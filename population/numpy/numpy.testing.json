{
  "description": "Common test support for all numpy test scripts.\n\nThis single module should provide all the common functionality for numpy tests\nin a single location, so that test scripts can just import it and work right\naway.",
  "functions": [
    {
      "name": "IgnoreException",
      "signature": "IgnoreException(...)",
      "docstring": {
        "description": "Ignoring this exception due to disabled feature",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "KnownFailureException",
      "signature": "KnownFailureException(...)",
      "docstring": {
        "description": "Raise this exception to mark a test as a known failing test.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "SkipTest",
      "signature": "SkipTest(...)",
      "docstring": {
        "description": "Raise this exception in a test to skip it.\n\nUsually you can use TestCase.skipTest() or one of the skipping decorators\ninstead of raising this directly.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "TestCase",
      "signature": "TestCase(methodName='runTest')",
      "docstring": {
        "description": "A class whose instances are single test cases.\n\nBy default, the test code itself should be placed in a method named\n'runTest'.\n\nIf the fixture may be used for many test cases, create as\nmany test methods as are needed. When instantiating such a TestCase\nsubclass, specify in the constructor arguments the name of the test method\nthat the instance is to execute.\n\nTest authors should subclass TestCase for their own tests. Construction\nand deconstruction of the test's environment ('fixture') can be\nimplemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\nIf it is necessary to override the __init__ method, the base class\n__init__ method must always be called. It is important that subclasses\nshould not change the signature of their __init__ method, since instances\nof the classes are instantiated automatically by parts of the framework\nin order to be run.\n\nWhen subclassing TestCase, you can set these attributes:\n* failureException: determines which exception will be raised when\n    the instance's assertion methods fail; test methods raising this\n    exception will be deemed to have 'failed' rather than 'errored'.\n* longMessage: determines whether long messages (including repr of\n    objects used in assert methods) will be printed on failure in *addition*\n    to any explicit message passed.\n* maxDiff: sets the maximum length of a diff in failure messages\n    by assert methods using difflib. It is looked up as an instance\n    attribute so can be configured by individual tests if required.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "assert_",
      "signature": "assert_(val, msg='')",
      "docstring": {
        "description": "Assert that works in release mode.\nAccepts callable msg to allow deferring evaluation until failure.\n\nThe Python built-in ``assert`` does not work when executing code in\noptimized mode (the ``-O`` flag) - no byte-code is generated for it.\n\nFor documentation on usage, refer to the Python documentation.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "assert_allclose",
      "signature": "assert_allclose(actual, desired, rtol=1e-07, atol=0, equal_nan=True, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two objects are not equal up to desired\ntolerance.\n\nGiven two array_like objects, check that their shapes and all elements\nare equal (but see the Notes for the special handling of a scalar). An\nexception is raised if the shapes mismatch or any values conflict. In\ncontrast to the standard usage in numpy, NaNs are compared like numbers,\nno assertion is raised if both objects have NaNs in the same positions.\n\nThe test is equivalent to ``allclose(actual, desired, rtol, atol)`` (note\nthat ``allclose`` has different default values). It compares the difference\nbetween `actual` and `desired` to ``atol + rtol * abs(desired)``.\n\n.. versionadded:: 1.5.0",
        "parameters": {
          "actual": {
            "type": "array_like",
            "description": "Array obtained."
          },
          "desired": {
            "type": "array_like",
            "description": "Array desired."
          },
          "rtol": {
            "type": "float, optional",
            "description": "Relative tolerance."
          },
          "atol": {
            "type": "float, optional",
            "description": "Absolute tolerance."
          },
          "equal_nan": {
            "type": "bool, optional.",
            "description": "If True, NaNs will compare equal."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n    If actual and desired are not equal up to specified precision.",
        "see_also": "assert_array_almost_equal_nulp, assert_array_max_ulp",
        "notes": "When one of `actual` and `desired` is a scalar and the other is\narray_like, the function checks that each element of the array_like\nobject is equal to the scalar.",
        "examples": ">>> x = [1e-5, 1e-3, 1e-1]\n>>> y = np.arccos(np.cos(x))\n>>> np.testing.assert_allclose(x, y, rtol=1e-5, atol=0)"
      }
    },
    {
      "name": "assert_almost_equal",
      "signature": "assert_almost_equal(actual, desired, decimal=7, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two items are not equal up to desired\nprecision.\n\n.. note:: It is recommended to use one of `assert_allclose`,\n          `assert_array_almost_equal_nulp` or `assert_array_max_ulp`\n          instead of this function for more consistent floating point\n          comparisons.\n\nThe test verifies that the elements of `actual` and `desired` satisfy.\n\n    ``abs(desired-actual) < float64(1.5 * 10**(-decimal))``\n\nThat is a looser test than originally documented, but agrees with what the\nactual implementation in `assert_array_almost_equal` did up to rounding\nvagaries. An exception is raised at conflicting values. For ndarrays this\ndelegates to assert_array_almost_equal",
        "parameters": {
          "actual": {
            "type": "array_like",
            "description": "The object to check."
          },
          "desired": {
            "type": "array_like",
            "description": "The expected object."
          },
          "decimal": {
            "type": "int, optional",
            "description": "Desired precision, default is 7."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n  If actual and desired are not equal up to specified precision.",
        "see_also": "assert_allclose: Compare two array_like objects for equality with desired\n                 relative and/or absolute precision.\nassert_array_almost_equal_nulp, assert_array_max_ulp, assert_equal",
        "notes": "",
        "examples": ">>> from numpy.testing import assert_almost_equal\n>>> assert_almost_equal(2.3333333333333, 2.33333334)\n>>> assert_almost_equal(2.3333333333333, 2.33333334, decimal=10)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not almost equal to 10 decimals\n ACTUAL: 2.3333333333333\n DESIRED: 2.33333334\n\n>>> assert_almost_equal(np.array([1.0,2.3333333333333]),\n...                     np.array([1.0,2.33333334]), decimal=9)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not almost equal to 9 decimals\n<BLANKLINE>\nMismatched elements: 1 / 2 (50%)\nMax absolute difference: 6.66669964e-09\nMax relative difference: 2.85715698e-09\n x: array([1.         , 2.333333333])\n y: array([1.        , 2.33333334])"
      }
    },
    {
      "name": "assert_approx_equal",
      "signature": "assert_approx_equal(actual, desired, significant=7, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two items are not equal up to significant\ndigits.\n\n.. note:: It is recommended to use one of `assert_allclose`,\n          `assert_array_almost_equal_nulp` or `assert_array_max_ulp`\n          instead of this function for more consistent floating point\n          comparisons.\n\nGiven two numbers, check that they are approximately equal.\nApproximately equal is defined as the number of significant digits\nthat agree.",
        "parameters": {
          "actual": {
            "type": "scalar",
            "description": "The object to check."
          },
          "desired": {
            "type": "scalar",
            "description": "The expected object."
          },
          "significant": {
            "type": "int, optional",
            "description": "Desired precision, default is 7."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n  If actual and desired are not equal up to specified precision.",
        "see_also": "assert_allclose: Compare two array_like objects for equality with desired\n                 relative and/or absolute precision.\nassert_array_almost_equal_nulp, assert_array_max_ulp, assert_equal",
        "notes": "",
        "examples": ">>> np.testing.assert_approx_equal(0.12345677777777e-20, 0.1234567e-20)\n>>> np.testing.assert_approx_equal(0.12345670e-20, 0.12345671e-20,\n...                                significant=8)\n>>> np.testing.assert_approx_equal(0.12345670e-20, 0.12345672e-20,\n...                                significant=8)\nTraceback (most recent call last):\n    ...\nAssertionError:\nItems are not equal to 8 significant digits:\n ACTUAL: 1.234567e-21\n DESIRED: 1.2345672e-21\n\nthe evaluated condition that raises the exception is\n\n>>> abs(0.12345670e-20/1e-21 - 0.12345672e-20/1e-21) >= 10**-(8-1)\nTrue"
      }
    },
    {
      "name": "assert_array_almost_equal",
      "signature": "assert_array_almost_equal(x, y, decimal=6, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two objects are not equal up to desired\nprecision.\n\n.. note:: It is recommended to use one of `assert_allclose`,\n          `assert_array_almost_equal_nulp` or `assert_array_max_ulp`\n          instead of this function for more consistent floating point\n          comparisons.\n\nThe test verifies identical shapes and that the elements of ``actual`` and\n``desired`` satisfy.\n\n    ``abs(desired-actual) < 1.5 * 10**(-decimal)``\n\nThat is a looser test than originally documented, but agrees with what the\nactual implementation did up to rounding vagaries. An exception is raised\nat shape mismatch or conflicting values. In contrast to the standard usage\nin numpy, NaNs are compared like numbers, no assertion is raised if both\nobjects have NaNs in the same positions.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": "The actual object to check."
          },
          "y": {
            "type": "array_like",
            "description": "The desired, expected object."
          },
          "decimal": {
            "type": "int, optional",
            "description": "Desired precision, default is 6."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n    If actual and desired are not equal up to specified precision.",
        "see_also": "assert_allclose: Compare two array_like objects for equality with desired\n                 relative and/or absolute precision.\nassert_array_almost_equal_nulp, assert_array_max_ulp, assert_equal",
        "notes": "",
        "examples": "the first assert does not raise an exception\n\n>>> np.testing.assert_array_almost_equal([1.0,2.333,np.nan],\n...                                      [1.0,2.333,np.nan])\n\n>>> np.testing.assert_array_almost_equal([1.0,2.33333,np.nan],\n...                                      [1.0,2.33339,np.nan], decimal=5)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not almost equal to 5 decimals\n<BLANKLINE>\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference: 6.e-05\nMax relative difference: 2.57136612e-05\n x: array([1.     , 2.33333,     nan])\n y: array([1.     , 2.33339,     nan])\n\n>>> np.testing.assert_array_almost_equal([1.0,2.33333,np.nan],\n...                                      [1.0,2.33333, 5], decimal=5)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not almost equal to 5 decimals\n<BLANKLINE>\nx and y nan location mismatch:\n x: array([1.     , 2.33333,     nan])\n y: array([1.     , 2.33333, 5.     ])"
      }
    },
    {
      "name": "assert_array_almost_equal_nulp",
      "signature": "assert_array_almost_equal_nulp(x, y, nulp=1)",
      "docstring": {
        "description": "Compare two arrays relatively to their spacing.\n\nThis is a relatively robust method to compare two arrays whose amplitude\nis variable.",
        "parameters": {
          "nulp": {
            "type": "int, optional",
            "description": "The maximum number of unit in the last place for tolerance (see Notes).\n    Default is 1."
          }
        },
        "returns": "None",
        "raises": "AssertionError\n    If the spacing between `x` and `y` for one or more elements is larger\n    than `nulp`.",
        "see_also": "assert_array_max_ulp : Check that all items of arrays differ in at most\n    N Units in the Last Place.\nspacing : Return the distance between x and the nearest adjacent number.",
        "notes": "An assertion is raised if the following condition is not met::\n\n    abs(x - y) <= nulp * spacing(maximum(abs(x), abs(y)))",
        "examples": ">>> x = np.array([1., 1e-10, 1e-20])\n>>> eps = np.finfo(x.dtype).eps\n>>> np.testing.assert_array_almost_equal_nulp(x, x*eps/2 + x)\n\n>>> np.testing.assert_array_almost_equal_nulp(x, x*eps + x)\nTraceback (most recent call last):\n  ...\nAssertionError: X and Y are not equal to 1 ULP (max is 2)"
      }
    },
    {
      "name": "assert_array_compare",
      "signature": "assert_array_compare(comparison, x, y, err_msg='', verbose=True, header='', precision=6, equal_nan=True, equal_inf=True, *, strict=False)",
      "docstring": {}
    },
    {
      "name": "assert_array_equal",
      "signature": "assert_array_equal(x, y, err_msg='', verbose=True, *, strict=False)",
      "docstring": {
        "description": "Raises an AssertionError if two array_like objects are not equal.\n\nGiven two array_like objects, check that the shape is equal and all\nelements of these objects are equal (but see the Notes for the special\nhandling of a scalar). An exception is raised at shape mismatch or\nconflicting values. In contrast to the standard usage in numpy, NaNs\nare compared like numbers, no assertion is raised if both objects have\nNaNs in the same positions.\n\nThe usual caution for verifying equality with floating point numbers is\nadvised.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": "The actual object to check."
          },
          "y": {
            "type": "array_like",
            "description": "The desired, expected object."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          },
          "strict": {
            "type": "bool, optional",
            "description": "If True, raise an AssertionError when either the shape or the data\n    type of the array_like objects does not match. The special\n    handling for scalars mentioned in the Notes section is disabled.\n\n    .. versionadded:: 1.24.0"
          }
        },
        "returns": "",
        "raises": "AssertionError\n    If actual and desired objects are not equal.",
        "see_also": "assert_allclose: Compare two array_like objects for equality with desired\n                 relative and/or absolute precision.\nassert_array_almost_equal_nulp, assert_array_max_ulp, assert_equal",
        "notes": "When one of `x` and `y` is a scalar and the other is array_like, the\nfunction checks that each element of the array_like object is equal to\nthe scalar. This behaviour can be disabled with the `strict` parameter.",
        "examples": "The first assert does not raise an exception:\n\n>>> np.testing.assert_array_equal([1.0,2.33333,np.nan],\n...                               [np.exp(0),2.33333, np.nan])\n\nAssert fails with numerical imprecision with floats:\n\n>>> np.testing.assert_array_equal([1.0,np.pi,np.nan],\n...                               [1, np.sqrt(np.pi)**2, np.nan])\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not equal\n<BLANKLINE>\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference: 4.4408921e-16\nMax relative difference: 1.41357986e-16\n x: array([1.      , 3.141593,      nan])\n y: array([1.      , 3.141593,      nan])\n\nUse `assert_allclose` or one of the nulp (number of floating point values)\nfunctions for these cases instead:\n\n>>> np.testing.assert_allclose([1.0,np.pi,np.nan],\n...                            [1, np.sqrt(np.pi)**2, np.nan],\n...                            rtol=1e-10, atol=0)\n\nAs mentioned in the Notes section, `assert_array_equal` has special\nhandling for scalars. Here the test checks that each value in `x` is 3:\n\n>>> x = np.full((2, 5), fill_value=3)\n>>> np.testing.assert_array_equal(x, 3)\n\nUse `strict` to raise an AssertionError when comparing a scalar with an\narray:\n\n>>> np.testing.assert_array_equal(x, 3, strict=True)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not equal\n<BLANKLINE>\n(shapes (2, 5), () mismatch)\n x: array([[3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3]])\n y: array(3)\n\nThe `strict` parameter also ensures that the array data types match:\n\n>>> x = np.array([2, 2, 2])\n>>> y = np.array([2., 2., 2.], dtype=np.float32)\n>>> np.testing.assert_array_equal(x, y, strict=True)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not equal\n<BLANKLINE>\n(dtypes int64, float32 mismatch)\n x: array([2, 2, 2])\n y: array([2., 2., 2.], dtype=float32)"
      }
    },
    {
      "name": "assert_array_less",
      "signature": "assert_array_less(x, y, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two array_like objects are not ordered by less\nthan.\n\nGiven two array_like objects, check that the shape is equal and all\nelements of the first object are strictly smaller than those of the\nsecond object. An exception is raised at shape mismatch or incorrectly\nordered values. Shape mismatch does not raise if an object has zero\ndimension. In contrast to the standard usage in numpy, NaNs are\ncompared, no assertion is raised if both objects have NaNs in the same\npositions.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": "The smaller object to check."
          },
          "y": {
            "type": "array_like",
            "description": "The larger object to compare."
          },
          "err_msg": {
            "type": "string",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n  If x is not strictly smaller than y, element-wise.",
        "see_also": "assert_array_equal: tests objects for equality\nassert_array_almost_equal: test objects for equality up to precision",
        "notes": "",
        "examples": ">>> np.testing.assert_array_less([1.0, 1.0, np.nan], [1.1, 2.0, np.nan])\n>>> np.testing.assert_array_less([1.0, 1.0, np.nan], [1, 2.0, np.nan])\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not less-ordered\n<BLANKLINE>\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference: 1.\nMax relative difference: 0.5\n x: array([ 1.,  1., nan])\n y: array([ 1.,  2., nan])\n\n>>> np.testing.assert_array_less([1.0, 4.0], 3)\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not less-ordered\n<BLANKLINE>\nMismatched elements: 1 / 2 (50%)\nMax absolute difference: 2.\nMax relative difference: 0.66666667\n x: array([1., 4.])\n y: array(3)\n\n>>> np.testing.assert_array_less([1.0, 2.0, 3.0], [4])\nTraceback (most recent call last):\n    ...\nAssertionError:\nArrays are not less-ordered\n<BLANKLINE>\n(shapes (3,), (1,) mismatch)\n x: array([1., 2., 3.])\n y: array([4])"
      }
    },
    {
      "name": "assert_array_max_ulp",
      "signature": "assert_array_max_ulp(a, b, maxulp=1, dtype=None)",
      "docstring": {
        "description": "Check that all items of arrays differ in at most N Units in the Last Place.",
        "parameters": {
          "maxulp": {
            "type": "int, optional",
            "description": "The maximum number of units in the last place that elements of `a` and\n    `b` can differ. Default is 1."
          },
          "dtype": {
            "type": "dtype, optional",
            "description": "Data-type to convert `a` and `b` to if given. Default is None."
          }
        },
        "returns": "ret : ndarray\n    Array containing number of representable floating point numbers between\n    items in `a` and `b`.",
        "raises": "AssertionError\n    If one or more elements differ by more than `maxulp`.",
        "see_also": "assert_array_almost_equal_nulp : Compare two arrays relatively to their\n    spacing.",
        "notes": "For computing the ULP difference, this API does not differentiate between\nvarious representations of NAN (ULP difference between 0x7fc00000 and 0xffc00000\nis zero).",
        "examples": ">>> a = np.linspace(0., 1., 100)\n>>> res = np.testing.assert_array_max_ulp(a, np.arcsin(np.sin(a)))"
      }
    },
    {
      "name": "assert_equal",
      "signature": "assert_equal(actual, desired, err_msg='', verbose=True)",
      "docstring": {
        "description": "Raises an AssertionError if two objects are not equal.\n\nGiven two objects (scalars, lists, tuples, dictionaries or numpy arrays),\ncheck that all elements of these objects are equal. An exception is raised\nat the first conflicting values.\n\nWhen one of `actual` and `desired` is a scalar and the other is array_like,\nthe function checks that each element of the array_like object is equal to\nthe scalar.\n\nThis function handles NaN comparisons as if NaN was a \"normal\" number.\nThat is, AssertionError is not raised if both objects have NaNs in the same\npositions.  This is in contrast to the IEEE standard on NaNs, which says\nthat NaN compared to anything must return False.",
        "parameters": {
          "actual": {
            "type": "array_like",
            "description": "The object to check."
          },
          "desired": {
            "type": "array_like",
            "description": "The expected object."
          },
          "err_msg": {
            "type": "str, optional",
            "description": "The error message to be printed in case of failure."
          },
          "verbose": {
            "type": "bool, optional",
            "description": "If True, the conflicting values are appended to the error message."
          }
        },
        "returns": "",
        "raises": "AssertionError\n    If actual and desired are not equal.",
        "see_also": "",
        "notes": "",
        "examples": ">>> np.testing.assert_equal([4,5], [4,6])\nTraceback (most recent call last):\n    ...\nAssertionError:\nItems are not equal:\nitem=1\n ACTUAL: 5\n DESIRED: 6\n\nThe following comparison does not raise an exception.  There are NaNs\nin the inputs, but they are in the same positions.\n\n>>> np.testing.assert_equal(np.array([1.0, 2.0, np.nan]), [1, 2, np.nan])"
      }
    },
    {
      "name": "assert_no_gc_cycles",
      "signature": "assert_no_gc_cycles(*args, **kwargs)",
      "docstring": {
        "description": "Fail if the given callable produces any reference cycles.\n\nIf called with all arguments omitted, may be used as a context manager:\n\n    with assert_no_gc_cycles():\n        do_something()\n\n.. versionadded:: 1.15.0",
        "parameters": {
          "func": {
            "type": "callable",
            "description": "The callable to test."
          },
          "\\*args": {
            "type": "Arguments",
            "description": "Arguments passed to `func`."
          },
          "\\*\\*kwargs": {
            "type": "Kwargs",
            "description": "Keyword arguments passed to `func`."
          }
        },
        "returns": "Nothing. The result is deliberately discarded to ensure that all cycles\nare found.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "assert_no_warnings",
      "signature": "assert_no_warnings(*args, **kwargs)",
      "docstring": {
        "description": "Fail if the given callable produces any warnings.\n\nIf called with all arguments omitted, may be used as a context manager:\n\n    with assert_no_warnings():\n        do_something()\n\nThe ability to be used as a context manager is new in NumPy v1.11.0.\n\n.. versionadded:: 1.7.0",
        "parameters": {
          "func": {
            "type": "callable",
            "description": "The callable to test."
          },
          "\\*args": {
            "type": "Arguments",
            "description": "Arguments passed to `func`."
          },
          "\\*\\*kwargs": {
            "type": "Kwargs",
            "description": "Keyword arguments passed to `func`."
          }
        },
        "returns": "The value returned by `func`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "assert_raises",
      "signature": "assert_raises(*args, **kwargs)",
      "docstring": {
        "description": "assert_raises(exception_class, callable, *args, **kwargs)\nassert_raises(exception_class)\n\nFail unless an exception of class exception_class is thrown\nby callable when invoked with arguments args and keyword\narguments kwargs. If a different type of exception is\nthrown, it will not be caught, and the test case will be\ndeemed to have suffered an error, exactly as for an\nunexpected exception.\n\nAlternatively, `assert_raises` can be used as a context manager:\n\n>>> from numpy.testing import assert_raises\n>>> with assert_raises(ZeroDivisionError):\n...     1 / 0\n\nis equivalent to\n\n>>> def div(x, y):\n...     return x / y\n>>> assert_raises(ZeroDivisionError, div, 1, 0)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "assert_raises_regex",
      "signature": "assert_raises_regex(exception_class, expected_regexp, *args, **kwargs)",
      "docstring": {
        "description": "assert_raises_regex(exception_class, expected_regexp, callable, *args,\n                    **kwargs)\nassert_raises_regex(exception_class, expected_regexp)\n\nFail unless an exception of class exception_class and with message that\nmatches expected_regexp is thrown by callable when invoked with arguments\nargs and keyword arguments kwargs.\n\nAlternatively, can be used as a context manager like `assert_raises`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": ".. versionadded:: 1.9.0",
        "examples": ""
      }
    },
    {
      "name": "assert_string_equal",
      "signature": "assert_string_equal(actual, desired)",
      "docstring": {
        "description": "Test if two strings are equal.\n\nIf the given strings are equal, `assert_string_equal` does nothing.\nIf they are not equal, an AssertionError is raised, and the diff\nbetween the strings is shown.",
        "parameters": {
          "actual": {
            "type": "str",
            "description": "The string to test for equality against the expected string."
          },
          "desired": {
            "type": "str",
            "description": "The expected string."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> np.testing.assert_string_equal('abc', 'abc')\n>>> np.testing.assert_string_equal('abc', 'abcd')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n...\nAssertionError: Differences in strings:\n- abc+ abcd?    +"
      }
    },
    {
      "name": "assert_warns",
      "signature": "assert_warns(warning_class, *args, **kwargs)",
      "docstring": {
        "description": "Fail unless the given callable throws the specified warning.\n\nA warning of class warning_class should be thrown by the callable when\ninvoked with arguments args and keyword arguments kwargs.\nIf a different type of warning is thrown, it will not be caught.\n\nIf called with all arguments other than the warning class omitted, may be\nused as a context manager:\n\n    with assert_warns(SomeWarning):\n        do_something()\n\nThe ability to be used as a context manager is new in NumPy v1.11.0.\n\n.. versionadded:: 1.4.0",
        "parameters": {
          "warning_class": {
            "type": "class",
            "description": "The class defining the warning that `func` is expected to throw."
          },
          "func": {
            "type": "callable, optional",
            "description": "Callable to test"
          },
          "*args": {
            "type": "Arguments",
            "description": "Arguments for `func`."
          },
          "**kwargs": {
            "type": "Kwargs",
            "description": "Keyword arguments for `func`."
          }
        },
        "returns": "The value returned by `func`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import warnings\n>>> def deprecated_func(num):\n...     warnings.warn(\"Please upgrade\", DeprecationWarning)\n...     return num*num\n>>> with np.testing.assert_warns(DeprecationWarning):\n...     assert deprecated_func(4) == 16\n>>> # or passing a func\n>>> ret = np.testing.assert_warns(DeprecationWarning, deprecated_func, 4)\n>>> assert ret == 16"
      }
    },
    {
      "name": "break_cycles",
      "signature": "break_cycles()",
      "docstring": {
        "description": "Break reference cycles by calling gc.collect\nObjects can call other objects' methods (for instance, another object's\n __del__) inside their own __del__. On PyPy, the interpreter only runs\nbetween calls to gc.collect, so multiple calls are needed to completely\nrelease all cycles.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "build_err_msg",
      "signature": "build_err_msg(arrays, err_msg, header='Items are not equal:', verbose=True, names=('ACTUAL', 'DESIRED'), precision=8)",
      "docstring": {}
    },
    {
      "name": "clear_and_catch_warnings",
      "signature": "clear_and_catch_warnings(record=False, modules=())",
      "docstring": {
        "description": "Context manager that resets warning registry for catching warnings\n\nWarnings can be slippery, because, whenever a warning is triggered, Python\nadds a ``__warningregistry__`` member to the *calling* module.  This makes\nit impossible to retrigger the warning in this module, whatever you put in\nthe warnings filters.  This context manager accepts a sequence of `modules`\nas a keyword argument to its constructor and:\n\n* stores and removes any ``__warningregistry__`` entries in given `modules`\n  on entry;\n* resets ``__warningregistry__`` to its previous state on exit.\n\nThis makes it possible to trigger any warning afresh inside the context\nmanager without disturbing the state of warnings outside.\n\nFor compatibility with Python 3.0, please consider all arguments to be\nkeyword-only.",
        "parameters": {
          "record": {
            "type": "bool, optional",
            "description": "Specifies whether warnings should be captured by a custom\n    implementation of ``warnings.showwarning()`` and be appended to a list\n    returned by the context manager. Otherwise None is returned by the\n    context manager. The objects appended to the list are arguments whose\n    attributes mirror the arguments to ``showwarning()``."
          },
          "modules": {
            "type": "sequence, optional",
            "description": "Sequence of modules for which to reset warnings registry on entry and\n    restore on exit. To work correctly, all 'ignore' filters should\n    filter by one of these modules."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import warnings\n>>> with np.testing.clear_and_catch_warnings(\n...         modules=[np.core.fromnumeric]):\n...     warnings.simplefilter('always')\n...     warnings.filterwarnings('ignore', module='np.core.fromnumeric')\n...     # do something that raises a warning but ignore those in\n...     # np.core.fromnumeric"
      }
    },
    {
      "name": "decorate_methods",
      "signature": "decorate_methods(cls, decorator, testmatch=None)",
      "docstring": {
        "description": "Apply a decorator to all methods in a class matching a regular expression.\n\nThe given decorator is applied to all public methods of `cls` that are\nmatched by the regular expression `testmatch`\n(``testmatch.search(methodname)``). Methods that are private, i.e. start\nwith an underscore, are ignored.",
        "parameters": {
          "cls": {
            "type": "class",
            "description": "Class whose methods to decorate."
          },
          "decorator": {
            "type": "function",
            "description": "Decorator to apply to methods"
          },
          "testmatch": {
            "type": "compiled regexp or str, optional",
            "description": "The regular expression. Default value is None, in which case the\n    nose default (``re.compile(r'(?:^|[\\b_\\.%s-])[Tt]est' % os.sep)``)\n    is used.\n    If `testmatch` is a string, it is compiled to a regular expression\n    first."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jiffies",
      "signature": "jiffies(_proc_pid_stat='/proc/159357/stat', _load_time=[])",
      "docstring": {
        "description": "Return number of jiffies elapsed.\n\nReturn number of jiffies (1/100ths of a second) that this\nprocess has been scheduled in user mode. See man 5 proc.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "measure",
      "signature": "measure(code_str, times=1, label=None)",
      "docstring": {
        "description": "Return elapsed time for executing code in the namespace of the caller.\n\nThe supplied code string is compiled with the Python builtin ``compile``.\nThe precision of the timing is 10 milli-seconds. If the code will execute\nfast on this timescale, it can be executed many times to get reasonable\ntiming accuracy.",
        "parameters": {
          "code_str": {
            "type": "str",
            "description": "The code to be timed."
          },
          "times": {
            "type": "int, optional",
            "description": "The number of times the code is executed. Default is 1. The code is\n    only compiled once."
          },
          "label": {
            "type": "str, optional",
            "description": "A label to identify `code_str` with. This is passed into ``compile``\n    as the second argument (for run-time error messages)."
          }
        },
        "returns": "elapsed : float\n    Total elapsed time in seconds for executing `code_str` `times` times.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> times = 10\n>>> etime = np.testing.measure('for i in range(1000): np.sqrt(i**2)', times=times)\n>>> print(\"Time for a single execution : \", etime / times, \"s\")  # doctest: +SKIP\nTime for a single execution :  0.005 s"
      }
    },
    {
      "name": "memusage",
      "signature": "memusage(_proc_pid_stat='/proc/159357/stat')",
      "docstring": {
        "description": "Return virtual memory size in bytes of the running python.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "print_assert_equal",
      "signature": "print_assert_equal(test_string, actual, desired)",
      "docstring": {
        "description": "Test if two objects are equal, and print an error message if test fails.\n\nThe test is performed with ``actual == desired``.",
        "parameters": {
          "test_string": {
            "type": "str",
            "description": "The message supplied to AssertionError."
          },
          "actual": {
            "type": "object",
            "description": "The object to test for equality against `desired`."
          },
          "desired": {
            "type": "object",
            "description": "The expected result."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> np.testing.print_assert_equal('Test XYZ of func xyz', [0, 1], [0, 1])\n>>> np.testing.print_assert_equal('Test XYZ of func xyz', [0, 1], [0, 2])\nTraceback (most recent call last):\n...\nAssertionError: Test XYZ of func xyz failed\nACTUAL:\n[0, 1]\nDESIRED:\n[0, 2]"
      }
    },
    {
      "name": "rundocs",
      "signature": "rundocs(filename=None, raise_on_error=True)",
      "docstring": {
        "description": "Run doctests found in the given file.\n\nBy default `rundocs` raises an AssertionError on failure.",
        "parameters": {
          "filename": {
            "type": "str",
            "description": "The path to the file for which the doctests are run."
          },
          "raise_on_error": {
            "type": "bool",
            "description": "Whether to raise an AssertionError when a doctest fails. Default is\n    True."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "The doctests can be run by the user/developer by adding the ``doctests``\nargument to the ``test()`` call. For example, to run all tests (including\ndoctests) for `numpy.lib`:\n\n>>> np.lib.test(doctests=True)  # doctest: +SKIP",
        "examples": ""
      }
    },
    {
      "name": "runstring",
      "signature": "runstring(astr, dict)",
      "docstring": {}
    },
    {
      "name": "suppress_warnings",
      "signature": "suppress_warnings(forwarding_rule='always')",
      "docstring": {
        "description": "Context manager and decorator doing much the same as\n``warnings.catch_warnings``.\n\nHowever, it also provides a filter mechanism to work around\nhttps://bugs.python.org/issue4180.\n\nThis bug causes Python before 3.4 to not reliably show warnings again\nafter they have been ignored once (even within catch_warnings). It\nmeans that no \"ignore\" filter can be used easily, since following\ntests might need to see the warning. Additionally it allows easier\nspecificity for testing warnings and can be nested.",
        "parameters": {
          "forwarding_rule": {
            "type": "str, optional",
            "description": "One of \"always\", \"once\", \"module\", or \"location\". Analogous to\n    the usual warnings module filter mode, it is useful to reduce\n    noise mostly on the outmost level. Unsuppressed and unrecorded\n    warnings will be forwarded based on this rule. Defaults to \"always\".\n    \"location\" is equivalent to the warnings \"default\", match by exact\n    location the warning warning originated from."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "Filters added inside the context manager will be discarded again\nwhen leaving it. Upon entering all filters defined outside a\ncontext will be applied automatically.\n\nWhen a recording filter is added, matching warnings are stored in the\n``log`` attribute as well as in the list returned by ``record``.\n\nIf filters are added and the ``module`` keyword is given, the\nwarning registry of this module will additionally be cleared when\napplying it, entering the context, or exiting it. This could cause\nwarnings to appear a second time after leaving the context if they\nwere configured to be printed once (default) and were already\nprinted before the context was entered.\n\nNesting this context manager will work as expected when the\nforwarding rule is \"always\" (default). Unfiltered and unrecorded\nwarnings will be passed out and be matched by the outer level.\nOn the outmost level they will be printed (or caught by another\nwarnings context). The forwarding rule argument can modify this\nbehaviour.\n\nLike ``catch_warnings`` this context manager is not threadsafe.",
        "examples": "With a context manager::\n\n    with np.testing.suppress_warnings() as sup:\n        sup.filter(DeprecationWarning, \"Some text\")\n        sup.filter(module=np.ma.core)\n        log = sup.record(FutureWarning, \"Does this occur?\")\n        command_giving_warnings()\n        # The FutureWarning was given once, the filtered warnings were\n        # ignored. All other warnings abide outside settings (may be\n        # printed/error)\n        assert_(len(log) == 1)\n        assert_(len(sup.log) == 1)  # also stored in log attribute\n\nOr as a decorator::\n\n    sup = np.testing.suppress_warnings()\n    sup.filter(module=np.ma.core)  # module must match exactly\n    @sup\n    def some_function():\n        # do something which causes a warning in np.ma.core\n        pass"
      }
    },
    {
      "name": "tempdir",
      "signature": "tempdir(*args, **kwargs)",
      "docstring": {
        "description": "Context manager to provide a temporary test folder.\n\nAll arguments are passed as this to the underlying tempfile.mkdtemp\nfunction.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "temppath",
      "signature": "temppath(*args, **kwargs)",
      "docstring": {
        "description": "Context manager for temporary files.\n\nContext manager that returns the path to a closed temporary file. Its\nparameters are the same as for tempfile.mkstemp and are passed directly\nto that function. The underlying file is removed when the context is\nexited, so it should be closed at that time.\n\nWindows does not allow a temporary file to be opened if it is already\nopen, so the underlying file must be closed after opening before it\ncan be opened again.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "test",
      "signature": "test(...)",
      "docstring": {}
    }
  ],
  "classes": [
    {
      "name": "IgnoreException",
      "docstring": {
        "description": "Ignoring this exception due to disabled feature",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "docstring": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "docstring": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KnownFailureException",
      "docstring": {
        "description": "Raise this exception to mark a test as a known failing test.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "docstring": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "docstring": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SkipTest",
      "docstring": {
        "description": "Raise this exception in a test to skip it.\n\nUsually you can use TestCase.skipTest() or one of the skipping decorators\ninstead of raising this directly.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "docstring": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "docstring": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TestCase",
      "docstring": {
        "description": "A class whose instances are single test cases.\n\nBy default, the test code itself should be placed in a method named\n'runTest'.\n\nIf the fixture may be used for many test cases, create as\nmany test methods as are needed. When instantiating such a TestCase\nsubclass, specify in the constructor arguments the name of the test method\nthat the instance is to execute.\n\nTest authors should subclass TestCase for their own tests. Construction\nand deconstruction of the test's environment ('fixture') can be\nimplemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\nIf it is necessary to override the __init__ method, the base class\n__init__ method must always be called. It is important that subclasses\nshould not change the signature of their __init__ method, since instances\nof the classes are instantiated automatically by parts of the framework\nin order to be run.\n\nWhen subclassing TestCase, you can set these attributes:\n* failureException: determines which exception will be raised when\n    the instance's assertion methods fail; test methods raising this\n    exception will be deemed to have 'failed' rather than 'errored'.\n* longMessage: determines whether long messages (including repr of\n    objects used in assert methods) will be printed on failure in *addition*\n    to any explicit message passed.\n* maxDiff: sets the maximum length of a diff in failure messages\n    by assert methods using difflib. It is looked up as an instance\n    attribute so can be configured by individual tests if required.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addClassCleanup",
          "signature": "addClassCleanup(function, /, *args, **kwargs)",
          "docstring": {
            "description": "Same as addCleanup, except the cleanup items are called even if\nsetUpClass fails (unlike tearDownClass).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "addCleanup",
          "signature": "addCleanup(self, function, /, *args, **kwargs)",
          "docstring": {
            "description": "Add a function, with arguments, to be called when the test is\ncompleted. Functions added are called on a LIFO basis and are\ncalled after tearDown on test failure or success.\n\nCleanup items are called even if setUp fails (unlike tearDown).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "addTypeEqualityFunc",
          "signature": "addTypeEqualityFunc(self, typeobj, function)",
          "docstring": {
            "description": "Add a type specific assertEqual style function to compare a type.\n\nThis method is for use by TestCase subclasses that need to register\ntheir own type equality functions to provide nicer error messages.\n\nArgs:\n    typeobj: The data type to call this function on when both values\n            are of the same type in assertEqual().\n    function: The callable taking two arguments and an optional\n            msg= argument that raises self.failureException with a\n            useful error message when the two arguments are not equal.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertAlmostEqual",
          "signature": "assertAlmostEqual(self, first, second, places=None, msg=None, delta=None)",
          "docstring": {
            "description": "Fail if the two objects are unequal as determined by their\ndifference rounded to the given number of decimal places\n(default 7) and comparing to zero, or by comparing that the\ndifference between the two objects is more than the given\ndelta.\n\nNote that decimal places (from zero) are usually not the same\nas significant digits (measured from the most significant digit).\n\nIf the two objects compare equal then they will automatically\ncompare almost equal.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertCountEqual",
          "signature": "assertCountEqual(self, first, second, msg=None)",
          "docstring": {
            "description": "Asserts that two iterables have the same elements, the same number of\ntimes, without regard to order.\n\n    self.assertEqual(Counter(list(first)),\n                     Counter(list(second)))\n\n Example:\n    - [0, 1, 1] and [1, 0, 1] compare equal.\n    - [0, 0, 1] and [0, 1] compare unequal.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertDictEqual",
          "signature": "assertDictEqual(self, d1, d2, msg=None)",
          "docstring": {}
        },
        {
          "name": "assertEqual",
          "signature": "assertEqual(self, first, second, msg=None)",
          "docstring": {
            "description": "Fail if the two objects are unequal as determined by the '=='\noperator.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertFalse",
          "signature": "assertFalse(self, expr, msg=None)",
          "docstring": {
            "description": "Check that the expression is false.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertGreater",
          "signature": "assertGreater(self, a, b, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a > b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertGreaterEqual",
          "signature": "assertGreaterEqual(self, a, b, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a >= b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIn",
          "signature": "assertIn(self, member, container, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a in b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIs",
          "signature": "assertIs(self, expr1, expr2, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a is b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIsInstance",
          "signature": "assertIsInstance(self, obj, cls, msg=None)",
          "docstring": {
            "description": "Same as self.assertTrue(isinstance(obj, cls)), with a nicer\ndefault message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIsNone",
          "signature": "assertIsNone(self, obj, msg=None)",
          "docstring": {
            "description": "Same as self.assertTrue(obj is None), with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIsNot",
          "signature": "assertIsNot(self, expr1, expr2, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a is not b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertIsNotNone",
          "signature": "assertIsNotNone(self, obj, msg=None)",
          "docstring": {
            "description": "Included for symmetry with assertIsNone.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertLess",
          "signature": "assertLess(self, a, b, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a < b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertLessEqual",
          "signature": "assertLessEqual(self, a, b, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a <= b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertListEqual",
          "signature": "assertListEqual(self, list1, list2, msg=None)",
          "docstring": {
            "description": "A list-specific equality assertion.\n\nArgs:\n    list1: The first list to compare.\n    list2: The second list to compare.\n    msg: Optional message to use on failure instead of a list of\n            differences.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertLogs",
          "signature": "assertLogs(self, logger=None, level=None)",
          "docstring": {
            "description": "Fail unless a log message of level *level* or higher is emitted\non *logger_name* or its children.  If omitted, *level* defaults to\nINFO and *logger* defaults to the root logger.\n\nThis method must be used as a context manager, and will yield\na recording object with two attributes: `output` and `records`.\nAt the end of the context manager, the `output` attribute will\nbe a list of the matching formatted log messages and the\n`records` attribute will be a list of the corresponding LogRecord\nobjects.\n\nExample::\n\n    with self.assertLogs('foo', level='INFO') as cm:\n        logging.getLogger('foo').info('first message')\n        logging.getLogger('foo.bar').error('second message')\n    self.assertEqual(cm.output, ['INFO:foo:first message',\n                                 'ERROR:foo.bar:second message'])",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertMultiLineEqual",
          "signature": "assertMultiLineEqual(self, first, second, msg=None)",
          "docstring": {
            "description": "Assert that two multi-line strings are equal.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNoLogs",
          "signature": "assertNoLogs(self, logger=None, level=None)",
          "docstring": {
            "description": "Fail unless no log messages of level *level* or higher are emitted\non *logger_name* or its children.\n\nThis method must be used as a context manager.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNotAlmostEqual",
          "signature": "assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None)",
          "docstring": {
            "description": "Fail if the two objects are equal as determined by their\ndifference rounded to the given number of decimal places\n(default 7) and comparing to zero, or by comparing that the\ndifference between the two objects is less than the given delta.\n\nNote that decimal places (from zero) are usually not the same\nas significant digits (measured from the most significant digit).\n\nObjects that are equal automatically fail.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNotEqual",
          "signature": "assertNotEqual(self, first, second, msg=None)",
          "docstring": {
            "description": "Fail if the two objects are equal as determined by the '!='\noperator.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNotIn",
          "signature": "assertNotIn(self, member, container, msg=None)",
          "docstring": {
            "description": "Just like self.assertTrue(a not in b), but with a nicer default message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNotIsInstance",
          "signature": "assertNotIsInstance(self, obj, cls, msg=None)",
          "docstring": {
            "description": "Included for symmetry with assertIsInstance.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertNotRegex",
          "signature": "assertNotRegex(self, text, unexpected_regex, msg=None)",
          "docstring": {
            "description": "Fail the test if the text matches the regular expression.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertRaises",
          "signature": "assertRaises(self, expected_exception, *args, **kwargs)",
          "docstring": {
            "description": "Fail unless an exception of class expected_exception is raised\nby the callable when invoked with specified positional and\nkeyword arguments. If a different type of exception is\nraised, it will not be caught, and the test case will be\ndeemed to have suffered an error, exactly as for an\nunexpected exception.\n\nIf called with the callable and arguments omitted, will return a\ncontext object used like this::\n\n     with self.assertRaises(SomeException):\n         do_something()\n\nAn optional keyword argument 'msg' can be provided when assertRaises\nis used as a context object.\n\nThe context manager keeps a reference to the exception as\nthe 'exception' attribute. This allows you to inspect the\nexception after the assertion::\n\n    with self.assertRaises(SomeException) as cm:\n        do_something()\n    the_exception = cm.exception\n    self.assertEqual(the_exception.error_code, 3)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertRaisesRegex",
          "signature": "assertRaisesRegex(self, expected_exception, expected_regex, *args, **kwargs)",
          "docstring": {
            "description": "Asserts that the message in a raised exception matches a regex.\n\nArgs:\n    expected_exception: Exception class expected to be raised.\n    expected_regex: Regex (re.Pattern object or string) expected\n            to be found in error message.\n    args: Function to be called and extra positional args.\n    kwargs: Extra kwargs.\n    msg: Optional message used in case of failure. Can only be used\n            when assertRaisesRegex is used as a context manager.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertRegex",
          "signature": "assertRegex(self, text, expected_regex, msg=None)",
          "docstring": {
            "description": "Fail the test unless the text matches the regular expression.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertSequenceEqual",
          "signature": "assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None)",
          "docstring": {
            "description": "An equality assertion for ordered sequences (like lists and tuples).\n\nFor the purposes of this function, a valid ordered sequence type is one\nwhich can be indexed, has a length, and has an equality operator.\n\nArgs:\n    seq1: The first sequence to compare.\n    seq2: The second sequence to compare.\n    seq_type: The expected datatype of the sequences, or None if no\n            datatype should be enforced.\n    msg: Optional message to use on failure instead of a list of\n            differences.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertSetEqual",
          "signature": "assertSetEqual(self, set1, set2, msg=None)",
          "docstring": {
            "description": "A set-specific equality assertion.\n\nArgs:\n    set1: The first set to compare.\n    set2: The second set to compare.\n    msg: Optional message to use on failure instead of a list of\n            differences.\n\nassertSetEqual uses ducktyping to support different types of sets, and\nis optimized for sets specifically (parameters must support a\ndifference method).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertTrue",
          "signature": "assertTrue(self, expr, msg=None)",
          "docstring": {
            "description": "Check that the expression is true.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertTupleEqual",
          "signature": "assertTupleEqual(self, tuple1, tuple2, msg=None)",
          "docstring": {
            "description": "A tuple-specific equality assertion.\n\nArgs:\n    tuple1: The first tuple to compare.\n    tuple2: The second tuple to compare.\n    msg: Optional message to use on failure instead of a list of\n            differences.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertWarns",
          "signature": "assertWarns(self, expected_warning, *args, **kwargs)",
          "docstring": {
            "description": "Fail unless a warning of class warnClass is triggered\nby the callable when invoked with specified positional and\nkeyword arguments.  If a different type of warning is\ntriggered, it will not be handled: depending on the other\nwarning filtering rules in effect, it might be silenced, printed\nout, or raised as an exception.\n\nIf called with the callable and arguments omitted, will return a\ncontext object used like this::\n\n     with self.assertWarns(SomeWarning):\n         do_something()\n\nAn optional keyword argument 'msg' can be provided when assertWarns\nis used as a context object.\n\nThe context manager keeps a reference to the first matching\nwarning as the 'warning' attribute; similarly, the 'filename'\nand 'lineno' attributes give you information about the line\nof Python code from which the warning was triggered.\nThis allows you to inspect the warning after the assertion::\n\n    with self.assertWarns(SomeWarning) as cm:\n        do_something()\n    the_warning = cm.warning\n    self.assertEqual(the_warning.some_attribute, 147)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "assertWarnsRegex",
          "signature": "assertWarnsRegex(self, expected_warning, expected_regex, *args, **kwargs)",
          "docstring": {
            "description": "Asserts that the message in a triggered warning matches a regexp.\nBasic functioning is similar to assertWarns() with the addition\nthat only warnings whose messages also match the regular expression\nare considered successful matches.\n\nArgs:\n    expected_warning: Warning class expected to be triggered.\n    expected_regex: Regex (re.Pattern object or string) expected\n            to be found in error message.\n    args: Function to be called and extra positional args.\n    kwargs: Extra kwargs.\n    msg: Optional message used in case of failure. Can only be used\n            when assertWarnsRegex is used as a context manager.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "countTestCases",
          "signature": "countTestCases(self)",
          "docstring": {}
        },
        {
          "name": "debug",
          "signature": "debug(self)",
          "docstring": {
            "description": "Run the test without collecting errors in a TestResult",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "defaultTestResult",
          "signature": "defaultTestResult(self)",
          "docstring": {}
        },
        {
          "name": "doClassCleanups",
          "signature": "doClassCleanups()",
          "docstring": {
            "description": "Execute all class cleanup functions. Normally called for you after\ntearDownClass.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "doCleanups",
          "signature": "doCleanups(self)",
          "docstring": {
            "description": "Execute all cleanup functions. Normally called for you after\ntearDown.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enterClassContext",
          "signature": "enterClassContext(cm)",
          "docstring": {
            "description": "Same as enterContext, but class-wide.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enterContext",
          "signature": "enterContext(self, cm)",
          "docstring": {
            "description": "Enters the supplied context manager.\n\nIf successful, also adds its __exit__ method as a cleanup\nfunction and returns the result of the __enter__ method.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fail",
          "signature": "fail(self, msg=None)",
          "docstring": {
            "description": "Fail immediately, with the given message.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "failureException",
          "signature": "AssertionError(...)",
          "docstring": {
            "description": "Assertion failed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "id",
          "signature": "id(self)",
          "docstring": {}
        },
        {
          "name": "run",
          "signature": "run(self, result=None)",
          "docstring": {}
        },
        {
          "name": "setUp",
          "signature": "setUp(self)",
          "docstring": {
            "description": "Hook method for setting up the test fixture before exercising it.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setUpClass",
          "signature": "setUpClass()",
          "docstring": {
            "description": "Hook method for setting up class fixture before running tests in the class.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shortDescription",
          "signature": "shortDescription(self)",
          "docstring": {
            "description": "Returns a one-line description of the test, or None if no\ndescription has been provided.\n\nThe default implementation of this method returns the first line of\nthe specified test method's docstring.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "skipTest",
          "signature": "skipTest(self, reason)",
          "docstring": {
            "description": "Skip this test.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "subTest",
          "signature": "subTest(self, msg=<object object at 0x77381f17a7d0>, **params)",
          "docstring": {
            "description": "Return a context manager that will return the enclosed block\nof code in a subtest identified by the optional message and\nkeyword parameters.  A failure in the subtest marks the test\ncase as failed but resumes execution at the end of the enclosed\nblock, allowing further test code to be executed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "tearDown",
          "signature": "tearDown(self)",
          "docstring": {
            "description": "Hook method for deconstructing the test fixture after testing it.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "tearDownClass",
          "signature": "tearDownClass()",
          "docstring": {
            "description": "Hook method for deconstructing the class fixture after running all tests in the class.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "clear_and_catch_warnings",
      "docstring": {
        "description": "Context manager that resets warning registry for catching warnings\n\nWarnings can be slippery, because, whenever a warning is triggered, Python\nadds a ``__warningregistry__`` member to the *calling* module.  This makes\nit impossible to retrigger the warning in this module, whatever you put in\nthe warnings filters.  This context manager accepts a sequence of `modules`\nas a keyword argument to its constructor and:\n\n* stores and removes any ``__warningregistry__`` entries in given `modules`\n  on entry;\n* resets ``__warningregistry__`` to its previous state on exit.\n\nThis makes it possible to trigger any warning afresh inside the context\nmanager without disturbing the state of warnings outside.\n\nFor compatibility with Python 3.0, please consider all arguments to be\nkeyword-only.",
        "parameters": {
          "record": {
            "type": "bool, optional",
            "description": "Specifies whether warnings should be captured by a custom\n    implementation of ``warnings.showwarning()`` and be appended to a list\n    returned by the context manager. Otherwise None is returned by the\n    context manager. The objects appended to the list are arguments whose\n    attributes mirror the arguments to ``showwarning()``."
          },
          "modules": {
            "type": "sequence, optional",
            "description": "Sequence of modules for which to reset warnings registry on entry and\n    restore on exit. To work correctly, all 'ignore' filters should\n    filter by one of these modules."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import warnings\n>>> with np.testing.clear_and_catch_warnings(\n...         modules=[np.core.fromnumeric]):\n...     warnings.simplefilter('always')\n...     warnings.filterwarnings('ignore', module='np.core.fromnumeric')\n...     # do something that raises a warning but ignore those in\n...     # np.core.fromnumeric"
      },
      "methods": []
    },
    {
      "name": "suppress_warnings",
      "docstring": {
        "description": "Context manager and decorator doing much the same as\n``warnings.catch_warnings``.\n\nHowever, it also provides a filter mechanism to work around\nhttps://bugs.python.org/issue4180.\n\nThis bug causes Python before 3.4 to not reliably show warnings again\nafter they have been ignored once (even within catch_warnings). It\nmeans that no \"ignore\" filter can be used easily, since following\ntests might need to see the warning. Additionally it allows easier\nspecificity for testing warnings and can be nested.",
        "parameters": {
          "forwarding_rule": {
            "type": "str, optional",
            "description": "One of \"always\", \"once\", \"module\", or \"location\". Analogous to\n    the usual warnings module filter mode, it is useful to reduce\n    noise mostly on the outmost level. Unsuppressed and unrecorded\n    warnings will be forwarded based on this rule. Defaults to \"always\".\n    \"location\" is equivalent to the warnings \"default\", match by exact\n    location the warning warning originated from."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "Filters added inside the context manager will be discarded again\nwhen leaving it. Upon entering all filters defined outside a\ncontext will be applied automatically.\n\nWhen a recording filter is added, matching warnings are stored in the\n``log`` attribute as well as in the list returned by ``record``.\n\nIf filters are added and the ``module`` keyword is given, the\nwarning registry of this module will additionally be cleared when\napplying it, entering the context, or exiting it. This could cause\nwarnings to appear a second time after leaving the context if they\nwere configured to be printed once (default) and were already\nprinted before the context was entered.\n\nNesting this context manager will work as expected when the\nforwarding rule is \"always\" (default). Unfiltered and unrecorded\nwarnings will be passed out and be matched by the outer level.\nOn the outmost level they will be printed (or caught by another\nwarnings context). The forwarding rule argument can modify this\nbehaviour.\n\nLike ``catch_warnings`` this context manager is not threadsafe.",
        "examples": "With a context manager::\n\n    with np.testing.suppress_warnings() as sup:\n        sup.filter(DeprecationWarning, \"Some text\")\n        sup.filter(module=np.ma.core)\n        log = sup.record(FutureWarning, \"Does this occur?\")\n        command_giving_warnings()\n        # The FutureWarning was given once, the filtered warnings were\n        # ignored. All other warnings abide outside settings (may be\n        # printed/error)\n        assert_(len(log) == 1)\n        assert_(len(sup.log) == 1)  # also stored in log attribute\n\nOr as a decorator::\n\n    sup = np.testing.suppress_warnings()\n    sup.filter(module=np.ma.core)  # module must match exactly\n    @sup\n    def some_function():\n        # do something which causes a warning in np.ma.core\n        pass"
      },
      "methods": [
        {
          "name": "filter",
          "signature": "filter(self, category=<class 'Warning'>, message='', module=None)",
          "docstring": {
            "description": "Add a new suppressing filter or apply it if the state is entered.",
            "parameters": {
              "category": {
                "type": "class, optional",
                "description": "Warning class to filter"
              },
              "message": {
                "type": "string, optional",
                "description": "Regular expression matching the warning message."
              },
              "module": {
                "type": "module, optional",
                "description": "Module to filter for. Note that the module (and its file)\n    must match exactly and cannot be a submodule. This may make\n    it unreliable for external modules."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "When added within a context, filters are only added inside\nthe context and will be forgotten when the context is exited.",
            "examples": ""
          }
        },
        {
          "name": "record",
          "signature": "record(self, category=<class 'Warning'>, message='', module=None)",
          "docstring": {
            "description": "Append a new recording filter or apply it if the state is entered.\n\nAll warnings matching will be appended to the ``log`` attribute.",
            "parameters": {
              "category": {
                "type": "class, optional",
                "description": "Warning class to filter"
              },
              "message": {
                "type": "string, optional",
                "description": "Regular expression matching the warning message."
              },
              "module": {
                "type": "module, optional",
                "description": "Module to filter for. Note that the module (and its file)\n    must match exactly and cannot be a submodule. This may make\n    it unreliable for external modules."
              }
            },
            "returns": "log : list\n    A list which will be filled with all matched warnings.",
            "raises": "",
            "see_also": "",
            "notes": "When added within a context, filters are only added inside\nthe context and will be forgotten when the context is exited.",
            "examples": ""
          }
        }
      ]
    }
  ],
  "constants": [
    {
      "name": "HAS_LAPACK64",
      "value": "False",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "HAS_REFCOUNT",
      "value": "True",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "IS_MUSL",
      "value": "False",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "IS_PYPY",
      "value": "False",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "IS_PYSTON",
      "value": "False",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "IS_WASM",
      "value": "False",
      "docstring": {
        "description": "bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and False are the only two instances of the class bool.\nThe class bool is a subclass of the class int, and cannot be subclassed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ]
}