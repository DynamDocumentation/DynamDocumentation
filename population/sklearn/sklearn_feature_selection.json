{
  "description": "Feature selection algorithms.\n\nThese include univariate filter selection methods and the recursive feature elimination\nalgorithm.",
  "functions": [
    {
      "name": "GenericUnivariateSelect",
      "signature": "GenericUnivariateSelect(score_func=<function f_classif at 0x76fcdc4ffd80>, *, mode='percentile', param=1e-05)",
      "docstring": {
        "description": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n    a single array scores."
          },
          "mode": {
            "type": "{'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'",
            "description": "Feature selection mode. Note that the `'percentile'` and `'kbest'`\n    modes are supporting unsupervised feature selection (when `y` is `None`)."
          },
          "param": {
            "type": "\"all\", float or int, default=1e-5",
            "description": "Parameter of the corresponding mode.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned scores only."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n>>> X_new = transformer.fit_transform(X, y)\n>>> X_new.shape\n(569, 20)"
      }
    },
    {
      "name": "RFE",
      "signature": "RFE(estimator, *, n_features_to_select=None, step=1, verbose=0, importance_getter='auto')",
      "docstring": {
        "description": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.",
        "parameters": {
          "estimator": {
            "type": "``Estimator`` instance",
            "description": "A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance\n    (e.g. `coef_`, `feature_importances_`)."
          },
          "n_features_to_select": {
            "type": "int or float, default=None",
            "description": "The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\n    .. versionchanged:: 0.24\n       Added float values for fractions."
          },
          "step": {
            "type": "int or float, default=1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "Controls verbosity of output."
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only available when `estimator` is a classifier."
          },
          "estimator_": {
            "type": "``Estimator`` instance",
            "description": "The fitted estimator used to select features."
          },
          "n_features_": {
            "type": "int",
            "description": "The number of selected features."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "ranking_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The feature ranking, such that ``ranking_[i]`` corresponds to the\n    ranking position of the i-th feature. Selected (i.e., estimated\n    best) features are assigned rank 1."
          },
          "support_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.",
        "notes": "Allows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
        "examples": "The following example shows how to retrieve the 5 most informative\nfeatures in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFE(estimator, n_features_to_select=5, step=1)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
      }
    },
    {
      "name": "RFECV",
      "signature": "RFECV(estimator, *, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')",
      "docstring": {
        "description": "Recursive feature elimination with cross-validation to select features.\n\nThe number of features selected is tuned automatically by fitting an :class:`RFE`\nselector on the different cross-validation splits (provided by the `cv` parameter).\nThe performance of the :class:`RFE` selector are evaluated using `scorer` for\ndifferent number of selected features and aggregated together. Finally, the scores\nare averaged across folds and the number of features selected is set to the number\nof features that maximize the cross-validation score.\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.",
        "parameters": {
          "estimator": {
            "type": "``Estimator`` instance",
            "description": "A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute."
          },
          "step": {
            "type": "int or float, default=1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n    Note that the last iteration may remove fewer than ``step`` features in\n    order to reach ``min_features_to_select``."
          },
          "min_features_to_select": {
            "type": "int, default=1",
            "description": "The minimum number of features to be selected. This number of features\n    will always be scored, even if the difference between the original\n    feature count and ``min_features_to_select`` isn't divisible by\n    ``step``.\n\n    .. versionadded:: 0.20"
          },
          "cv": {
            "type": "int, cross-validation generator or an iterable, default=None",
            "description": "Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n    estimator is not a classifier or if ``y`` is neither binary nor multiclass,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value of None changed from 3-fold to 5-fold."
          },
          "scoring": {
            "type": "str, callable or None, default=None",
            "description": "A string (see :ref:`scoring_parameter`) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "Controls verbosity of output."
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": "Number of cores to run in parallel while fitting across folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18"
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance.\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only available when `estimator` is a classifier."
          },
          "estimator_": {
            "type": "``Estimator`` instance",
            "description": "The fitted estimator used to select features."
          },
          "cv_results_": {
            "type": "dict of ndarrays",
            "description": "All arrays (values of the dictionary) are sorted in ascending order\n    by the number of features used (i.e., the first element of the array\n    represents the models that used the least number of features, while the\n    last element represents the models that used all available features).\n\n    .. versionadded:: 1.0\n\n    This dictionary contains the following keys:\n\n    split(k)_test_score : ndarray of shape (n_subsets_of_features,)\n        The cross-validation scores across (k)th fold.\n\n    mean_test_score : ndarray of shape (n_subsets_of_features,)\n        Mean of scores over the folds.\n\n    std_test_score : ndarray of shape (n_subsets_of_features,)\n        Standard deviation of scores over the folds.\n\n    n_features : ndarray of shape (n_subsets_of_features,)\n        Number of features used at each step.\n\n        .. versionadded:: 1.5"
          },
          "n_features_": {
            "type": "int",
            "description": "The number of selected features with cross-validation."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "ranking_": {
            "type": "narray of shape (n_features,)",
            "description": "The feature ranking, such that `ranking_[i]`\n    corresponds to the ranking\n    position of the i-th feature.\n    Selected (i.e., estimated best)\n    features are assigned rank 1."
          },
          "support_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFE : Recursive feature elimination.",
        "notes": "The size of all values in ``cv_results_`` is equal to\n``ceil((n_features - min_features_to_select) / step) + 1``,\nwhere step is the number of features removed at each iteration.\n\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
        "examples": "The following example shows how to retrieve the a-priori not known 5\ninformative features in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFECV\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFECV(estimator, step=1, cv=5)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
      }
    },
    {
      "name": "SelectFdr",
      "signature": "SelectFdr(score_func=<function f_classif at 0x76fcdc4ffd80>, *, alpha=0.05)",
      "docstring": {
        "description": "Filter: Select the p-values for an estimated false discovery rate.\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "The highest uncorrected p-value for features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/False_discovery_rate",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFdr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
      }
    },
    {
      "name": "SelectFpr",
      "signature": "SelectFpr(score_func=<function f_classif at 0x76fcdc4ffd80>, *, alpha=0.05)",
      "docstring": {
        "description": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "Features with p-values less than `alpha` are selected.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFpr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
      }
    },
    {
      "name": "SelectFromModel",
      "signature": "SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto')",
      "docstring": {
        "description": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.",
        "parameters": {
          "estimator": {
            "type": "object",
            "description": "The base estimator from which the transformer is built.\n    This can be both a fitted (if ``prefit`` is set to True)\n    or a non-fitted estimator. The estimator should have a\n    ``feature_importances_`` or ``coef_`` attribute after fitting.\n    Otherwise, the ``importance_getter`` parameter should be used."
          },
          "threshold": {
            "type": "str or float, default=None",
            "description": "The threshold value to use for feature selection. Features whose\n    absolute importance value is greater or equal are kept while the others\n    are discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value\n    is the median (resp. the mean) of the feature importances. A scaling\n    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n    estimator has a parameter penalty set to l1, either explicitly\n    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n    Otherwise, \"mean\" is used by default."
          },
          "prefit": {
            "type": "bool, default=False",
            "description": "Whether a prefit model is expected to be passed into the constructor\n    directly or not.\n    If `True`, `estimator` must be a fitted estimator.\n    If `False`, `estimator` is fitted and updated by calling\n    `fit` and `partial_fit`, respectively."
          },
          "norm_order": {
            "type": "non-zero int, inf, -inf, default=1",
            "description": "Order of the norm used to filter the vectors of coefficients below\n    ``threshold`` in the case where the ``coef_`` attribute of the\n    estimator is of dimension 2."
          },
          "max_features": {
            "type": "int, callable, default=None",
            "description": "The maximum number of features to select.\n\n    - If an integer, then it specifies the maximum number of features to\n      allow.\n    - If a callable, then it specifies how to calculate the maximum number of\n      features allowed by using the output of `max_features(X)`.\n    - If `None`, then all features are kept.\n\n    To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n    .. versionadded:: 0.20\n    .. versionchanged:: 1.1\n       `max_features` accepts a callable."
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a ``coef_``\n    attribute or ``feature_importances_`` attribute of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "estimator_": {
            "type": "estimator",
            "description": "The base estimator from which the transformer is built. This attribute\n    exist only when `fit` has been called.\n\n    - If `prefit=True`, it is a deep copy of `estimator`.\n    - If `prefit=False`, it is a clone of `estimator` and fit on the data\n      passed to `fit` or `partial_fit`."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "max_features_": {
            "type": "int",
            "description": "Maximum number of features calculated during :term:`fit`. Only defined\n    if the ``max_features`` is not `None`.\n\n    - If `max_features` is an `int`, then `max_features_ = max_features`.\n    - If `max_features` is a callable, then `max_features_ = max_features(X)`.\n\n    .. versionadded:: 1.1"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "threshold_": {
            "type": "float",
            "description": "The threshold value used for feature selection."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.",
        "notes": "Allows NaN/Inf in the input if the underlying estimator does as well.",
        "examples": ">>> from sklearn.feature_selection import SelectFromModel\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[ 0.87, -1.34,  0.31 ],\n...      [-2.79, -0.02, -0.85 ],\n...      [-1.34, -0.48, -2.55 ],\n...      [ 1.92,  1.48,  0.65 ]]\n>>> y = [0, 1, 0, 1]\n>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n>>> selector.estimator_.coef_\narray([[-0.3252...,  0.8345...,  0.4976...]])\n>>> selector.threshold_\nnp.float64(0.55249...)\n>>> selector.get_support()\narray([False,  True, False])\n>>> selector.transform(X)\narray([[-1.34],\n       [-0.02],\n       [-0.48],\n       [ 1.48]])\n\nUsing a callable to create a selector that can use no more than half\nof the input features.\n\n>>> def half_callable(X):\n...     return round(len(X[0]) / 2)\n>>> half_selector = SelectFromModel(estimator=LogisticRegression(),\n...                                 max_features=half_callable)\n>>> _ = half_selector.fit(X, y)\n>>> half_selector.max_features_\n2"
      }
    },
    {
      "name": "SelectFwe",
      "signature": "SelectFwe(score_func=<function f_classif at 0x76fcdc4ffd80>, *, alpha=0.05)",
      "docstring": {
        "description": "Filter: Select the p-values corresponding to Family-wise error rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "The highest uncorrected p-value for features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFwe, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 15)"
      }
    },
    {
      "name": "SelectKBest",
      "signature": "SelectKBest(score_func=<function f_classif at 0x76fcdc4ffd80>, *, k=10)",
      "docstring": {
        "description": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18"
          },
          "k": {
            "type": "int or \"all\", default=10",
            "description": "Number of top features to select.\n    The \"all\" option bypasses selection, for use in a parameter search.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned only scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif: ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nchi2: Chi-squared stats of non-negative features for classification tasks.\nf_regression: F-value between label/feature for regression tasks.\nmutual_info_regression: Mutual information for a continuous target.\nSelectPercentile: Select features based on percentile of the highest\n    scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "Ties between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.",
        "examples": ">>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n>>> X_new.shape\n(1797, 20)"
      }
    },
    {
      "name": "SelectPercentile",
      "signature": "SelectPercentile(score_func=<function f_classif at 0x76fcdc4ffd80>, *, percentile=10)",
      "docstring": {
        "description": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18"
          },
          "percentile": {
            "type": "int, default=10",
            "description": "Percent of features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned only scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "Ties between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.",
        "examples": ">>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectPercentile, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n>>> X_new.shape\n(1797, 7)"
      }
    },
    {
      "name": "SelectorMixin",
      "signature": "SelectorMixin()",
      "docstring": {
        "description": "Transformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.base import BaseEstimator\n>>> from sklearn.feature_selection import SelectorMixin\n>>> class FeatureSelector(SelectorMixin, BaseEstimator):\n...    def fit(self, X, y=None):\n...        self.n_features_in_ = X.shape[1]\n...        return self\n...    def _get_support_mask(self):\n...        mask = np.zeros(self.n_features_in_, dtype=bool)\n...        mask[:2] = True  # select the first two features\n...        return mask\n>>> X, y = load_iris(return_X_y=True)\n>>> FeatureSelector().fit_transform(X, y).shape\n(150, 2)"
      }
    },
    {
      "name": "SequentialFeatureSelector",
      "signature": "SequentialFeatureSelector(estimator, *, n_features_to_select='auto', tol=None, direction='forward', scoring=None, cv=5, n_jobs=None)",
      "docstring": {
        "description": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator. In\nthe case of unsupervised learning, this Sequential Feature Selector\nlooks only at the features (X), not the desired outputs (y).\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": "An unfitted estimator."
          },
          "n_features_to_select": {
            "type": "\"auto\", int or float, default=\"auto\"",
            "description": "If `\"auto\"`, the behaviour depends on the `tol` parameter:\n\n    - if `tol` is not `None`, then features are selected while the score\n      change does not exceed `tol`.\n    - otherwise, half of the features are selected.\n\n    If integer, the parameter is the absolute number of features to select.\n    If float between 0 and 1, it is the fraction of features to select.\n\n    .. versionadded:: 1.1\n       The option `\"auto\"` was added in version 1.1.\n\n    .. versionchanged:: 1.3\n       The default changed from `\"warn\"` to `\"auto\"` in 1.3."
          },
          "tol": {
            "type": "float, default=None",
            "description": "If the score is not incremented by at least `tol` between two\n    consecutive feature additions or removals, stop adding or removing.\n\n    `tol` can be negative when removing features using `direction=\"backward\"`.\n    `tol` is required to be strictly positive when doing forward selection.\n    It can be useful to reduce the number of features at the cost of a small\n    decrease in the score.\n\n    `tol` is enabled only when `n_features_to_select` is `\"auto\"`.\n\n    .. versionadded:: 1.1"
          },
          "direction": {
            "type": "{'forward', 'backward'}, default='forward'",
            "description": "Whether to perform forward selection or backward selection."
          },
          "scoring": {
            "type": "str or callable, default=None",
            "description": "A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring_callable`) to evaluate the predictions on the test set.\n\n    NOTE that when using a custom scorer, it should return a single\n    value.\n\n    If None, the estimator's score method is used."
          },
          "cv": {
            "type": "int, cross-validation generator or an iterable, default=None",
            "description": "Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other\n    cases, :class:`~sklearn.model_selection.KFold` is used. These splitters\n    are instantiated with `shuffle=False` so the splits will be the same\n    across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "Number of jobs to run in parallel. When evaluating a new feature to\n    add or remove, the cross-validation procedure is parallel over the\n    folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_features_to_select_": {
            "type": "int",
            "description": "The number of features that were selected."
          },
          "support_": {
            "type": "ndarray of shape (n_features,), dtype=bool",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "GenericUnivariateSelect : Univariate feature selector with configurable\n    strategy.\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination based on importance weights, with\n    automatic selection of the number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.",
        "notes": "",
        "examples": ">>> from sklearn.feature_selection import SequentialFeatureSelector\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n>>> sfs.fit(X, y)\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=3)\n>>> sfs.get_support()\narray([ True, False,  True,  True])\n>>> sfs.transform(X).shape\n(150, 3)"
      }
    },
    {
      "name": "VarianceThreshold",
      "signature": "VarianceThreshold(threshold=0.0)",
      "docstring": {
        "description": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.",
        "parameters": {
          "threshold": {
            "type": "float, default=0",
            "description": "Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n\nAttributes\n----------"
          },
          "variances_": {
            "type": "array, shape (n_features,)",
            "description": "Variances of individual features."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "SelectFromModel: Meta-transformer for selecting features based on\n    importance weights.\nSelectPercentile : Select features according to a percentile of the highest\n    scores.\nSequentialFeatureSelector : Transformer that performs Sequential Feature\n    Selection.",
        "notes": "Allows NaN in the input.\nRaises ValueError if no feature in X meets the variance threshold.",
        "examples": "The following dataset has integer features, two of which are the same\nin every sample. These are removed with the default setting for threshold::\n\n    >>> from sklearn.feature_selection import VarianceThreshold\n    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    >>> selector = VarianceThreshold()\n    >>> selector.fit_transform(X)\n    array([[2, 0],\n           [1, 4],\n           [1, 1]])"
      }
    },
    {
      "name": "chi2",
      "signature": "chi2(X, y)",
      "docstring": {
        "description": "Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the `n_features` features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only **non-negative integer feature values** such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nIf some of your features are continuous, you need to bin them, for\nexample by using :class:`~sklearn.preprocessing.KBinsDiscretizer`.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "Sample vectors."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "Target vector (class labels)."
          }
        },
        "returns": "chi2 : ndarray of shape (n_features,)\n    Chi2 statistics for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values for each feature.",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nf_regression : F-value between label/feature for regression tasks.",
        "notes": "Complexity of this algorithm is O(n_classes * n_features).",
        "examples": ">>> import numpy as np\n>>> from sklearn.feature_selection import chi2\n>>> X = np.array([[1, 1, 3],\n...               [0, 1, 5],\n...               [5, 4, 1],\n...               [6, 6, 2],\n...               [1, 4, 0],\n...               [0, 0, 0]])\n>>> y = np.array([1, 1, 0, 0, 2, 2])\n>>> chi2_stats, p_values = chi2(X, y)\n>>> chi2_stats\narray([15.3...,  6.5       ,  8.9...])\n>>> p_values\narray([0.0004..., 0.0387..., 0.0116... ])"
      }
    },
    {
      "name": "f_classif",
      "signature": "f_classif(X, y)",
      "docstring": {
        "description": "Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "The set of regressors that will be tested sequentially."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "The target vector."
          }
        },
        "returns": "f_statistic : ndarray of shape (n_features,)\n    F-statistic for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values associated with the F-statistic.",
        "raises": "",
        "see_also": "chi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import make_classification\n>>> from sklearn.feature_selection import f_classif\n>>> X, y = make_classification(\n...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,\n...     shuffle=False, random_state=42\n... )\n>>> f_statistic, p_values = f_classif(X, y)\n>>> f_statistic\narray([2.2...e+02, 7.0...e-01, 1.6...e+00, 9.3...e-01,\n       5.4...e+00, 3.2...e-01, 4.7...e-02, 5.7...e-01,\n       7.5...e-01, 8.9...e-02])\n>>> p_values\narray([7.1...e-27, 4.0...e-01, 1.9...e-01, 3.3...e-01,\n       2.2...e-02, 5.7...e-01, 8.2...e-01, 4.5...e-01,\n       3.8...e-01, 7.6...e-01])"
      }
    },
    {
      "name": "f_oneway",
      "signature": "f_oneway(*args)",
      "docstring": {
        "description": "Perform a 1-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that 2 or more groups have\nthe same population mean. The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "*args": {
            "type": "{array-like, sparse matrix}",
            "description": "Sample1, sample2... The sample measurements should be given as\n    arguments."
          }
        },
        "returns": "f_statistic : float\n    The computed F-value of the test.\np_value : float\n    The associated p-value from the F-distribution.",
        "raises": "",
        "see_also": "",
        "notes": "The ANOVA test has important assumptions that must be satisfied in order\nfor the associated p-value to be valid.\n\n1. The samples are independent\n2. Each sample is from a normally distributed population\n3. The population standard deviations of the groups are all equal. This\n   property is known as homoscedasticity.\n\nIf these assumptions are not true for a given set of data, it may still be\npossible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although\nwith some loss of power.\n\nThe algorithm is from Heiman[2], pp.394-7.\n\nSee ``scipy.stats.f_oneway`` that should give the same results while\nbeing less efficient.\n\nReferences\n----------\n.. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n       Statistics\". Chapter 14.\n       http://vassarstats.net/textbook\n\n.. [2] Heiman, G.W.  Research Methods in Statistics. 2002.",
        "examples": ""
      }
    },
    {
      "name": "f_regression",
      "signature": "f_regression(X, y, *, center=True, force_finite=True)",
      "docstring": {
        "description": "Univariate linear regression tests returning F-statistic and p-values.\n\nQuick linear model for testing the effect of a single regressor,\nsequentially for many regressors.\n\nThis is done in 2 steps:\n\n1. The cross correlation between each regressor and the target is computed\n   using :func:`r_regression` as::\n\n       E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))\n\n2. It is converted to an F score and then to a p-value.\n\n:func:`f_regression` is derived from :func:`r_regression` and will rank\nfeatures in the same order if all the features are positively correlated\nwith the target.\n\nNote however that contrary to :func:`f_regression`, :func:`r_regression`\nvalues lie in [-1, 1] and can thus be negative. :func:`f_regression` is\ntherefore recommended as a feature selection criterion to identify\npotentially predictive feature for a downstream classifier, irrespective of\nthe sign of the association with the target variable.\n\nFurthermore :func:`f_regression` returns p-values while\n:func:`r_regression` does not.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "The data matrix."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "The target vector."
          },
          "center": {
            "type": "bool, default=True",
            "description": "Whether or not to center the data matrix `X` and the target vector `y`.\n    By default, `X` and `y` will be centered."
          },
          "force_finite": {
            "type": "bool, default=True",
            "description": "Whether or not to force the F-statistics and associated p-values to\n    be finite. There are two cases where the F-statistic is expected to not\n    be finite:\n\n    - when the target `y` or some features in `X` are constant. In this\n      case, the Pearson's R correlation is not defined leading to obtain\n      `np.nan` values in the F-statistic and p-value. When\n      `force_finite=True`, the F-statistic is set to `0.0` and the\n      associated p-value is set to `1.0`.\n    - when a feature in `X` is perfectly correlated (or\n      anti-correlated) with the target `y`. In this case, the F-statistic\n      is expected to be `np.inf`. When `force_finite=True`, the F-statistic\n      is set to `np.finfo(dtype).max` and the associated p-value is set to\n      `0.0`.\n\n    .. versionadded:: 1.1"
          }
        },
        "returns": "f_statistic : ndarray of shape (n_features,)\n    F-statistic for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values associated with the F-statistic.",
        "raises": "",
        "see_also": "r_regression: Pearson's R between label/feature for regression tasks.\nf_classif: ANOVA F-value between label/feature for classification tasks.\nchi2: Chi-squared stats of non-negative features for classification tasks.\nSelectKBest: Select features based on the k highest scores.\nSelectFpr: Select features based on a false positive rate test.\nSelectFdr: Select features based on an estimated false discovery rate.\nSelectFwe: Select features based on family-wise error rate.\nSelectPercentile: Select features based on percentile of the highest\n    scores.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import make_regression\n>>> from sklearn.feature_selection import f_regression\n>>> X, y = make_regression(\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\n... )\n>>> f_statistic, p_values = f_regression(X, y)\n>>> f_statistic\narray([1.2...+00, 2.6...+13, 2.6...+00])\n>>> p_values\narray([2.7..., 1.5..., 1.0...])"
      }
    },
    {
      "name": "mutual_info_classif",
      "signature": "mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None, n_jobs=None)",
      "docstring": {
        "description": "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "Feature matrix."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "Target vector."
          },
          "discrete_features": {
            "type": "'auto', bool or array-like, default='auto'",
            "description": "If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`."
          },
          "n_neighbors": {
            "type": "int, default=3",
            "description": "Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias."
          },
          "copy": {
            "type": "bool, default=True",
            "description": "Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten."
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of jobs to use for computing the mutual information.\n    The parallelization is done on the columns of `X`.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 1.5"
          }
        },
        "returns": "mi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target in\n    nat units.",
        "raises": "",
        "see_also": "",
        "notes": "1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16",
        "examples": ">>> from sklearn.datasets import make_classification\n>>> from sklearn.feature_selection import mutual_info_classif\n>>> X, y = make_classification(\n...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,\n...     shuffle=False, random_state=42\n... )\n>>> mutual_info_classif(X, y)\narray([0.58..., 0.10..., 0.19..., 0.09... , 0.        ,\n       0.     , 0.     , 0.     , 0.      , 0.        ])"
      }
    },
    {
      "name": "mutual_info_regression",
      "signature": "mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None, n_jobs=None)",
      "docstring": {
        "description": "Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "X": {
            "type": "array-like or sparse matrix, shape (n_samples, n_features)",
            "description": "Feature matrix."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "Target vector."
          },
          "discrete_features": {
            "type": "{'auto', bool, array-like}, default='auto'",
            "description": "If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`."
          },
          "n_neighbors": {
            "type": "int, default=3",
            "description": "Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias."
          },
          "copy": {
            "type": "bool, default=True",
            "description": "Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten."
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of jobs to use for computing the mutual information.\n    The parallelization is done on the columns of `X`.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 1.5"
          }
        },
        "returns": "mi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target in\n    nat units.",
        "raises": "",
        "see_also": "",
        "notes": "1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16",
        "examples": ">>> from sklearn.datasets import make_regression\n>>> from sklearn.feature_selection import mutual_info_regression\n>>> X, y = make_regression(\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\n... )\n>>> mutual_info_regression(X, y)\narray([0.1..., 2.6...  , 0.0...])"
      }
    },
    {
      "name": "r_regression",
      "signature": "r_regression(X, y, *, center=True, force_finite=True)",
      "docstring": {
        "description": "Compute Pearson's r for each features and the target.\n\nPearson's r is also known as the Pearson correlation coefficient.\n\nLinear model for testing the individual effect of each of many regressors.\nThis is a scoring function to be used in a feature selection procedure, not\na free standing feature selection procedure.\n\nThe cross correlation between each regressor and the target is computed\nas::\n\n    E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))\n\nFor more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n\n.. versionadded:: 1.0",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "The data matrix."
          },
          "y": {
            "type": "array-like of shape (n_samples,)",
            "description": "The target vector."
          },
          "center": {
            "type": "bool, default=True",
            "description": "Whether or not to center the data matrix `X` and the target vector `y`.\n    By default, `X` and `y` will be centered."
          },
          "force_finite": {
            "type": "bool, default=True",
            "description": "Whether or not to force the Pearson's R correlation to be finite.\n    In the particular case where some features in `X` or the target `y`\n    are constant, the Pearson's R correlation is not defined. When\n    `force_finite=False`, a correlation of `np.nan` is returned to\n    acknowledge this case. When `force_finite=True`, this value will be\n    forced to a minimal correlation of `0.0`.\n\n    .. versionadded:: 1.1"
          }
        },
        "returns": "correlation_coefficient : ndarray of shape (n_features,)\n    Pearson's R correlation coefficients of features.",
        "raises": "",
        "see_also": "f_regression: Univariate linear regression tests returning f-statistic\n    and p-values.\nmutual_info_regression: Mutual information for a continuous target.\nf_classif: ANOVA F-value between label/feature for classification tasks.\nchi2: Chi-squared stats of non-negative features for classification tasks.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import make_regression\n>>> from sklearn.feature_selection import r_regression\n>>> X, y = make_regression(\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\n... )\n>>> r_regression(X, y)\narray([-0.15...,  1.        , -0.22...])"
      }
    }
  ],
  "classes": [
    {
      "name": "GenericUnivariateSelect",
      "docstring": {
        "description": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n    a single array scores."
          },
          "mode": {
            "type": "{'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'",
            "description": "Feature selection mode. Note that the `'percentile'` and `'kbest'`\n    modes are supporting unsupervised feature selection (when `y` is `None`)."
          },
          "param": {
            "type": "\"all\", float or int, default=1e-5",
            "description": "Parameter of the corresponding mode.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned scores only."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n>>> X_new = transformer.fit_transform(X, y)\n>>> X_new.shape\n(569, 20)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RFE",
      "docstring": {
        "description": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.",
        "parameters": {
          "estimator": {
            "type": "``Estimator`` instance",
            "description": "A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance\n    (e.g. `coef_`, `feature_importances_`)."
          },
          "n_features_to_select": {
            "type": "int or float, default=None",
            "description": "The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\n    .. versionchanged:: 0.24\n       Added float values for fractions."
          },
          "step": {
            "type": "int or float, default=1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "Controls verbosity of output."
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only available when `estimator` is a classifier."
          },
          "estimator_": {
            "type": "``Estimator`` instance",
            "description": "The fitted estimator used to select features."
          },
          "n_features_": {
            "type": "int",
            "description": "The number of selected features."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "ranking_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The feature ranking, such that ``ranking_[i]`` corresponds to the\n    ranking position of the i-th feature. Selected (i.e., estimated\n    best) features are assigned rank 1."
          },
          "support_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.",
        "notes": "Allows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
        "examples": "The following example shows how to retrieve the 5 most informative\nfeatures in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFE(estimator, n_features_to_select=5, step=1)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "docstring": {
            "description": "Compute the decision function of ``X``.",
            "parameters": {
              "X": {
                "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
                "description": "The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``."
              }
            },
            "returns": "score : array, shape = [n_samples, n_classes] or [n_samples]\n    The decision function of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification produce an array of shape\n    [n_samples].",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, **fit_params)",
          "docstring": {
            "description": "Fit the RFE model and then the underlying estimator on the selected features.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "The target values."
              },
              "**fit_params": {
                "type": "dict",
                "description": "- If `enable_metadata_routing=False` (default): Parameters directly passed\n      to the ``fit`` method of the underlying estimator.\n\n    - If `enable_metadata_routing=True`: Parameters safely routed to the ``fit``\n      method of the underlying estimator.\n\n    .. versionchanged:: 1.6\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\n.. versionadded:: 1.6",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, **predict_params)",
          "docstring": {
            "description": "Reduce X to the selected features and predict using the estimator.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              },
              "**predict_params": {
                "type": "dict",
                "description": "Parameters to route to the ``predict`` method of the\n    underlying estimator.\n\n    .. versionadded:: 1.6\n        Only available if `enable_metadata_routing=True`,\n        which can be set by using\n        ``sklearn.set_config(enable_metadata_routing=True)``.\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "y : array of shape [n_samples]\n    The predicted target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Predict class log-probabilities for X.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "p : array of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Predict class probabilities for X.",
            "parameters": {
              "X": {
                "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
                "description": "The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``."
              }
            },
            "returns": "p : array of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, **score_params)",
          "docstring": {
            "description": "Reduce X to the selected features and return the score of the estimator.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              },
              "y": {
                "type": "array of shape [n_samples]",
                "description": "The target values."
              },
              "**score_params": {
                "type": "dict",
                "description": "- If `enable_metadata_routing=False` (default): Parameters directly passed\n      to the ``score`` method of the underlying estimator.\n\n    - If `enable_metadata_routing=True`: Parameters safely routed to the `score`\n      method of the underlying estimator.\n\n    .. versionadded:: 1.0\n\n    .. versionchanged:: 1.6\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "score : float\n    Score of the underlying base estimator computed with the selected\n    features returned by `rfe.transform(X)` and `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RFECV",
      "docstring": {
        "description": "Recursive feature elimination with cross-validation to select features.\n\nThe number of features selected is tuned automatically by fitting an :class:`RFE`\nselector on the different cross-validation splits (provided by the `cv` parameter).\nThe performance of the :class:`RFE` selector are evaluated using `scorer` for\ndifferent number of selected features and aggregated together. Finally, the scores\nare averaged across folds and the number of features selected is set to the number\nof features that maximize the cross-validation score.\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.",
        "parameters": {
          "estimator": {
            "type": "``Estimator`` instance",
            "description": "A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute."
          },
          "step": {
            "type": "int or float, default=1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n    Note that the last iteration may remove fewer than ``step`` features in\n    order to reach ``min_features_to_select``."
          },
          "min_features_to_select": {
            "type": "int, default=1",
            "description": "The minimum number of features to be selected. This number of features\n    will always be scored, even if the difference between the original\n    feature count and ``min_features_to_select`` isn't divisible by\n    ``step``.\n\n    .. versionadded:: 0.20"
          },
          "cv": {
            "type": "int, cross-validation generator or an iterable, default=None",
            "description": "Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n    estimator is not a classifier or if ``y`` is neither binary nor multiclass,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value of None changed from 3-fold to 5-fold."
          },
          "scoring": {
            "type": "str, callable or None, default=None",
            "description": "A string (see :ref:`scoring_parameter`) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "Controls verbosity of output."
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": "Number of cores to run in parallel while fitting across folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18"
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance.\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only available when `estimator` is a classifier."
          },
          "estimator_": {
            "type": "``Estimator`` instance",
            "description": "The fitted estimator used to select features."
          },
          "cv_results_": {
            "type": "dict of ndarrays",
            "description": "All arrays (values of the dictionary) are sorted in ascending order\n    by the number of features used (i.e., the first element of the array\n    represents the models that used the least number of features, while the\n    last element represents the models that used all available features).\n\n    .. versionadded:: 1.0\n\n    This dictionary contains the following keys:\n\n    split(k)_test_score : ndarray of shape (n_subsets_of_features,)\n        The cross-validation scores across (k)th fold.\n\n    mean_test_score : ndarray of shape (n_subsets_of_features,)\n        Mean of scores over the folds.\n\n    std_test_score : ndarray of shape (n_subsets_of_features,)\n        Standard deviation of scores over the folds.\n\n    n_features : ndarray of shape (n_subsets_of_features,)\n        Number of features used at each step.\n\n        .. versionadded:: 1.5"
          },
          "n_features_": {
            "type": "int",
            "description": "The number of selected features with cross-validation."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "ranking_": {
            "type": "narray of shape (n_features,)",
            "description": "The feature ranking, such that `ranking_[i]`\n    corresponds to the ranking\n    position of the i-th feature.\n    Selected (i.e., estimated best)\n    features are assigned rank 1."
          },
          "support_": {
            "type": "ndarray of shape (n_features,)",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFE : Recursive feature elimination.",
        "notes": "The size of all values in ``cv_results_`` is equal to\n``ceil((n_features - min_features_to_select) / step) + 1``,\nwhere step is the number of features removed at each iteration.\n\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
        "examples": "The following example shows how to retrieve the a-priori not known 5\ninformative features in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFECV\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFECV(estimator, step=1, cv=5)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "docstring": {
            "description": "Compute the decision function of ``X``.",
            "parameters": {
              "X": {
                "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
                "description": "The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``."
              }
            },
            "returns": "score : array, shape = [n_samples, n_classes] or [n_samples]\n    The decision function of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification produce an array of shape\n    [n_samples].",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, *, groups=None, **params)",
          "docstring": {
            "description": "Fit the RFE model and automatically tune the number of selected features.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vector, where `n_samples` is the number of samples and\n    `n_features` is the total number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values (integers for classification, real numbers for\n    regression)."
              },
              "groups": {
                "type": "array-like of shape (n_samples,) or None, default=None",
                "description": "Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n    .. versionadded:: 0.20"
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "Parameters passed to the ``fit`` method of the estimator,\n    the scorer, and the CV splitter.\n\n    .. versionadded:: 1.6\n        Only available if `enable_metadata_routing=True`,\n        which can be set by using\n        ``sklearn.set_config(enable_metadata_routing=True)``.\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\n.. versionadded:: 1.6",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, **predict_params)",
          "docstring": {
            "description": "Reduce X to the selected features and predict using the estimator.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              },
              "**predict_params": {
                "type": "dict",
                "description": "Parameters to route to the ``predict`` method of the\n    underlying estimator.\n\n    .. versionadded:: 1.6\n        Only available if `enable_metadata_routing=True`,\n        which can be set by using\n        ``sklearn.set_config(enable_metadata_routing=True)``.\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "y : array of shape [n_samples]\n    The predicted target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Predict class log-probabilities for X.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "p : array of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Predict class probabilities for X.",
            "parameters": {
              "X": {
                "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
                "description": "The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``."
              }
            },
            "returns": "p : array of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, **score_params)",
          "docstring": {
            "description": "Score using the `scoring` option on the given test data and labels.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "True labels for X."
              },
              "**score_params": {
                "type": "dict",
                "description": "Parameters to pass to the `score` method of the underlying scorer.\n\n    .. versionadded:: 1.6\n        Only available if `enable_metadata_routing=True`,\n        which can be set by using\n        ``sklearn.set_config(enable_metadata_routing=True)``.\n        See :ref:`Metadata Routing User Guide <metadata_routing>`\n        for more details."
              }
            },
            "returns": "score : float\n    Score of self.predict(X) w.r.t. y defined by `scoring`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectFdr",
      "docstring": {
        "description": "Filter: Select the p-values for an estimated false discovery rate.\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "The highest uncorrected p-value for features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/False_discovery_rate",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFdr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectFpr",
      "docstring": {
        "description": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "Features with p-values less than `alpha` are selected.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFpr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectFromModel",
      "docstring": {
        "description": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.",
        "parameters": {
          "estimator": {
            "type": "object",
            "description": "The base estimator from which the transformer is built.\n    This can be both a fitted (if ``prefit`` is set to True)\n    or a non-fitted estimator. The estimator should have a\n    ``feature_importances_`` or ``coef_`` attribute after fitting.\n    Otherwise, the ``importance_getter`` parameter should be used."
          },
          "threshold": {
            "type": "str or float, default=None",
            "description": "The threshold value to use for feature selection. Features whose\n    absolute importance value is greater or equal are kept while the others\n    are discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value\n    is the median (resp. the mean) of the feature importances. A scaling\n    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n    estimator has a parameter penalty set to l1, either explicitly\n    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n    Otherwise, \"mean\" is used by default."
          },
          "prefit": {
            "type": "bool, default=False",
            "description": "Whether a prefit model is expected to be passed into the constructor\n    directly or not.\n    If `True`, `estimator` must be a fitted estimator.\n    If `False`, `estimator` is fitted and updated by calling\n    `fit` and `partial_fit`, respectively."
          },
          "norm_order": {
            "type": "non-zero int, inf, -inf, default=1",
            "description": "Order of the norm used to filter the vectors of coefficients below\n    ``threshold`` in the case where the ``coef_`` attribute of the\n    estimator is of dimension 2."
          },
          "max_features": {
            "type": "int, callable, default=None",
            "description": "The maximum number of features to select.\n\n    - If an integer, then it specifies the maximum number of features to\n      allow.\n    - If a callable, then it specifies how to calculate the maximum number of\n      features allowed by using the output of `max_features(X)`.\n    - If `None`, then all features are kept.\n\n    To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n    .. versionadded:: 0.20\n    .. versionchanged:: 1.1\n       `max_features` accepts a callable."
          },
          "importance_getter": {
            "type": "str or callable, default='auto'",
            "description": "If 'auto', uses the feature importance either through a ``coef_``\n    attribute or ``feature_importances_`` attribute of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "estimator_": {
            "type": "estimator",
            "description": "The base estimator from which the transformer is built. This attribute\n    exist only when `fit` has been called.\n\n    - If `prefit=True`, it is a deep copy of `estimator`.\n    - If `prefit=False`, it is a clone of `estimator` and fit on the data\n      passed to `fit` or `partial_fit`."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "max_features_": {
            "type": "int",
            "description": "Maximum number of features calculated during :term:`fit`. Only defined\n    if the ``max_features`` is not `None`.\n\n    - If `max_features` is an `int`, then `max_features_ = max_features`.\n    - If `max_features` is a callable, then `max_features_ = max_features(X)`.\n\n    .. versionadded:: 1.1"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "threshold_": {
            "type": "float",
            "description": "The threshold value used for feature selection."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.",
        "notes": "Allows NaN/Inf in the input if the underlying estimator does as well.",
        "examples": ">>> from sklearn.feature_selection import SelectFromModel\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[ 0.87, -1.34,  0.31 ],\n...      [-2.79, -0.02, -0.85 ],\n...      [-1.34, -0.48, -2.55 ],\n...      [ 1.92,  1.48,  0.65 ]]\n>>> y = [0, 1, 0, 1]\n>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n>>> selector.estimator_.coef_\narray([[-0.3252...,  0.8345...,  0.4976...]])\n>>> selector.threshold_\nnp.float64(0.55249...)\n>>> selector.get_support()\narray([False,  True, False])\n>>> selector.transform(X)\narray([[-1.34],\n       [-0.02],\n       [-0.48],\n       [ 1.48]])\n\nUsing a callable to create a selector that can use no more than half\nof the input features.\n\n>>> def half_callable(X):\n...     return round(len(X[0]) / 2)\n>>> half_selector = SelectFromModel(estimator=LogisticRegression(),\n...                                 max_features=half_callable)\n>>> _ = half_selector.fit(X, y)\n>>> half_selector.max_features_\n2"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit the SelectFromModel meta-transformer.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "The target values (integers that correspond to classes in\n    classification, real numbers in regression)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "- If `enable_metadata_routing=False` (default): Parameters directly passed\n      to the `fit` method of the sub-estimator. They are ignored if\n      `prefit=True`.\n\n    - If `enable_metadata_routing=True`: Parameters safely routed to the `fit`\n      method of the sub-estimator. They are ignored if `prefit=True`.\n\n    .. versionchanged:: 1.4\n        See :ref:`Metadata Routing User Guide <metadata_routing>` for\n        more details."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\n.. versionadded:: 1.4",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, **partial_fit_params)",
          "docstring": {
            "description": "Fit the SelectFromModel meta-transformer only once.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "The target values (integers that correspond to classes in\n    classification, real numbers in regression)."
              },
              "**partial_fit_params": {
                "type": "dict",
                "description": "- If `enable_metadata_routing=False` (default): Parameters directly passed\n      to the `partial_fit` method of the sub-estimator.\n\n    - If `enable_metadata_routing=True`: Parameters passed to the `partial_fit`\n      method of the sub-estimator. They are ignored if `prefit=True`.\n\n    .. versionchanged:: 1.4\n\n        `**partial_fit_params` are routed to the sub-estimator, if\n        `enable_metadata_routing=True` is set via\n        :func:`~sklearn.set_config`, which allows for aliasing.\n\n        See :ref:`Metadata Routing User Guide <metadata_routing>` for\n        more details."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectFwe",
      "docstring": {
        "description": "Filter: Select the p-values corresponding to Family-wise error rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks."
          },
          "alpha": {
            "type": "float, default=5e-2",
            "description": "The highest uncorrected p-value for features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "",
        "examples": ">>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFwe, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 15)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectKBest",
      "docstring": {
        "description": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18"
          },
          "k": {
            "type": "int or \"all\", default=10",
            "description": "Number of top features to select.\n    The \"all\" option bypasses selection, for use in a parameter search.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned only scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif: ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nchi2: Chi-squared stats of non-negative features for classification tasks.\nf_regression: F-value between label/feature for regression tasks.\nmutual_info_regression: Mutual information for a continuous target.\nSelectPercentile: Select features based on percentile of the highest\n    scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "Ties between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.",
        "examples": ">>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n>>> X_new.shape\n(1797, 20)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectPercentile",
      "docstring": {
        "description": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
        "parameters": {
          "score_func": {
            "type": "callable, default=f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18"
          },
          "percentile": {
            "type": "int, default=10",
            "description": "Percent of features to keep.\n\nAttributes\n----------"
          },
          "scores_": {
            "type": "array-like of shape (n_features,)",
            "description": "Scores of features."
          },
          "pvalues_": {
            "type": "array-like of shape (n_features,)",
            "description": "p-values of feature scores, None if `score_func` returned only scores."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "f_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
        "notes": "Ties between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.",
        "examples": ">>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectPercentile, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n>>> X_new.shape\n(1797, 7)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Run score function on (X, y) and get the appropriate features.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or None",
                "description": "The target values (class labels in classification, real numbers in\n    regression). If the selector is unsupervised then `y` can be set to `None`."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SelectorMixin",
      "docstring": {
        "description": "Transformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.base import BaseEstimator\n>>> from sklearn.feature_selection import SelectorMixin\n>>> class FeatureSelector(SelectorMixin, BaseEstimator):\n...    def fit(self, X, y=None):\n...        self.n_features_in_ = X.shape[1]\n...        return self\n...    def _get_support_mask(self):\n...        mask = np.zeros(self.n_features_in_, dtype=bool)\n...        mask[:2] = True  # select the first two features\n...        return mask\n>>> X, y = load_iris(return_X_y=True)\n>>> FeatureSelector().fit_transform(X, y).shape\n(150, 2)"
      },
      "methods": [
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SequentialFeatureSelector",
      "docstring": {
        "description": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator. In\nthe case of unsupervised learning, this Sequential Feature Selector\nlooks only at the features (X), not the desired outputs (y).\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": "An unfitted estimator."
          },
          "n_features_to_select": {
            "type": "\"auto\", int or float, default=\"auto\"",
            "description": "If `\"auto\"`, the behaviour depends on the `tol` parameter:\n\n    - if `tol` is not `None`, then features are selected while the score\n      change does not exceed `tol`.\n    - otherwise, half of the features are selected.\n\n    If integer, the parameter is the absolute number of features to select.\n    If float between 0 and 1, it is the fraction of features to select.\n\n    .. versionadded:: 1.1\n       The option `\"auto\"` was added in version 1.1.\n\n    .. versionchanged:: 1.3\n       The default changed from `\"warn\"` to `\"auto\"` in 1.3."
          },
          "tol": {
            "type": "float, default=None",
            "description": "If the score is not incremented by at least `tol` between two\n    consecutive feature additions or removals, stop adding or removing.\n\n    `tol` can be negative when removing features using `direction=\"backward\"`.\n    `tol` is required to be strictly positive when doing forward selection.\n    It can be useful to reduce the number of features at the cost of a small\n    decrease in the score.\n\n    `tol` is enabled only when `n_features_to_select` is `\"auto\"`.\n\n    .. versionadded:: 1.1"
          },
          "direction": {
            "type": "{'forward', 'backward'}, default='forward'",
            "description": "Whether to perform forward selection or backward selection."
          },
          "scoring": {
            "type": "str or callable, default=None",
            "description": "A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring_callable`) to evaluate the predictions on the test set.\n\n    NOTE that when using a custom scorer, it should return a single\n    value.\n\n    If None, the estimator's score method is used."
          },
          "cv": {
            "type": "int, cross-validation generator or an iterable, default=None",
            "description": "Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other\n    cases, :class:`~sklearn.model_selection.KFold` is used. These splitters\n    are instantiated with `shuffle=False` so the splits will be the same\n    across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "Number of jobs to run in parallel. When evaluating a new feature to\n    add or remove, the cross-validation procedure is parallel over the\n    folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_features_to_select_": {
            "type": "int",
            "description": "The number of features that were selected."
          },
          "support_": {
            "type": "ndarray of shape (n_features,), dtype=bool",
            "description": "The mask of selected features."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "GenericUnivariateSelect : Univariate feature selector with configurable\n    strategy.\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination based on importance weights, with\n    automatic selection of the number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.",
        "notes": "",
        "examples": ">>> from sklearn.feature_selection import SequentialFeatureSelector\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n>>> sfs.fit(X, y)\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=3)\n>>> sfs.get_support()\narray([ True, False,  True,  True])\n>>> sfs.transform(X).shape\n(150, 3)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "docstring": {
            "description": "Learn the features to select from X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of predictors."
              },
              "y": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Target values. This parameter may be ignored for\n    unsupervised learning."
              },
              "**params": {
                "type": "dict, default=None",
                "description": "Parameters to be passed to the underlying `estimator`, `cv`\n    and `scorer` objects.\n\n    .. versionadded:: 1.6\n\n        Only available if `enable_metadata_routing=True`,\n        which can be set by using\n        ``sklearn.set_config(enable_metadata_routing=True)``.\n        See :ref:`Metadata Routing User Guide <metadata_routing>` for\n        more details."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\n.. versionadded:: 1.6",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "VarianceThreshold",
      "docstring": {
        "description": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.",
        "parameters": {
          "threshold": {
            "type": "float, default=0",
            "description": "Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n\nAttributes\n----------"
          },
          "variances_": {
            "type": "array, shape (n_features,)",
            "description": "Variances of individual features."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "SelectFromModel: Meta-transformer for selecting features based on\n    importance weights.\nSelectPercentile : Select features according to a percentile of the highest\n    scores.\nSequentialFeatureSelector : Transformer that performs Sequential Feature\n    Selection.",
        "notes": "Allows NaN in the input.\nRaises ValueError if no feature in X meets the variance threshold.",
        "examples": "The following dataset has integer features, two of which are the same\nin every sample. These are removed with the default setting for threshold::\n\n    >>> from sklearn.feature_selection import VarianceThreshold\n    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    >>> selector = VarianceThreshold()\n    >>> selector.fit_transform(X)\n    array([[2, 0],\n           [1, 4],\n           [1, 1]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Learn empirical variances from X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_samples, n_features)",
                "description": "Data from which to compute variances, where `n_samples` is\n    the number of samples and `n_features` is the number of features."
              },
              "y": {
                "type": "any, default=None",
                "description": "Ignored. This parameter exists only for compatibility with\n    sklearn.pipeline.Pipeline."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Mask feature names according to selected features.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features.\n\n    - If `input_features` is `None`, then `feature_names_in_` is\n      used as feature names in. If `feature_names_in_` is not defined,\n      then the following input feature names are generated:\n      `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n    - If `input_features` is an array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_support",
          "signature": "get_support(self, indices=False)",
          "docstring": {
            "description": "Get a mask, or integer index, of the features selected.",
            "parameters": {
              "indices": {
                "type": "bool, default=False",
                "description": "If True, the return value will be an array of integers, rather\n    than a boolean mask."
              }
            },
            "returns": "support : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Reverse the transformation operation.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_selected_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Reduce X to the selected features.",
            "parameters": {
              "X": {
                "type": "array of shape [n_samples, n_features]",
                "description": "The input samples."
              }
            },
            "returns": "X_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ],
  "constants": []
}