{
  "description": "The k-nearest neighbors algorithms.",
  "functions": [
    {
      "name": "BallTree",
      "signature": "BallTree(...)",
      "docstring": {
        "description": "BallTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "array-like of shape (n_samples, n_features)",
            "description": "n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made."
          },
          "leaf_size": {
            "type": "positive int, default=40",
            "description": "Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``."
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for BallTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class."
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": "and Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": "The training data"
          },
          "valid_metrics": {
            "type": "list of str",
            "description": "List of valid distance metrics."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Query for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import BallTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      }
    },
    {
      "name": "KDTree",
      "signature": "KDTree(...)",
      "docstring": {
        "description": "KDTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "array-like of shape (n_samples, n_features)",
            "description": "n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made."
          },
          "leaf_size": {
            "type": "positive int, default=40",
            "description": "Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``."
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for KDTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class."
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": "and Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": "The training data"
          },
          "valid_metrics": {
            "type": "list of str",
            "description": "List of valid distance metrics."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Query for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import KDTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      }
    },
    {
      "name": "KNeighborsClassifier",
      "signature": "KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Classifier implementing the k-nearest neighbors vote.\n\nRead more in the :ref:`User Guide <classification>`.",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Refer to the example entitled\n    :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n    showing the impact of the `weights` parameter on the decision\n    boundary."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent\n    to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n    For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n    to be positive."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "effective_metric_": {
            "type": "str or callble",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          },
          "outputs_2d_": {
            "type": "bool",
            "description": "False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\nKNeighborsRegressor: Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\nNearestNeighbors: Unsupervised learner for implementing neighbor searches.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances\n   but different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> neigh = KNeighborsClassifier(n_neighbors=3)\n>>> neigh.fit(X, y)\nKNeighborsClassifier(...)\n>>> print(neigh.predict([[1.1]]))\n[0]\n>>> print(neigh.predict_proba([[0.9]]))\n[[0.666... 0.333...]]"
      }
    },
    {
      "name": "KNeighborsRegressor",
      "signature": "KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Regression based on k-nearest neighbors.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\n    See the following example for a demonstration of the impact of\n    different weighting schemes on predictions:\n    :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str, DistanceMetric object or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    If metric is a DistanceMetric object, it will be passed directly to\n    the underlying computation routines."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\nKNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\nRadiusNeighborsClassifier : Classifier implementing\n    a vote among neighbors within a given radius.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances but\n   different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> neigh = KNeighborsRegressor(n_neighbors=2)\n>>> neigh.fit(X, y)\nKNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
      }
    },
    {
      "name": "KNeighborsTransformer",
      "signature": "KNeighborsTransformer(*, mode='distance', n_neighbors=5, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Transform X into a (weighted) graph of k nearest neighbors.\n\nThe transformed data is a sparse graph as returned by kneighbors_graph.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors for each sample in the transformed sparse graph.\n    For compatibility reasons, as each sample is considered as its own\n    neighbor, one extra neighbor will be computed when mode == 'distance'.\n    In this case, the sparse graph contains (n_neighbors + 1) neighbors."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "kneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nRadiusNeighborsTransformer : Transform X into a weighted graph of\n    neighbors nearer than a radius.",
        "notes": "For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\nin combination with :class:`~sklearn.manifold.TSNE` see\n:ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.",
        "examples": ">>> from sklearn.datasets import load_wine\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> X, _ = load_wine(return_X_y=True)\n>>> X.shape\n(178, 13)\n>>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n>>> X_dist_graph = transformer.fit_transform(X)\n>>> X_dist_graph.shape\n(178, 178)"
      }
    },
    {
      "name": "KernelDensity",
      "signature": "KernelDensity(*, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)",
      "docstring": {
        "description": "Kernel Density Estimation.\n\nRead more in the :ref:`User Guide <kernel_density>`.",
        "parameters": {
          "bandwidth": {
            "type": "float or {\"scott\", \"silverman\"}, default=1.0",
            "description": "The bandwidth of the kernel. If bandwidth is a float, it defines the\n    bandwidth of the kernel. If bandwidth is a string, one of the estimation\n    methods is implemented."
          },
          "algorithm": {
            "type": "{'kd_tree', 'ball_tree', 'auto'}, default='auto'",
            "description": "The tree algorithm to use."
          },
          "kernel": {
            "type": "{'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'",
            "description": "The kernel to use."
          },
          "metric": {
            "type": "str, default='euclidean'",
            "description": "Metric to use for distance computation. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    Not all metrics are valid with all algorithms: refer to the\n    documentation of :class:`BallTree` and :class:`KDTree`. Note that the\n    normalization of the density output is correct only for the Euclidean\n    distance metric."
          },
          "atol": {
            "type": "float, default=0",
            "description": "The desired absolute tolerance of the result.  A larger tolerance will\n    generally lead to faster execution."
          },
          "rtol": {
            "type": "float, default=0",
            "description": "The desired relative tolerance of the result.  A larger tolerance will\n    generally lead to faster execution."
          },
          "breadth_first": {
            "type": "bool, default=True",
            "description": "If true (default), use a breadth-first approach to the problem.\n    Otherwise use a depth-first approach."
          },
          "leaf_size": {
            "type": "int, default=40",
            "description": "Specify the leaf size of the underlying tree.  See :class:`BallTree`\n    or :class:`KDTree` for details."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional parameters to be passed to the tree for use with the\n    metric.  For more information, see the documentation of\n    :class:`BallTree` or :class:`KDTree`.\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "tree_": {
            "type": "``BinaryTree`` instance",
            "description": "The tree algorithm for fast generalized N-point problems."
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings."
          },
          "bandwidth_": {
            "type": "float",
            "description": "Value of the bandwidth, given directly by the bandwidth parameter or\n    estimated using the 'scott' or 'silverman' method.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n    problems.\nsklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n    problems.",
        "notes": "",
        "examples": "Compute a gaussian kernel density estimate with a fixed bandwidth.\n\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> rng = np.random.RandomState(42)\n>>> X = rng.random_sample((100, 3))\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n>>> log_density = kde.score_samples(X[:3])\n>>> log_density\narray([-1.52955942, -1.51462041, -1.60244657])"
      }
    },
    {
      "name": "LocalOutlierFactor",
      "signature": "LocalOutlierFactor(n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)",
      "docstring": {
        "description": "Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n\nThe anomaly score of each sample is called the Local Outlier Factor.\nIt measures the local deviation of the density of a given sample with respect\nto its neighbors.\nIt is local in that the anomaly score depends on how isolated the object\nis with respect to the surrounding neighborhood.\nMore precisely, locality is given by k-nearest neighbors, whose distance\nis used to estimate the local density.\nBy comparing the local density of a sample to the local densities of its\nneighbors, one can identify samples that have a substantially lower density\nthan their neighbors. These are considered outliers.\n\n.. versionadded:: 0.19",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=20",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries.\n    If n_neighbors is larger than the number of samples provided,\n    all samples will be used."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    :func:`sklearn.metrics.pairwise_distances`. When p = 1, this\n    is equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "contamination": {
            "type": "'auto' or float, default='auto'",
            "description": "The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. When fitting this is used to define the\n    threshold on the scores of the samples.\n\n    - if 'auto', the threshold is determined as in the\n      original paper,\n    - if a float, the contamination should be in the range (0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``."
          },
          "novelty": {
            "type": "bool, default=False",
            "description": "By default, LocalOutlierFactor is only meant to be used for outlier\n    detection (novelty=False). Set novelty to True if you want to use\n    LocalOutlierFactor for novelty detection. In this case be aware that\n    you should only use predict, decision_function and score_samples\n    on new unseen data and not on the training set; and note that the\n    results obtained this way may differ from the standard LOF results.\n\n    .. versionadded:: 0.20"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "negative_outlier_factor_": {
            "type": "ndarray of shape (n_samples,)",
            "description": "The opposite LOF of the training samples. The higher, the more normal.\n    Inliers tend to have a LOF score close to 1\n    (``negative_outlier_factor_`` close to -1), while outliers tend to have\n    a larger LOF score.\n\n    The local outlier factor (LOF) of a sample captures its\n    supposed 'degree of abnormality'.\n    It is the average of the ratio of the local reachability density of\n    a sample and those of its k-nearest neighbors."
          },
          "n_neighbors_": {
            "type": "int",
            "description": "The actual number of neighbors used for :meth:`kneighbors` queries."
          },
          "offset_": {
            "type": "float",
            "description": "Offset used to obtain binary labels from the raw scores.\n    Observations having a negative_outlier_factor smaller than `offset_`\n    are detected as abnormal.\n    The offset is set to -1.5 (inliers score around -1), except when a\n    contamination parameter different than \"auto\" is provided. In that\n    case, the offset is defined in such a way we obtain the expected\n    number of outliers in training.\n\n    .. versionadded:: 0.20"
          },
          "effective_metric_": {
            "type": "str",
            "description": "The effective metric used for the distance computation."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "The effective additional keyword arguments for the metric function."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "It is the number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n    Support Vector Machine.\n\nReferences\n----------\n.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n       LOF: identifying density-based local outliers. In ACM sigmod record.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.neighbors import LocalOutlierFactor\n>>> X = [[-1.1], [0.2], [101.1], [0.3]]\n>>> clf = LocalOutlierFactor(n_neighbors=2)\n>>> clf.fit_predict(X)\narray([ 1,  1, -1,  1])\n>>> clf.negative_outlier_factor_\narray([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])"
      }
    },
    {
      "name": "NearestCentroid",
      "signature": "NearestCentroid(metric='euclidean', *, shrink_threshold=None, priors='uniform')",
      "docstring": {
        "description": "Nearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to\nthe class with the nearest centroid.\n\nRead more in the :ref:`User Guide <nearest_centroid_classifier>`.",
        "parameters": {
          "metric": {
            "type": "{\"euclidean\", \"manhattan\"}, default=\"euclidean\"",
            "description": "Metric to use for distance computation.\n\n    If `metric=\"euclidean\"`, the centroid for the samples corresponding to each\n    class is the arithmetic mean, which minimizes the sum of squared L1 distances.\n    If `metric=\"manhattan\"`, the centroid is the feature-wise median, which\n    minimizes the sum of L1 distances.\n\n    .. versionchanged:: 1.5\n        All metrics but `\"euclidean\"` and `\"manhattan\"` were deprecated and\n        now raise an error.\n\n    .. versionchanged:: 0.19\n        `metric='precomputed'` was deprecated and now raises an error"
          },
          "shrink_threshold": {
            "type": "float, default=None",
            "description": "Threshold for shrinking centroids to remove features."
          },
          "priors": {
            "type": "{\"uniform\", \"empirical\"} or array-like of shape (n_classes,),         default=\"uniform\"",
            "description": "The class prior probabilities. By default, the class proportions are\n    inferred from the training data.\n\n    .. versionadded:: 1.6\n\nAttributes\n----------"
          },
          "centroids_": {
            "type": "array-like of shape (n_classes, n_features)",
            "description": "Centroid of each class."
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": "The unique classes labels."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "deviations_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Deviations (or shrinkages) of the centroids of each class from the\n    overall centroid. Equal to eq. (18.4) if `shrink_threshold=None`,\n    else (18.5) p. 653 of [2]. Can be used to identify features used\n    for classification.\n\n    .. versionadded:: 1.6"
          },
          "within_class_std_dev_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Pooled or within-class standard deviation of input data.\n\n    .. versionadded:: 1.6"
          },
          "class_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The class prior probabilities.\n\n    .. versionadded:: 1.6"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Nearest neighbors classifier.",
        "notes": "When used for text classification with tf-idf vectors, this classifier is\nalso known as the Rocchio classifier.\n\nReferences\n----------\n[1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\nmultiple cancer types by shrunken centroids of gene expression. Proceedings\nof the National Academy of Sciences of the United States of America,\n99(10), 6567-6572. The National Academy of Sciences.\n\n[2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical\nLearning Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.",
        "examples": ">>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
      }
    },
    {
      "name": "NearestNeighbors",
      "signature": "NearestNeighbors(*, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Unsupervised learner for implementing neighbor searches.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "p": {
            "type": "float (positive), default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str",
            "description": "Metric used to compute distances to neighbors."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Parameters for the metric used to compute distances to neighbors."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n    within a given radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed\n    radius.\nBallTree : Space partitioning data structure for organizing points in a\n    multi-dimensional space, used for nearest neighbor search.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "examples": ">>> import numpy as np\n>>> from sklearn.neighbors import NearestNeighbors\n>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n>>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n>>> neigh.fit(samples)\nNearestNeighbors(...)\n>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\narray([[2, 0]]...)\n>>> nbrs = neigh.radius_neighbors(\n...    [[0, 0, 1.3]], 0.4, return_distance=False\n... )\n>>> np.asarray(nbrs[0][0])\narray(2)"
      }
    },
    {
      "name": "NeighborhoodComponentsAnalysis",
      "signature": "NeighborhoodComponentsAnalysis(n_components=None, *, init='auto', warm_start=False, max_iter=50, tol=1e-05, callback=None, verbose=0, random_state=None)",
      "docstring": {
        "description": "Neighborhood Components Analysis.\n\nNeighborhood Component Analysis (NCA) is a machine learning algorithm for\nmetric learning. It learns a linear transformation in a supervised fashion\nto improve the classification accuracy of a stochastic nearest neighbors\nrule in the transformed space.\n\nRead more in the :ref:`User Guide <nca>`.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": "Preferred dimensionality of the projected space.\n    If None it will be set to `n_features`."
          },
          "init": {
            "type": "{'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'",
            "description": "Initialization of the linear transformation. Possible options are\n    `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy\n    array of shape `(n_features_a, n_features_b)`.\n\n    - `'auto'`\n        Depending on `n_components`, the most reasonable initialization\n        is chosen. If `n_components <= min(n_features, n_classes - 1)`\n        we use `'lda'`, as it uses labels information. If not, but\n        `n_components < min(n_features, n_samples)`, we use `'pca'`, as\n        it projects data in meaningful directions (those of higher\n        variance). Otherwise, we just use `'identity'`.\n\n    - `'pca'`\n        `n_components` principal components of the inputs passed\n        to :meth:`fit` will be used to initialize the transformation.\n        (See :class:`~sklearn.decomposition.PCA`)\n\n    - `'lda'`\n        `min(n_components, n_classes)` most discriminative\n        components of the inputs passed to :meth:`fit` will be used to\n        initialize the transformation. (If `n_components > n_classes`,\n        the rest of the components will be zero.) (See\n        :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n\n    - `'identity'`\n        If `n_components` is strictly smaller than the\n        dimensionality of the inputs passed to :meth:`fit`, the identity\n        matrix will be truncated to the first `n_components` rows.\n\n    - `'random'`\n        The initial transformation will be a random array of shape\n        `(n_components, n_features)`. Each value is sampled from the\n        standard normal distribution.\n\n    - numpy array\n        `n_features_b` must match the dimensionality of the inputs passed\n        to :meth:`fit` and n_features_a must be less than or equal to that.\n        If `n_components` is not `None`, `n_features_a` must match it."
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": "If `True` and :meth:`fit` has been called before, the solution of the\n    previous call to :meth:`fit` is used as the initial linear\n    transformation (`n_components` and `init` will be ignored)."
          },
          "max_iter": {
            "type": "int, default=50",
            "description": "Maximum number of iterations in the optimization."
          },
          "tol": {
            "type": "float, default=1e-5",
            "description": "Convergence tolerance for the optimization."
          },
          "callback": {
            "type": "callable, default=None",
            "description": "If not `None`, this function is called after every iteration of the\n    optimizer, taking as arguments the current solution (flattened\n    transformation matrix) and the number of iterations. This might be\n    useful in case one wants to examine or store the transformation\n    found after each iteration."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "If 0, no progress messages will be printed.\n    If 1, progress messages will be printed to stdout.\n    If > 1, progress messages will be printed and the `disp`\n    parameter of :func:`scipy.optimize.minimize` will be set to\n    `verbose - 2`."
          },
          "random_state": {
            "type": "int or numpy.RandomState, default=None",
            "description": "A pseudo random number generator object or a seed for it if int. If\n    `init='random'`, `random_state` is used to initialize the random\n    transformation. If `init='pca'`, `random_state` is passed as an\n    argument to PCA when initializing the transformation. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": "The linear transformation learned during fitting."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "n_iter_": {
            "type": "int",
            "description": "Counts the number of iterations performed by the optimizer."
          },
          "random_state_": {
            "type": "numpy.RandomState",
            "description": "Pseudo random number generator object used during initialization."
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n    Discriminant Analysis.\nsklearn.decomposition.PCA : Principal component analysis (PCA).\n\nReferences\n----------\n.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n       \"Neighbourhood Components Analysis\". Advances in Neural Information\n       Processing Systems. 17, 513-520, 2005.\n       http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n.. [2] Wikipedia entry on Neighborhood Components Analysis\n       https://en.wikipedia.org/wiki/Neighbourhood_components_analysis",
        "notes": "",
        "examples": ">>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> nca.fit(X_train, y_train)\nNeighborhoodComponentsAnalysis(...)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> knn.fit(X_train, y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(X_test, y_test))\n0.933333...\n>>> knn.fit(nca.transform(X_train), y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(nca.transform(X_test), y_test))\n0.961904..."
      }
    },
    {
      "name": "RadiusNeighborsClassifier",
      "signature": "RadiusNeighborsClassifier(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Classifier implementing a vote among neighbors within a given radius.\n\nRead more in the :ref:`User Guide <classification>`.",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "outlier_label": {
            "type": "{manual label, 'most_frequent'}, default=None",
            "description": "Label for outlier samples (samples with no neighbors in given radius).\n\n    - manual label: str or int label (should be the same type as y)\n      or list of manual labels if multi-output is used.\n    - 'most_frequent' : assign the most frequent label of y to outliers.\n    - None : when any outlier is detected, ValueError will be raised.\n\n    The outlier label should be selected from among the unique 'Y' labels.\n    If it is specified with a different value a warning will be raised and\n    all class probabilities of outliers will be assigned to be 0."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier."
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          },
          "outlier_label_": {
            "type": "int or array-like of shape (n_class,)",
            "description": "Label which is given for outlier samples (samples with no neighbors\n    on given radius)."
          },
          "outputs_2d_": {
            "type": "bool",
            "description": "False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsRegressor : Regression based on neighbors within a\n    fixed radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nNearestNeighbors : Unsupervised learner for implementing neighbor\n    searches.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsClassifier\n>>> neigh = RadiusNeighborsClassifier(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsClassifier(...)\n>>> print(neigh.predict([[1.5]]))\n[0]\n>>> print(neigh.predict_proba([[1.0]]))\n[[0.66666667 0.33333333]]"
      }
    },
    {
      "name": "RadiusNeighborsRegressor",
      "signature": "RadiusNeighborsRegressor(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Regression based on neighbors within a fixed radius.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nKNeighborsClassifier : Classifier based on the k-nearest neighbors.\nRadiusNeighborsClassifier : Classifier based on neighbors within a given radius.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsRegressor\n>>> neigh = RadiusNeighborsRegressor(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
      }
    },
    {
      "name": "RadiusNeighborsTransformer",
      "signature": "RadiusNeighborsTransformer(*, mode='distance', radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)",
      "docstring": {
        "description": "Transform X into a (weighted) graph of neighbors nearer than a radius.\n\nThe transformed data is a sparse graph as returned by\n`radius_neighbors_graph`.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "radius": {
            "type": "float, default=1.0",
            "description": "Radius of neighborhood in the transformed sparse graph."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "kneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nKNeighborsTransformer : Transform X into a weighted graph of k\n    nearest neighbors.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.cluster import DBSCAN\n>>> from sklearn.neighbors import RadiusNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> X, _ = load_wine(return_X_y=True)\n>>> estimator = make_pipeline(\n...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n...     DBSCAN(eps=25.0, metric='precomputed'))\n>>> X_clustered = estimator.fit_predict(X)\n>>> clusters, counts = np.unique(X_clustered, return_counts=True)\n>>> print(counts)\n[ 29  15 111  11  12]"
      }
    },
    {
      "name": "kneighbors_graph",
      "signature": "kneighbors_graph(X, n_neighbors, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)",
      "docstring": {
        "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "Sample data."
          },
          "n_neighbors": {
            "type": "int",
            "description": "Number of neighbors for each sample."
          },
          "mode": {
            "type": "{'connectivity', 'distance'}, default='connectivity'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "metric": {
            "type": "str, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent\n    to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n    For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n    to be positive."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "include_self": {
            "type": "bool or 'auto', default=False",
            "description": "Whether or not to mark each sample as the first nearest neighbor to\n    itself. If 'auto', then True is used for mode='connectivity' and False\n    for mode='distance'."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details."
          }
        },
        "returns": "A : sparse matrix of shape (n_samples, n_samples)\n    Graph where A[i, j] is assigned the weight of edge that\n    connects i to j. The matrix is of CSR format.",
        "raises": "",
        "see_also": "radius_neighbors_graph: Compute the (weighted) graph of Neighbors for points in X.",
        "notes": "",
        "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import kneighbors_graph\n>>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
      }
    },
    {
      "name": "radius_neighbors_graph",
      "signature": "radius_neighbors_graph(X, radius, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)",
      "docstring": {
        "description": "Compute the (weighted) graph of Neighbors for points in X.\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "Sample data."
          },
          "radius": {
            "type": "float",
            "description": "Radius of neighborhoods."
          },
          "mode": {
            "type": "{'connectivity', 'distance'}, default='connectivity'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "metric": {
            "type": "str, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "include_self": {
            "type": "bool or 'auto', default=False",
            "description": "Whether or not to mark each sample as the first nearest neighbor to\n    itself. If 'auto', then True is used for mode='connectivity' and False\n    for mode='distance'."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details."
          }
        },
        "returns": "A : sparse matrix of shape (n_samples, n_samples)\n    Graph where A[i, j] is assigned the weight of edge that connects\n    i to j. The matrix is of CSR format.",
        "raises": "",
        "see_also": "kneighbors_graph: Compute the weighted graph of k-neighbors for points in X.",
        "notes": "",
        "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import radius_neighbors_graph\n>>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n...                            include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])"
      }
    },
    {
      "name": "sort_graph_by_row_values",
      "signature": "sort_graph_by_row_values(graph, copy=False, warn_when_not_sorted=True)",
      "docstring": {
        "description": "Sort a sparse graph such that each row is stored with increasing values.\n\n.. versionadded:: 1.2",
        "parameters": {
          "graph": {
            "type": "sparse matrix of shape (n_samples, n_samples)",
            "description": "Distance matrix to other samples, where only non-zero elements are\n    considered neighbors. Matrix is converted to CSR format if not already."
          },
          "copy": {
            "type": "bool, default=False",
            "description": "If True, the graph is copied before sorting. If False, the sorting is\n    performed inplace. If the graph is not of CSR format, `copy` must be\n    True to allow the conversion to CSR format, otherwise an error is\n    raised."
          },
          "warn_when_not_sorted": {
            "type": "bool, default=True",
            "description": "If True, a :class:`~sklearn.exceptions.EfficiencyWarning` is raised\n    when the input graph is not sorted by row values."
          }
        },
        "returns": "graph : sparse matrix of shape (n_samples, n_samples)\n    Distance matrix to other samples, where only non-zero elements are\n    considered neighbors. Matrix is in CSR format.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from scipy.sparse import csr_matrix\n>>> from sklearn.neighbors import sort_graph_by_row_values\n>>> X = csr_matrix(\n...     [[0., 3., 1.],\n...      [3., 0., 2.],\n...      [1., 2., 0.]])\n>>> X.data\narray([3., 1., 3., 2., 1., 2.])\n>>> X_ = sort_graph_by_row_values(X)\n>>> X_.data\narray([1., 3., 2., 3., 1., 2.])"
      }
    }
  ],
  "classes": [
    {
      "name": "BallTree",
      "docstring": {
        "description": "BallTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "array-like of shape (n_samples, n_features)",
            "description": "n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made."
          },
          "leaf_size": {
            "type": "positive int, default=40",
            "description": "Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``."
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for BallTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class."
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": "and Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": "The training data"
          },
          "valid_metrics": {
            "type": "list of str",
            "description": "List of valid distance metrics."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Query for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import BallTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      },
      "methods": [
        {
          "name": "get_arrays",
          "signature": "get_arrays(self)",
          "docstring": {
            "description": "get_arrays()\n\nGet data and node arrays.",
            "parameters": {},
            "returns": "arrays: tuple of array\n    Arrays for storing tree data, index, node data and node bounds.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_n_calls",
          "signature": "get_n_calls(self)",
          "docstring": {
            "description": "get_n_calls()\n\nGet number of calls.",
            "parameters": {},
            "returns": "n_calls: int\n    number of distance computation calls",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tree_stats",
          "signature": "get_tree_stats(self)",
          "docstring": {
            "description": "get_tree_stats()\n\nGet tree status.",
            "parameters": {},
            "returns": "tree_stats: tuple of int\n    (number of trims, number of leaves, number of splits)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kernel_density",
          "signature": "kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1e-08, breadth_first=True, return_log=False)",
          "docstring": {
            "description": "kernel_density(X, h, kernel='gaussian', atol=0, rtol=1E-8,\n               breadth_first=True, return_log=False)\n\nCompute the kernel density estimate at points X with the given kernel,\nusing the distance metric specified at tree creation.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query.  Last dimension should match dimension\n    of training data."
              },
              "h": {
                "type": "float",
                "description": "the bandwidth of the kernel"
              },
              "kernel": {
                "type": "str, default=\"gaussian\"",
                "description": "specify the kernel to use.  Options are\n    - 'gaussian'\n    - 'tophat'\n    - 'epanechnikov'\n    - 'exponential'\n    - 'linear'\n    - 'cosine'\n    Default is kernel = 'gaussian'"
              },
              "atol": {
                "type": "float, default=0",
                "description": "Specify the desired absolute tolerance of the result.\n    If the true result is `K_true`, then the returned result `K_ret`\n    satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n    The default is zero (i.e. machine precision)."
              },
              "rtol": {
                "type": "float, default=1e-8",
                "description": "Specify the desired relative tolerance of the result.\n    If the true result is `K_true`, then the returned result `K_ret`\n    satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n    The default is `1e-8` (i.e. machine precision)."
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": "If True, use a breadth-first search.  If False (default) use a\n    depth-first search.  Breadth-first is generally faster for\n    compact kernels and/or high tolerances."
              },
              "return_log": {
                "type": "bool, default=False",
                "description": "Return the logarithm of the result.  This can be more accurate\n    than returning the result itself for narrow kernels."
              }
            },
            "returns": "density : ndarray of shape X.shape[:-1]\n    The array of (log)-density evaluations",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query",
          "signature": "query(self, X, k=1, return_distance=True, dualtree=False, breadth_first=False, sort_results=True)",
          "docstring": {
            "description": "query(X, k=1, return_distance=True,\n      dualtree=False, breadth_first=False)\n\nquery the tree for the k nearest neighbors",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query"
              },
              "k": {
                "type": "int, default=1",
                "description": "The number of nearest neighbors to return"
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "if True, return a tuple (d, i) of distances and indices\n    if False, return array i"
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": "if True, use the dual tree formalism for the query: a tree is\n    built for the query points, and the pair of trees is used to\n    efficiently search this space.  This can lead to better\n    performance as the number of points grows large."
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": "if True, then query the nodes in a breadth-first manner.\n    Otherwise, query the nodes in a depth-first manner."
              },
              "sort_results": {
                "type": "bool, default=True",
                "description": "if True, then distances and indices of each point are sorted\n    on return, so that the first column contains the closest points.\n    Otherwise, neighbors are returned in an arbitrary order."
              }
            },
            "returns": "i    : if return_distance == False\n(d,i) : if return_distance == True\n\nd : ndarray of shape X.shape[:-1] + (k,), dtype=double\n    Each entry gives the list of distances to the neighbors of the\n    corresponding point.\n\ni : ndarray of shape X.shape[:-1] + (k,), dtype=int\n    Each entry gives the list of indices of neighbors of the\n    corresponding point.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query_radius",
          "signature": "query_radius(self, X, r, return_distance=False, count_only=False, sort_results=False)",
          "docstring": {
            "description": "query_radius(X, r, return_distance=False,\ncount_only=False, sort_results=False)\n\nquery the tree for neighbors within a radius r",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query"
              },
              "r": {
                "type": "distance within which neighbors are returned",
                "description": "r can be a single value, or an array of values of shape\n    x.shape[:-1] if different radii are desired for each point."
              },
              "return_distance": {
                "type": "bool, default=False",
                "description": "if True,  return distances to neighbors of each point\n    if False, return only neighbors\n    Note that unlike the query() method, setting return_distance=True\n    here adds to the computation time.  Not all distances need to be\n    calculated explicitly for return_distance=False.  Results are\n    not sorted by default: see ``sort_results`` keyword."
              },
              "count_only": {
                "type": "bool, default=False",
                "description": "if True,  return only the count of points within distance r\n    if False, return the indices of all points within distance r\n    If return_distance==True, setting count_only=True will\n    result in an error."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "if True, the distances and indices will be sorted before being\n    returned.  If False, the results will not be sorted.  If\n    return_distance == False, setting sort_results = True will\n    result in an error."
              }
            },
            "returns": "count       : if count_only == True\nind         : if count_only == False and return_distance == False\n(ind, dist) : if count_only == False and return_distance == True\n\ncount : ndarray of shape X.shape[:-1], dtype=int\n    Each entry gives the number of neighbors within a distance r of the\n    corresponding point.\n\nind : ndarray of shape X.shape[:-1], dtype=object\n    Each element is a numpy integer array listing the indices of\n    neighbors of the corresponding point.  Note that unlike\n    the results of a k-neighbors query, the returned neighbors\n    are not sorted by distance by default.\n\ndist : ndarray of shape X.shape[:-1], dtype=object\n    Each element is a numpy double array listing the distances\n    corresponding to indices in i.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_n_calls",
          "signature": "reset_n_calls(self)",
          "docstring": {
            "description": "reset_n_calls()\n\nReset number of calls to 0.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "two_point_correlation",
          "signature": "two_point_correlation(self, X, r, dualtree=False)",
          "docstring": {
            "description": "two_point_correlation(X, r, dualtree=False)\n\nCompute the two-point correlation function",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query.  Last dimension should match dimension\n    of training data."
              },
              "r": {
                "type": "array-like",
                "description": "A one-dimensional array of distances"
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": "If True, use a dualtree algorithm.  Otherwise, use a single-tree\n    algorithm.  Dual tree algorithms can have better scaling for\n    large N."
              }
            },
            "returns": "counts : ndarray\n    counts[i] contains the number of pairs of points with distance\n    less than or equal to r[i]",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KDTree",
      "docstring": {
        "description": "KDTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "parameters": {
          "X": {
            "type": "array-like of shape (n_samples, n_features)",
            "description": "n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made."
          },
          "leaf_size": {
            "type": "positive int, default=40",
            "description": "Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``."
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for KDTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class."
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": "and Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": "The training data"
          },
          "valid_metrics": {
            "type": "list of str",
            "description": "List of valid distance metrics."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Query for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import KDTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      },
      "methods": [
        {
          "name": "get_arrays",
          "signature": "get_arrays(self)",
          "docstring": {
            "description": "get_arrays()\n\nGet data and node arrays.",
            "parameters": {},
            "returns": "arrays: tuple of array\n    Arrays for storing tree data, index, node data and node bounds.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_n_calls",
          "signature": "get_n_calls(self)",
          "docstring": {
            "description": "get_n_calls()\n\nGet number of calls.",
            "parameters": {},
            "returns": "n_calls: int\n    number of distance computation calls",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tree_stats",
          "signature": "get_tree_stats(self)",
          "docstring": {
            "description": "get_tree_stats()\n\nGet tree status.",
            "parameters": {},
            "returns": "tree_stats: tuple of int\n    (number of trims, number of leaves, number of splits)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kernel_density",
          "signature": "kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1e-08, breadth_first=True, return_log=False)",
          "docstring": {
            "description": "kernel_density(X, h, kernel='gaussian', atol=0, rtol=1E-8,\n               breadth_first=True, return_log=False)\n\nCompute the kernel density estimate at points X with the given kernel,\nusing the distance metric specified at tree creation.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query.  Last dimension should match dimension\n    of training data."
              },
              "h": {
                "type": "float",
                "description": "the bandwidth of the kernel"
              },
              "kernel": {
                "type": "str, default=\"gaussian\"",
                "description": "specify the kernel to use.  Options are\n    - 'gaussian'\n    - 'tophat'\n    - 'epanechnikov'\n    - 'exponential'\n    - 'linear'\n    - 'cosine'\n    Default is kernel = 'gaussian'"
              },
              "atol": {
                "type": "float, default=0",
                "description": "Specify the desired absolute tolerance of the result.\n    If the true result is `K_true`, then the returned result `K_ret`\n    satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n    The default is zero (i.e. machine precision)."
              },
              "rtol": {
                "type": "float, default=1e-8",
                "description": "Specify the desired relative tolerance of the result.\n    If the true result is `K_true`, then the returned result `K_ret`\n    satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n    The default is `1e-8` (i.e. machine precision)."
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": "If True, use a breadth-first search.  If False (default) use a\n    depth-first search.  Breadth-first is generally faster for\n    compact kernels and/or high tolerances."
              },
              "return_log": {
                "type": "bool, default=False",
                "description": "Return the logarithm of the result.  This can be more accurate\n    than returning the result itself for narrow kernels."
              }
            },
            "returns": "density : ndarray of shape X.shape[:-1]\n    The array of (log)-density evaluations",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query",
          "signature": "query(self, X, k=1, return_distance=True, dualtree=False, breadth_first=False, sort_results=True)",
          "docstring": {
            "description": "query(X, k=1, return_distance=True,\n      dualtree=False, breadth_first=False)\n\nquery the tree for the k nearest neighbors",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query"
              },
              "k": {
                "type": "int, default=1",
                "description": "The number of nearest neighbors to return"
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "if True, return a tuple (d, i) of distances and indices\n    if False, return array i"
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": "if True, use the dual tree formalism for the query: a tree is\n    built for the query points, and the pair of trees is used to\n    efficiently search this space.  This can lead to better\n    performance as the number of points grows large."
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": "if True, then query the nodes in a breadth-first manner.\n    Otherwise, query the nodes in a depth-first manner."
              },
              "sort_results": {
                "type": "bool, default=True",
                "description": "if True, then distances and indices of each point are sorted\n    on return, so that the first column contains the closest points.\n    Otherwise, neighbors are returned in an arbitrary order."
              }
            },
            "returns": "i    : if return_distance == False\n(d,i) : if return_distance == True\n\nd : ndarray of shape X.shape[:-1] + (k,), dtype=double\n    Each entry gives the list of distances to the neighbors of the\n    corresponding point.\n\ni : ndarray of shape X.shape[:-1] + (k,), dtype=int\n    Each entry gives the list of indices of neighbors of the\n    corresponding point.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query_radius",
          "signature": "query_radius(self, X, r, return_distance=False, count_only=False, sort_results=False)",
          "docstring": {
            "description": "query_radius(X, r, return_distance=False,\ncount_only=False, sort_results=False)\n\nquery the tree for neighbors within a radius r",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query"
              },
              "r": {
                "type": "distance within which neighbors are returned",
                "description": "r can be a single value, or an array of values of shape\n    x.shape[:-1] if different radii are desired for each point."
              },
              "return_distance": {
                "type": "bool, default=False",
                "description": "if True,  return distances to neighbors of each point\n    if False, return only neighbors\n    Note that unlike the query() method, setting return_distance=True\n    here adds to the computation time.  Not all distances need to be\n    calculated explicitly for return_distance=False.  Results are\n    not sorted by default: see ``sort_results`` keyword."
              },
              "count_only": {
                "type": "bool, default=False",
                "description": "if True,  return only the count of points within distance r\n    if False, return the indices of all points within distance r\n    If return_distance==True, setting count_only=True will\n    result in an error."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "if True, the distances and indices will be sorted before being\n    returned.  If False, the results will not be sorted.  If\n    return_distance == False, setting sort_results = True will\n    result in an error."
              }
            },
            "returns": "count       : if count_only == True\nind         : if count_only == False and return_distance == False\n(ind, dist) : if count_only == False and return_distance == True\n\ncount : ndarray of shape X.shape[:-1], dtype=int\n    Each entry gives the number of neighbors within a distance r of the\n    corresponding point.\n\nind : ndarray of shape X.shape[:-1], dtype=object\n    Each element is a numpy integer array listing the indices of\n    neighbors of the corresponding point.  Note that unlike\n    the results of a k-neighbors query, the returned neighbors\n    are not sorted by distance by default.\n\ndist : ndarray of shape X.shape[:-1], dtype=object\n    Each element is a numpy double array listing the distances\n    corresponding to indices in i.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_n_calls",
          "signature": "reset_n_calls(self)",
          "docstring": {
            "description": "reset_n_calls()\n\nReset number of calls to 0.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "two_point_correlation",
          "signature": "two_point_correlation(self, X, r, dualtree=False)",
          "docstring": {
            "description": "two_point_correlation(X, r, dualtree=False)\n\nCompute the two-point correlation function",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query.  Last dimension should match dimension\n    of training data."
              },
              "r": {
                "type": "array-like",
                "description": "A one-dimensional array of distances"
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": "If True, use a dualtree algorithm.  Otherwise, use a single-tree\n    algorithm.  Dual tree algorithms can have better scaling for\n    large N."
              }
            },
            "returns": "counts : ndarray\n    counts[i] contains the number of pairs of points with distance\n    less than or equal to r[i]",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsClassifier",
      "docstring": {
        "description": "Classifier implementing the k-nearest neighbors vote.\n\nRead more in the :ref:`User Guide <classification>`.",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Refer to the example entitled\n    :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n    showing the impact of the `weights` parameter on the decision\n    boundary."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent\n    to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n    For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n    to be positive."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "effective_metric_": {
            "type": "str or callble",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          },
          "outputs_2d_": {
            "type": "bool",
            "description": "False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\nKNeighborsRegressor: Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\nNearestNeighbors: Unsupervised learner for implementing neighbor searches.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances\n   but different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> neigh = KNeighborsClassifier(n_neighbors=3)\n>>> neigh.fit(X, y)\nKNeighborsClassifier(...)\n>>> print(neigh.predict([[1.1]]))\n[0]\n>>> print(neigh.predict_proba([[0.9]]))\n[[0.666... 0.333...]]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the k-nearest neighbors classifier from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)",
                "description": "Target values."
              }
            },
            "returns": "self : KNeighborsClassifier\n    The fitted k-nearest neighbors classifier.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "docstring": {
            "description": "Find the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors required for each sample. The default is the\n    value passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True.\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "In the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "docstring": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors for each sample. The default is the value\n    passed to the constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n    of Neighbors for points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Predict the class labels for the provided data.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n    Class labels for each data sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test data X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "p : ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.\n    The class probabilities of the input samples. Classes are ordered\n    by lexicographic order.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features), or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    used; in this case, points are not considered their own\n    neighbors. This means that `knn.fit(X, y).score(None, y)`\n    implicitly performs a leave-one-out cross-validation procedure\n    and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n    but typically much faster."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._classification.KNeighborsClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._classification.KNeighborsClassifier",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsRegressor",
      "docstring": {
        "description": "Regression based on k-nearest neighbors.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\n    See the following example for a demonstration of the impact of\n    different weighting schemes on predictions:\n    :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str, DistanceMetric object or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    If metric is a DistanceMetric object, it will be passed directly to\n    the underlying computation routines."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\nKNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\nRadiusNeighborsClassifier : Classifier implementing\n    a vote among neighbors within a given radius.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances but\n   different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> neigh = KNeighborsRegressor(n_neighbors=2)\n>>> neigh.fit(X, y)\nKNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the k-nearest neighbors regressor from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)",
                "description": "Target values."
              }
            },
            "returns": "self : KNeighborsRegressor\n    The fitted k-nearest neighbors regressor.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "docstring": {
            "description": "Find the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors required for each sample. The default is the\n    value passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True.\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "In the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "docstring": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors for each sample. The default is the value\n    passed to the constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n    of Neighbors for points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Predict the target for the provided data.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n    Target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the coefficient of determination of the prediction.\n\nThe coefficient of determination :math:`R^2` is defined as\n:math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\nsum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\nis the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\nThe best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\nthe expected value of `y`, disregarding the input features, would get\na :math:`R^2` score of 0.0.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True values for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "The :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._regression.KNeighborsRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._regression.KNeighborsRegressor",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsTransformer",
      "docstring": {
        "description": "Transform X into a (weighted) graph of k nearest neighbors.\n\nThe transformed data is a sparse graph as returned by kneighbors_graph.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors for each sample in the transformed sparse graph.\n    For compatibility reasons, as each sample is considered as its own\n    neighbor, one extra neighbor will be computed when mode == 'distance'.\n    In this case, the sparse graph contains (n_neighbors + 1) neighbors."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "kneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nRadiusNeighborsTransformer : Transform X into a weighted graph of\n    neighbors nearer than a radius.",
        "notes": "For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\nin combination with :class:`~sklearn.manifold.TSNE` see\n:ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.",
        "examples": ">>> from sklearn.datasets import load_wine\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> X, _ = load_wine(return_X_y=True)\n>>> X.shape\n(178, 13)\n>>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n>>> X_dist_graph = transformer.fit_transform(X)\n>>> X_dist_graph.shape\n(178, 178)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Fit the k-nearest neighbors transformer from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "self : KNeighborsTransformer\n    The fitted k-nearest neighbors transformer.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Training set."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "Xt : sparse matrix of shape (n_samples, n_samples)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.\n\nThe feature names out will prefixed by the lowercased class name. For\nexample, if the transformer outputs 3 features, then the feature names\nout are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Only used to validate feature names with the names seen in `fit`."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "docstring": {
            "description": "Find the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors required for each sample. The default is the\n    value passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True.\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "In the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "docstring": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors for each sample. The default is the value\n    passed to the constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n    of Neighbors for points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples_transform, n_features)",
                "description": "Sample data."
              }
            },
            "returns": "Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KernelDensity",
      "docstring": {
        "description": "Kernel Density Estimation.\n\nRead more in the :ref:`User Guide <kernel_density>`.",
        "parameters": {
          "bandwidth": {
            "type": "float or {\"scott\", \"silverman\"}, default=1.0",
            "description": "The bandwidth of the kernel. If bandwidth is a float, it defines the\n    bandwidth of the kernel. If bandwidth is a string, one of the estimation\n    methods is implemented."
          },
          "algorithm": {
            "type": "{'kd_tree', 'ball_tree', 'auto'}, default='auto'",
            "description": "The tree algorithm to use."
          },
          "kernel": {
            "type": "{'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'",
            "description": "The kernel to use."
          },
          "metric": {
            "type": "str, default='euclidean'",
            "description": "Metric to use for distance computation. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    Not all metrics are valid with all algorithms: refer to the\n    documentation of :class:`BallTree` and :class:`KDTree`. Note that the\n    normalization of the density output is correct only for the Euclidean\n    distance metric."
          },
          "atol": {
            "type": "float, default=0",
            "description": "The desired absolute tolerance of the result.  A larger tolerance will\n    generally lead to faster execution."
          },
          "rtol": {
            "type": "float, default=0",
            "description": "The desired relative tolerance of the result.  A larger tolerance will\n    generally lead to faster execution."
          },
          "breadth_first": {
            "type": "bool, default=True",
            "description": "If true (default), use a breadth-first approach to the problem.\n    Otherwise use a depth-first approach."
          },
          "leaf_size": {
            "type": "int, default=40",
            "description": "Specify the leaf size of the underlying tree.  See :class:`BallTree`\n    or :class:`KDTree` for details."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional parameters to be passed to the tree for use with the\n    metric.  For more information, see the documentation of\n    :class:`BallTree` or :class:`KDTree`.\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "tree_": {
            "type": "``BinaryTree`` instance",
            "description": "The tree algorithm for fast generalized N-point problems."
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings."
          },
          "bandwidth_": {
            "type": "float",
            "description": "Value of the bandwidth, given directly by the bandwidth parameter or\n    estimated using the 'scott' or 'silverman' method.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n    problems.\nsklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n    problems.",
        "notes": "",
        "examples": "Compute a gaussian kernel density estimate with a fixed bandwidth.\n\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> rng = np.random.RandomState(42)\n>>> X = rng.random_sample((100, 3))\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n>>> log_density = kde.score_samples(X[:3])\n>>> log_density\narray([-1.52955942, -1.51462041, -1.60244657])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "docstring": {
            "description": "Fit the Kernel Density model on the data.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "List of n_features-dimensional data points.  Each row\n    corresponds to a single data point."
              },
              "y": {
                "type": "None",
                "description": "Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "List of sample weights attached to the data X.\n\n    .. versionadded:: 0.20"
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "sample",
          "signature": "sample(self, n_samples=1, random_state=None)",
          "docstring": {
            "description": "Generate random samples from the model.\n\nCurrently, this is implemented only for gaussian and tophat kernels.",
            "parameters": {
              "n_samples": {
                "type": "int, default=1",
                "description": "Number of samples to generate."
              },
              "random_state": {
                "type": "int, RandomState instance or None, default=None",
                "description": "Determines random number generation used to generate\n    random samples. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`."
              }
            },
            "returns": "X : array-like of shape (n_samples, n_features)\n    List of samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None)",
          "docstring": {
            "description": "Compute the total log-likelihood under the model.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "List of n_features-dimensional data points.  Each row\n    corresponds to a single data point."
              },
              "y": {
                "type": "None",
                "description": "Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`."
              }
            },
            "returns": "logprob : float\n    Total log-likelihood of the data in X. This is normalized to be a\n    probability density, so the value will be low for high-dimensional\n    data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "docstring": {
            "description": "Compute the log-likelihood of each sample under the model.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "An array of points to query.  Last dimension should match dimension\n    of training data (n_features)."
              }
            },
            "returns": "density : ndarray of shape (n_samples,)\n    Log-likelihood of each sample in `X`. These are normalized to be\n    probability densities, so values will be low for high-dimensional\n    data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.neighbors._kde.KernelDensity, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._kde.KernelDensity",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LocalOutlierFactor",
      "docstring": {
        "description": "Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n\nThe anomaly score of each sample is called the Local Outlier Factor.\nIt measures the local deviation of the density of a given sample with respect\nto its neighbors.\nIt is local in that the anomaly score depends on how isolated the object\nis with respect to the surrounding neighborhood.\nMore precisely, locality is given by k-nearest neighbors, whose distance\nis used to estimate the local density.\nBy comparing the local density of a sample to the local densities of its\nneighbors, one can identify samples that have a substantially lower density\nthan their neighbors. These are considered outliers.\n\n.. versionadded:: 0.19",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=20",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries.\n    If n_neighbors is larger than the number of samples provided,\n    all samples will be used."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    :func:`sklearn.metrics.pairwise_distances`. When p = 1, this\n    is equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "contamination": {
            "type": "'auto' or float, default='auto'",
            "description": "The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. When fitting this is used to define the\n    threshold on the scores of the samples.\n\n    - if 'auto', the threshold is determined as in the\n      original paper,\n    - if a float, the contamination should be in the range (0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``."
          },
          "novelty": {
            "type": "bool, default=False",
            "description": "By default, LocalOutlierFactor is only meant to be used for outlier\n    detection (novelty=False). Set novelty to True if you want to use\n    LocalOutlierFactor for novelty detection. In this case be aware that\n    you should only use predict, decision_function and score_samples\n    on new unseen data and not on the training set; and note that the\n    results obtained this way may differ from the standard LOF results.\n\n    .. versionadded:: 0.20"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "negative_outlier_factor_": {
            "type": "ndarray of shape (n_samples,)",
            "description": "The opposite LOF of the training samples. The higher, the more normal.\n    Inliers tend to have a LOF score close to 1\n    (``negative_outlier_factor_`` close to -1), while outliers tend to have\n    a larger LOF score.\n\n    The local outlier factor (LOF) of a sample captures its\n    supposed 'degree of abnormality'.\n    It is the average of the ratio of the local reachability density of\n    a sample and those of its k-nearest neighbors."
          },
          "n_neighbors_": {
            "type": "int",
            "description": "The actual number of neighbors used for :meth:`kneighbors` queries."
          },
          "offset_": {
            "type": "float",
            "description": "Offset used to obtain binary labels from the raw scores.\n    Observations having a negative_outlier_factor smaller than `offset_`\n    are detected as abnormal.\n    The offset is set to -1.5 (inliers score around -1), except when a\n    contamination parameter different than \"auto\" is provided. In that\n    case, the offset is defined in such a way we obtain the expected\n    number of outliers in training.\n\n    .. versionadded:: 0.20"
          },
          "effective_metric_": {
            "type": "str",
            "description": "The effective metric used for the distance computation."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "The effective additional keyword arguments for the metric function."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "It is the number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n    Support Vector Machine.\n\nReferences\n----------\n.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n       LOF: identifying density-based local outliers. In ACM sigmod record.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.neighbors import LocalOutlierFactor\n>>> X = [[-1.1], [0.2], [101.1], [0.3]]\n>>> clf = LocalOutlierFactor(n_neighbors=2)\n>>> clf.fit_predict(X)\narray([ 1,  1, -1,  1])\n>>> clf.negative_outlier_factor_\narray([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "docstring": {
            "description": "Shifted opposite of the Local Outlier Factor of X.\n\nBigger is better, i.e. large values correspond to inliers.\n\n**Only available for novelty detection (when novelty is set to True).**\nThe shift offset allows a zero threshold for being an outlier.\nThe argument X is supposed to contain *new data*: if X contains a\npoint from training, it considers the later in its own neighborhood.\nAlso, the samples in X are not considered in the neighborhood of any\npoint.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples."
              }
            },
            "returns": "shifted_opposite_lof_scores : ndarray of shape (n_samples,)\n    The shifted opposite of the Local Outlier Factor of each input\n    samples. The lower, the more abnormal. Negative scores represent\n    outliers, positive scores represent inliers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Fit the local outlier factor detector from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "self : LocalOutlierFactor\n    The fitted local outlier factor detector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "docstring": {
            "description": "Fit the model to the training set X and return the labels.\n\n**Not available for novelty detection (when novelty is set to True).**\nLabel is 1 for an inlier and -1 for an outlier according to the LOF\nscore and the contamination parameter.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features), default=None",
                "description": "The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "is_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and 1 for inliers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "docstring": {
            "description": "Find the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors required for each sample. The default is the\n    value passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True.\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "In the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "docstring": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors for each sample. The default is the value\n    passed to the constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n    of Neighbors for points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X=None)",
          "docstring": {
            "description": "Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n\n**Only available for novelty detection (when novelty is set to True).**\nThis method allows to generalize prediction to *new observations* (not\nin the training set). Note that the result of ``clf.fit(X)`` then\n``clf.predict(X)`` with ``novelty=True`` may differ from the result\nobtained by ``clf.fit_predict(X)`` with ``novelty=False``.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples."
              }
            },
            "returns": "is_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and +1 for inliers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "docstring": {
            "description": "Opposite of the Local Outlier Factor of X.\n\nIt is the opposite as bigger is better, i.e. large values correspond\nto inliers.\n\n**Only available for novelty detection (when novelty is set to True).**\nThe argument X is supposed to contain *new data*: if X contains a\npoint from training, it considers the later in its own neighborhood.\nAlso, the samples in X are not considered in the neighborhood of any\npoint. Because of this, the scores obtained via ``score_samples`` may\ndiffer from the standard LOF scores.\nThe standard LOF scores for the training data is available via the\n``negative_outlier_factor_`` attribute.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples."
              }
            },
            "returns": "opposite_lof_scores : ndarray of shape (n_samples,)\n    The opposite of the Local Outlier Factor of each input samples.\n    The lower, the more abnormal.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NearestCentroid",
      "docstring": {
        "description": "Nearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to\nthe class with the nearest centroid.\n\nRead more in the :ref:`User Guide <nearest_centroid_classifier>`.",
        "parameters": {
          "metric": {
            "type": "{\"euclidean\", \"manhattan\"}, default=\"euclidean\"",
            "description": "Metric to use for distance computation.\n\n    If `metric=\"euclidean\"`, the centroid for the samples corresponding to each\n    class is the arithmetic mean, which minimizes the sum of squared L1 distances.\n    If `metric=\"manhattan\"`, the centroid is the feature-wise median, which\n    minimizes the sum of L1 distances.\n\n    .. versionchanged:: 1.5\n        All metrics but `\"euclidean\"` and `\"manhattan\"` were deprecated and\n        now raise an error.\n\n    .. versionchanged:: 0.19\n        `metric='precomputed'` was deprecated and now raises an error"
          },
          "shrink_threshold": {
            "type": "float, default=None",
            "description": "Threshold for shrinking centroids to remove features."
          },
          "priors": {
            "type": "{\"uniform\", \"empirical\"} or array-like of shape (n_classes,),         default=\"uniform\"",
            "description": "The class prior probabilities. By default, the class proportions are\n    inferred from the training data.\n\n    .. versionadded:: 1.6\n\nAttributes\n----------"
          },
          "centroids_": {
            "type": "array-like of shape (n_classes, n_features)",
            "description": "Centroid of each class."
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": "The unique classes labels."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "deviations_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Deviations (or shrinkages) of the centroids of each class from the\n    overall centroid. Equal to eq. (18.4) if `shrink_threshold=None`,\n    else (18.5) p. 653 of [2]. Can be used to identify features used\n    for classification.\n\n    .. versionadded:: 1.6"
          },
          "within_class_std_dev_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Pooled or within-class standard deviation of input data.\n\n    .. versionadded:: 1.6"
          },
          "class_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The class prior probabilities.\n\n    .. versionadded:: 1.6"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Nearest neighbors classifier.",
        "notes": "When used for text classification with tf-idf vectors, this classifier is\nalso known as the Rocchio classifier.\n\nReferences\n----------\n[1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\nmultiple cancer types by shrunken centroids of gene expression. Proceedings\nof the National Academy of Sciences of the United States of America,\n99(10), 6567-6572. The National Academy of Sciences.\n\n[2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical\nLearning Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.",
        "examples": ">>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "docstring": {
            "description": "Apply decision function to an array of samples.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Array of samples (test vectors)."
              }
            },
            "returns": "y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    Decision function values related to each class, per sample.\n    In the two-class case, the shape is `(n_samples,)`, giving the\n    log likelihood ratio of the positive class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the NearestCentroid model according to the given training data.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vector, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n    Note that centroid shrinking cannot be used with sparse matrices."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors `X`.\n\nThe predicted class `C` for each sample in `X` is returned.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Input data."
              }
            },
            "returns": "y_pred : ndarray of shape (n_samples,)\n    The predicted classes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Estimate log class probabilities.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Input data."
              }
            },
            "returns": "y_log_proba : ndarray of shape (n_samples, n_classes)\n    Estimated log probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Estimate class probabilities.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Input data."
              }
            },
            "returns": "y_proba : ndarray of shape (n_samples, n_classes)\n    Probability estimate of the sample for each class in the\n    model, where classes are ordered as they are in `self.classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._nearest_centroid.NearestCentroid, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._nearest_centroid.NearestCentroid",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NearestNeighbors",
      "docstring": {
        "description": "Unsupervised learner for implementing neighbor searches.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": "Number of neighbors to use by default for :meth:`kneighbors` queries."
          },
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "p": {
            "type": "float (positive), default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str",
            "description": "Metric used to compute distances to neighbors."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Parameters for the metric used to compute distances to neighbors."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n    within a given radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed\n    radius.\nBallTree : Space partitioning data structure for organizing points in a\n    multi-dimensional space, used for nearest neighbor search.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "examples": ">>> import numpy as np\n>>> from sklearn.neighbors import NearestNeighbors\n>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n>>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n>>> neigh.fit(samples)\nNearestNeighbors(...)\n>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\narray([[2, 0]]...)\n>>> nbrs = neigh.radius_neighbors(\n...    [[0, 0, 1.3]], 0.4, return_distance=False\n... )\n>>> np.asarray(nbrs[0][0])\narray(2)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Fit the nearest neighbors estimator from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "self : NearestNeighbors\n    The fitted nearest neighbors estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "docstring": {
            "description": "Find the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors required for each sample. The default is the\n    value passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True.\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "In the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "docstring": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": "Number of neighbors for each sample. The default is the value\n    passed to the constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n    of Neighbors for points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "docstring": {
            "description": "Find the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Limiting distance of neighbors to return. The default is the value\n    passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, the distances and indices will be sorted by increasing\n    distances before being returned. If False, the results may not\n    be sorted. If `return_distance=False`, setting `sort_results=True`\n    will result in an error.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_samples,) of arrays\n    Array representing the distances to each point, only present if\n    `return_distance=True`. The distance values are computed according\n    to the ``metric`` constructor parameter.\n\nneigh_ind : ndarray of shape (n_samples,) of arrays\n    An array of arrays of indices of the approximate nearest points\n    from the population matrix that lie within a ball of size\n    ``radius`` around the query points.",
            "raises": "",
            "see_also": "",
            "notes": "Because the number of neighbors of each point is not necessarily\nequal, the results for multiple query points cannot be fit in a\nstandard data array.\nFor efficiency, `radius_neighbors` returns arrays of objects, where\neach object is a 1D array of indices or distances.",
            "examples": "In the following example, we construct a NeighborsClassifier\nclass from an array representing our data set and ask who's\nthe closest point to [1, 1, 1]:\n\n>>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)\nNearestNeighbors(radius=1.6)\n>>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]\n\nThe first array returned contains the distances to all points which\nare closer than 1.6, while the second array returned contains their\nindices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\nNeighborhoods are restricted the points at a distance lower than\nradius.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Radius of neighborhoods. The default is the value passed to the\n    constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, in each row of the result, the non-zero entries will be\n    sorted by increasing distances. If False, the non-zero entries may\n    not be sorted. Only used with mode='distance'.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n    points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)\nNearestNeighbors(radius=1.5)\n>>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NeighborhoodComponentsAnalysis",
      "docstring": {
        "description": "Neighborhood Components Analysis.\n\nNeighborhood Component Analysis (NCA) is a machine learning algorithm for\nmetric learning. It learns a linear transformation in a supervised fashion\nto improve the classification accuracy of a stochastic nearest neighbors\nrule in the transformed space.\n\nRead more in the :ref:`User Guide <nca>`.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": "Preferred dimensionality of the projected space.\n    If None it will be set to `n_features`."
          },
          "init": {
            "type": "{'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'",
            "description": "Initialization of the linear transformation. Possible options are\n    `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy\n    array of shape `(n_features_a, n_features_b)`.\n\n    - `'auto'`\n        Depending on `n_components`, the most reasonable initialization\n        is chosen. If `n_components <= min(n_features, n_classes - 1)`\n        we use `'lda'`, as it uses labels information. If not, but\n        `n_components < min(n_features, n_samples)`, we use `'pca'`, as\n        it projects data in meaningful directions (those of higher\n        variance). Otherwise, we just use `'identity'`.\n\n    - `'pca'`\n        `n_components` principal components of the inputs passed\n        to :meth:`fit` will be used to initialize the transformation.\n        (See :class:`~sklearn.decomposition.PCA`)\n\n    - `'lda'`\n        `min(n_components, n_classes)` most discriminative\n        components of the inputs passed to :meth:`fit` will be used to\n        initialize the transformation. (If `n_components > n_classes`,\n        the rest of the components will be zero.) (See\n        :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n\n    - `'identity'`\n        If `n_components` is strictly smaller than the\n        dimensionality of the inputs passed to :meth:`fit`, the identity\n        matrix will be truncated to the first `n_components` rows.\n\n    - `'random'`\n        The initial transformation will be a random array of shape\n        `(n_components, n_features)`. Each value is sampled from the\n        standard normal distribution.\n\n    - numpy array\n        `n_features_b` must match the dimensionality of the inputs passed\n        to :meth:`fit` and n_features_a must be less than or equal to that.\n        If `n_components` is not `None`, `n_features_a` must match it."
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": "If `True` and :meth:`fit` has been called before, the solution of the\n    previous call to :meth:`fit` is used as the initial linear\n    transformation (`n_components` and `init` will be ignored)."
          },
          "max_iter": {
            "type": "int, default=50",
            "description": "Maximum number of iterations in the optimization."
          },
          "tol": {
            "type": "float, default=1e-5",
            "description": "Convergence tolerance for the optimization."
          },
          "callback": {
            "type": "callable, default=None",
            "description": "If not `None`, this function is called after every iteration of the\n    optimizer, taking as arguments the current solution (flattened\n    transformation matrix) and the number of iterations. This might be\n    useful in case one wants to examine or store the transformation\n    found after each iteration."
          },
          "verbose": {
            "type": "int, default=0",
            "description": "If 0, no progress messages will be printed.\n    If 1, progress messages will be printed to stdout.\n    If > 1, progress messages will be printed and the `disp`\n    parameter of :func:`scipy.optimize.minimize` will be set to\n    `verbose - 2`."
          },
          "random_state": {
            "type": "int or numpy.RandomState, default=None",
            "description": "A pseudo random number generator object or a seed for it if int. If\n    `init='random'`, `random_state` is used to initialize the random\n    transformation. If `init='pca'`, `random_state` is passed as an\n    argument to PCA when initializing the transformation. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": "The linear transformation learned during fitting."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "n_iter_": {
            "type": "int",
            "description": "Counts the number of iterations performed by the optimizer."
          },
          "random_state_": {
            "type": "numpy.RandomState",
            "description": "Pseudo random number generator object used during initialization."
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n    Discriminant Analysis.\nsklearn.decomposition.PCA : Principal component analysis (PCA).\n\nReferences\n----------\n.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n       \"Neighbourhood Components Analysis\". Advances in Neural Information\n       Processing Systems. 17, 513-520, 2005.\n       http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n.. [2] Wikipedia entry on Neighborhood Components Analysis\n       https://en.wikipedia.org/wiki/Neighbourhood_components_analysis",
        "notes": "",
        "examples": ">>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> nca.fit(X_train, y_train)\nNeighborhoodComponentsAnalysis(...)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> knn.fit(X_train, y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(X_test, y_test))\n0.933333...\n>>> knn.fit(nca.transform(X_train), y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(nca.transform(X_test), y_test))\n0.961904..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the model according to the given training data.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The training samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "The corresponding training labels."
              }
            },
            "returns": "self : object\n    Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.\n\nThe feature names out will prefixed by the lowercased class name. For\nexample, if the transformer outputs 3 features, then the feature names\nout are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Only used to validate feature names with the names seen in `fit`."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Apply the learned transformation to the given data.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Data samples."
              }
            },
            "returns": "X_embedded: ndarray of shape (n_samples, n_components)\n    The data samples transformed.",
            "raises": "NotFittedError\n    If :meth:`fit` has not been called before.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsClassifier",
      "docstring": {
        "description": "Classifier implementing a vote among neighbors within a given radius.\n\nRead more in the :ref:`User Guide <classification>`.",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "outlier_label": {
            "type": "{manual label, 'most_frequent'}, default=None",
            "description": "Label for outlier samples (samples with no neighbors in given radius).\n\n    - manual label: str or int label (should be the same type as y)\n      or list of manual labels if multi-output is used.\n    - 'most_frequent' : assign the most frequent label of y to outliers.\n    - None : when any outlier is detected, ValueError will be raised.\n\n    The outlier label should be selected from among the unique 'Y' labels.\n    If it is specified with a different value a warning will be raised and\n    all class probabilities of outliers will be assigned to be 0."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier."
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          },
          "outlier_label_": {
            "type": "int or array-like of shape (n_class,)",
            "description": "Label which is given for outlier samples (samples with no neighbors\n    on given radius)."
          },
          "outputs_2d_": {
            "type": "bool",
            "description": "False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsRegressor : Regression based on neighbors within a\n    fixed radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nNearestNeighbors : Unsupervised learner for implementing neighbor\n    searches.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsClassifier\n>>> neigh = RadiusNeighborsClassifier(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsClassifier(...)\n>>> print(neigh.predict([[1.5]]))\n[0]\n>>> print(neigh.predict_proba([[1.0]]))\n[[0.66666667 0.33333333]]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the radius neighbors classifier from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)",
                "description": "Target values."
              }
            },
            "returns": "self : RadiusNeighborsClassifier\n    The fitted radius neighbors classifier.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Predict the class labels for the provided data.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n    Class labels for each data sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test data X.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "p : ndarray of shape (n_queries, n_classes), or a list of                 n_outputs of such arrays if n_outputs > 1.\n    The class probabilities of the input samples. Classes are ordered\n    by lexicographic order.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "docstring": {
            "description": "Find the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Limiting distance of neighbors to return. The default is the value\n    passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, the distances and indices will be sorted by increasing\n    distances before being returned. If False, the results may not\n    be sorted. If `return_distance=False`, setting `sort_results=True`\n    will result in an error.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_samples,) of arrays\n    Array representing the distances to each point, only present if\n    `return_distance=True`. The distance values are computed according\n    to the ``metric`` constructor parameter.\n\nneigh_ind : ndarray of shape (n_samples,) of arrays\n    An array of arrays of indices of the approximate nearest points\n    from the population matrix that lie within a ball of size\n    ``radius`` around the query points.",
            "raises": "",
            "see_also": "",
            "notes": "Because the number of neighbors of each point is not necessarily\nequal, the results for multiple query points cannot be fit in a\nstandard data array.\nFor efficiency, `radius_neighbors` returns arrays of objects, where\neach object is a 1D array of indices or distances.",
            "examples": "In the following example, we construct a NeighborsClassifier\nclass from an array representing our data set and ask who's\nthe closest point to [1, 1, 1]:\n\n>>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)\nNearestNeighbors(radius=1.6)\n>>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]\n\nThe first array returned contains the distances to all points which\nare closer than 1.6, while the second array returned contains their\nindices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\nNeighborhoods are restricted the points at a distance lower than\nradius.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Radius of neighborhoods. The default is the value passed to the\n    constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, in each row of the result, the non-zero entries will be\n    sorted by increasing distances. If False, the non-zero entries may\n    not be sorted. Only used with mode='distance'.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n    points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)\nNearestNeighbors(radius=1.5)\n>>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features), or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    used; in this case, points are not considered their own\n    neighbors. This means that `knn.fit(X, y).score(None, y)`\n    implicitly performs a leave-one-out cross-validation procedure\n    and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n    but typically much faster."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._classification.RadiusNeighborsClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._classification.RadiusNeighborsClassifier",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsRegressor",
      "docstring": {
        "description": "Regression based on neighbors within a fixed radius.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": "Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": "Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "p": {
            "type": "float, default=2",
            "description": "Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nKNeighborsClassifier : Classifier based on the k-nearest neighbors.\nRadiusNeighborsClassifier : Classifier based on neighbors within a given radius.",
        "notes": "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "examples": ">>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsRegressor\n>>> neigh = RadiusNeighborsRegressor(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "docstring": {
            "description": "Fit the radius neighbors regressor from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "{array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)",
                "description": "Target values."
              }
            },
            "returns": "self : RadiusNeighborsRegressor\n    The fitted radius neighbors regressor.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Predict the target for the provided data.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None",
                "description": "Test samples. If `None`, predictions for all indexed points are\n    returned; in this case, points are not considered their own\n    neighbors."
              }
            },
            "returns": "y : ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double\n    Target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "docstring": {
            "description": "Find the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Limiting distance of neighbors to return. The default is the value\n    passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, the distances and indices will be sorted by increasing\n    distances before being returned. If False, the results may not\n    be sorted. If `return_distance=False`, setting `sort_results=True`\n    will result in an error.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_samples,) of arrays\n    Array representing the distances to each point, only present if\n    `return_distance=True`. The distance values are computed according\n    to the ``metric`` constructor parameter.\n\nneigh_ind : ndarray of shape (n_samples,) of arrays\n    An array of arrays of indices of the approximate nearest points\n    from the population matrix that lie within a ball of size\n    ``radius`` around the query points.",
            "raises": "",
            "see_also": "",
            "notes": "Because the number of neighbors of each point is not necessarily\nequal, the results for multiple query points cannot be fit in a\nstandard data array.\nFor efficiency, `radius_neighbors` returns arrays of objects, where\neach object is a 1D array of indices or distances.",
            "examples": "In the following example, we construct a NeighborsClassifier\nclass from an array representing our data set and ask who's\nthe closest point to [1, 1, 1]:\n\n>>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)\nNearestNeighbors(radius=1.6)\n>>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]\n\nThe first array returned contains the distances to all points which\nare closer than 1.6, while the second array returned contains their\nindices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\nNeighborhoods are restricted the points at a distance lower than\nradius.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Radius of neighborhoods. The default is the value passed to the\n    constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, in each row of the result, the non-zero entries will be\n    sorted by increasing distances. If False, the non-zero entries may\n    not be sorted. Only used with mode='distance'.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n    points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)\nNearestNeighbors(radius=1.5)\n>>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the coefficient of determination of the prediction.\n\nThe coefficient of determination :math:`R^2` is defined as\n:math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\nsum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\nis the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\nThe best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\nthe expected value of `y`, disregarding the input features, would get\na :math:`R^2` score of 0.0.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True values for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "The :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._regression.RadiusNeighborsRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._regression.RadiusNeighborsRegressor",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsTransformer",
      "docstring": {
        "description": "Transform X into a (weighted) graph of neighbors nearer than a radius.\n\nThe transformed data is a sparse graph as returned by\n`radius_neighbors_graph`.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": "Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric."
          },
          "radius": {
            "type": "float, default=1.0",
            "description": "Radius of neighborhood in the transformed sparse graph."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force."
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": "Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": "Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported."
          },
          "p": {
            "type": "float, default=2",
            "description": "Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": "Additional keyword arguments for the metric function."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": "The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": "Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": "Number of samples in the fitted data."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "kneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nKNeighborsTransformer : Transform X into a weighted graph of k\n    nearest neighbors.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.cluster import DBSCAN\n>>> from sklearn.neighbors import RadiusNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> X, _ = load_wine(return_X_y=True)\n>>> estimator = make_pipeline(\n...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n...     DBSCAN(eps=25.0, metric='precomputed'))\n>>> X_clustered = estimator.fit_predict(X)\n>>> clusters, counts = np.unique(X_clustered, return_counts=True)\n>>> print(counts)\n[ 29  15 111  11  12]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Fit the radius neighbors transformer from the training dataset.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'",
                "description": "Training data."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "self : RadiusNeighborsTransformer\n    The fitted radius neighbors transformer.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Training set."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present for API consistency by convention."
              }
            },
            "returns": "Xt : sparse matrix of shape (n_samples, n_samples)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.\n\nThe feature names out will prefixed by the lowercased class name. For\nexample, if the transformer outputs 3 features, then the feature names\nout are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Only used to validate feature names with the names seen in `fit`."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "docstring": {
            "description": "Find the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Limiting distance of neighbors to return. The default is the value\n    passed to the constructor."
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": "Whether or not to return the distances."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, the distances and indices will be sorted by increasing\n    distances before being returned. If False, the results may not\n    be sorted. If `return_distance=False`, setting `sort_results=True`\n    will result in an error.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "neigh_dist : ndarray of shape (n_samples,) of arrays\n    Array representing the distances to each point, only present if\n    `return_distance=True`. The distance values are computed according\n    to the ``metric`` constructor parameter.\n\nneigh_ind : ndarray of shape (n_samples,) of arrays\n    An array of arrays of indices of the approximate nearest points\n    from the population matrix that lie within a ball of size\n    ``radius`` around the query points.",
            "raises": "",
            "see_also": "",
            "notes": "Because the number of neighbors of each point is not necessarily\nequal, the results for multiple query points cannot be fit in a\nstandard data array.\nFor efficiency, `radius_neighbors` returns arrays of objects, where\neach object is a 1D array of indices or distances.",
            "examples": "In the following example, we construct a NeighborsClassifier\nclass from an array representing our data set and ask who's\nthe closest point to [1, 1, 1]:\n\n>>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)\nNearestNeighbors(radius=1.6)\n>>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]\n\nThe first array returned contains the distances to all points which\nare closer than 1.6, while the second array returned contains their\nindices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\nNeighborhoods are restricted the points at a distance lower than\nradius.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features), default=None",
                "description": "The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor."
              },
              "radius": {
                "type": "float, default=None",
                "description": "Radius of neighborhoods. The default is the value passed to the\n    constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": "Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are distances between points, type of distance\n    depends on the selected metric parameter in\n    NearestNeighbors class."
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": "If True, in each row of the result, the non-zero entries will be\n    sorted by increasing distances. If False, the non-zero entries may\n    not be sorted. Only used with mode='distance'.\n\n    .. versionadded:: 0.22"
              }
            },
            "returns": "A : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data.\n    `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n    points in X.",
            "notes": "",
            "examples": ">>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)\nNearestNeighbors(radius=1.5)\n>>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])"
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples_transform, n_features)",
                "description": "Sample data."
              }
            },
            "returns": "Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ],
  "constants": [
    {
      "name": "VALID_METRICS",
      "value": "{'ball_tree': ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc'], 'kd_tree': ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity'], 'brute': ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'haversine', 'jaccard', 'l1', 'l2', 'mahalanobis', 'manhattan', 'minkowski', 'nan_euclidean', 'precomputed', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']}",
      "docstring": {
        "description": "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "VALID_METRICS_SPARSE",
      "value": "{'ball_tree': [], 'kd_tree': [], 'brute': {'l1', 'precomputed', 'euclidean', 'manhattan', 'cityblock', 'l2', 'cosine'}}",
      "docstring": {
        "description": "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ]
}