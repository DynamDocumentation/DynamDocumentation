{
  "description": "No description available",
  "functions": [
    {
      "name": "custom_bwd",
      "signature": "custom_bwd(bwd=None, *, device_type: str)",
      "documentation": {
        "description": "Create a helper decorator for backward methods of custom autograd functions.\n\nAutograd functions are subclasses of :class:`torch.autograd.Function`.\nEnsures that ``backward`` executes with the same autocast state as ``forward``.\nSee the :ref:`example page<amp-custom-examples>` for more detail.",
        "parameters": {
          "device_type": {
            "type": "str",
            "description": "Device type to use. 'cuda', 'cpu', 'xpu' and so on."
          },
          "The": {
            "type": "",
            "description": "type is the same as the `type` attribute of a :class:`torch.device`."
          },
          "Thus": {
            "type": "",
            "description": ", you may obtain the device type of a tensor using `Tensor.device.type`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "custom_fwd",
      "signature": "custom_fwd(fwd=None, *, device_type: str, cast_inputs: Optional[torch.dtype] = None)",
      "documentation": {
        "description": "Create a helper decorator for ``forward`` methods of custom autograd functions.\n\nAutograd functions are subclasses of :class:`torch.autograd.Function`.\nSee the :ref:`example page<amp-custom-examples>` for more detail.",
        "parameters": {
          "device_type": {
            "type": "str",
            "description": "Device type to use. 'cuda', 'cpu', 'xpu' and so on."
          },
          "The": {
            "type": "",
            "description": "type is the same as the `type` attribute of a :class:`torch.device`."
          },
          "Thus": {
            "type": "",
            "description": ", you may obtain the device type of a tensor using `Tensor.device.type`."
          },
          "cast_inputs": {
            "type": ":class:`torch.dtype` or None, optional, default=None",
            "description": "If not ``None``,"
          },
          "when": {
            "type": "",
            "description": "``forward`` runs in an autocast-enabled region, casts incoming"
          },
          "floating": {
            "type": "",
            "description": "-point Tensors to the target dtype (non-floating-point Tensors are not affected),"
          },
          "then": {
            "type": "",
            "description": "executes ``forward`` with autocast disabled."
          },
          "If": {
            "type": "",
            "description": "the decorated ``forward`` is called outside an autocast-enabled region,\n:func:`custom_fwd<custom_fwd>` is a no-op and ``cast_inputs`` has no effect."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_autocast_available",
      "signature": "is_autocast_available(device_type: str) -> bool",
      "documentation": {
        "description": "Return a bool indicating if autocast is available on :attr:`device_type`.",
        "parameters": {
          "device_type": {
            "type": "str",
            "description": "Device type to use. Possible values are: 'cuda', 'cpu', 'xpu' and so on."
          },
          "The": {
            "type": "",
            "description": "type is the same as the `type` attribute of a :class:`torch.device`."
          },
          "Thus": {
            "type": "",
            "description": ", you may obtain the device type of a tensor using `Tensor.device.type`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "GradScaler",
      "documentation": {
        "description": "An instance ``scaler`` of :class:`GradScaler`.\n\nHelps perform the steps of gradient scaling\nconveniently.\n\n* ``scaler.scale(loss)`` multiplies a given loss by ``scaler``'s current scale factor.\n* ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.\n* ``scaler.update()`` updates ``scaler``'s scale factor.\n\nExample::\n\n    # Creates a GradScaler once at the beginning of training.\n    scaler = GradScaler()\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n            output = model(input)\n            loss = loss_fn(output, target)\n\n            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(loss).backward()\n\n            # scaler.step() first unscales gradients of the optimizer's params.\n            # If gradients don't contain infs/NaNs, optimizer.step() is then called,\n            # otherwise, optimizer.step() is skipped.\n            scaler.step(optimizer)\n\n            # Updates the scale for next iteration.\n            scaler.update()\n\nSee the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage\n(along with autocasting) in more complex cases like gradient clipping, gradient accumulation, gradient penalty,\nand multiple losses/optimizers.\n\n``scaler`` dynamically estimates the scale factor each iteration.  To minimize gradient underflow,\na large scale factor should be used.  However, ``float16`` values can \"overflow\" (become inf or NaN) if\nthe scale factor is too large.  Therefore, the optimal scale factor is the largest factor that can be used\nwithout incurring inf or NaN gradient values.\n``scaler`` approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every\n``scaler.step(optimizer)`` (or optional separate ``scaler.unscale_(optimizer)``, see :meth:`unscale_`).\n\n* If infs/NaNs are found, ``scaler.step(optimizer)`` skips the underlying ``optimizer.step()`` (so the params\n  themselves remain uncorrupted) and ``update()`` multiplies the scale by ``backoff_factor``.\n\n* If no infs/NaNs are found, ``scaler.step(optimizer)`` runs the underlying ``optimizer.step()`` as usual.\n  If ``growth_interval`` unskipped iterations occur consecutively, ``update()`` multiplies the scale by\n  ``growth_factor``.\n\nThe scale factor often causes infs/NaNs to appear in gradients for the first few iterations as its\nvalue calibrates.  ``scaler.step`` will skip the underlying ``optimizer.step()`` for these\niterations.  After that, step skipping should occur rarely (once every few hundred or thousand iterations).",
        "parameters": {
          "device": {
            "type": "str, optional, default=\"cuda\"",
            "description": "Device type to use. Possible values are: 'cuda' and 'cpu'."
          },
          "The": {
            "type": "",
            "description": "type is the same as the `type` attribute of a :class:`torch.device`."
          },
          "Thus": {
            "type": "",
            "description": ", you may obtain the device type of a tensor using `Tensor.device.type`."
          },
          "init_scale": {
            "type": "float, optional, default=2.**16",
            "description": "Initial scale factor."
          },
          "growth_factor": {
            "type": "float, optional, default=2.0",
            "description": "Factor by which the scale is multiplied during\n:meth:`update` if no inf/NaN gradients occur for ``growth_interval`` consecutive iterations."
          },
          "backoff_factor": {
            "type": "float, optional, default=0.5",
            "description": "Factor by which the scale is multiplied during\n:meth:`update` if inf/NaN gradients occur in an iteration."
          },
          "growth_interval": {
            "type": "int, optional, default=2000",
            "description": "Number of consecutive iterations without inf/NaN gradients"
          },
          "that": {
            "type": "",
            "description": "must occur for the scale to be multiplied by ``growth_factor``."
          },
          "enabled": {
            "type": "bool, optional",
            "description": "If ``False``, disables gradient scaling. :meth:`step` simply"
          },
          "invokes": {
            "type": "",
            "description": "the underlying ``optimizer.step()``, and other methods become no-ops."
          },
          "Default": {
            "type": "",
            "description": "``True``"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "get_backoff_factor",
          "signature": "get_backoff_factor(self) -> 'float'",
          "documentation": {
            "description": "Return a Python float containing the scale backoff factor.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_growth_factor",
          "signature": "get_growth_factor(self) -> 'float'",
          "documentation": {
            "description": "Return a Python float containing the scale growth factor.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_growth_interval",
          "signature": "get_growth_interval(self) -> 'int'",
          "documentation": {
            "description": "Return a Python int containing the growth interval.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_scale",
          "signature": "get_scale(self) -> 'float'",
          "documentation": {
            "description": "Return a Python float containing the current scale, or 1.0 if scaling is disabled.\n\n.. warning::\n    :meth:`get_scale` incurs a CPU-GPU sync.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_enabled",
          "signature": "is_enabled(self) -> 'bool'",
          "documentation": {
            "description": "Return a bool indicating whether this instance is enabled.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: 'Dict[str, Any]') -> 'None'",
          "documentation": {
            "description": "Load the scaler state.\n\nIf this instance is disabled, :meth:`load_state_dict` is a no-op.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "scaler state.  Should be an object returned from a call to :meth:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "scale",
          "signature": "scale(self, outputs: 'Union[torch.Tensor, Iterable[torch.Tensor]]') -> 'Union[torch.Tensor, Iterable[torch.Tensor]]'",
          "documentation": {
            "description": "Multiplies ('scales') a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs.  If this instance of :class:`GradScaler` is not enabled, outputs are returned\nunmodified.",
            "parameters": {
              "outputs": {
                "type": "Tensor or iterable of Tensors",
                "description": "Outputs to scale."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_backoff_factor",
          "signature": "set_backoff_factor(self, new_factor: 'float') -> 'None'",
          "documentation": {
            "description": "Set a new scale backoff factor.",
            "parameters": {
              "new_scale": {
                "type": "float",
                "description": "Value to use as the new scale backoff factor."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_growth_factor",
          "signature": "set_growth_factor(self, new_factor: 'float') -> 'None'",
          "documentation": {
            "description": "Set a new scale growth factor.",
            "parameters": {
              "new_scale": {
                "type": "float",
                "description": "Value to use as the new scale growth factor."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_growth_interval",
          "signature": "set_growth_interval(self, new_interval: 'int') -> 'None'",
          "documentation": {
            "description": "Set a new growth interval.",
            "parameters": {
              "new_interval": {
                "type": "int",
                "description": "Value to use as the new growth interval."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> 'Dict[str, Any]'",
          "documentation": {
            "description": "Return the state of the scaler as a :class:`dict`.\n\nIt contains five entries:\n\n* ``\"scale\"`` - a Python float containing the current scale\n* ``\"growth_factor\"`` - a Python float containing the current growth factor\n* ``\"backoff_factor\"`` - a Python float containing the current backoff factor\n* ``\"growth_interval\"`` - a Python int containing the current growth interval\n* ``\"_growth_tracker\"`` - a Python int containing the number of recent consecutive unskipped steps.\n\nIf this instance is not enabled, returns an empty dict.\n\n.. note::\n   If you wish to checkpoint the scaler's state after a particular iteration, :meth:`state_dict`\n   should be called after :meth:`update`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, optimizer: 'torch.optim.Optimizer', *args: 'Any', **kwargs: 'Any') -> 'Optional[float]'",
          "documentation": {
            "description": "Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\n\n:meth:`step` carries out the following two operations:\n\n1.  Internally invokes ``unscale_(optimizer)`` (unless :meth:`unscale_` was explicitly called for ``optimizer``\n    earlier in the iteration).  As part of the :meth:`unscale_`, gradients are checked for infs/NaNs.\n2.  If no inf/NaN gradients are found, invokes ``optimizer.step()`` using the unscaled\n    gradients.  Otherwise, ``optimizer.step()`` is skipped to avoid corrupting the params.\n\n``*args`` and ``**kwargs`` are forwarded to ``optimizer.step()``.\n\nReturns the return value of ``optimizer.step(*args, **kwargs)``.",
            "parameters": {
              "optimizer": {
                "type": "torch.optim.Optimizer",
                "description": "Optimizer that applies the gradients."
              },
              "args": {
                "type": "",
                "description": "Any arguments."
              },
              "kwargs": {
                "type": "",
                "description": "Any keyword arguments.\n.. warning::"
              },
              "Closure": {
                "type": "",
                "description": "use is not currently supported."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "unscale_",
          "signature": "unscale_(self, optimizer: 'torch.optim.Optimizer') -> 'None'",
          "documentation": {
            "description": "Divides (\"unscales\") the optimizer's gradient tensors by the scale factor.\n\n:meth:`unscale_` is optional, serving cases where you need to\n:ref:`modify or inspect gradients<working-with-unscaled-gradients>`\nbetween the backward pass(es) and :meth:`step`.\nIf :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.\n\nSimple example, using :meth:`unscale_` to enable clipping of unscaled gradients::\n\n    ...\n    scaler.scale(loss).backward()\n    scaler.unscale_(optimizer)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n    scaler.step(optimizer)\n    scaler.update()",
            "parameters": {
              "optimizer": {
                "type": "torch.optim.Optimizer",
                "description": "Optimizer that owns the gradients to be unscaled.\n.. note::\n:meth:`unscale_` does not incur a CPU-GPU sync.\n.. warning::\n:meth:`unscale_` should only be called once per optimizer per :meth:`step` call,"
              },
              "and": {
                "type": "",
                "description": "only after all gradients for that optimizer's assigned parameters have been accumulated."
              },
              "Calling": {
                "type": "",
                "description": "meth:`unscale_` twice for a given optimizer between each :meth:`step` triggers a RuntimeError.\n.. warning::\n:meth:`unscale_` may unscale sparse gradients out of place, replacing the ``.grad`` attribute."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update",
          "signature": "update(self, new_scale: 'Optional[Union[float, torch.Tensor]]' = None) -> 'None'",
          "documentation": {
            "description": "Update the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by ``backoff_factor``\nto reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,\nthe scale is multiplied by ``growth_factor`` to increase it.\n\nPassing ``new_scale`` sets the new scale value manually. (``new_scale`` is not\nused directly, it's used to fill GradScaler's internal scale tensor. So if\n``new_scale`` was a tensor, later in-place changes to that tensor will not further\naffect the scale GradScaler uses internally.)",
            "parameters": {
              "new_scale": {
                "type": "float or :class:`torch.Tensor`, optional, default=None",
                "description": "New scale factor.\n.. warning::\n:meth:`update` should only be called at the end of the iteration, after ``scaler.step(optimizer)`` has"
              },
              "been": {
                "type": "",
                "description": "invoked for all optimizers used this iteration.\n.. warning::"
              },
              "For": {
                "type": "",
                "description": "performance reasons, we do not check the scale factor value to avoid synchronizations,"
              },
              "so": {
                "type": "",
                "description": "the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or"
              },
              "you": {
                "type": "",
                "description": "are seeing NaNs in your gradients or loss, something is likely wrong. For example,"
              },
              "bf16": {
                "type": "",
                "description": "-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "autocast",
      "documentation": {
        "description": "Instances of :class:`autocast` serve as context managers or decorators that\nallow regions of your script to run in mixed precision.\n\nIn these regions, ops run in an op-specific dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee the :ref:`Autocast Op Reference<autocast-op-reference>` for details.\n\nWhen entering an autocast-enabled region, Tensors may be any type.\nYou should not call ``half()`` or ``bfloat16()`` on your model(s) or inputs when using autocasting.\n\n:class:`autocast` should wrap only the forward pass(es) of your network, including the loss\ncomputation(s).  Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward ops.\n\nExample for CUDA Devices::\n\n    # Creates model and optimizer in default precision\n    model = Net().cuda()\n    optimizer = optim.SGD(model.parameters(), ...)\n\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Enables autocasting for the forward pass (model + loss)\n        with torch.autocast(device_type=\"cuda\"):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Exits the context manager before backward()\n        loss.backward()\n        optimizer.step()\n\nSee the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).\n\n:class:`autocast` can also be used as a decorator, e.g., on the ``forward`` method of your model::\n\n    class AutocastModel(nn.Module):\n        ...\n        @torch.autocast(device_type=\"cuda\")\n        def forward(self, input):\n            ...\n\nFloating-point Tensors produced in an autocast-enabled region may be ``float16``.\nAfter returning to an autocast-disabled region, using them with floating-point\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\nproduced in the autocast region back to ``float32`` (or other dtype if desired).\nIf a Tensor from the autocast region is already ``float32``, the cast is a no-op,\nand incurs no additional overhead.\nCUDA Example::\n\n    # Creates some tensors in default dtype (here assumed to be float32)\n    a_float32 = torch.rand((8, 8), device=\"cuda\")\n    b_float32 = torch.rand((8, 8), device=\"cuda\")\n    c_float32 = torch.rand((8, 8), device=\"cuda\")\n    d_float32 = torch.rand((8, 8), device=\"cuda\")\n\n    with torch.autocast(device_type=\"cuda\"):\n        # torch.mm is on autocast's list of ops that should run in float16.\n        # Inputs are float32, but the op runs in float16 and produces float16 output.\n        # No manual casts are required.\n        e_float16 = torch.mm(a_float32, b_float32)\n        # Also handles mixed input types\n        f_float16 = torch.mm(d_float32, e_float16)\n\n    # After exiting autocast, calls f_float16.float() to use with d_float32\n    g_float32 = torch.mm(d_float32, f_float16.float())\n\nCPU Training Example::\n\n    # Creates model and optimizer in default precision\n    model = Net()\n    optimizer = optim.SGD(model.parameters(), ...)\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n\n            # Runs the forward pass with autocasting.\n            with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n                output = model(input)\n                loss = loss_fn(output, target)\n\n            loss.backward()\n            optimizer.step()\n\n\nCPU Inference Example::\n\n    # Creates model in default precision\n    model = Net().eval()\n\n    with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n        for input in data:\n            # Runs the forward pass with autocasting.\n            output = model(input)\n\nCPU Inference Example with Jit Trace::\n\n    class TestModel(nn.Module):\n        def __init__(self, input_size, num_classes):\n            super().__init__()\n            self.fc1 = nn.Linear(input_size, num_classes)\n        def forward(self, x):\n            return self.fc1(x)\n\n    input_size = 2\n    num_classes = 2\n    model = TestModel(input_size, num_classes).eval()\n\n    # For now, we suggest to disable the Jit Autocast Pass,\n    # As the issue: https://github.com/pytorch/pytorch/issues/75956\n    torch._C._jit_set_autocast_mode(False)\n\n    with torch.cpu.amp.autocast(cache_enabled=False):\n        model = torch.jit.trace(model, torch.randn(1, input_size))\n    model = torch.jit.freeze(model)\n    # Models Run\n    for _ in range(3):\n        model(torch.randn(1, input_size))\n\nType mismatch errors *in* an autocast-enabled region are a bug; if this is what you observe,\nplease file an issue.\n\n``autocast(enabled=False)`` subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particular ``dtype``.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast to ``dtype`` before use::\n\n    # Creates some tensors in default dtype (here assumed to be float32)\n    a_float32 = torch.rand((8, 8), device=\"cuda\")\n    b_float32 = torch.rand((8, 8), device=\"cuda\")\n    c_float32 = torch.rand((8, 8), device=\"cuda\")\n    d_float32 = torch.rand((8, 8), device=\"cuda\")\n\n    with torch.autocast(device_type=\"cuda\"):\n        e_float16 = torch.mm(a_float32, b_float32)\n        with torch.autocast(device_type=\"cuda\", enabled=False):\n            # Calls e_float16.float() to ensure float32 execution\n            # (necessary because e_float16 was created in an autocasted region)\n            f_float32 = torch.mm(c_float32, e_float16.float())\n\n        # No manual casts are required when re-entering the autocast-enabled region.\n        # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n        g_float16 = torch.mm(d_float32, f_float32)\n\nThe autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator\nmust be invoked in that thread.  This affects :class:`torch.nn.DataParallel` and\n:class:`torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process\n(see :ref:`Working with Multiple GPUs<amp-multigpu>`).",
        "parameters": {
          "device_type": {
            "type": "str, required",
            "description": "Device type to use. Possible values are: 'cuda', 'cpu', 'xpu' and 'hpu'."
          },
          "The": {
            "type": "",
            "description": "type is the same as the `type` attribute of a :class:`torch.device`."
          },
          "Thus": {
            "type": "",
            "description": ", you may obtain the device type of a tensor using `Tensor.device.type`."
          },
          "enabled": {
            "type": "bool, optional",
            "description": "Whether autocasting should be enabled in the region."
          },
          "Default": {
            "type": "",
            "description": "``True``"
          },
          "dtype": {
            "type": "torch_dtype, optional",
            "description": "Data type for ops run in autocast. It uses the default value\n(``torch.float16`` for CUDA and ``torch.bfloat16`` for CPU), given by\n:func:`~torch.get_autocast_dtype`, if :attr:`dtype` is ``None``."
          },
          "cache_enabled": {
            "type": "bool, optional",
            "description": "Whether the weight cache inside autocast should be enabled."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    }
  ]
}