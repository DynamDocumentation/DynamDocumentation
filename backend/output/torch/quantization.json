{
  "description": "No description available",
  "functions": [
    {
      "name": "QConfigAny",
      "signature": "Optional(*args, **kwargs)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "add_quant_dequant",
      "signature": "add_quant_dequant(module)",
      "documentation": {
        "description": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig\nNote that this function will modify the children of module inplace and it\ncan return a new module which wraps the input module as well.",
        "parameters": {
          "module": {
            "type": "",
            "description": "is a leaf module and we want to quantize it."
          },
          "that": {
            "type": "",
            "description": "we want to quantize"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "Either": {
            "type": "",
            "description": "the inplace modified module with submodules wrapped in\n`QuantWrapper` based on qconfig or a new `QuantWrapper` module which"
          },
          "wraps": {
            "type": "",
            "description": "the input module, the latter case only happens when the input"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "convert",
      "signature": "convert(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None, use_precomputed_fake_quant=False)",
      "documentation": {
        "description": "Converts submodules in input module to a different module according to `mapping`\nby calling `from_float` method on the target module class. And remove qconfig at the\nend if remove_qconfig is set to True.",
        "parameters": {
          "module": {
            "type": "",
            "description": "type, can be overwritten to allow swapping user defined"
          },
          "Modules": {
            "type": "",
            "description": "`inplace`: carry out model transformations in-place, the original module"
          },
          "is": {
            "type": "",
            "description": "mutated\n`convert_custom_config_dict`: custom configuration dictionary for convert function\n`use_precomputed_fake_quant`: a flag to enable use of precomputed fake quant\n.. code-block:: python\n# Example of convert_custom_config_dict:"
          },
          "convert_custom_config_dict": {
            "type": "",
            "description": "= {\n# user will manually define the corresponding quantized\n# module class which has a from_observed class method that converts\n# observed custom module to quantized custom module\n\"observed_to_quantized_custom_module_class\": {"
          },
          "ObservedCustomModule": {
            "type": "",
            "description": "QuantizedCustomModule\n}\n}"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "convert_dynamic_jit",
      "signature": "convert_dynamic_jit(model, inplace=False, debug=False, preserved_attrs=None)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "convert_jit",
      "signature": "convert_jit(model, inplace=False, debug=False, preserved_attrs=None)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "default_eval_fn",
      "signature": "default_eval_fn(model, calib_data)",
      "documentation": {
        "description": "Default evaluation function takes a torch.utils.data.Dataset or a list of\ninput Tensors and run the model on the dataset",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "disable_fake_quant",
      "signature": "disable_fake_quant(mod)",
      "documentation": {
        "description": "Disable fake quantization for the module.\n\nDisable fake quantization for this module, if applicable. Example usage::\n\n  # model is any PyTorch model\n  model.apply(torch.ao.quantization.disable_fake_quant)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "disable_observer",
      "signature": "disable_observer(mod)",
      "documentation": {
        "description": "Disable observation for this module.\n\nDisable observation for this module, if applicable. Example usage::\n\n  # model is any PyTorch model\n  model.apply(torch.ao.quantization.disable_observer)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_fake_quant",
      "signature": "enable_fake_quant(mod)",
      "documentation": {
        "description": "Enable fake quantization for the module.\n\nEnable fake quantization for this module, if applicable. Example usage::\n\n  # model is any PyTorch model\n  model.apply(torch.ao.quantization.enable_fake_quant)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_observer",
      "signature": "enable_observer(mod)",
      "documentation": {
        "description": "Enable observation for this module.\n\nEnable observation for this module, if applicable. Example usage::\n\n  # model is any PyTorch model\n  model.apply(torch.ao.quantization.enable_observer)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fuse_conv_bn",
      "signature": "fuse_conv_bn(is_qat, conv, bn)",
      "documentation": {
        "description": "Return the fused the conv and bn modules.\nGiven the conv and bn modules, fuses them and returns the fused module",
        "parameters": {
          "is_qat": {
            "type": "",
            "description": "a flag for whether we are using quantization aware training fusion"
          },
          "or": {
            "type": "",
            "description": "post training quantization fusion"
          },
          "conv": {
            "type": "",
            "description": "Module instance of type conv2d/conv3d"
          },
          "bn": {
            "type": "",
            "description": "Spatial BN instance that needs to be fused with the conv"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fuse_conv_bn_jit",
      "signature": "fuse_conv_bn_jit(model, inplace=False)",
      "documentation": {
        "description": "Fuse conv - bn module\nWorks for eval model only.",
        "parameters": {
          "model": {
            "type": "",
            "description": "TorchScript model from scripting or tracing"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fuse_conv_bn_relu",
      "signature": "fuse_conv_bn_relu(is_qat, conv, bn, relu)",
      "documentation": {
        "description": "Return the fused conv and bv modules.\n\nGiven the conv and bn modules, fuses them and returns the fused module",
        "parameters": {
          "is_qat": {
            "type": "",
            "description": "a flag for whether we are using quantization aware training fusion"
          },
          "or": {
            "type": "",
            "description": "post training quantization fusion"
          },
          "conv": {
            "type": "",
            "description": "Module instance of type conv2d/conv3d"
          },
          "bn": {
            "type": "",
            "description": "Spatial BN instance that needs to be fused with the conv"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fuse_linear_bn",
      "signature": "fuse_linear_bn(is_qat, linear, bn)",
      "documentation": {
        "description": "Return the fused linear and bn modules.\nGiven the linear and bn modules, fuses them and returns the fused module",
        "parameters": {
          "is_qat": {
            "type": "",
            "description": "a flag for whether we are using quantization aware training fusion"
          },
          "or": {
            "type": "",
            "description": "post training quantization fusion"
          },
          "linear": {
            "type": "",
            "description": "Module instance of type Linear"
          },
          "bn": {
            "type": "",
            "description": "BatchNorm1d instance that needs to be fused with the linear layer"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fuse_modules",
      "signature": "fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules at 0x76e2605a11c0>, fuse_custom_config_dict=None)",
      "documentation": {
        "description": "Fuse a list of modules into a single module.\n\nFuses only the following sequence of modules:\nconv, bn\nconv, bn, relu\nconv, relu\nlinear, relu\nbn, relu\nAll other sequences are left unchanged.\nFor these sequences, replaces the first item in the list\nwith the fused module, replacing the rest of the modules\nwith identity.",
        "parameters": {
          "model": {
            "type": "",
            "description": "Model containing the modules to be fused"
          },
          "modules_to_fuse": {
            "type": "",
            "description": "list of list of module names to fuse. Can also be a list"
          },
          "of": {
            "type": "",
            "description": "the same length. For example,"
          },
          "inplace": {
            "type": "",
            "description": "bool specifying if fusion happens in place on the model, by default"
          },
          "a": {
            "type": "",
            "description": "new model is returned"
          },
          "fuser_func": {
            "type": "[convModule, BNModule]",
            "description": "returns the list [ConvBNModule, nn.Identity()]"
          },
          "Defaults": {
            "type": "",
            "description": "to torch.ao.quantization.fuse_known_modules\n`fuse_custom_config_dict`: custom configuration for fusion\n.. code-block:: python\n# Example of fuse_custom_config_dict"
          },
          "fuse_custom_config_dict": {
            "type": "",
            "description": "= {\n# Additional fuser_method mapping\n\"additional_fuser_method_mapping\": {\n(torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n},\n}"
          }
        },
        "returns": "model with fused modules. A new copy is created if inplace=True.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_compare_output_module_list",
      "signature": "get_default_compare_output_module_list() -> Set[Callable]",
      "documentation": {
        "description": "Get list of module class types that we will record output\nin numeric suite",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_dynamic_quant_module_mappings",
      "signature": "get_default_dynamic_quant_module_mappings() -> Dict[Callable, Any]",
      "documentation": {
        "description": "Get module mapping for post training dynamic quantization",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_float_to_quantized_operator_mappings",
      "signature": "get_default_float_to_quantized_operator_mappings() -> Dict[Union[Callable, str], Callable]",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_qat_module_mappings",
      "signature": "get_default_qat_module_mappings() -> Dict[Callable, Any]",
      "documentation": {
        "description": "Get default module mapping for quantization aware training",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_qat_qconfig",
      "signature": "get_default_qat_qconfig(backend='x86', version=1)",
      "documentation": {
        "description": "Returns the default QAT qconfig for the specified backend.",
        "parameters": {
          "Return": {
            "type": "",
            "description": ""
          },
          "qconfig": {
            "type": "",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_qconfig",
      "signature": "get_default_qconfig(backend='x86', version=0)",
      "documentation": {
        "description": "Returns the default PTQ qconfig for the specified backend.",
        "parameters": {
          "Return": {
            "type": "",
            "description": ""
          },
          "qconfig": {
            "type": "",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_qconfig_propagation_list",
      "signature": "get_default_qconfig_propagation_list() -> Set[Callable]",
      "documentation": {
        "description": "Get the default list of module types that we'll attach qconfig\nattribute to in prepare",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_static_quant_module_mappings",
      "signature": "get_default_static_quant_module_mappings() -> Dict[Callable, Any]",
      "documentation": {
        "description": "Get module mapping for post training static quantization",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_dynamic_quant_module_class",
      "signature": "get_dynamic_quant_module_class(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[Dict[Callable, Any]] = None) -> Any",
      "documentation": {
        "description": "n Get the dynamically quantized module class corresponding to\nthe floating point module class",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_fuser_method",
      "signature": "get_fuser_method(op_list, additional_fuser_method_mapping=None)",
      "documentation": {
        "description": "Get fuser method for the given list of module types.\n\nGet fuser method for the given list of module types,\nreturn None if fuser method does not exist",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_observer_state_dict",
      "signature": "get_observer_state_dict(mod)",
      "documentation": {
        "description": "Returns the state dict corresponding to the observer stats.\nTraverse the model state_dict and extract out the stats.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_quantized_operator",
      "signature": "get_quantized_operator(float_op: Union[Callable, str]) -> Callable",
      "documentation": {
        "description": "Get the quantized operator corresponding to the float operator",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_static_quant_module_class",
      "signature": "get_static_quant_module_class(float_module_class: Callable, additional_static_quant_mapping: Optional[Dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
      "documentation": {
        "description": "n Get the statically quantized module class corresponding to\nthe floating point module class",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "load_observer_state_dict",
      "signature": "load_observer_state_dict(mod, obs_dict)",
      "documentation": {
        "description": "Given input model and a state_dict containing model observer stats,\nload the stats back into the model. The observer state_dict can be saved\nusing torch.ao.quantization.get_observer_state_dict",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "no_observer_set",
      "signature": "no_observer_set() -> Set[Any]",
      "documentation": {
        "description": "These modules cannot have observers inserted by default.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "prepare",
      "signature": "prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)",
      "documentation": {
        "description": "Prepares a copy of the model for quantization calibration or quantization-aware training.\n\nQuantization configuration should be assigned preemptively\nto individual submodules in `.qconfig` attribute.\n\nThe model will be attached with observer or fake quant modules, and qconfig\nwill be propagated.",
        "parameters": {
          "prepare_custom_config_dict": {
            "type": "",
            "description": "= {\n# user will manually define the corresponding observed\n# module class which has a from_float class method that converts\n# float custom module to observed custom module\n\"float_to_observed_custom_module_class\": {"
          },
          "CustomModule": {
            "type": "",
            "description": "ObservedCustomModule\n}\n}"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "prepare_dynamic_jit",
      "signature": "prepare_dynamic_jit(model, qconfig_dict, inplace=False)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "prepare_jit",
      "signature": "prepare_jit(model, qconfig_dict, inplace=False)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "prepare_qat",
      "signature": "prepare_qat(model, mapping=None, inplace=False)",
      "documentation": {
        "description": "Prepares a copy of the model for quantization calibration or\nquantization-aware training and converts it to quantized version.\n\nQuantization configuration should be assigned preemptively\nto individual submodules in `.qconfig` attribute.",
        "parameters": {
          "model": {
            "type": "",
            "description": "input model to be modified in-place"
          },
          "mapping": {
            "type": "",
            "description": "dictionary that maps float modules to quantized modules to be"
          },
          "replaced": {
            "type": "",
            "description": "."
          },
          "inplace": {
            "type": "",
            "description": "carry out model transformations in-place, the original module"
          },
          "is": {
            "type": "",
            "description": "mutated"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "propagate_qconfig_",
      "signature": "propagate_qconfig_(module, qconfig_dict=None, prepare_custom_config_dict=None)",
      "documentation": {
        "description": "Propagate qconfig through the module hierarchy and assign `qconfig`\nattribute on each leaf module",
        "parameters": {
          "module": {
            "type": "",
            "description": "input module"
          },
          "qconfig_dict": {
            "type": "",
            "description": "dictionary that maps from name or type of submodule to"
          },
          "quantization": {
            "type": "",
            "description": "configuration, qconfig applies to all submodules of a"
          },
          "given": {
            "type": "",
            "description": "module unless qconfig for the submodules are specified (when"
          },
          "the": {
            "type": "",
            "description": "submodule already has qconfig attribute)"
          },
          "prepare_custom_config_dict": {
            "type": "",
            "description": "dictionary for custom handling of modules"
          },
          "see": {
            "type": "",
            "description": "docs for :func:`~torch.ao.quantization.prepare_fx`"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "None": {
            "type": "",
            "description": ", module is modified inplace with qconfig attached"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "qconfig_equals",
      "signature": "qconfig_equals(q1: typing.Optional[torch.ao.quantization.qconfig.QConfig], q2: typing.Optional[torch.ao.quantization.qconfig.QConfig])",
      "documentation": {
        "description": "Returns `True` if `q1` equals `q2`, and `False` otherwise.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "quantize",
      "signature": "quantize(model, run_fn, run_args, mapping=None, inplace=False)",
      "documentation": {
        "description": "Quantize the input float model with post training static quantization.\n\nFirst it will prepare the model for calibration, then it calls\n`run_fn` which will run the calibration step, after that we will\nconvert the model to a quantized model.",
        "parameters": {
          "model": {
            "type": "",
            "description": "input float model"
          },
          "run_fn": {
            "type": "",
            "description": "a calibration function for calibrating the prepared model"
          },
          "run_args": {
            "type": "",
            "description": "positional arguments for `run_fn`"
          },
          "inplace": {
            "type": "",
            "description": "carry out model transformations in-place, the original module is mutated"
          },
          "mapping": {
            "type": "",
            "description": "correspondence between original module types and quantized counterparts"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "Quantized": {
            "type": "",
            "description": "model."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "quantize_dynamic",
      "signature": "quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)",
      "documentation": {
        "description": "Converts a float model to dynamic (i.e. weights-only) quantized model.\n\nReplaces specified modules with dynamic weight-only quantized versions and output the quantized model.\n\nFor simplest usage provide `dtype` argument that can be float16 or qint8. Weight-only quantization\nby default is performed for layers with large weights size - i.e. Linear and RNN variants.\n\nFine grained control is possible with `qconfig` and `mapping` that act similarly to `quantize()`.\nIf `qconfig` is provided, the `dtype` argument is ignored.",
        "parameters": {
          "model": {
            "type": "",
            "description": "input model"
          },
          "qconfig_spec": {
            "type": "",
            "description": "Either:\n- A dictionary that maps from name or type of submodule to quantization"
          },
          "configuration": {
            "type": "",
            "description": ", qconfig applies to all submodules of a given"
          },
          "module": {
            "type": "",
            "description": "unless qconfig for the submodules are specified (when the"
          },
          "submodule": {
            "type": "",
            "description": "already has qconfig attribute). Entries in the dictionary"
          },
          "need": {
            "type": "",
            "description": "to be QConfig instances.\n- A set of types and/or submodule names to apply dynamic quantization to,"
          },
          "in": {
            "type": "",
            "description": "which case the `dtype` argument is used to specify the bit-width"
          },
          "inplace": {
            "type": "",
            "description": "carry out model transformations in-place, the original module is mutated"
          },
          "mapping": {
            "type": "",
            "description": "maps type of a submodule to a type of corresponding dynamically quantized version"
          },
          "with": {
            "type": "",
            "description": "which the submodule needs to be replaced"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "quantize_dynamic_jit",
      "signature": "quantize_dynamic_jit(model, qconfig_dict, inplace=False, debug=False)",
      "documentation": {
        "description": "Quantize the input float TorchScript model with\npost training dynamic quantization.\nCurrently only qint8 quantization of torch.nn.Linear is supported.",
        "parameters": {
          "qconfig": {
            "type": "",
            "description": "= get_default_qconfig('fbgemm')"
          },
          "descriptions": {
            "type": "",
            "description": "in :func:`~torch.ao.quantization.quantize_jit`\n`inplace`: carry out model transformations in-place, the original module is"
          },
          "mutated": {
            "type": "",
            "description": "`debug`: flag for producing a debug friendly model (preserve weight attribute)"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "Quantized": {
            "type": "",
            "description": "TorchSciprt model."
          },
          "Example": {
            "type": "",
            "description": "```python"
          },
          "import": {
            "type": "",
            "description": "torch"
          },
          "from": {
            "type": "",
            "description": "torch.ao.quantization import quantize_dynamic_jit"
          },
          "ts_model": {
            "type": "",
            "description": ",\n{'': qconfig},"
          },
          "def": {
            "type": "",
            "description": "calibrate(model, data_loader):"
          },
          "model": {
            "type": "image",
            "description": ""
          },
          "with": {
            "type": "",
            "description": "torch.no_grad():"
          },
          "for": {
            "type": "",
            "description": "image, target in data_loader:"
          },
          "quantized_model": {
            "type": "",
            "description": "= quantize_dynamic_jit("
          },
          "calibrate": {
            "type": "",
            "description": ",\n[data_loader_test])\n```"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "```python\nimport torch\nfrom torch.ao.quantization import per_channel_dynamic_qconfig\nfrom torch.ao.quantization import quantize_dynamic_jit\n\nts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\nqconfig = get_default_qconfig('fbgemm')\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\n\nquantized_model = quantize_dynamic_jit(\n    ts_model,\n    {'': qconfig},\n    calibrate,\n    [data_loader_test])\n```"
      }
    },
    {
      "name": "quantize_jit",
      "signature": "quantize_jit(model, qconfig_dict, run_fn, run_args, inplace=False, debug=False)",
      "documentation": {
        "description": "Quantize the input float TorchScript model with\npost training static quantization.\n\nFirst it will prepare the model for calibration, then it calls\n`run_fn` which will run the calibration step, after that we will\nconvert the model to a quantized model.",
        "parameters": {
          "qconfig": {
            "type": "",
            "description": "= get_default_qconfig('fbgemm')"
          },
          "to": {
            "type": "",
            "description": "whole model unless it's overwritten by more specific configurations, the"
          },
          "the": {
            "type": "",
            "description": "qconfig of parent module."
          },
          "Right": {
            "type": "",
            "description": "now qconfig_dict is the only way to configure how the model is quantized,"
          },
          "and": {
            "type": "",
            "description": "it is done in the granularity of module, that is, we only support one type"
          },
          "of": {
            "type": "",
            "description": "qconfig for each torch.nn.Module, and the qconfig for sub module will"
          },
          "override": {
            "type": "",
            "description": "the qconfig for parent module, empty string means global configuration.\n`run_fn`: a calibration function for calibrating the prepared model\n`run_args`: positional arguments for `run_fn`\n`inplace`: carry out model transformations in-place, the original module is"
          },
          "mutated": {
            "type": "",
            "description": "`debug`: flag for producing a debug friendly model (preserve weight attribute)"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "Quantized": {
            "type": "",
            "description": "TorchSciprt model."
          },
          "Example": {
            "type": "",
            "description": "```python"
          },
          "import": {
            "type": "",
            "description": "torch"
          },
          "from": {
            "type": "",
            "description": "torch.ao.quantization import quantize_jit"
          },
          "ts_model": {
            "type": "",
            "description": ",\n{'': qconfig},"
          },
          "def": {
            "type": "",
            "description": "calibrate(model, data_loader):"
          },
          "model": {
            "type": "image",
            "description": ""
          },
          "with": {
            "type": "",
            "description": "torch.no_grad():"
          },
          "for": {
            "type": "",
            "description": "image, target in data_loader:"
          },
          "quantized_model": {
            "type": "",
            "description": "= quantize_jit("
          },
          "calibrate": {
            "type": "",
            "description": ",\n[data_loader_test])\n```"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "```python\nimport torch\nfrom torch.ao.quantization import get_default_qconfig\nfrom torch.ao.quantization import quantize_jit\n\nts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\nqconfig = get_default_qconfig('fbgemm')\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\n\nquantized_model = quantize_jit(\n    ts_model,\n    {'': qconfig},\n    calibrate,\n    [data_loader_test])\n```"
      }
    },
    {
      "name": "quantize_qat",
      "signature": "quantize_qat(model, run_fn, run_args, inplace=False)",
      "documentation": {
        "description": "Do quantization aware training and output a quantized model",
        "parameters": {
          "model": {
            "type": "",
            "description": "input model"
          },
          "run_fn": {
            "type": "",
            "description": "a function for evaluating the prepared model, can be a"
          },
          "function": {
            "type": "",
            "description": "that simply runs the prepared model or a training"
          },
          "loop": {
            "type": "",
            "description": ""
          },
          "run_args": {
            "type": "",
            "description": "positional arguments for `run_fn`"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "Quantized": {
            "type": "",
            "description": "model."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "script_qconfig",
      "signature": "script_qconfig(qconfig)",
      "documentation": {
        "description": "Instantiate the activation and weight observer modules and script\nthem, these observer module instances will be deepcopied during\nprepare_jit step.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "script_qconfig_dict",
      "signature": "script_qconfig_dict(qconfig_dict)",
      "documentation": {
        "description": "Helper function used by `prepare_jit`.\nApply `script_qconfig` for all entries in `qconfig_dict` that is\nnot None.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "swap_module",
      "signature": "swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant=False)",
      "documentation": {
        "description": "Swaps the module if it has a quantized counterpart and it has an\n`observer` attached.",
        "parameters": {
          "mod": {
            "type": "",
            "description": "input module"
          },
          "mapping": {
            "type": "",
            "description": "a dictionary that maps from nn module to nnq module"
          },
          "Return": {
            "type": "",
            "description": ""
          },
          "The": {
            "type": "",
            "description": "corresponding quantized module of `mod`"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "ABC",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "DeQuantStub",
      "documentation": {
        "description": "Dequantize stub module, before calibration, this is same as identity,\nthis will be swapped as `nnq.DeQuantize` in `convert`.",
        "parameters": {
          "qconfig": {
            "type": "",
            "description": "quantization configuration for the tensor,"
          },
          "if": {
            "type": "",
            "description": "qconfig is not provided, we will get qconfig from parent modules"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FakeQuantize",
      "documentation": {
        "description": "Simulate the quantize and dequantize operations in training time.\n\nThe output of this module is given by::\n\n    x_out = (\n      clamp(round(x/scale + zero_point), quant_min, quant_max) - zero_point\n    ) * scale\n\n* :attr:`is_dynamic` indicates whether the fake quantie is a placeholder for dynamic quantization\n  operators (choose_qparams -> q -> dq) or static quantization operators (q -> dq)\n\n* :attr:`scale` defines the scale factor used for quantization.\n\n* :attr:`zero_point` specifies the quantized value to which 0 in floating point maps to\n\n* :attr:`fake_quant_enabled` controls the application of fake quantization on tensors, note that\n  statistics can still be updated.\n\n* :attr:`observer_enabled` controls statistics collection on tensors\n\n* :attr:`dtype` specifies the quantized dtype that is being emulated with fake-quantization,\n    allowable values are torch.qint8 and torch.quint8.",
        "parameters": {
          "observer": {
            "type": "module",
            "description": "Module for observing statistics on input tensors and calculating scale"
          },
          "and": {
            "type": "",
            "description": "zero-point."
          },
          "observer_kwargs": {
            "type": "optional",
            "description": "Arguments for the observer module"
          },
          "Attributes": {
            "type": "",
            "description": ""
          },
          "activation_post_process": {
            "type": "Module",
            "description": "User provided module that collects statistics on the input tensor and"
          },
          "provides": {
            "type": "",
            "description": "a method to calculate scale and zero-point."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_fake_quant",
          "signature": "disable_fake_quant(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_observer",
          "signature": "disable_observer(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_fake_quant",
          "signature": "enable_fake_quant(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_observer",
          "signature": "enable_observer(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, X)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "with_args(**kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FakeQuantizeBase",
      "documentation": {
        "description": "Base fake quantize module.\n\nBase fake quantize module\nAny fake quantize implementation should derive from this class.\n\nConcrete fake quantize module should follow the same API. In forward, they will update\nthe statistics of the observed Tensor and fake quantize the input. They should also provide a\n`calculate_qparams` function that computes the quantization parameters given\nthe collected statistics.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_fake_quant",
          "signature": "disable_fake_quant(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_observer",
          "signature": "disable_observer(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_fake_quant",
          "signature": "enable_fake_quant(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_observer",
          "signature": "enable_observer(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "with_args(**kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FixedQParamsFakeQuantize",
      "documentation": {
        "description": "Simulate quantize and dequantize in training time.\n\nSimulate quantize and dequantize with fixed quantization\nparameters in training time. Only per tensor quantization\nis supported.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_fake_quant",
          "signature": "disable_fake_quant(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_observer",
          "signature": "disable_observer(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_fake_quant",
          "signature": "enable_fake_quant(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_observer",
          "signature": "enable_observer(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Define a string representation of the object's attributes.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, X)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "with_args(**kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FusedMovingAvgObsFakeQuantize",
      "documentation": {
        "description": "Define a fused module to observe the tensor.\n\nFused module that is used to observe the input tensor (compute min/max), compute\nscale/zero_point and fake_quantize the tensor.\nThis module uses calculation similar MovingAverageMinMaxObserver for the inputs,\nto compute the min/max values in order to compute the scale/zero_point.\nThe qscheme input in the observer is used to differentiate between symmetric/affine\nquantization scheme.\n\nThe output of this module is given by\nx_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale\n\nSimilar to :class:`~torch.ao.quantization.FakeQuantize`, and accepts the same attributes as the\nbase class.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self) -> Tuple[torch.Tensor, torch.Tensor]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_fake_quant",
          "signature": "disable_fake_quant(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "disable_observer",
          "signature": "disable_observer(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_fake_quant",
          "signature": "enable_fake_quant(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "enable_observer",
          "signature": "enable_observer(self, enabled: bool = True) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, X: torch.Tensor) -> torch.Tensor",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "with_args(**kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HistogramObserver",
      "documentation": {
        "description": "The module records the running histogram of tensor values along with\nmin/max values. ``calculate_qparams`` will calculate scale and zero_point.",
        "parameters": {
          "bins": {
            "type": "",
            "description": "Number of bins to use for the histogram"
          },
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec"
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          },
          "eps": {
            "type": "",
            "description": "Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`."
          },
          "The": {
            "type": "",
            "description": "search for the min/max values ensures the minimization of the"
          },
          "1": {
            "type": "",
            "description": ". Create the histogram of the incoming inputs."
          },
          "with": {
            "type": "",
            "description": "every new tensor observed."
          },
          "2": {
            "type": "",
            "description": ". Search the distribution in the histogram for optimal min/max values."
          },
          "quantization": {
            "type": "",
            "description": "error with respect to the floating point model."
          },
          "3": {
            "type": "",
            "description": ". Compute the scale and zero point the same way as in the\n:class:`~torch.ao.quantization.MinMaxObserver`"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x_orig: torch.Tensor) -> torch.Tensor",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_histogram",
          "signature": "reset_histogram(self, x: torch.Tensor, min_val: torch.Tensor, max_val: torch.Tensor) -> None",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_min_max_vals",
          "signature": "reset_min_max_vals(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MinMaxObserver",
      "documentation": {
        "description": "Observer module for computing the quantization parameters based on the\nrunning min and max values.\n\nThis observer uses the tensor min/max statistics to compute the quantization\nparameters. The module records the running minimum and maximum of incoming\ntensors, and uses this statistic to compute the quantization parameters.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          },
          "quant_min": {
            "type": "",
            "description": "Minimum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "quant_max": {
            "type": "",
            "description": "Maximum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "eps": {
            "type": "",
            "description": "Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`."
          },
          "Given": {
            "type": "",
            "description": "running min/max as :math:`x_\\text{min}` and :math:`x_\\text{max}`,"
          },
          "scale": {
            "type": "",
            "description": "math:`s` and zero point :math:`z` are computed as:"
          },
          "The": {
            "type": "",
            "description": "scale :math:`s` and zero point :math:`z` are then computed as:\n.. math::\n\\begin{aligned}\n\\text{if Symmetric:}&\\\\\n&s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n\\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n&z = \\begin{cases}"
          },
          "x_": {
            "type": "",
            "description": "\\text{max} &= \\begin{cases}\n\\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n\\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise}\n\\end{cases}\\\\\n\\end{array}"
          },
          "where": {
            "type": "",
            "description": "math:`Q_\\text{min}` and :math:`Q_\\text{max}` are the minimum and"
          },
          "0": {
            "type": "",
            "description": "& \\text{if dtype is qint8} \\\\"
          },
          "128": {
            "type": "",
            "description": "& \\text{otherwise}\n\\end{cases}\\\\\n\\text{Otherwise:}&\\\\\n&s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n\\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n&z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n\\end{aligned}"
          },
          "maximum": {
            "type": "",
            "description": "of the quantized data type.\n.. warning:: :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.\n.. note:: If the running minimum equals to the running maximum, the scale"
          },
          "and": {
            "type": "",
            "description": "zero_point are set to 1.0 and 0."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "Calculates the quantization parameters.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x_orig)",
          "documentation": {
            "description": "Records the running minimum and maximum of ``x``.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_min_max_vals",
          "signature": "reset_min_max_vals(self)",
          "documentation": {
            "description": "Resets the min/max values.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MovingAverageMinMaxObserver",
      "documentation": {
        "description": "Observer module for computing the quantization parameters based on the\nmoving average of the min and max values.\n\nThis observer computes the quantization parameters based on the moving\naverages of minimums and maximums of the incoming tensors. The module\nrecords the average minimum and maximum of incoming tensors, and uses this\nstatistic to compute the quantization parameters.",
        "parameters": {
          "averaging_constant": {
            "type": "",
            "description": "Averaging constant for min/max."
          },
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          },
          "quant_min": {
            "type": "",
            "description": "Minimum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "quant_max": {
            "type": "",
            "description": "Maximum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "eps": {
            "type": "",
            "description": "Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`."
          },
          "The": {
            "type": "",
            "description": "scale and zero point are then computed as in\n:class:`~torch.ao.quantization.observer.MinMaxObserver`.\n.. note:: Only works with ``torch.per_tensor_affine`` quantization scheme.\n.. note:: If the running minimum equals to the running maximum, the scale"
          },
          "x_": {
            "type": "",
            "description": "\\text{max} = \\begin{cases}\n\\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n(1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise}\n\\end{cases}\\\\\n\\end{array}"
          },
          "where": {
            "type": "",
            "description": "math:`x_\\text{min/max}` is the running average min/max, :math:`X` is"
          },
          "is": {
            "type": "",
            "description": "the incoming tensor, and :math:`c` is the ``averaging_constant``."
          },
          "and": {
            "type": "",
            "description": "zero_point are set to 1.0 and 0."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "Calculates the quantization parameters.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x_orig)",
          "documentation": {
            "description": "Records the running minimum and maximum of ``x``.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_min_max_vals",
          "signature": "reset_min_max_vals(self)",
          "documentation": {
            "description": "Resets the min/max values.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MovingAveragePerChannelMinMaxObserver",
      "documentation": {
        "description": "Observer module for computing the quantization parameters based on the\nrunning per channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum\nof incoming tensors, and uses this statistic to compute the quantization\nparameters.",
        "parameters": {
          "averaging_constant": {
            "type": "",
            "description": "Averaging constant for min/max."
          },
          "ch_axis": {
            "type": "",
            "description": "Channel axis"
          },
          "dtype": {
            "type": "",
            "description": "Quantized data type"
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          },
          "quant_min": {
            "type": "",
            "description": "Minimum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "quant_max": {
            "type": "",
            "description": "Maximum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "eps": {
            "type": "",
            "description": "Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`."
          },
          "The": {
            "type": "",
            "description": "quantization parameters are computed the same way as in\n:class:`~torch.ao.quantization.observer.MovingAverageMinMaxObserver`, with the"
          },
          "difference": {
            "type": "",
            "description": "that the running min/max values are stored per channel."
          },
          "Scales": {
            "type": "",
            "description": "and zero points are thus computed per channel as well.\n.. note:: If the running minimum equals to the running maximum, the scales"
          },
          "and": {
            "type": "",
            "description": "zero_points are set to 1.0 and 0."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x_orig)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_min_max_vals",
          "signature": "reset_min_max_vals(self)",
          "documentation": {
            "description": "Resets the min/max values.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NoopObserver",
      "documentation": {
        "description": "Observer that doesn't do anything and just passes its configuration to the\nquantized module's ``.from_float()``.\n\nPrimarily used for quantization to float16 which doesn't require determining\nranges.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "Quantized data type"
          },
          "custom_op_name": {
            "type": "",
            "description": "(temporary) specify this observer for an operator that doesn't require any observation\n(Can be used in Graph Mode Passes for special case ops)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ObserverBase",
      "documentation": {
        "description": "Base observer Module.\nAny observer implementation should derive from this class.\n\nConcrete observers should follow the same API. In forward, they will update\nthe statistics of the observed Tensor. And they should provide a\n`calculate_qparams` function that computes the quantization parameters given\nthe collected statistics.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "is_dynamic": {
            "type": "",
            "description": "indicator for whether the observer is a placeholder for dynamic quantization"
          },
          "or": {
            "type": "",
            "description": "static quantization"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PerChannelMinMaxObserver",
      "documentation": {
        "description": "Observer module for computing the quantization parameters based on the\nrunning per channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum\nof incoming tensors, and uses this statistic to compute the quantization\nparameters.",
        "parameters": {
          "ch_axis": {
            "type": "",
            "description": "Channel axis"
          },
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          },
          "quant_min": {
            "type": "",
            "description": "Minimum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "quant_max": {
            "type": "",
            "description": "Maximum quantization value. If unspecified, it will follow the 8-bit setup."
          },
          "eps": {
            "type": "",
            "description": "Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`."
          },
          "The": {
            "type": "",
            "description": "quantization parameters are computed the same way as in\n:class:`~torch.ao.quantization.observer.MinMaxObserver`, with the difference"
          },
          "that": {
            "type": "",
            "description": "the running min/max values are stored per channel."
          },
          "Scales": {
            "type": "",
            "description": "and zero points are thus computed per channel as well.\n.. note:: If the running minimum equals to the running maximum, the scales"
          },
          "and": {
            "type": "",
            "description": "zero_points are set to 1.0 and 0."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x_orig)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reset_min_max_vals",
          "signature": "reset_min_max_vals(self)",
          "documentation": {
            "description": "Resets the min/max values.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PlaceholderObserver",
      "documentation": {
        "description": "Observer that doesn't do anything and just passes its configuration to the\nquantized module's ``.from_float()``.\n\nCan be used for quantization to float16 which doesn't require determining\nranges.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "quant_min": {
            "type": "",
            "description": "minimum value in quantized domain (TODO: align behavior with other observers)"
          },
          "quant_max": {
            "type": "",
            "description": "maximum value in quantized domain"
          },
          "custom_op_name": {
            "type": "",
            "description": "(temporary) specify this observer for an operator that doesn't require any observation\n(Can be used in Graph Mode Passes for special case ops)."
          },
          "compute_dtype": {
            "type": "deprecated",
            "description": "if set, marks the future quantize function to use"
          },
          "dynamic": {
            "type": "",
            "description": "quantization instead of static quantization."
          },
          "This": {
            "type": "",
            "description": "field is deprecated, use `is_dynamic=True` instead."
          },
          "is_dynamic": {
            "type": "",
            "description": "if True, the `quantize` function in the reference model"
          },
          "representation": {
            "type": "",
            "description": "taking stats from this observer instance will"
          },
          "use": {
            "type": "",
            "description": "dynamic quantization."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QConfig",
      "documentation": {
        "description": "Describes how to quantize a layer or a part of the network by providing\nsettings (observer classes) for activations and weights respectively.\n\n\nNote that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns\ninstances on invocation, not the concrete observer instances themselves.\nQuantization preparation function will instantiate observers multiple times for each of the layers.\n\n\nObserver classes have usually reasonable default arguments, but they can be overwritten with `with_args`\nmethod (that behaves like functools.partial)::\n\n  my_qconfig = QConfig(\n      activation=MinMaxObserver.with_args(dtype=torch.qint8),\n      weight=default_observer.with_args(dtype=torch.qint8))",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.\n\nRaises ValueError if the value is not present.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QConfigDynamic",
      "documentation": {
        "description": "Describes how to dynamically quantize a layer or a part of the network by providing\nsettings (observer classes) for weights.\n\nIt's like QConfig, but for dynamic quantization.\n\nNote that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns\ninstances on invocation, not the concrete observer instances themselves.\nQuantization function will instantiate observers multiple times for each of the layers.\n\nObserver classes have usually reasonable default arguments, but they can be overwritten with `with_args`\nmethod (that behaves like functools.partial)::\n\n  my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.\n\nRaises ValueError if the value is not present.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QuantStub",
      "documentation": {
        "description": "Quantize stub module, before calibration, this is same as an observer,\nit will be swapped as `nnq.Quantize` in `convert`.",
        "parameters": {
          "qconfig": {
            "type": "",
            "description": "quantization configuration for the tensor,"
          },
          "if": {
            "type": "",
            "description": "qconfig is not provided, we will get qconfig from parent modules"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QuantType",
      "documentation": {
        "description": "Enum where members are also (and must be) ints",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "as_integer_ratio",
          "signature": "as_integer_ratio(self, /)",
          "documentation": {
            "description": "Return a pair of integers, whose ratio is equal to the original int.\n\nThe ratio is in lowest terms and has a positive denominator.\n\n>>> (10).as_integer_ratio()\n(10, 1)\n>>> (-10).as_integer_ratio()\n(-10, 1)\n>>> (0).as_integer_ratio()\n(0, 1)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_count",
          "signature": "bit_count(self, /)",
          "documentation": {
            "description": "Number of ones in the binary representation of the absolute value of self.\n\nAlso known as the population count.\n\n>>> bin(13)\n'0b1101'\n>>> (13).bit_count()\n3",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_length",
          "signature": "bit_length(self, /)",
          "documentation": {
            "description": "Number of bits necessary to represent self in binary.\n\n>>> bin(37)\n'0b100101'\n>>> (37).bit_length()\n6",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "conjugate(...)",
          "documentation": {
            "description": "Returns self, the complex conjugate of any int.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "from_bytes",
          "signature": "from_bytes(bytes, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return the integer represented by the given array of bytes.\n\nbytes\n  Holds the array of bytes to convert.  The argument must either\n  support the buffer protocol or be an iterable object producing bytes.\n  Bytes and bytearray are examples of built-in objects that support the\n  buffer protocol.\nbyteorder\n  The byte order used to represent the integer.  If byteorder is 'big',\n  the most significant byte is at the beginning of the byte array.  If\n  byteorder is 'little', the most significant byte is at the end of the\n  byte array.  To request the native byte order of the host system, use\n  `sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\n  Indicates whether two's complement is used to represent the integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_integer",
          "signature": "is_integer(self, /)",
          "documentation": {
            "description": "Returns True. Exists for duck type compatibility with float.is_integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bytes",
          "signature": "to_bytes(self, /, length=1, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return an array of bytes representing an integer.\n\nlength\n  Length of bytes object to use.  An OverflowError is raised if the\n  integer is not representable with the given number of bytes.  Default\n  is length 1.\nbyteorder\n  The byte order used to represent the integer.  If byteorder is 'big',\n  the most significant byte is at the beginning of the byte array.  If\n  byteorder is 'little', the most significant byte is at the end of the\n  byte array.  To request the native byte order of the host system, use\n  `sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\n  Determines whether two's complement is used to represent the integer.\n  If signed is False and a negative integer is given, an OverflowError\n  is raised.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QuantWrapper",
      "documentation": {
        "description": "A wrapper class that wraps the input module, adds QuantStub and\nDeQuantStub and surround the call to module with call to quant and dequant\nmodules.\n\nThis is used by the `quantization` utility functions to add the quant and\ndequant modules, before `convert` function `QuantStub` will just be observer,\nit observes the input tensor, after `convert`, `QuantStub`\nwill be swapped to `nnq.Quantize` which does actual quantization. Similarly\nfor `DeQuantStub`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, X)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RecordingObserver",
      "documentation": {
        "description": "The module is mainly for debug and records the tensor values during runtime.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "Quantized data type"
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tensor_value",
          "signature": "get_tensor_value(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "default_debug_observer",
      "documentation": {
        "description": "The module is mainly for debug and records the tensor values during runtime.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "Quantized data type"
          },
          "qscheme": {
            "type": "",
            "description": "Quantization scheme to be used"
          },
          "reduce_range": {
            "type": "",
            "description": "Reduces the range of the quantized data type by 1 bit"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tensor_value",
          "signature": "get_tensor_value(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "default_placeholder_observer",
      "documentation": {
        "description": "Observer that doesn't do anything and just passes its configuration to the\nquantized module's ``.from_float()``.\n\nCan be used for quantization to float16 which doesn't require determining\nranges.",
        "parameters": {
          "dtype": {
            "type": "",
            "description": "dtype argument to the `quantize` node needed to implement the"
          },
          "reference": {
            "type": "",
            "description": "model spec."
          },
          "quant_min": {
            "type": "",
            "description": "minimum value in quantized domain (TODO: align behavior with other observers)"
          },
          "quant_max": {
            "type": "",
            "description": "maximum value in quantized domain"
          },
          "custom_op_name": {
            "type": "",
            "description": "(temporary) specify this observer for an operator that doesn't require any observation\n(Can be used in Graph Mode Passes for special case ops)."
          },
          "compute_dtype": {
            "type": "deprecated",
            "description": "if set, marks the future quantize function to use"
          },
          "dynamic": {
            "type": "",
            "description": "quantization instead of static quantization."
          },
          "This": {
            "type": "",
            "description": "field is deprecated, use `is_dynamic=True` instead."
          },
          "is_dynamic": {
            "type": "",
            "description": "if True, the `quantize` function in the reference model"
          },
          "representation": {
            "type": "",
            "description": "taking stats from this observer instance will"
          },
          "use": {
            "type": "",
            "description": "dynamic quantization."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "calculate_qparams",
          "signature": "calculate_qparams(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, x)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_args",
          "signature": "_with_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n    >>> foo_instance1 = foo_builder()\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1) == id(foo_instance2)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_callable_args",
          "signature": "_with_callable_args(**kwargs)",
          "documentation": {
            "description": "Wrapper that allows creation of class factories args that need to be\ncalled at construction time.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances and those arguments should only\nbe calculated at construction time. Can be used in conjunction with _with_args\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"Undefined vars\")\n    >>> Foo.with_callable_args = classmethod(_with_callable_args)\n    >>> Foo.with_args = classmethod(_with_args)\n    >>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n    >>> foo_instance1 = foo_builder()\n    >>> # wait 50\n    >>> foo_instance2 = foo_builder()\n    >>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\n    False",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}