{
  "description": ":mod:`torch.optim` is a package implementing various optimization algorithms.\n\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can also be easily integrated in the\nfuture.",
  "functions": [],
  "classes": [
    {
      "name": "ASGD",
      "documentation": {
        "description": "Implements Averaged Stochastic Gradient Descent.\n\nIt has been proposed in `Acceleration of stochastic approximation by\naveraging`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-2)"
          },
          "lambd": {
            "type": "float, optional",
            "description": "decay term (default: 1e-4)"
          },
          "alpha": {
            "type": "float, optional",
            "description": "power for eta update (default: 0.75)"
          },
          "t0": {
            "type": "float, optional",
            "description": "point at which to start averaging (default: 1e6)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)\n.. _Acceleration of stochastic approximation by averaging:"
          },
          "https": {
            "type": "",
            "description": "//dl.acm.org/citation.cfm?id=131098"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Adadelta",
      "documentation": {
        "description": "Implements Adadelta algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)},\n            \\: f(\\theta) \\text{ (objective)}, \\: \\rho \\text{ (decay)},\n            \\: \\lambda \\text{ (weight decay)}                                                \\\\\n        &\\textbf{initialize} :  v_0  \\leftarrow 0 \\: \\text{ (square avg)},\n            \\: u_0 \\leftarrow 0 \\: \\text{ (accumulate variables)}                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm} v_t      \\leftarrow v_{t-1} \\rho + g^2_t (1 - \\rho)                    \\\\\n        &\\hspace{5mm}\\Delta x_t    \\leftarrow   \\frac{\\sqrt{u_{t-1} +\n            \\epsilon }}{ \\sqrt{v_t + \\epsilon}  }g_t \\hspace{21mm}                           \\\\\n        &\\hspace{5mm} u_t  \\leftarrow   u_{t-1}  \\rho +\n             \\Delta x^2_t  (1 - \\rho)                                                        \\\\\n        &\\hspace{5mm}\\theta_t      \\leftarrow   \\theta_{t-1} - \\gamma  \\Delta x_t            \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `ADADELTA: An Adaptive Learning Rate Method`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "coefficient that scale delta before it is applied"
          },
          "to": {
            "type": "",
            "description": "the parameters (default: 1.0)"
          },
          "rho": {
            "type": "float, optional",
            "description": "coefficient used for computing a running average"
          },
          "of": {
            "type": "",
            "description": "squared gradients (default: 0.9). A higher value of `rho` will"
          },
          "result": {
            "type": "",
            "description": "in a slower average, which can be helpful for preventing"
          },
          "oscillations": {
            "type": "",
            "description": "in the learning process."
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-6)."
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)\n.. _ADADELTA\\: An Adaptive Learning Rate Method:"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1212.5701"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Adafactor",
      "documentation": {
        "description": "Implements Adafactor algorithm.\n\n.. math::\n    \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\tau\n            \\text{(}\\beta_2\\text{ decay)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},    \\\\\n        &\\hspace{15mm}      \\: \\epsilon_1, \\epsilon_2 \\text{ (epsilons)}, \\: d \\text{(clipping threshold)}, \\\\\n        &\\hspace{15mm}      \\: \\lambda \\text{(weight decay)},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : \\: R_0 \\leftarrow 0 \\text{ (second moment row factor)},       \\\\\n        &\\hspace{23mm} \\: C_0 \\leftarrow 0 \\text{ (second moment col factor)},               \\\\\n        &\\hspace{23mm} \\: \\widehat{V}_0 \\leftarrow 0 \\text{ (second moment for vectors)}     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}G_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}G_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\widehat{\\beta}_{2_t} \\leftarrow 1 - t^{\\tau}                           \\\\\n        &\\hspace{5mm}\\rho_t         \\leftarrow min(lr, \\frac{1}{\\sqrt{t}})                   \\\\\n        &\\hspace{5mm}\\alpha_t       \\leftarrow max(\\epsilon_2,\n            \\text{RMS}(\\theta_{t-1}))\\rho_t                                                  \\\\\n        &\\hspace{5mm}\\theta_t       \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}    \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\text{dim}(G_t) > 1:                                     \\\\\n        &\\hspace{10mm}R_t           \\leftarrow \\widehat{\\beta}_{2_t}R_{t-1}+\n            (1-\\widehat{\\beta}_{2_t})(G_t \\odot G_t) \\cdot 1_m                               \\\\\n        &\\hspace{10mm}C_t           \\leftarrow \\widehat{\\beta}_{2_t}C_{t-1}+\n            (1-\\widehat{\\beta}_{2_t}) 1^\\top_n \\cdot (G_t \\odot G_t)                         \\\\\n        &\\hspace{10mm}\\widehat{V}_t \\leftarrow\n            \\frac{R_t \\cdot C_t}{max(1^\\top_n \\cdot R_t, \\epsilon_1)}                        \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\widehat{V}_t \\leftarrow \\widehat{\\beta}_{2_t}\\widehat{V}_{t-1}+\n            (1-\\widehat{\\beta}_{2_t}) \\cdot (G_t \\odot G_t)                                  \\\\\n        &\\hspace{5mm}U_t            \\leftarrow\n            \\frac{G_t}{max(\\sqrt{\\widehat{V}_t}, \\epsilon_1)}                                \\\\\n        &\\hspace{5mm}\\widehat{U}_t  \\leftarrow \\frac{U_t}{max(1, \\frac{\\text{RMS}(U_t)}{d})} \\\\\n        &\\hspace{5mm}\\theta_t       \\leftarrow \\theta_{t-1} - \\alpha_t \\widehat{U}_t         \\\\\n\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n    \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)\n.. Note::"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "unlike other optimizers, Adafactor does not require a"
          },
          "learning": {
            "type": "",
            "description": "rate, and Shazeer, Noam, and Mitchell Stern do not use lr at all."
          },
          "Deviating": {
            "type": "",
            "description": "from the paper, this implementation uses lr for applying weight"
          },
          "decay": {
            "type": "",
            "description": ", similar to what is suggested in `Decoupled Weight Decay Regularization`_."
          },
          "the": {
            "type": "",
            "description": "variance estimate :math:`\\widehat{V}_t` and for the update :math:`U_t`."
          },
          "step": {
            "type": "",
            "description": "size, and so we set 0.01 as the default value. (default: 1e-2)"
          },
          "beta2_decay": {
            "type": "float, optional",
            "description": "the decay rate of beta2. beta2 standardly refers"
          },
          "to": {
            "type": "",
            "description": "foreach on CUDA for faster runtime. (default: None)"
          },
          "squared": {
            "type": "",
            "description": ". (default: -0.8)"
          },
          "eps": {
            "type": "Tuple[float, float], optional",
            "description": "epsilon1 is the term added to the denominator"
          },
          "of": {
            "type": "",
            "description": "the update calculation to improve numerical stability. This use of epsilon1"
          },
          "deviates": {
            "type": "",
            "description": "from the algorithm written in the paper! See note below for more details."
          },
          "epsilon2": {
            "type": "",
            "description": "is the term used to avoid having too small a weight update when applying"
          },
          "parameter": {
            "type": "",
            "description": "scaling. (default: (None, 1e-3))"
          },
          "d": {
            "type": "float, optional",
            "description": "the clipping threshold, used to avoid larger-than-desired"
          },
          "updates": {
            "type": "",
            "description": "."
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay coefficient (default: 1e-2)"
          },
          "foreach": {
            "type": "bool, optional",
            "description": "whether foreach implementation of optimizer is used. Note"
          },
          "that": {
            "type": "",
            "description": "the foreach implementation uses ~ sizeof(params) more peak memory than the"
          },
          "for": {
            "type": "",
            "description": "-loop version due to the intermediates being a tensorlist vs just one tensor."
          },
          "As": {
            "type": "",
            "description": "Adafactor is commonly used when memory is prohibitive, Adafactor will default"
          },
          "True": {
            "type": "",
            "description": ". This behavior is contrary to other optimizers, which will attempt defaulting"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "The": {
            "type": "",
            "description": "implementation of Adafactor subtly differs from Shazeer, Noam, and Mitchell Stern"
          },
          "and": {
            "type": "",
            "description": "implementations in some other frameworks with its use of learning rate and\n:math:`\\epsilon_1`."
          },
          "Regarding": {
            "type": "",
            "description": "the use of :math:`\\epsilon_1`: The implementation attempts to replicate the"
          },
          "use": {
            "type": "",
            "description": "lr at all, as the stated algorithm uses :math:`\\rho_t` and update clipping to"
          },
          "affect": {
            "type": "",
            "description": "the step size."
          },
          "This": {
            "type": "",
            "description": "is in contrast to Shazeer, Noam, and Mitchell Stern and other frameworks which"
          },
          "Shazeer": {
            "type": "",
            "description": ", Noam, and Mitchell Stern do not enforce an opinion on how weight decay should"
          },
          "be": {
            "type": "",
            "description": "computed, and so we use the learning rate as a coefficient for decoupled weight"
          },
          "presumed": {
            "type": "",
            "description": "intention of Shazeer, Noam, and Mitchell Stern to use :math:`\\epsilon_1` as"
          },
          "a": {
            "type": "",
            "description": "stabilizing term when the squared gradient becomes small."
          },
          "where": {
            "type": "",
            "description": "the row and column factors of gradient squared :math:`R_t` and :math:`C_t`"
          },
          "are": {
            "type": "",
            "description": "left alone, and we apply :math:`\\epsilon_1` at the final calculation of"
          },
          "apply": {
            "type": "",
            "description": "math:`\\epsilon_1` to both row and column factors of the squared gradient, but"
          },
          "not": {
            "type": "",
            "description": "in the calculations after:\n.. math::\n\\begin{aligned}\n&\\hspace{5mm}R_t \\leftarrow \\widehat{\\beta}_{2_t}R_{t-1}+\n(1-\\widehat{\\beta}_{2_t})(G_t \\odot G_t + \\epsilon_1 1_n \\cdot 1^\\top_m) \\cdot 1_m          \\\\\n&\\hspace{5mm}C_t \\leftarrow \\widehat{\\beta}_{2_t}C_{t-1}+\n(1-\\widehat{\\beta}_{2_t}) 1^\\top_n \\cdot (G_t \\odot G_t + \\epsilon_1 1_n \\cdot 1^\\top_m)    \\\\\n&\\hspace{5mm}\\widehat{V}_t \\leftarrow \\frac{R_t \\cdot C_t}{1^\\top_n \\cdot R_t}                          \\\\\n&\\hspace{5mm}U_t \\leftarrow \\frac{G_t}{\\sqrt{\\widehat{V}_t}}                                            \\\\\n\\end{aligned}\n.. _Adafactor\\: Adaptive Learning Rates with Sublinear Memory Cost:"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1711.05101"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Adagrad",
      "documentation": {
        "description": "Implements Adagrad algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{12mm}    \\tau \\text{ (initial accumulator value)}, \\: \\eta\\text{ (lr decay)}\\\\\n        &\\textbf{initialize} :  state\\_sum_0 \\leftarrow \\tau                          \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\tilde{\\gamma}    \\leftarrow \\gamma / (1 +(t-1) \\eta)                  \\\\\n        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n        &\\hspace{5mm}state\\_sum_t  \\leftarrow  state\\_sum_{t-1} + g^2_t                      \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow\n            \\theta_{t-1}- \\tilde{\\gamma} \\frac{g_t}{\\sqrt{state\\_sum_t}+\\epsilon}            \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adaptive Subgradient Methods for Online Learning\nand Stochastic Optimization`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-2)"
          },
          "lr_decay": {
            "type": "float, optional",
            "description": "learning rate decay (default: 0)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "initial_accumulator_value": {
            "type": "float, optional",
            "description": "initial value of the"
          },
          "sum": {
            "type": "",
            "description": "of squares of gradients (default: 0)"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-10)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "fused": {
            "type": "bool, optional",
            "description": "whether the fused implementation (CPU only) is used."
          },
          "Currently": {
            "type": "",
            "description": ", `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`"
          },
          "are": {
            "type": "",
            "description": "supported. (default: None). Please note that the fused implementations does not"
          },
          "support": {
            "type": "",
            "description": "sparse or complex gradients.\n.. _Adaptive Subgradient Methods for Online Learning and Stochastic"
          },
          "Optimization": {
            "type": "",
            "description": "http://jmlr.org/papers/v12/duchi11a.html"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Adam",
      "documentation": {
        "description": "Implements Adam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n        &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n            \\:\\textit{maximize},  \\: \\epsilon \\text{ (epsilon)}                              \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_{t-1}}^{max},\n            \\widehat{v_t})                                                                   \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-3). A tensor LR"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "LR": {
            "type": "",
            "description": "if you are not also specifying fused=True or capturable=True."
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square (default: (0.9, 0.999))"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "amsgrad": {
            "type": "bool, optional",
            "description": "whether to use the AMSGrad variant of this"
          },
          "algorithm": {
            "type": "",
            "description": "from the paper `On the Convergence of Adam and Beyond`_\n(default: False)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "fused": {
            "type": "bool, optional",
            "description": "whether the fused implementation is used."
          },
          "Currently": {
            "type": "",
            "description": ", `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`"
          },
          "are": {
            "type": "",
            "description": "supported. (default: None)\n.. note:: The foreach and fused implementations are typically faster than the for-loop,"
          },
          "single": {
            "type": "",
            "description": "-tensor implementation, with fused being theoretically fastest with both"
          },
          "vertical": {
            "type": "",
            "description": "and horizontal fusion. As such, if the user has not specified either"
          },
          "flag": {
            "type": "i.e., when foreach = fused = None",
            "description": ", we will attempt defaulting to the foreach"
          },
          "implementation": {
            "type": "",
            "description": ", pass False for either foreach or fused.\n.. Note::"
          },
          "To": {
            "type": "",
            "description": "specify fused, pass True for fused. To force running the for-loop"
          },
          "A": {
            "type": "",
            "description": "prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Adam\\: A Method for Stochastic Optimization:"
          },
          "https": {
            "type": "",
            "description": "//openreview.net/forum?id=ryQu7f-RZ"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "AdamW",
      "documentation": {
        "description": "Implements AdamW algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\beta_1, \\beta_2\n            \\text{(betas)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},\n            \\: \\epsilon \\text{ (epsilon)}                                                    \\\\\n        &\\hspace{13mm}      \\lambda \\text{(weight decay)},  \\: \\textit{amsgrad},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ (first moment)}, v_0 \\leftarrow 0\n            \\text{ ( second moment)}, \\: \\widehat{v_0}^{max}\\leftarrow 0              \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}         \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_{t-1}}^{max},\n            \\widehat{v_t})                                                                   \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Decoupled Weight Decay Regularization`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-3). A tensor LR"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "LR": {
            "type": "",
            "description": "if you are not also specifying fused=True or capturable=True."
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square (default: (0.9, 0.999))"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay coefficient (default: 1e-2)"
          },
          "amsgrad": {
            "type": "bool, optional",
            "description": "whether to use the AMSGrad variant of this"
          },
          "algorithm": {
            "type": "",
            "description": "from the paper `On the Convergence of Adam and Beyond`_\n(default: False)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "fused": {
            "type": "bool, optional",
            "description": "whether the fused implementation is used."
          },
          "Currently": {
            "type": "",
            "description": ", `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`"
          },
          "are": {
            "type": "",
            "description": "supported. (default: None)\n.. note:: The foreach and fused implementations are typically faster than the for-loop,"
          },
          "single": {
            "type": "",
            "description": "-tensor implementation, with fused being theoretically fastest with both"
          },
          "vertical": {
            "type": "",
            "description": "and horizontal fusion. As such, if the user has not specified either"
          },
          "flag": {
            "type": "i.e., when foreach = fused = None",
            "description": ", we will attempt defaulting to the foreach"
          },
          "implementation": {
            "type": "",
            "description": ", pass False for either foreach or fused.\n.. Note::"
          },
          "To": {
            "type": "",
            "description": "specify fused, pass True for fused. To force running the for-loop"
          },
          "A": {
            "type": "",
            "description": "prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Decoupled Weight Decay Regularization:"
          },
          "https": {
            "type": "",
            "description": "//openreview.net/forum?id=ryQu7f-RZ"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Adamax",
      "documentation": {
        "description": "Implements Adamax algorithm (a variant of Adam based on infinity norm).\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)},\n            \\: \\lambda \\text{ (weight decay)},                                                \\\\\n        &\\hspace{13mm}    \\epsilon \\text{ (epsilon)}                                          \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            u_0 \\leftarrow 0 \\text{ ( infinity norm)}                                 \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t      \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t               \\\\\n        &\\hspace{5mm}u_t      \\leftarrow   \\mathrm{max}(\\beta_2 u_{t-1}, |g_{t}|+\\epsilon)   \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\frac{\\gamma m_t}{(1-\\beta^t_1) u_t} \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 2e-3)"
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)\n.. _Adam\\: A Method for Stochastic Optimization:"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1412.6980"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Performs a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LBFGS",
      "documentation": {
        "description": "Implements L-BFGS algorithm.\n\nHeavily inspired by `minFunc\n<https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`_.\n\n.. warning::\n    This optimizer doesn't support per-parameter options and parameter\n    groups (there can be only one).\n\n.. warning::\n    Right now all parameters have to be on a single device. This will be\n    improved in the future.\n\n.. note::\n    This is a very memory intensive optimizer (it requires additional\n    ``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory\n    try reducing the history size, or use a different algorithm.",
        "parameters": {
          "params": {
            "type": "iterable",
            "description": "iterable of parameters to optimize. Parameters must be real."
          },
          "lr": {
            "type": "float",
            "description": "learning rate (default: 1)"
          },
          "max_iter": {
            "type": "int",
            "description": "maximal number of iterations per optimization step\n(default: 20)"
          },
          "max_eval": {
            "type": "int",
            "description": "maximal number of function evaluations per optimization"
          },
          "step": {
            "type": "default: max_iter * 1.25",
            "description": "."
          },
          "tolerance_grad": {
            "type": "float",
            "description": "termination tolerance on first order optimality\n(default: 1e-7)."
          },
          "tolerance_change": {
            "type": "float",
            "description": "termination tolerance on function"
          },
          "value": {
            "type": "",
            "description": "/parameter changes (default: 1e-9)."
          },
          "history_size": {
            "type": "int",
            "description": "update history size (default: 100)."
          },
          "line_search_fn": {
            "type": "str",
            "description": "either 'strong_wolfe' or None (default: None)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NAdam",
      "documentation": {
        "description": "Implements NAdam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma_t \\text{ (lr)}, \\: \\beta_1,\\beta_2 \\text{ (betas)},\n            \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n        &\\hspace{13mm} \\: \\lambda \\text{ (weight decay)}, \\:\\psi \\text{ (momentum decay)}    \\\\\n        &\\hspace{13mm} \\: \\textit{decoupled\\_weight\\_decay}, \\:\\textit{maximize}             \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0 \\leftarrow 0 \\text{ ( second moment)}                                 \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1}                                       \\\\\n        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{decoupled\\_weight\\_decay}                       \\\\\n        &\\hspace{15mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}                    \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n        &\\hspace{5mm} \\mu_t \\leftarrow \\beta_1 \\big(1 - \\frac{1}{2}  0.96^{t \\psi} \\big)     \\\\\n        &\\hspace{5mm} \\mu_{t+1} \\leftarrow \\beta_1 \\big(1 - \\frac{1}{2} 0.96^{(t+1)\\psi}\\big)\\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow \\mu_{t+1} m_t/(1-\\prod_{i=1}^{t+1}\\mu_i)\\\\[-1.ex]\n        & \\hspace{11mm} + (1-\\mu_t) g_t /(1-\\prod_{i=1}^{t} \\mu_{i})                         \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Incorporating Nesterov Momentum into Adam`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 2e-3)"
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square (default: (0.9, 0.999))"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "momentum_decay": {
            "type": "float, optional",
            "description": "momentum momentum_decay (default: 4e-3)"
          },
          "decoupled_weight_decay": {
            "type": "bool, optional",
            "description": "whether to use decoupled weight"
          },
          "decay": {
            "type": "",
            "description": "as in AdamW to obtain NAdamW (default: False)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)\n.. _Incorporating Nesterov Momentum into Adam:"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1711.05101"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Optimizer",
      "documentation": {
        "description": "Base class for all optimizers.\n\n.. warning::\n    Parameters need to be specified as collections that have a deterministic\n    ordering that is consistent between runs. Examples of objects that don't\n    satisfy those properties are sets and iterators over values of dictionaries.",
        "parameters": {
          "params": {
            "type": "iterable",
            "description": "an iterable of :class:`torch.Tensor` s or\n:class:`dict` s. Specifies what Tensors should be optimized."
          },
          "defaults": {
            "type": "",
            "description": "(dict): a dict containing default values of optimization"
          },
          "options": {
            "type": "used when a parameter group doesn't specify them",
            "description": "."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]",
          "documentation": {
            "description": "Perform a single optimization step to update parameter.",
            "parameters": {
              "closure": {
                "type": "Callable",
                "description": "A closure that reevaluates the model and"
              },
              "returns": {
                "type": "",
                "description": "the loss. Optional for most optimizers."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RAdam",
      "documentation": {
        "description": "Implements RAdam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\beta_1, \\beta_2\n            \\text{ (betas)}, \\: \\theta_0 \\text{ (params)}, \\:f(\\theta) \\text{ (objective)}, \\:\n            \\lambda \\text{ (weightdecay)}, \\:\\textit{maximize}                               \\\\\n        &\\hspace{13mm} \\epsilon \\text{ (epsilon)}, \\textit{decoupled\\_weight\\_decay}         \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0 \\leftarrow 0 \\text{ ( second moment)},                                       \\\\\n        &\\hspace{18mm} \\rho_{\\infty} \\leftarrow 2/(1-\\beta_2) -1                      \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}  \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{6mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{12mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{6mm}\\textbf{else}                                                           \\\\\n        &\\hspace{12mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{6mm} \\theta_t \\leftarrow \\theta_{t-1}                                       \\\\\n        &\\hspace{6mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{12mm}\\textbf{if} \\: \\textit{decoupled\\_weight\\_decay}                       \\\\\n        &\\hspace{18mm} \\theta_t \\leftarrow \\theta_{t} - \\gamma \\lambda \\theta_{t}            \\\\\n        &\\hspace{12mm}\\textbf{else}                                                          \\\\\n        &\\hspace{18mm} g_t \\leftarrow g_t + \\lambda \\theta_{t}                               \\\\\n        &\\hspace{6mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{6mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{6mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{6mm}\\rho_t \\leftarrow \\rho_{\\infty} -\n            2 t \\beta^t_2 /\\big(1-\\beta_2^t \\big)                                    \\\\[0.1.ex]\n        &\\hspace{6mm}\\textbf{if} \\: \\rho_t > 5                                               \\\\\n        &\\hspace{12mm} l_t \\leftarrow \\frac{\\sqrt{ (1-\\beta^t_2) }}{ \\sqrt{v_t} +\\epsilon  } \\\\\n        &\\hspace{12mm} r_t \\leftarrow\n  \\sqrt{\\frac{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2) \\rho_t}} \\\\\n        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t} r_t l_t        \\\\\n        &\\hspace{6mm}\\textbf{else}                                                           \\\\\n        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}                \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `On the variance of the adaptive learning rate and beyond`_.\n\nThis implementation provides an option to use either the original weight_decay implementation as in Adam\n(where the weight_decay is applied to the gradient) or the one from AdamW (where weight_decay is applied\nto the weight) through the decoupled_weight_decay option. When decoupled_weight_decay is set to False\n(default), it uses the original Adam style weight decay, otherwise, it uses the AdamW style which\ncorresponds more closely to the `author's implementation`_ in the RAdam paper. Further information\nabout decoupled weight decay can be found in `Decoupled Weight Decay Regularization`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-3)"
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square (default: (0.9, 0.999))"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "decoupled_weight_decay": {
            "type": "bool, optional",
            "description": "whether to use decoupled weight"
          },
          "decay": {
            "type": "",
            "description": "as in AdamW to obtain RAdamW (default: False)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)\n.. _On the variance of the adaptive learning rate and beyond:"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1711.05101"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RMSprop",
      "documentation": {
        "description": "Implements RMSprop algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\alpha \\text{ (alpha)}, \\: \\gamma \\text{ (lr)},\n            \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n        &\\hspace{13mm}   \\lambda \\text{ (weight decay)},\\: \\mu \\text{ (momentum)},\n            \\: centered, \\: \\epsilon \\text{ (epsilon)}                                       \\\\\n        &\\textbf{initialize} : v_0 \\leftarrow 0 \\text{ (square average)}, \\:\n            \\textbf{b}_0 \\leftarrow 0 \\text{ (buffer)}, \\: g^{ave}_0 \\leftarrow 0     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\alpha v_{t-1} + (1 - \\alpha) g^2_t\n            \\hspace{8mm}                                                                     \\\\\n        &\\hspace{5mm} \\tilde{v_t} \\leftarrow v_t                                             \\\\\n        &\\hspace{5mm}if \\: centered                                                          \\\\\n        &\\hspace{10mm} g^{ave}_t \\leftarrow g^{ave}_{t-1} \\alpha + (1-\\alpha) g_t            \\\\\n        &\\hspace{10mm} \\tilde{v_t} \\leftarrow \\tilde{v_t} -  \\big(g^{ave}_{t} \\big)^2        \\\\\n        &\\hspace{5mm}if \\: \\mu > 0                                                           \\\\\n        &\\hspace{10mm} \\textbf{b}_t\\leftarrow \\mu \\textbf{b}_{t-1} +\n            g_t/ \\big(\\sqrt{\\tilde{v_t}} +  \\epsilon \\big)                                   \\\\\n        &\\hspace{10mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\textbf{b}_t                \\\\\n        &\\hspace{5mm} else                                                                   \\\\\n        &\\hspace{10mm}\\theta_t      \\leftarrow   \\theta_{t-1} -\n            \\gamma  g_t/ \\big(\\sqrt{\\tilde{v_t}} + \\epsilon \\big)  \\hspace{3mm}              \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to\n`lecture notes <https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_ by G. Hinton.\nand centered version `Generating Sequences\nWith Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\nThe implementation here takes the square root of the gradient average before\nadding epsilon (note that TensorFlow interchanges these two operations). The effective\nlearning rate is thus :math:`\\gamma/(\\sqrt{v} + \\epsilon)` where :math:`\\gamma`\nis the scheduled learning rate and :math:`v` is the weighted moving average\nof the squared gradient.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-2)"
          },
          "alpha": {
            "type": "float, optional",
            "description": "smoothing constant (default: 0.99)"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "momentum": {
            "type": "float, optional",
            "description": "momentum factor (default: 0)"
          },
          "centered": {
            "type": "bool, optional",
            "description": "if ``True``, compute the centered RMSProp,"
          },
          "the": {
            "type": "",
            "description": "gradient is normalized by an estimation of its variance"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Rprop",
      "documentation": {
        "description": "Implements the resilient backpropagation algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\theta_0 \\in \\mathbf{R}^d \\text{ (params)},f(\\theta)\n            \\text{ (objective)},                                                             \\\\\n        &\\hspace{13mm}      \\eta_{+/-} \\text{ (etaplus, etaminus)}, \\Gamma_{max/min}\n            \\text{ (step sizes)}                                                             \\\\\n        &\\textbf{initialize} :   g^0_{prev} \\leftarrow 0,\n            \\: \\eta_0 \\leftarrow \\text{lr (learning rate)}                                   \\\\\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\textbf{for} \\text{  } i = 0, 1, \\ldots, d-1 \\: \\mathbf{do}            \\\\\n        &\\hspace{10mm}  \\textbf{if} \\:   g^i_{prev} g^i_t  > 0                               \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{min}(\\eta^i_{t-1} \\eta_{+},\n            \\Gamma_{max})                                                                    \\\\\n        &\\hspace{10mm}  \\textbf{else if}  \\:  g^i_{prev} g^i_t < 0                           \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{max}(\\eta^i_{t-1} \\eta_{-},\n            \\Gamma_{min})                                                                    \\\\\n        &\\hspace{15mm}  g^i_t \\leftarrow 0                                                   \\\\\n        &\\hspace{10mm}  \\textbf{else}  \\:                                                    \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\eta^i_{t-1}                                     \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1}- \\eta_t \\mathrm{sign}(g_t)             \\\\\n        &\\hspace{5mm}g_{prev} \\leftarrow  g_t                                                \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to the paper\n`A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm\n<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417>`_.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, optional",
            "description": "learning rate (default: 1e-2)"
          },
          "etas": {
            "type": "Tuple[float, float], optional",
            "description": "pair of (etaminus, etaplus), that"
          },
          "are": {
            "type": "",
            "description": "multiplicative increase and decrease factors\n(default: (0.5, 1.2))"
          },
          "step_sizes": {
            "type": "Tuple[float, float], optional",
            "description": "a pair of minimal and"
          },
          "maximal": {
            "type": "",
            "description": "allowed step sizes (default: (1e-6, 50))"
          },
          "capturable": {
            "type": "bool, optional",
            "description": "whether this instance is safe to"
          },
          "capture": {
            "type": "",
            "description": "in a CUDA graph. Passing True can impair ungraphed performance,"
          },
          "so": {
            "type": "",
            "description": "if you don't intend to graph capture this instance, leave it False\n(default: False)"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": "through the optimizer at a time or switch this flag to False (default: None)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SGD",
      "documentation": {
        "description": "Implements stochastic gradient descent (optionally with momentum).\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-3)"
          },
          "momentum": {
            "type": "float, optional",
            "description": "momentum factor (default: 0)"
          },
          "dampening": {
            "type": "float, optional",
            "description": "dampening for momentum (default: 0)"
          },
          "weight_decay": {
            "type": "float, optional",
            "description": "weight decay (L2 penalty) (default: 0)"
          },
          "nesterov": {
            "type": "bool, optional",
            "description": "enables Nesterov momentum. Only applicable"
          },
          "when": {
            "type": "",
            "description": "momentum is non-zero. (default: False)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "foreach": {
            "type": "",
            "description": "over the for-loop implementation on CUDA, since it is usually"
          },
          "is": {
            "type": "",
            "description": "used. If unspecified by the user (so foreach is None), we will try to use"
          },
          "significantly": {
            "type": "",
            "description": "more performant. Note that the foreach implementation uses\n~ sizeof(params) more peak memory than the for-loop version due to the intermediates"
          },
          "being": {
            "type": "",
            "description": "a tensorlist vs just one tensor. If memory is prohibitive, batch fewer"
          },
          "parameters": {
            "type": "",
            "description": ", gradient, velocity, and momentum respectively."
          },
          "differentiable": {
            "type": "bool, optional",
            "description": "whether autograd should"
          },
          "occur": {
            "type": "",
            "description": "through the optimizer step in training. Otherwise, the step()"
          },
          "function": {
            "type": "",
            "description": "runs in a torch.no_grad() context. Setting to True can impair"
          },
          "performance": {
            "type": "",
            "description": ", so leave it False if you don't intend to run autograd"
          },
          "through": {
            "type": "",
            "description": "this instance (default: False)"
          },
          "fused": {
            "type": "bool, optional",
            "description": "whether the fused implementation is used."
          },
          "Currently": {
            "type": "",
            "description": ", `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`"
          },
          "are": {
            "type": "",
            "description": "supported. (default: None)\n.. note:: The foreach and fused implementations are typically faster than the for-loop,"
          },
          "single": {
            "type": "",
            "description": "-tensor implementation, with fused being theoretically fastest with both"
          },
          "vertical": {
            "type": "",
            "description": "and horizontal fusion. As such, if the user has not specified either"
          },
          "flag": {
            "type": "i.e., when foreach = fused = None",
            "description": ", we will attempt defaulting to the foreach"
          },
          "implementation": {
            "type": "",
            "description": ", pass False for either foreach or fused."
          },
          "To": {
            "type": "",
            "description": "specify fused, pass True for fused. To force running the for-loop"
          },
          "Example": {
            "type": "",
            "description": ">>> # xdoctest: +SKIP\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> optimizer.zero_grad()\n>>> loss_fn(model(input), target).backward()\n>>> optimizer.step()"
          },
          "__": {
            "type": "",
            "description": "http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n.. note::"
          },
          "The": {
            "type": "",
            "description": "Nesterov version is analogously modified."
          },
          "Sutskever": {
            "type": "",
            "description": "et al. and implementations in some other frameworks."
          },
          "Considering": {
            "type": "",
            "description": "the specific case of Momentum, the update can be written as\n.. math::\n\\begin{aligned}"
          },
          "v_": {
            "type": "",
            "description": "{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\"
          },
          "p_": {
            "type": "",
            "description": "{t+1} & = p_{t} - v_{t+1}.\n\\end{aligned}"
          },
          "where": {
            "type": "",
            "description": "math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the"
          },
          "This": {
            "type": "",
            "description": "is in contrast to Sutskever et al. and"
          },
          "other": {
            "type": "",
            "description": "frameworks which employ an update of the form\n.. math::\n\\begin{aligned}"
          },
          "Moreover": {
            "type": "",
            "description": ", the initial value of the momentum buffer is set to the"
          },
          "gradient": {
            "type": "",
            "description": "value at the first step. This is in contrast to some other"
          },
          "frameworks": {
            "type": "",
            "description": "that initialize it to all zeros."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> optimizer.zero_grad()\n    >>> loss_fn(model(input), target).backward()\n    >>> optimizer.step()\n\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et al. and implementations in some other frameworks.\n\n    Considering the specific case of Momentum, the update can be written as\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n\n    This is in contrast to Sutskever et al. and\n    other frameworks which employ an update of the form\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n\n    The Nesterov version is analogously modified.\n\n    Moreover, the initial value of the momentum buffer is set to the\n    gradient value at the first step. This is in contrast to some other\n    frameworks that initialize it to all zeros."
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SparseAdam",
      "documentation": {
        "description": "SparseAdam implements a masked version of the Adam algorithm\nsuitable for sparse gradients. Currently, due to implementation constraints (explained\nbelow), SparseAdam is only intended for a narrow subset of use cases, specifically\nparameters of a dense layout with gradients of a sparse layout. This occurs in a\nspecial case where the module backwards produces grads already in a sparse layout.\nOne example NN module that behaves as such is ``nn.Embedding(sparse=True)``.\n\nSparseAdam approximates the Adam algorithm by masking out the parameter and moment\nupdates corresponding to the zero values in the gradients. Whereas the Adam algorithm\nwill update the first moment, the second moment, and the parameters based on all values\nof the gradients, SparseAdam only updates the moments and parameters corresponding\nto the non-zero values of the gradients.\n\nA simplified way of thinking about the `intended` implementation is as such:\n\n1. Create a mask of the non-zero values in the sparse gradients. For example,\n   if your gradient looks like [0, 5, 0, 0, 9], the mask would be [0, 1, 0, 0, 1].\n2. Apply this mask over the running moments and do computation on only the\n   non-zero values.\n3. Apply this mask over the parameters and only apply an update on non-zero values.\n\nIn actuality, we use sparse layout Tensors to optimize this approximation, which means the\nmore gradients that are masked by not being materialized, the more performant the optimization.\nSince we rely on using sparse layout tensors, we infer that any materialized value in the\nsparse layout is non-zero and we do NOT actually verify that all values are not zero!\nIt is important to not conflate a semantically sparse tensor (a tensor where many\nof its values are zeros) with a sparse layout tensor (a tensor where ``.is_sparse``\nreturns ``True``). The SparseAdam approximation is intended for `semantically` sparse\ntensors and the sparse layout is only a implementation detail. A clearer implementation\nwould be to use MaskedTensors, but those are experimental.\n\n\n.. note::\n\n    If you suspect your gradients are semantically sparse (but do not have sparse\n    layout), this variant may not be the best for you. Ideally, you want to avoid\n    materializing anything that is suspected to be sparse in the first place, since\n    needing to convert all your grads from dense layout to sparse layout may outweigh\n    the performance gain. Here, using Adam may be the best alternative, unless you\n    can easily rig up your module to output sparse grads similar to\n    ``nn.Embedding(sparse=True)``. If you insist on converting your grads, you can do\n    so by manually overriding your parameters' ``.grad`` fields with their sparse\n    equivalents before calling ``.step()``.",
        "parameters": {
          "params": {
            "type": "",
            "description": ", instead of minimizing (default: False)\n.. _Adam\\: A Method for Stochastic Optimization:"
          },
          "or": {
            "type": "",
            "description": "iterable of dicts defining parameter groups. When using named_parameters,"
          },
          "all": {
            "type": "",
            "description": "parameters in all groups should be named"
          },
          "lr": {
            "type": "float, Tensor, optional",
            "description": "learning rate (default: 1e-3)"
          },
          "betas": {
            "type": "Tuple[float, float], optional",
            "description": "coefficients used for computing"
          },
          "running": {
            "type": "",
            "description": "averages of gradient and its square (default: (0.9, 0.999))"
          },
          "eps": {
            "type": "float, optional",
            "description": "term added to the denominator to improve"
          },
          "numerical": {
            "type": "",
            "description": "stability (default: 1e-8)"
          },
          "maximize": {
            "type": "bool, optional",
            "description": "maximize the objective with respect to the"
          },
          "https": {
            "type": "",
            "description": "//arxiv.org/abs/1412.6980"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "OptimizerPostHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "OptimizerPreHook",
          "signature": "Callable(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_param_group",
          "signature": "add_param_group(self, param_group: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Add a param group to the :class:`Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can be made\ntrainable and added to the :class:`Optimizer` as training progresses.",
            "parameters": {
              "param_group": {
                "type": "dict",
                "description": "Specifies what Tensors should be optimized along with group"
              },
              "specific": {
                "type": "",
                "description": "optimization options."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Dict[str, Any]) -> None",
          "documentation": {
            "description": "Load the optimizer state.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "optimizer state. Should be an object returned"
              },
              "from": {
                "type": "",
                "description": "a call to :meth:`state_dict`.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "names of the parameters (if they exist under the \"param_names\" key of each param group"
              },
              "in": {
                "type": "",
                "description": "meth:`state_dict`) will not affect the loading process."
              },
              "To": {
                "type": "",
                "description": "use the parameters' names for custom cases (such as when the parameters in the loaded state dict"
              },
              "differ": {
                "type": "",
                "description": "from those initialized in the optimizer),"
              },
              "a": {
                "type": "",
                "description": "custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict"
              },
              "accordingly": {
                "type": "",
                "description": "."
              },
              "If": {
                "type": "",
                "description": "``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override"
              },
              "the": {
                "type": "",
                "description": "optimizer ``param_names`` will remain unchanged."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "profile_hook_step",
          "signature": "profile_hook_step(func: Callable[~_P, ~R]) -> Callable[~_P, ~R]",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict post-hook which will be called after\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\n\nThe hook will be called with argument ``self`` after calling\n``load_state_dict`` on ``self``. The registered hook can be used to\nperform post-processing after ``load_state_dict`` has loaded the\n``state_dict``.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a load_state_dict pre-hook which will be called before\n:meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\nfollowing signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe ``optimizer`` argument is the optimizer instance being used and the\n``state_dict`` argument is a shallow copy of the ``state_dict`` the user\npassed in to ``load_state_dict``. The hook may modify the state_dict inplace\nor optionally return a new one. If a state_dict is returned, it will be used\nto be loaded into the optimizer.\n\nThe hook will be called with argument ``self`` and ``state_dict`` before\ncalling ``load_state_dict`` on ``self``. The registered hook can be used to\nperform pre-processing before the ``load_state_dict`` call is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``load_state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer, state_dict) -> state_dict or None\n\nThe hook will be called with arguments ``self`` and ``state_dict`` after generating\na ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\nreturn a new one. The registered hook can be used to perform post-processing\non the ``state_dict`` before it is returned.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided post ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered post-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "post": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n\nIt should have the following signature::\n\n    hook(optimizer) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.\nThe hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\nThe registered hook can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If True, the provided pre ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "the already registered pre-hooks on ``state_dict``. Otherwise,"
              },
              "the": {
                "type": "",
                "description": "provided ``hook`` will be fired after all the already registered"
              },
              "pre": {
                "type": "",
                "description": "-hooks. (default: False)"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemoveableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_post_hook",
          "signature": "register_step_post_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step post hook which will be called after optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None\n\nThe ``optimizer`` argument is the optimizer instance being used.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_step_pre_hook",
          "signature": "register_step_pre_hook(self, hook: Callable[[Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register an optimizer step pre hook which will be called before optimizer step.\n\nIt should have the following signature::\n\n    hook(optimizer, args, kwargs) -> None or modified args and kwargs\n\nThe ``optimizer`` argument is the optimizer instance being used. If\nargs and kwargs are modified by the pre-hook, then the transformed\nvalues are returned as a tuple containing the new_args and new_kwargs.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self) -> Dict[str, Any]",
          "documentation": {
            "description": "Return the state of the optimizer as a :class:`dict`.\n\nIt contains two entries:\n\n* ``state``: a Dict holding current optimization state. Its content\n    differs between optimizer classes, but some common characteristics\n    hold. For example, state is saved per parameter, and the parameter\n    itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n    to a Dict with state corresponding to each parameter.\n* ``param_groups``: a List containing all parameter groups where each\n    parameter group is a Dict. Each parameter group contains metadata\n    specific to the optimizer, such as learning rate and weight decay,\n    as well as a List of parameter IDs of the parameters in the group.\n    If a param group was initialized with ``named_parameters()`` the names\n    content will also be saved in the state dict.\n\nNOTE: The parameter IDs may look like indices but they are just IDs\nassociating state with param_group. When loading from a state_dict,\nthe optimizer will zip the param_group ``params`` (int IDs) and the\noptimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\nmatch state WITHOUT additional verification.\n\nA returned state dict might look something like:\n\n.. code-block:: text\n\n    {\n        'state': {\n            0: {'momentum_buffer': tensor(...), ...},\n            1: {'momentum_buffer': tensor(...), ...},\n            2: {'momentum_buffer': tensor(...), ...},\n            3: {'momentum_buffer': tensor(...), ...}\n        },\n        'param_groups': [\n            {\n                'lr': 0.01,\n                'weight_decay': 0,\n                ...\n                'params': [0]\n                'param_names' ['param0']  (optional)\n            },\n            {\n                'lr': 0.001,\n                'weight_decay': 0.5,\n                ...\n                'params': [1, 2, 3]\n                'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n            }\n        ]\n    }",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "step",
          "signature": "step(self, closure=None)",
          "documentation": {
            "description": "Perform a single optimization step.",
            "parameters": {
              "closure": {
                "type": "Callable, optional",
                "description": "A closure that reevaluates the model"
              },
              "and": {
                "type": "",
                "description": "returns the loss."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset the gradients of all optimized :class:`torch.Tensor` s.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "This": {
                "type": "",
                "description": "will in general have lower memory footprint, and can modestly improve performance."
              },
              "However": {
                "type": "",
                "description": ", it changes certain behaviors. For example:"
              },
              "1": {
                "type": "",
                "description": ". When the user tries to access a gradient and perform manual ops on it,"
              },
              "a": {
                "type": "",
                "description": "None attribute or a Tensor full of 0s will behave differently."
              },
              "2": {
                "type": "",
                "description": ". If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s"
              },
              "are": {
                "type": "",
                "description": "guaranteed to be None for params that did not receive a gradient."
              },
              "3": {
                "type": "",
                "description": ". ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n(in one case it does the step with a gradient of 0 and in the other it skips"
              },
              "the": {
                "type": "",
                "description": "step altogether)."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}