{
  "description": "No description available",
  "functions": [
    {
      "name": "Callable",
      "signature": "Callable(*args, **kwargs)",
      "documentation": {
        "description": "Deprecated alias to collections.abc.Callable.\n\nCallable[[int], str] signifies a function that takes a single\nparameter of type int and returns a str.\n\nThe subscription syntax must always be used with exactly two\nvalues: the argument list and the return type.\nThe argument list must be a list of types, a ParamSpec,\nConcatenate or ellipsis. The return type must be a single type.\n\nThere is no syntax to indicate optional or keyword arguments;\nsuch function types are rarely used as callback types.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Collection",
      "signature": "Collection(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of collections.abc.Collection.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Mapping",
      "signature": "Mapping(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of collections.abc.Mapping.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Sequence",
      "signature": "Sequence(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of collections.abc.Sequence.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "dynamo_export",
      "signature": "dynamo_export(model: 'torch.nn.Module | Callable | torch.export.ExportedProgram', /, *model_args, export_options: 'ExportOptions | None' = None, **model_kwargs) -> 'ONNXProgram'",
      "documentation": {
        "description": "Export a torch.nn.Module to an ONNX graph.",
        "parameters": {
          "model": {
            "type": "",
            "description": "The PyTorch model to be exported to ONNX."
          },
          "model_args": {
            "type": "",
            "description": "Positional inputs to ``model``."
          },
          "model_kwargs": {
            "type": "",
            "description": "Keyword inputs to ``model``."
          },
          "export_options": {
            "type": "",
            "description": "Options to influence the export to ONNX."
          }
        },
        "returns": "An in-memory representation of the exported ONNX model.\n\n**Example 1 - Simplest export**\n::\n\n    class MyModel(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.linear = torch.nn.Linear(2, 2)\n\n        def forward(self, x, bias=None):\n            out = self.linear(x)\n            out = out + bias\n            return out\n\n\n    model = MyModel()\n    kwargs = {\"bias\": 3.0}\n    args = (torch.randn(2, 2, 2),)\n    onnx_program = torch.onnx.dynamo_export(model, *args, **kwargs).save(\n        \"my_simple_model.onnx\"\n    )\n\n**Example 2 - Exporting with dynamic shapes**\n::\n\n    # The previous model can be exported with dynamic shapes\n    export_options = torch.onnx.ExportOptions(dynamic_shapes=True)\n    onnx_program = torch.onnx.dynamo_export(\n        model, *args, **kwargs, export_options=export_options\n    )\n    onnx_program.save(\"my_dynamic_model.onnx\")",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_fake_mode",
      "signature": "enable_fake_mode()",
      "documentation": {
        "description": "Enable fake mode for the duration of the context.\n\nInternally it instantiates a :class:`torch._subclasses.fake_tensor.FakeTensorMode` context manager\nthat converts user input and model parameters into :class:`torch._subclasses.fake_tensor.FakeTensor`.\n\nA :class:`torch._subclasses.fake_tensor.FakeTensor`\nis a :class:`torch.Tensor` with the ability to run PyTorch code without having to\nactually do computation through tensors allocated on a ``meta`` device. Because\nthere is no actual data being allocated on the device, this API allows for\ninitializing and exporting large models without the actual memory footprint needed for executing it.\n\nIt is highly recommended to initialize the model in fake mode when exporting models that\nare too large to fit into memory.\n\nExample::\n\n    # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n    >>> import torch\n    >>> class MyModel(torch.nn.Module):  # Model with a parameter\n    ...     def __init__(self) -> None:\n    ...         super().__init__()\n    ...         self.weight = torch.nn.Parameter(torch.tensor(42.0))\n    ...     def forward(self, x):\n    ...         return self.weight + x\n    >>> with torch.onnx.enable_fake_mode():\n    ...     # When initialized in fake mode, the model's parameters are fake tensors\n    ...     # They do not take up memory so we can initialize large models\n    ...     my_nn_module = MyModel()\n    ...     arg1 = torch.randn(2, 2, 2)\n    >>> onnx_program = torch.onnx.export(my_nn_module, (arg1,), dynamo=True)\n    >>> # Saving model WITHOUT initializers (only the architecture)\n    >>> onnx_program.save(\n    ...     \"my_model_without_initializers.onnx\",\n    ...     include_initializers=False,\n    ...     keep_initializers_as_inputs=True,\n    ... )\n    >>> # Saving model WITH initializers after applying concrete weights\n    >>> onnx_program.apply_weights({\"weight\": torch.tensor(42.0)})\n    >>> onnx_program.save(\"my_model_with_initializers.onnx\")\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "export",
      "signature": "export(model: 'torch.nn.Module | torch.export.ExportedProgram | torch.jit.ScriptModule | torch.jit.ScriptFunction', args: 'tuple[Any, ...]' = (), f: 'str | os.PathLike | None' = None, *, kwargs: 'dict[str, Any] | None' = None, export_params: 'bool' = True, verbose: 'bool | None' = None, input_names: 'Sequence[str] | None' = None, output_names: 'Sequence[str] | None' = None, opset_version: 'int | None' = None, dynamic_axes: 'Mapping[str, Mapping[int, str]] | Mapping[str, Sequence[int]] | None' = None, keep_initializers_as_inputs: 'bool' = False, dynamo: 'bool' = False, external_data: 'bool' = True, dynamic_shapes: 'dict[str, Any] | tuple[Any, ...] | list[Any] | None' = None, custom_translation_table: 'dict[Callable, Callable | Sequence[Callable]] | None' = None, report: 'bool' = False, optimize: 'bool' = False, verify: 'bool' = False, profile: 'bool' = False, dump_exported_program: 'bool' = False, artifacts_dir: 'str | os.PathLike' = '.', fallback: 'bool' = False, training: '_C_onnx.TrainingMode' = <TrainingMode.EVAL: 0>, operator_export_type: '_C_onnx.OperatorExportTypes' = <OperatorExportTypes.ONNX: 0>, do_constant_folding: 'bool' = True, custom_opsets: 'Mapping[str, int] | None' = None, export_modules_as_functions: 'bool | Collection[type[torch.nn.Module]]' = False, autograd_inlining: 'bool' = True, **_: 'Any') -> 'ONNXProgram | None'",
      "documentation": {
        "description": "Exports a model into ONNX format.",
        "parameters": {
          "model": {
            "type": "",
            "description": "The model to be exported."
          },
          "args": {
            "type": "",
            "description": "Example positional inputs. Any non-Tensor arguments will be hard-coded into the"
          },
          "exported": {
            "type": "",
            "description": "program. This option is only valid when dynamo is True."
          },
          "in": {
            "type": "",
            "description": "the order they occur in the tuple."
          },
          "f": {
            "type": "",
            "description": "Path to the output ONNX model file. E.g. \"model.onnx\"."
          },
          "kwargs": {
            "type": "",
            "description": "Optional example keyword inputs."
          },
          "export_params": {
            "type": "",
            "description": "If false, parameters (weights) will not be exported."
          },
          "verbose": {
            "type": "",
            "description": "Whether to enable verbose logging."
          },
          "input_names": {
            "type": "",
            "description": "=[\"x\"],"
          },
          "output_names": {
            "type": "",
            "description": "=[\"sum\"],"
          },
          "opset_version": {
            "type": "",
            "description": "The version of the\n`default (ai.onnx) opset <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_"
          },
          "to": {
            "type": "",
            "description": "determine whether or not to replace the function with a particular fused kernel."
          },
          "dynamic_axes": {
            "type": "",
            "description": "is used when dynamo=False."
          },
          "By": {
            "type": "",
            "description": "default the exported model will have the shapes of all input and output tensors"
          },
          "set": {
            "type": "",
            "description": "to exactly match those given in ``args``. To specify axes of tensors as"
          },
          "dynamic": {
            "type": "i.e. known only at run-time",
            "description": ", set ``dynamic_axes`` to a dict with schema:\n* KEY (str): an input or output name. Each name must also be provided in ``input_names`` or\n``output_names``.\n* VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a"
          },
          "list": {
            "type": "",
            "description": ", each element is an axis index."
          },
          "For": {
            "type": "",
            "description": "example::"
          },
          "class": {
            "type": "",
            "description": "SumModule(torch.nn.Module):"
          },
          "def": {
            "type": "",
            "description": "forward(self, x):"
          },
          "return": {
            "type": "",
            "description": "torch.sum(x, dim=1)"
          },
          "torch": {
            "type": "",
            "description": ".onnx.export("
          },
          "SumModule": {
            "type": "",
            "description": ",\n(torch.ones(2, 2),),\n\"onnx.pb\","
          },
          "Produces": {
            "type": "",
            "description": ":"
          },
          "input": {
            "type": "",
            "description": "{"
          },
          "name": {
            "type": "",
            "description": "\"sum\"\n..."
          },
          "shape": {
            "type": "",
            "description": "{"
          },
          "dim": {
            "type": "",
            "description": "{"
          },
          "dim_value": {
            "type": "",
            "description": "2  # axis 1\n..."
          },
          "output": {
            "type": "",
            "description": "{"
          },
          "While": {
            "type": "",
            "description": ":"
          },
          "dim_param": {
            "type": "",
            "description": "\"sum_dynamic_axes_1\"  # axis 0\n..."
          },
          "keep_initializers_as_inputs": {
            "type": "",
            "description": "If True, all the"
          },
          "initializers": {
            "type": "typically corresponding to model weights",
            "description": "in the"
          },
          "then": {
            "type": "",
            "description": "initializers are not added as inputs to the graph, and only"
          },
          "the": {
            "type": "",
            "description": "opset version is set to 1. Only custom opset domain name and version should be"
          },
          "Set": {
            "type": "",
            "description": "it to False if the weights are static to allow for better optimizations\n(e.g. constant folding) by backends/runtimes."
          },
          "dynamo": {
            "type": "",
            "description": "Whether to export the model with ``torch.export`` ExportedProgram instead of TorchScript."
          },
          "external_data": {
            "type": "",
            "description": "Whether to save the model weights as an external data file."
          },
          "This": {
            "type": "",
            "description": "feature requires ``opset_version`` >= 15, otherwise the export will fail. This is because\n``opset_version`` < 15 implies IR version < 8, which means no local function support."
          },
          "When": {
            "type": "",
            "description": "False, the weights are saved in the ONNX file with the model architecture."
          },
          "dynamic_shapes": {
            "type": "",
            "description": "A dictionary or a tuple of dynamic shapes for the model inputs. Refer to\n:func:`torch.export.export` for more details. This is only used (and preferred) when dynamo is True."
          },
          "Note": {
            "type": "",
            "description": "that dynamic_shapes is designed to be used when the model is exported with dynamo=True, while"
          },
          "custom_translation_table": {
            "type": "",
            "description": "A dictionary of custom decompositions for operators in the model."
          },
          "The": {
            "type": "",
            "description": "dictionary should have the callable target in the fx Node as the key (e.g. ``torch.ops.aten.stft.default``),"
          },
          "and": {
            "type": "",
            "description": "the value should be a function that builds that graph using ONNX Script. This option"
          },
          "is": {
            "type": "",
            "description": "only valid when dynamo is True."
          },
          "report": {
            "type": "",
            "description": "Whether to generate a markdown report for the export process. This option"
          },
          "optimize": {
            "type": "",
            "description": "Whether to optimize the exported model. This option"
          },
          "verify": {
            "type": "",
            "description": "Whether to verify the exported model using ONNX Runtime. This option"
          },
          "profile": {
            "type": "",
            "description": "Whether to profile the export process. This option"
          },
          "dump_exported_program": {
            "type": "",
            "description": "Whether to dump the :class:`torch.export.ExportedProgram` to a file."
          },
          "artifacts_dir": {
            "type": "",
            "description": "The directory to save the debugging artifacts like the report and the serialized"
          },
          "fallback": {
            "type": "",
            "description": "Whether to fallback to the TorchScript exporter if the dynamo exporter fails."
          },
          "recommended": {
            "type": "",
            "description": "to set dynamic_axes even when dynamic_shapes is provided."
          },
          "training": {
            "type": "",
            "description": "Deprecated option. Instead, set the training mode of the model before exporting."
          },
          "operator_export_type": {
            "type": "",
            "description": "Deprecated option. Only ONNX is supported."
          },
          "do_constant_folding": {
            "type": "",
            "description": "Deprecated option."
          },
          "custom_opsets": {
            "type": "",
            "description": "Deprecated."
          },
          "A": {
            "type": "",
            "description": "dictionary:\n* KEY (str): opset domain name\n* VALUE (int): opset version"
          },
          "If": {
            "type": "",
            "description": "a custom opset is referenced by ``model`` but not mentioned in this dictionary,"
          },
          "indicated": {
            "type": "",
            "description": "through this argument."
          },
          "export_modules_as_functions": {
            "type": "",
            "description": "Deprecated option."
          },
          "Flag": {
            "type": "",
            "description": "used to control whether to inline autograd functions."
          },
          "exporting": {
            "type": "",
            "description": "all ``nn.Module`` forward calls as local functions in ONNX. Or a set to indicate the"
          },
          "particular": {
            "type": "",
            "description": "types of modules to export as local functions in ONNX."
          },
          "Module": {
            "type": "",
            "description": "variables will be exported as function attributes. There are two categories of function"
          },
          "attributes": {
            "type": "",
            "description": "."
          },
          "1": {
            "type": "",
            "description": ". Annotated attributes: class variables that have type annotations via\n`PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_"
          },
          "will": {
            "type": "",
            "description": "have prefix \"inferred::\". This is to differentiate from predefined attributes retrieved from"
          },
          "Annotated": {
            "type": "",
            "description": "attributes are not used inside the subgraph of ONNX local function because"
          },
          "they": {
            "type": "",
            "description": "are not created by PyTorch JIT tracing, but they may be used by consumers"
          },
          "2": {
            "type": "",
            "description": ". Inferred attributes: variables that are used by operators inside the module. Attribute names"
          },
          "python": {
            "type": "",
            "description": "module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\n* ``False`` (default): export ``nn.Module`` forward calls as fine grained nodes.\n* ``True``: export all ``nn.Module`` forward calls as local function nodes.\n* Set of type of nn.Module: export ``nn.Module`` forward calls as local function nodes,"
          },
          "only": {
            "type": "",
            "description": "if the type of the ``nn.Module`` is found in the set."
          },
          "autograd_inlining": {
            "type": "",
            "description": "Deprecated."
          },
          "Refer": {
            "type": "",
            "description": "to https://github.com/pytorch/pytorch/pull/74765 for more details."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_in_onnx_export",
      "signature": "is_in_onnx_export() -> 'bool'",
      "documentation": {
        "description": "Returns whether it is in the middle of ONNX export.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_onnxrt_backend_supported",
      "signature": "is_onnxrt_backend_supported() -> bool",
      "documentation": {
        "description": "Returns ``True`` if ONNX Runtime dependencies are installed and usable\nto support TorchDynamo backend integration; ``False`` otherwise.\n\nExample::\n\n    # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n    >>> import torch\n    >>> if torch.onnx.is_onnxrt_backend_supported():\n    ...     @torch.compile(backend=\"onnxrt\")\n    ...     def f(x):\n    ...             return x * x\n    ...     print(f(torch.randn(10)))\n    ... else:\n    ...     print(\"pip install onnx onnxscript onnxruntime\")\n    ...",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "register_custom_op_symbolic",
      "signature": "register_custom_op_symbolic(symbolic_name: 'str', symbolic_fn: 'Callable', opset_version: 'int')",
      "documentation": {
        "description": "Registers a symbolic function for a custom operator.\n\nWhen the user registers symbolic for custom/contrib ops,\nit is highly recommended to add shape inference for that operator via setType API,\notherwise the exported graph may have incorrect shape inference in some extreme cases.\nAn example of setType is `test_aten_embedding_2` in `test_operators.py`.\n\nSee \"Custom Operators\" in the module documentation for an example usage.",
        "parameters": {
          "symbolic_name": {
            "type": "str",
            "description": "The name of the custom operator in \"<domain>::<op>\""
          },
          "format": {
            "type": "",
            "description": "."
          },
          "symbolic_fn": {
            "type": "Callable",
            "description": "A function that takes in the ONNX graph and"
          },
          "the": {
            "type": "",
            "description": "input arguments to the current operator, and returns new"
          },
          "operator": {
            "type": "",
            "description": "nodes to add to the graph."
          },
          "opset_version": {
            "type": "int",
            "description": "The ONNX opset version in which to register."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "select_model_mode_for_export",
      "signature": "select_model_mode_for_export(model, mode: '_C_onnx.TrainingMode')",
      "documentation": {
        "description": "A context manager to temporarily set the training mode of ``model``\nto ``mode``, resetting it when we exit the with-block.",
        "parameters": {
          "model": {
            "type": "",
            "description": "Same type and meaning as ``model`` arg to :func:`export`."
          },
          "mode": {
            "type": "",
            "description": "Same type and meaning as ``training`` arg to :func:`export`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "unregister_custom_op_symbolic",
      "signature": "unregister_custom_op_symbolic(symbolic_name: 'str', opset_version: 'int')",
      "documentation": {
        "description": "Unregisters ``symbolic_name``.\n\nSee \"Custom Operators\" in the module documentation for an example usage.",
        "parameters": {
          "symbolic_name": {
            "type": "str",
            "description": "The name of the custom operator in \"<domain>::<op>\""
          },
          "format": {
            "type": "",
            "description": "."
          },
          "opset_version": {
            "type": "int",
            "description": "The ONNX opset version in which to unregister."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "Any",
      "documentation": {
        "description": "Special type indicating an unconstrained type.\n\n- Any is compatible with every type.\n- Any assumed to have all methods.\n- All values assumed to be instances of Any.\n\nNote that all the above statements are true from the point of view of\nstatic type checkers. At runtime, Any should not be used with instance\nchecks.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "DiagnosticOptions",
      "documentation": {
        "description": "Options for diagnostic context.\n\nAttributes:\n    verbosity_level: Set the amount of information logged for each diagnostics,\n        equivalent to the 'level' in Python logging module.\n    warnings_as_errors: When True, warning diagnostics are treated as error diagnostics.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "ExportOptions",
      "documentation": {
        "description": "Options to influence the TorchDynamo ONNX exporter.\n\nAttributes:\n    dynamic_shapes: Shape information hint for input/output tensors.\n        When ``None``, the exporter determines the most compatible setting.\n        When ``True``, all input shapes are considered dynamic.\n        When ``False``, all input shapes are considered static.\n    diagnostic_options: The diagnostic options for the exporter.\n    fake_context: The fake context used for symbolic tracing.\n    onnx_registry: The ONNX registry used to register ATen operators to ONNX functions.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "JitScalarType",
      "documentation": {
        "description": "Scalar types defined in torch.\n\nUse ``JitScalarType`` to convert from torch and JIT scalar types to ONNX scalar types.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n    >>> # xdoctest: +IGNORE_WANT(\"win32 has different output\")\n    >>> JitScalarType.from_value(torch.ones(1, 2)).onnx_type()\n    TensorProtoDataType.FLOAT\n\n    >>> JitScalarType.from_value(torch_c_value_with_type_float).onnx_type()\n    TensorProtoDataType.FLOAT\n\n    >>> JitScalarType.from_dtype(torch.get_default_dtype).onnx_type()\n    TensorProtoDataType.FLOAT"
      },
      "methods": [
        {
          "name": "as_integer_ratio",
          "signature": "as_integer_ratio(self, /)",
          "documentation": {
            "description": "Return a pair of integers, whose ratio is equal to the original int.\n\nThe ratio is in lowest terms and has a positive denominator.\n\n>>> (10).as_integer_ratio()\n(10, 1)\n>>> (-10).as_integer_ratio()\n(-10, 1)\n>>> (0).as_integer_ratio()\n(0, 1)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_count",
          "signature": "bit_count(self, /)",
          "documentation": {
            "description": "Number of ones in the binary representation of the absolute value of self.\n\nAlso known as the population count.\n\n>>> bin(13)\n'0b1101'\n>>> (13).bit_count()\n3",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_length",
          "signature": "bit_length(self, /)",
          "documentation": {
            "description": "Number of bits necessary to represent self in binary.\n\n>>> bin(37)\n'0b100101'\n>>> (37).bit_length()\n6",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "conjugate(...)",
          "documentation": {
            "description": "Returns self, the complex conjugate of any int.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "from_bytes",
          "signature": "from_bytes(bytes, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return the integer represented by the given array of bytes.\n\nbytes\n  Holds the array of bytes to convert.  The argument must either\n  support the buffer protocol or be an iterable object producing bytes.\n  Bytes and bytearray are examples of built-in objects that support the\n  buffer protocol.\nbyteorder\n  The byte order used to represent the integer.  If byteorder is 'big',\n  the most significant byte is at the beginning of the byte array.  If\n  byteorder is 'little', the most significant byte is at the end of the\n  byte array.  To request the native byte order of the host system, use\n  `sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\n  Indicates whether two's complement is used to represent the integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_integer",
          "signature": "is_integer(self, /)",
          "documentation": {
            "description": "Returns True. Exists for duck type compatibility with float.is_integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bytes",
          "signature": "to_bytes(self, /, length=1, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return an array of bytes representing an integer.\n\nlength\n  Length of bytes object to use.  An OverflowError is raised if the\n  integer is not representable with the given number of bytes.  Default\n  is length 1.\nbyteorder\n  The byte order used to represent the integer.  If byteorder is 'big',\n  the most significant byte is at the beginning of the byte array.  If\n  byteorder is 'little', the most significant byte is at the end of the\n  byte array.  To request the native byte order of the host system, use\n  `sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\n  Determines whether two's complement is used to represent the integer.\n  If signed is False and a negative integer is given, an OverflowError\n  is raised.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ONNXProgram",
      "documentation": {
        "description": "A class to represent an ONNX program that is callable with torch tensors.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "apply_weights",
          "signature": "apply_weights(self, state_dict: 'dict[str, torch.Tensor]') -> 'None'",
          "documentation": {
            "description": "Apply the weights from the specified state dict to the ONNX model.\n\nUse this method to replace FakeTensors or other weights.",
            "parameters": {
              "state_dict": {
                "type": "",
                "description": "The state dict containing the weights to apply to the ONNX model."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "initialize_inference_session",
          "signature": "initialize_inference_session(self, initializer: 'Callable[[str | bytes], ort.InferenceSession]' = <function _ort_session_initializer at 0x76e2173bcf40>) -> 'None'",
          "documentation": {
            "description": "Initialize the ONNX Runtime inference session.",
            "parameters": {
              "initializer": {
                "type": "",
                "description": "The function to initialize the ONNX Runtime inference"
              },
              "session": {
                "type": "",
                "description": "with the specified model. By default, it uses the\n:func:`_ort_session_initializer` function."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "optimize",
          "signature": "optimize(self) -> 'None'",
          "documentation": {
            "description": "Optimize the ONNX model.\n\nThis method optimizes the ONNX model by performing constant folding and\neliminating redundancies in the graph. The optimization is done in-place.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "release",
          "signature": "release(self) -> 'None'",
          "documentation": {
            "description": "Release the inference session.\n\nYou may call this method to release the resources used by the inference session.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self, destination: 'str | os.PathLike', *, include_initializers: 'bool' = True, keep_initializers_as_inputs: 'bool' = False, external_data: 'bool | None' = None)",
          "documentation": {
            "description": "Save the ONNX model to the specified destination.\n\nWhen ``external_data`` is ``True`` or the model is larger than 2GB,\nthe weights are saved as external data in a separate file.\n\nInitializer (model weights) serialization behaviors:\n* ``include_initializers=True``, ``keep_initializers_as_inputs=False`` (default):\nThe initializers are included in the saved model.\n* ``include_initializers=True``, ``keep_initializers_as_inputs=True``:\nThe initializers are included in the saved model and kept as model inputs.\nChoose this option if you want the ability to override the model weights\nduring inference.\n* ``include_initializers=False``, ``keep_initializers_as_inputs=False``:\nThe initializers are not included in the saved model and are not listed\nas model inputs. Choose this option if you want to attach the initializers\nto the ONNX model in a separate, post-processing, step.\n* ``include_initializers=False``, ``keep_initializers_as_inputs=True``:\nThe initializers are not included in the saved model but are listed as model\ninputs. Choose this option if you want to supply the initializers during\ninference and want to minimize the size of the saved model.",
            "parameters": {
              "destination": {
                "type": "",
                "description": "The path to save the ONNX model to."
              },
              "include_initializers": {
                "type": "",
                "description": "Whether to include the initializers in the saved model."
              },
              "keep_initializers_as_inputs": {
                "type": "",
                "description": "Whether to keep the initializers as inputs in the saved model."
              },
              "If": {
                "type": "",
                "description": "`True`, the initializers are added as inputs to the model which means they can be overwritten."
              },
              "by": {
                "type": "",
                "description": "providing the initializers as model inputs."
              },
              "external_data": {
                "type": "",
                "description": "Whether to save the weights as external data in a separate file."
              }
            },
            "returns": "",
            "raises": "TypeError: If ``external_data`` is ``True`` and ``destination`` is not a file path.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ONNXRuntimeOptions",
      "documentation": {
        "description": "Options to influence the execution of the ONNX model through ONNX Runtime.\n\nAttributes:\n    session_options: ONNX Runtime session options.\n    execution_providers: ONNX Runtime execution providers to use during model execution.\n    execution_provider_options: ONNX Runtime execution provider options.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "OnnxExporterError",
      "documentation": {
        "description": "Errors raised by the ONNX exporter. This is the base class for all exporter errors.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OnnxRegistry",
      "documentation": {
        "description": "Registry for ONNX functions.\n\nThe registry maintains a mapping from qualified names to symbolic functions under a\nfixed opset version. It supports registering custom onnx-script functions and for\ndispatcher to dispatch calls to the appropriate function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "get_op_functions",
          "signature": "get_op_functions(self, namespace: 'str', op_name: 'str', overload: 'str | None' = None) -> 'list[registration.ONNXFunction] | None'",
          "documentation": {
            "description": "Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>.\n\nThe list is ordered by the time of registration. The custom operators should be\nin the second half of the list.",
            "parameters": {
              "namespace": {
                "type": "",
                "description": "The namespace of the operator to get."
              },
              "op_name": {
                "type": "",
                "description": "The name of the operator to get."
              },
              "overload": {
                "type": "",
                "description": "The overload of the operator to get. If it's default overload,"
              },
              "leave": {
                "type": "",
                "description": "it to None."
              }
            },
            "returns": "A list of ONNXFunctions corresponding to the given name, or None if\n    the name is not in the registry.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_registered_op",
          "signature": "is_registered_op(self, namespace: 'str', op_name: 'str', overload: 'str | None' = None) -> 'bool'",
          "documentation": {
            "description": "Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>.",
            "parameters": {
              "namespace": {
                "type": "",
                "description": "The namespace of the operator to check."
              },
              "op_name": {
                "type": "",
                "description": "The name of the operator to check."
              },
              "overload": {
                "type": "",
                "description": "The overload of the operator to check. If it's default overload,"
              },
              "leave": {
                "type": "",
                "description": "it to None."
              }
            },
            "returns": "True if the given op is registered, otherwise False.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_op",
          "signature": "register_op(self, function: 'onnxscript.OnnxFunction | onnxscript.TracedOnnxFunction', namespace: 'str', op_name: 'str', overload: 'str | None' = None, is_complex: 'bool' = False) -> 'None'",
          "documentation": {
            "description": "Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>.",
            "parameters": {
              "function": {
                "type": "",
                "description": "The onnx-sctip function to register."
              },
              "namespace": {
                "type": "",
                "description": "The namespace of the operator to register."
              },
              "op_name": {
                "type": "",
                "description": "The name of the operator to register."
              },
              "overload": {
                "type": "",
                "description": "The overload of the operator to register. If it's default overload,"
              },
              "leave": {
                "type": "",
                "description": "it to None."
              },
              "is_complex": {
                "type": "",
                "description": "Whether the function is a function that handles complex valued inputs."
              }
            },
            "returns": "",
            "raises": "ValueError: If the name is not in the form of 'namespace::op'.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OperatorExportTypes",
      "documentation": {
        "description": "Members:\n\nONNX\n\nONNX_ATEN\n\nONNX_ATEN_FALLBACK\n\nONNX_FALLTHROUGH",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "TensorProtoDataType",
      "documentation": {
        "description": "Members:\n\nUNDEFINED\n\nFLOAT\n\nUINT8\n\nINT8\n\nUINT16\n\nINT16\n\nINT32\n\nINT64\n\nSTRING\n\nBOOL\n\nFLOAT16\n\nDOUBLE\n\nUINT32\n\nUINT64\n\nCOMPLEX64\n\nCOMPLEX128\n\nBFLOAT16\n\nFLOAT8E4M3FN\n\nFLOAT8E4M3FNUZ\n\nFLOAT8E5M2\n\nFLOAT8E5M2FNUZ",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "TrainingMode",
      "documentation": {
        "description": "Members:\n\nEVAL\n\nPRESERVE\n\nTRAINING",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    }
  ]
}