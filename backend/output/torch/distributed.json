{
  "description": "No description available",
  "functions": [
    {
      "name": "all_gather",
      "signature": "all_gather(tensor_list, tensor, group=None, async_op=False)",
      "documentation": {
        "description": "Gathers tensors from the whole group in a list.\n\nComplex and uneven sized tensors are supported.",
        "parameters": {
          "tensor_list": {
            "type": "list[Tensor]",
            "description": "Output list. It should contain"
          },
          "correctly": {
            "type": "",
            "description": "-sized tensors to be used for output of the collective."
          },
          "Uneven": {
            "type": "",
            "description": "sized tensors are supported."
          },
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to be broadcast from current process."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # All tensors below are of torch.int64 dtype.\n    >>> # We have 2 process groups, 2 ranks.\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> tensor_list = [torch.zeros(2, dtype=torch.int64, device=device) for _ in range(2)]\n    >>> tensor_list\n    [tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0\n    [tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1\n    >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n    >>> tensor\n    tensor([1, 2], device='cuda:0') # Rank 0\n    tensor([3, 4], device='cuda:1') # Rank 1\n    >>> dist.all_gather(tensor_list, tensor)\n    >>> tensor_list\n    [tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0\n    [tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1\n\n    >>> # All tensors below are of torch.cfloat dtype.\n    >>> # We have 2 process groups, 2 ranks.\n    >>> tensor_list = [torch.zeros(2, dtype=torch.cfloat, device=device) for _ in range(2)]\n    >>> tensor_list\n    [tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0\n    [tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1\n    >>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat, device=device) + 2 * rank * (1+1j)\n    >>> tensor\n    tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n    tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n    >>> dist.all_gather(tensor_list, tensor)\n    >>> tensor_list\n    [tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0\n    [tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1"
      }
    },
    {
      "name": "all_gather_coalesced",
      "signature": "all_gather_coalesced(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
      "documentation": {
        "description": "Gathers input tensors from the whole group in a list in a coalesced manner.\n\nComplex tensors are supported.",
        "parameters": {
          "output_tensor_lists": {
            "type": "list[list[Tensor]]",
            "description": "Output list. It should contain"
          },
          "correctly": {
            "type": "",
            "description": "-sized tensors to be used for output of the collective."
          },
          "input_tensor_list": {
            "type": "list[Tensor]",
            "description": "Tensors to be broadcast from"
          },
          "current": {
            "type": "",
            "description": "process. At least one tensor has to be non empty."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group\n\nExample:\n    we have 2 process groups, 2 ranks.\n    rank 0 passes:\n        input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]\n        output_tensor_lists =\n           [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n            [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n    rank 1 passes:\n        input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]\n        output_tensor_lists =\n           [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n            [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n    both rank 0 and 1 get:\n        output_tensor_lists =\n           [[[1, 1], [1, 1]], [2], [3, 3]],\n            [[3, 3], [3, 3]], [5], [1, 1]]].\n\nWARNING: at this time individual shape checking is not implemented across nodes.\nFor example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\nrank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the\nall_gather_coalesced operation will proceed without complaint and return\nerroneous outputs. This lack of shape checking results in significant\nperformance improvements but users of this function should take extra care\nto ensure that each node passes in tensors whose shapes match across nodes.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "we have 2 process groups, 2 ranks.\n    rank 0 passes:\n        input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]\n        output_tensor_lists =\n           [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n            [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n    rank 1 passes:\n        input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]\n        output_tensor_lists =\n           [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n            [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n    both rank 0 and 1 get:\n        output_tensor_lists =\n           [[[1, 1], [1, 1]], [2], [3, 3]],\n            [[3, 3], [3, 3]], [5], [1, 1]]].\n\nWARNING: at this time individual shape checking is not implemented across nodes.\nFor example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\nrank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the\nall_gather_coalesced operation will proceed without complaint and return\nerroneous outputs. This lack of shape checking results in significant\nperformance improvements but users of this function should take extra care\nto ensure that each node passes in tensors whose shapes match across nodes."
      }
    },
    {
      "name": "all_gather_into_tensor",
      "signature": "all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)",
      "documentation": {
        "description": "Gather tensors from all ranks and put them in a single output tensor.\n\nThis function requires all tensors to be the same size on each process.",
        "parameters": {
          "output_tensor": {
            "type": "Tensor",
            "description": "Output tensor to accommodate tensor elements"
          },
          "from": {
            "type": "",
            "description": "all ranks. It must be correctly sized to have one of the"
          },
          "following": {
            "type": "",
            "description": "forms:\n(i) a concatenation of all the input tensors along the primary"
          },
          "dimension": {
            "type": "",
            "description": "; for definition of \"concatenation\", see ``torch.cat()``;\n(ii) a stack of all the input tensors along the primary dimension;"
          },
          "for": {
            "type": "",
            "description": "definition of \"stack\", see ``torch.stack()``."
          },
          "Examples": {
            "type": "",
            "description": "below may better explain the supported output forms."
          },
          "input_tensor": {
            "type": "Tensor",
            "description": "Tensor to be gathered from current rank."
          },
          "Different": {
            "type": "",
            "description": "from the ``all_gather`` API, the input tensors in this"
          },
          "API": {
            "type": "",
            "description": "must have the same size across all ranks."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n    >>> # We have two ranks.\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n    >>> tensor_in\n    tensor([1, 2], device='cuda:0') # Rank 0\n    tensor([3, 4], device='cuda:1') # Rank 1\n    >>> # Output in concatenation form\n    >>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n    >>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n    >>> tensor_out\n    tensor([1, 2, 3, 4], device='cuda:0') # Rank 0\n    tensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n    >>> # Output in stack form\n    >>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n    >>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n    >>> tensor_out2\n    tensor([[1, 2],\n            [3, 4]], device='cuda:0') # Rank 0\n    tensor([[1, 2],\n            [3, 4]], device='cuda:1') # Rank 1\n\n.. warning::\n    The Gloo backend does not support this API."
      }
    },
    {
      "name": "all_gather_object",
      "signature": "all_gather_object(object_list, obj, group=None)",
      "documentation": {
        "description": "Gathers picklable objects from the whole group into a list.\n\nSimilar to :func:`all_gather`, but Python objects can be passed in.\nNote that the object must be picklable in order to be gathered.",
        "parameters": {
          "object_list": {
            "type": "list[Any]",
            "description": "Output list. It should be correctly sized as the"
          },
          "size": {
            "type": "",
            "description": "of the group for this collective and will contain the output."
          },
          "obj": {
            "type": "Any",
            "description": "Pickable Python object to be broadcast from current process."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          }
        },
        "returns": "None. If the calling rank is part of this group, the output of the\n    collective will be populated into the input ``object_list``. If the\n    calling rank is not part of the group, the passed in ``object_list`` will\n    be unmodified.\n\n.. note:: Note that this API differs slightly from the :func:`all_gather`\n    collective since it does not provide an ``async_op`` handle and thus\n    will be a blocking call.\n\n.. note:: For NCCL-based processed groups, internal tensor representations\n    of objects must be moved to the GPU device before communication takes\n    place. In this case, the device used is given by\n    ``torch.cuda.current_device()`` and it is the user's responsiblity to\n    ensure that this is set so that each rank has an individual GPU, via\n    ``torch.cuda.set_device()``.\n\n.. warning::\n    :func:`all_gather_object` uses ``pickle`` module implicitly, which is\n    known to be insecure. It is possible to construct malicious pickle data\n    which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`all_gather_object` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`all_gather` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> # Assumes world_size of 3.\n    >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>> output = [None for _ in gather_objects]\n    >>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n    >>> output\n    ['foo', 12, {1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "all_reduce",
      "signature": "all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "documentation": {
        "description": "Reduces the tensor data across all machines in a way that all get the final result.\n\nAfter the call ``tensor`` is going to be bitwise identical in all processes.\n\nComplex tensors are supported.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Input and output of the collective. The function"
          },
          "operates": {
            "type": "",
            "description": "in-place."
          },
          "op": {
            "type": "optional",
            "description": "One of the values from\n``torch.distributed.ReduceOp``"
          },
          "enum": {
            "type": "",
            "description": ".  Specifies an operation used for element-wise reductions."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"no rank\")\n    >>> # All tensors below are of torch.int64 type.\n    >>> # We have 2 process groups, 2 ranks.\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n    >>> tensor\n    tensor([1, 2], device='cuda:0') # Rank 0\n    tensor([3, 4], device='cuda:1') # Rank 1\n    >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n    >>> tensor\n    tensor([4, 6], device='cuda:0') # Rank 0\n    tensor([4, 6], device='cuda:1') # Rank 1\n\n    >>> # All tensors below are of torch.cfloat type.\n    >>> # We have 2 process groups, 2 ranks.\n    >>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat, device=device) + 2 * rank * (1+1j)\n    >>> tensor\n    tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n    tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n    >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n    >>> tensor\n    tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0\n    tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1"
      }
    },
    {
      "name": "all_reduce_coalesced",
      "signature": "all_reduce_coalesced(tensors, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "documentation": {
        "description": "WARNING: at this time individual shape checking is not implemented across nodes.\n\nFor example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\nrank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce\noperation will proceed without complaint and return erroneous outputs. This lack\nof shape checking results in significant performance improvements but users of this\nfunction should take extra care to ensure that each node passes in tensors whose\nshapes match across nodes.\n\nReduces each tensor in tensors (residing on the same device) across all machines\nin such a way that all get the final result.\n\nAfter the call each tensor in tensors is going to bitwise identical\nin all processes.\n\nComplex tensors are supported.",
        "parameters": {
          "tensors": {
            "type": "Union[List[Tensor], Tensor]",
            "description": "Input and output of the collective."
          },
          "The": {
            "type": "",
            "description": "function operates in-place."
          },
          "op": {
            "type": "Optional[ReduceOp]",
            "description": "One of the values from\n``torch.distributed.ReduceOp`` enum. Specifies an operation used for"
          },
          "element": {
            "type": "",
            "description": "-wise reductions."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "Optional[bool]",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "all_to_all",
      "signature": "all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)",
      "documentation": {
        "description": "Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.\n\nComplex tensors are supported.",
        "parameters": {
          "output_tensor_list": {
            "type": "list[Tensor]",
            "description": "List of tensors to be gathered one"
          },
          "per": {
            "type": "",
            "description": "rank."
          },
          "input_tensor_list": {
            "type": "list[Tensor]",
            "description": "List of tensors to scatter one per rank."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group.\n\n.. warning::\n    `all_to_all` is experimental and subject to change.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"Undefined rank\")\n    >>> input = torch.arange(4) + rank * 4\n    >>> input = list(input.chunk(4))\n    >>> input\n    [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n    [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n    [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n    [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n    >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n    >>> dist.all_to_all(output, input)\n    >>> output\n    [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n    [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n    [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n    [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n\n    >>> # Essentially, it is similar to following operation:\n    >>> scatter_list = input\n    >>> gather_list  = output\n    >>> for i in range(world_size):\n    >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n\n    >>> input\n    tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n    tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n    tensor([20, 21, 22, 23, 24])                                     # Rank 2\n    tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n    >>> input_splits\n    [2, 2, 1, 1]                                                     # Rank 0\n    [3, 2, 2, 2]                                                     # Rank 1\n    [2, 1, 1, 1]                                                     # Rank 2\n    [2, 2, 2, 1]                                                     # Rank 3\n    >>> output_splits\n    [2, 3, 2, 2]                                                     # Rank 0\n    [2, 2, 1, 2]                                                     # Rank 1\n    [1, 2, 1, 2]                                                     # Rank 2\n    [1, 2, 1, 1]                                                     # Rank 3\n    >>> input = list(input.split(input_splits))\n    >>> input\n    [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n    [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n    [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n    [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n    >>> output = ...\n    >>> dist.all_to_all(output, input)\n    >>> output\n    [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n    [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n    [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n    [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n    >>> # Another example with tensors of torch.cfloat type.\n    >>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n    >>> input = list(input.chunk(4))\n    >>> input\n    [tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n    [tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n    [tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n    [tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n    >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n    >>> dist.all_to_all(output, input)\n    >>> output\n    [tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n    [tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n    [tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n    [tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3"
      }
    },
    {
      "name": "all_to_all_single",
      "signature": "all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)",
      "documentation": {
        "description": "Split input tensor and then scatter the split list to all processes in a group.\n\nLater the received tensors are concatenated from all the processes in the group\nand returned as a single output tensor.\n\nComplex tensors are supported.",
        "parameters": {
          "output": {
            "type": "Tensor",
            "description": "Gathered concatenated output tensor."
          },
          "input": {
            "type": "Tensor",
            "description": "Input tensor to scatter."
          },
          "output_split_sizes": {
            "type": "",
            "description": "(list[Int], optional): Output split sizes for dim 0"
          },
          "if": {
            "type": "",
            "description": "specified None or empty, dim 0 of ``input`` tensor must divide"
          },
          "equally": {
            "type": "",
            "description": "by ``world_size``."
          },
          "input_split_sizes": {
            "type": "",
            "description": "(list[Int], optional): Input split sizes for dim 0"
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group.\n\n.. warning::\n    `all_to_all_single` is experimental and subject to change.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"Undefined rank\")\n    >>> input = torch.arange(4) + rank * 4\n    >>> input\n    tensor([0, 1, 2, 3])     # Rank 0\n    tensor([4, 5, 6, 7])     # Rank 1\n    tensor([8, 9, 10, 11])   # Rank 2\n    tensor([12, 13, 14, 15]) # Rank 3\n    >>> output = torch.empty([4], dtype=torch.int64)\n    >>> dist.all_to_all_single(output, input)\n    >>> output\n    tensor([0, 4, 8, 12])    # Rank 0\n    tensor([1, 5, 9, 13])    # Rank 1\n    tensor([2, 6, 10, 14])   # Rank 2\n    tensor([3, 7, 11, 15])   # Rank 3\n\n    >>> # Essentially, it is similar to following operation:\n    >>> scatter_list = list(input.chunk(world_size))\n    >>> gather_list  = list(output.chunk(world_size))\n    >>> for i in range(world_size):\n    >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n\n    >>> # Another example with uneven split\n    >>> input\n    tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n    tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n    tensor([20, 21, 22, 23, 24])                                     # Rank 2\n    tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n    >>> input_splits\n    [2, 2, 1, 1]                                                     # Rank 0\n    [3, 2, 2, 2]                                                     # Rank 1\n    [2, 1, 1, 1]                                                     # Rank 2\n    [2, 2, 2, 1]                                                     # Rank 3\n    >>> output_splits\n    [2, 3, 2, 2]                                                     # Rank 0\n    [2, 2, 1, 2]                                                     # Rank 1\n    [1, 2, 1, 2]                                                     # Rank 2\n    [1, 2, 1, 1]                                                     # Rank 3\n    >>> output = ...\n    >>> dist.all_to_all_single(output, input, output_splits, input_splits)\n    >>> output\n    tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\n    tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\n    tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\n    tensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n\n    >>> # Another example with tensors of torch.cfloat type.\n    >>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n    >>> input\n    tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\n    tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\n    tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\n    tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n    >>> output = torch.empty([4], dtype=torch.int64)\n    >>> dist.all_to_all_single(output, input)\n    >>> output\n    tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\n    tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\n    tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\n    tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3"
      }
    },
    {
      "name": "barrier",
      "signature": "barrier(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op=False, device_ids=None)",
      "documentation": {
        "description": "Synchronize all processes.\n\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          },
          "device_ids": {
            "type": "[int], optional",
            "description": "List of device/GPU ids."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group\n\n.. note:: `ProcessGroupNCCL` now blocks the cpu thread till the completion of the barrier collective.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "batch_isend_irecv",
      "signature": "batch_isend_irecv(p2p_op_list: List[torch.distributed.distributed_c10d.P2POp]) -> List[torch.distributed.distributed_c10d.Work]",
      "documentation": {
        "description": "Send or Receive a batch of tensors asynchronously and return a list of requests.\n\nProcess each of the operations in ``p2p_op_list`` and return the corresponding\nrequests. NCCL, Gloo, and UCC backend are currently supported.",
        "parameters": {
          "p2p_op_list": {
            "type": "",
            "description": "A list of point-to-point operations(type of each operator is\n``torch.distributed.P2POp``). The order of the isend/irecv in the list"
          },
          "matters": {
            "type": "",
            "description": "and it needs to match with corresponding isend/irecv on the"
          },
          "remote": {
            "type": "",
            "description": "end."
          }
        },
        "returns": "A list of distributed request objects returned by calling the corresponding\n    op in the op_list.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"no rank\")\n    >>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank\n    >>> recv_tensor = torch.randn(2, dtype=torch.float32)\n    >>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)\n    >>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)\n    >>> reqs = batch_isend_irecv([send_op, recv_op])\n    >>> for req in reqs:\n    >>>     req.wait()\n    >>> recv_tensor\n    tensor([2, 3])     # Rank 0\n    tensor([0, 1])     # Rank 1\n\n.. note:: Note that when this API is used with the NCCL PG backend, users must set\n    the current GPU device with `torch.cuda.set_device`, otherwise it will\n    lead to unexpected hang issues.\n\n    In addition, if this API is the first collective call in the ``group``\n    passed to ``dist.P2POp``, all ranks of the ``group`` must participate in\n    this API call; otherwise, the behavior is undefined. If this API call is\n    not the first collective call in the ``group``, batched P2P operations\n    involving only a subset of ranks of the ``group`` are allowed."
      }
    },
    {
      "name": "breakpoint",
      "signature": "breakpoint(rank: int = 0, skip: int = 0)",
      "documentation": {
        "description": "Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be\ndone with the breakpoint before continuing.",
        "parameters": {
          "rank": {
            "type": "int",
            "description": "Which rank to break on.  Default: ``0``"
          },
          "skip": {
            "type": "int",
            "description": "Skip the first ``skip`` calls to this breakpoint. Default: ``0``."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcast",
      "signature": "broadcast(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_src: Optional[int] = None)",
      "documentation": {
        "description": "Broadcasts the tensor to the whole group.\n\n``tensor`` must have the same number of elements in all processes\nparticipating in the collective.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Data to be sent if ``src`` is the rank of current"
          },
          "process": {
            "type": "",
            "description": ", and tensor to be used to save received data otherwise."
          },
          "src": {
            "type": "int",
            "description": "Source rank on global process group (regardless of ``group`` argument)."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          },
          "group_src": {
            "type": "int",
            "description": "Source rank on ``group``.  Must specify one of ``group_src``"
          },
          "and": {
            "type": "",
            "description": "``src`` but not both."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcast_object_list",
      "signature": "broadcast_object_list(object_list: List[Any], src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_src: Optional[int] = None)",
      "documentation": {
        "description": "Broadcasts picklable objects in ``object_list`` to the whole group.\n\nSimilar to :func:`broadcast`, but Python objects can be passed in.\nNote that all objects in ``object_list`` must be picklable in order to be\nbroadcasted.",
        "parameters": {
          "object_list": {
            "type": "List[Any]",
            "description": "List of input objects to broadcast."
          },
          "Each": {
            "type": "",
            "description": "object must be picklable. Only objects on the ``src`` rank will"
          },
          "be": {
            "type": "",
            "description": "broadcast, but each rank must provide lists of equal sizes."
          },
          "src": {
            "type": "int",
            "description": "Source rank from which to broadcast ``object_list``."
          },
          "Source": {
            "type": "",
            "description": "rank is based on global process group (regardless of ``group`` argument)"
          },
          "group": {
            "type": "",
            "description": "(ProcessGroup, optional): The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          },
          "device": {
            "type": "``torch.device``, optional",
            "description": "If not None, the objects are"
          },
          "serialized": {
            "type": "",
            "description": "and converted to tensors which are moved to the\n``device`` before broadcasting. Default is ``None``."
          },
          "group_src": {
            "type": "int",
            "description": "Source rank on ``group``.  Must not specify one of ``group_src``"
          },
          "and": {
            "type": "",
            "description": "``src`` but not both."
          }
        },
        "returns": "``None``. If rank is part of the group, ``object_list`` will contain the\n    broadcasted objects from ``src`` rank.\n\n.. note:: For NCCL-based process groups, internal tensor representations\n    of objects must be moved to the GPU device before communication takes\n    place. In this case, the device used is given by\n    ``torch.cuda.current_device()`` and it is the user's responsibility to\n    ensure that this is set so that each rank has an individual GPU, via\n    ``torch.cuda.set_device()``.\n\n.. note:: Note that this API differs slightly from the :func:`broadcast`\n    collective since it does not provide an ``async_op`` handle and thus\n    will be a blocking call.\n\n.. warning::\n    :func:`broadcast_object_list` uses ``pickle`` module implicitly, which\n    is known to be insecure. It is possible to construct malicious pickle\n    data which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`broadcast_object_list` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`broadcast` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> if dist.get_rank() == 0:\n    >>>     # Assumes world_size of 3.\n    >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>> else:\n    >>>     objects = [None, None, None]\n    >>> # Assumes backend is not NCCL\n    >>> device = torch.device(\"cpu\")\n    >>> dist.broadcast_object_list(objects, src=0, device=device)\n    >>> objects\n    ['foo', 12, {1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "destroy_process_group",
      "signature": "destroy_process_group(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None)",
      "documentation": {
        "description": "Destroy a given process group, and deinitialize the distributed package.",
        "parameters": {
          "group": {
            "type": "",
            "description": ".WORLD is given, all process"
          },
          "groups": {
            "type": "",
            "description": "including the default one will"
          },
          "be": {
            "type": "",
            "description": "destroyed."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gather",
      "signature": "gather(tensor: torch.Tensor, gather_list: Optional[List[torch.Tensor]] = None, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_dst: Optional[int] = None)",
      "documentation": {
        "description": "Gathers a list of tensors in a single process.\n\nThis function requires all tensors to be the same size on each process.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Input tensor."
          },
          "gather_list": {
            "type": "list[Tensor], optional",
            "description": "List of appropriately,"
          },
          "same": {
            "type": "",
            "description": "-sized tensors to use for gathered data\n(default is None, must be specified on the destination rank)"
          },
          "dst": {
            "type": "int, optional",
            "description": "Destination rank on global process group (regardless of ``group`` argument).\n(If both ``dst`` and ``group_dst`` are None, default is global rank 0)"
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          },
          "group_dst": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``"
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group\n\n.. note:: Note that all Tensors in gather_list must have the same size.\n\nExample::\n    >>> # xdoctest: +SKIP(\"no rank\")\n    >>> # We have 2 process groups, 2 ranks.\n    >>> tensor_size = 2\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> tensor = torch.ones(tensor_size, device=device) + rank\n    >>> if dist.get_rank() == 0:\n    >>>     gather_list = [torch.zeros_like(tensor, device=device) for i in range(2)]\n    >>> else:\n    >>>     gather_list = None\n    >>> dist.gather(tensor, gather_list, dst=0)\n    >>> # Rank 0 gets gathered data.\n    >>> gather_list\n    [tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0\n    None                                                                   # Rank 1",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gather_object",
      "signature": "gather_object(obj: Any, object_gather_list: Optional[List[Any]] = None, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, group_dst: Optional[int] = None)",
      "documentation": {
        "description": "Gathers picklable objects from the whole group in a single process.\n\nSimilar to :func:`gather`, but Python objects can be passed in. Note that the\nobject must be picklable in order to be gathered.",
        "parameters": {
          "obj": {
            "type": "Any",
            "description": "Input object. Must be picklable."
          },
          "object_gather_list": {
            "type": "list[Any]",
            "description": "Output list. On the ``dst`` rank, it"
          },
          "should": {
            "type": "",
            "description": "be correctly sized as the size of the group for this"
          },
          "collective": {
            "type": "",
            "description": "and will contain the output. Must be ``None`` on non-dst"
          },
          "ranks": {
            "type": "",
            "description": ". (default is ``None``)"
          },
          "dst": {
            "type": "int, optional",
            "description": "Destination rank on global process group (regardless of ``group`` argument).\n(If both ``dst`` and ``group_dst`` are None, default is global rank 0)"
          },
          "group": {
            "type": "",
            "description": "(ProcessGroup, optional): The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          },
          "group_dst": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``"
          }
        },
        "returns": "None. On the ``dst`` rank, ``object_gather_list`` will contain the\n    output of the collective.\n\n.. note:: Note that this API differs slightly from the gather collective\n    since it does not provide an async_op handle and thus will be a blocking\n    call.\n\n.. note:: For NCCL-based processed groups, internal tensor representations\n    of objects must be moved to the GPU device before communication takes\n    place. In this case, the device used is given by\n    ``torch.cuda.current_device()`` and it is the user's responsiblity to\n    ensure that this is set so that each rank has an individual GPU, via\n    ``torch.cuda.set_device()``.\n\n.. warning::\n    :func:`gather_object` uses ``pickle`` module implicitly, which is\n    known to be insecure. It is possible to construct malicious pickle data\n    which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`gather_object` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`gather` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> # Assumes world_size of 3.\n    >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>> output = [None for _ in gather_objects]\n    >>> dist.gather_object(\n    ...     gather_objects[dist.get_rank()],\n    ...     output if dist.get_rank() == 0 else None,\n    ...     dst=0\n    ... )\n    >>> # On rank 0\n    >>> output\n    ['foo', 12, {1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_backend",
      "signature": "get_backend(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> torch.distributed.distributed_c10d.Backend",
      "documentation": {
        "description": "Return the backend of the given process group.",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. The"
          },
          "default": {
            "type": "",
            "description": "is the general main process group. If another specific group"
          },
          "is": {
            "type": "",
            "description": "specified, the calling process must be part of :attr:`group`."
          }
        },
        "returns": "The backend of the given process group as a lower case string.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_backend_config",
      "signature": "get_backend_config(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> str",
      "documentation": {
        "description": "Return the backend configuration of the given process group.",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. The"
          },
          "default": {
            "type": "",
            "description": "is the general main process group. If another specific group"
          },
          "is": {
            "type": "",
            "description": "specified, the calling process must be part of :attr:`group`."
          }
        },
        "returns": "The backend configuration of the given process group as a lower case string.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_debug_level",
      "signature": "get_debug_level()",
      "documentation": {
        "description": "get_debug_level() -> torch._C._distributed_c10d.DebugLevel\n\nGets the debug level of the torch.distributed package.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_default_backend_for_device",
      "signature": "get_default_backend_for_device(device: Union[str, torch.device]) -> str",
      "documentation": {
        "description": "Return the default backend for the given device.",
        "parameters": {
          "Union": {
            "type": "",
            "description": "[str, torch.device]: The device to get the default backend for."
          }
        },
        "returns": "The default backend for the given device as a lower case string.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_global_rank",
      "signature": "get_global_rank(group: torch.distributed.distributed_c10d.ProcessGroup, group_rank: int) -> int",
      "documentation": {
        "description": "Translate a group rank into a global rank.\n\n``group_rank`` must be part of `group` otherwise this raises RuntimeError.",
        "parameters": {
          "group": {
            "type": "ProcessGroup",
            "description": "ProcessGroup to find the global rank from."
          },
          "group_rank": {
            "type": "int",
            "description": "Group rank to query."
          }
        },
        "returns": "Global rank of ``group_rank`` relative to ``group``\n\nN.B. calling this function on the default process group returns identity",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_group_rank",
      "signature": "get_group_rank(group: torch.distributed.distributed_c10d.ProcessGroup, global_rank: int) -> int",
      "documentation": {
        "description": "Translate a global rank into a group rank.\n\n``global_rank`` must be part of ``group`` otherwise this raises RuntimeError.",
        "parameters": {
          "group": {
            "type": "ProcessGroup",
            "description": "ProcessGroup to find the relative rank."
          },
          "global_rank": {
            "type": "int",
            "description": "Global rank to query."
          }
        },
        "returns": "Group rank of ``global_rank`` relative to ``group``\n\nN.B. calling this function on the default process group returns identity",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_node_local_rank",
      "signature": "get_node_local_rank(fallback_rank: Optional[int] = None) -> int",
      "documentation": {
        "description": "Return the local rank of the current process relative to the node.\n\nSemantically, this is a useful concept for mapping processes to devices.\nFor example, on a node with 8 accelerator you could use the node local rank to decide\nwhich accelerator device to bind the process to.\n\nIn practice, the actual assignment of node local ranks is handled by the process launcher outside of pytorch,\nand communicated via the `LOCAL_RANK` environment variable.\n\nTorchrun will automatically populate `LOCAL_RANK`, but other launchers may not.  If `LOCAL_RANK` is unspecified,\nthis API will fall back to the provided kwarg 'fallback_rank' if specified, otherwise it will raise an error. The\nintent is to allow writing an application that runs either in single or multi device contexts without error.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_pg_count",
      "signature": "get_pg_count() -> int",
      "documentation": {
        "description": "Return the number of process groups.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_process_group_ranks",
      "signature": "get_process_group_ranks(group: torch.distributed.distributed_c10d.ProcessGroup) -> List[int]",
      "documentation": {
        "description": "Get all ranks associated with ``group``.",
        "parameters": {
          "group": {
            "type": "ProcessGroup",
            "description": "ProcessGroup to get all ranks from."
          }
        },
        "returns": "List of global ranks ordered by group rank.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_rank",
      "signature": "get_rank(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> int",
      "documentation": {
        "description": "Return the rank of the current process in the provided ``group``, default otherwise.\n\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 to\n``world_size``.",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          }
        },
        "returns": "The rank of the process group\n    -1, if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_world_size",
      "signature": "get_world_size(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> int",
      "documentation": {
        "description": "Return the number of processes in the current process group.",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          }
        },
        "returns": "The world size of the process group\n    -1, if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "init_device_mesh",
      "signature": "init_device_mesh(device_type: str, mesh_shape: Tuple[int, ...], *, mesh_dim_names: Optional[Tuple[str, ...]] = None) -> torch.distributed.device_mesh.DeviceMesh",
      "documentation": {
        "description": "Initializes a `DeviceMesh` based on `device_type`, `mesh_shape`, and `mesh_dim_names` parameters.\n\nThis creates a DeviceMesh with an n-dimensional array layout, where `n` is the length of `mesh_shape`.\nIf `mesh_dim_names` is provided, each dimension is labeled as `mesh_dim_names[i]`.\n\n.. note::\n    `init_device_mesh` follows SPMD programming model, meaning the same PyTorch Python program\n    runs on all processes/ranks in the cluster. Ensure `mesh_shape` (the dimensions of the nD array\n    describing device layout) is identical across all ranks. Inconsistent `mesh_shape` may lead to hanging.\n\n.. note::\n    If no process group is found, init_device_mesh will initialize distributed process group/groups\n    required for distributed communications behind the scene.",
        "parameters": {
          "device_type": {
            "type": "str",
            "description": "The device type of the mesh. Currently supports: \"cpu\", \"cuda/cuda-like\"."
          },
          "Passing": {
            "type": "",
            "description": "in a device type with a GPU index, such as \"cuda:0\", is not allowed."
          },
          "mesh_shape": {
            "type": "Tuple[int]",
            "description": "A tuple defining the dimensions of the multi-dimensional array"
          },
          "describing": {
            "type": "",
            "description": "the layout of devices."
          },
          "mesh_dim_names": {
            "type": "Tuple[str], optional",
            "description": "A tuple of mesh dimension names to assign to each dimension"
          },
          "of": {
            "type": "",
            "description": "`mesh_shape`. Each string in `mesh_dim_names` must be unique."
          }
        },
        "returns": "DeviceMesh: A :class:`DeviceMesh` object representing the device layout.\n\nExample::\n    >>> # xdoctest: +SKIP(\"no rank\")\n    >>> from torch.distributed.device_mesh import init_device_mesh\n    >>>\n    >>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n    >>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "init_process_group",
      "signature": "init_process_group(backend: Optional[str] = None, init_method: Optional[str] = None, timeout: Optional[datetime.timedelta] = None, world_size: int = -1, rank: int = -1, store: Optional[torch.distributed.distributed_c10d.Store] = None, group_name: str = '', pg_options: Optional[Any] = None, device_id: Optional[torch.device] = None) -> None",
      "documentation": {
        "description": "Initialize the default distributed process group.\n\nThis will also initialize the distributed package.\n\nThere are 2 main ways to initialize a process group:\n    1. Specify ``store``, ``rank``, and ``world_size`` explicitly.\n    2. Specify ``init_method`` (a URL string) which indicates where/how\n       to discover peers. Optionally specify ``rank`` and ``world_size``,\n       or encode all required parameters in the URL and omit them.\n\nIf neither is specified, ``init_method`` is assumed to be \"env://\".",
        "parameters": {
          "backend": {
            "type": "",
            "description": ", ``is_high_priority_stream`` can be specified so that"
          },
          "build": {
            "type": "",
            "description": "-time configurations, valid values include ``mpi``, ``gloo``,\n``nccl``, ``ucc``, or one that is registered by a third-party"
          },
          "plugin": {
            "type": "",
            "description": "."
          },
          "Since": {
            "type": "",
            "description": "2.6, if ``backend`` is not provided, c10d will use a backend"
          },
          "registered": {
            "type": "",
            "description": "for that detected accelerator (or ``cpu``)."
          },
          "for": {
            "type": "",
            "description": "collectives with CUDA tensors. A custom backend can be specified by passing in"
          },
          "If": {
            "type": "",
            "description": "using multiple processes per machine with ``nccl`` backend, each"
          },
          "detect": {
            "type": "",
            "description": "the accelerator on the run-time machine and use a backend"
          },
          "This": {
            "type": "",
            "description": "is done since CUDA execution is async and it is no longer safe to continue executing user code since"
          },
          "which": {
            "type": "",
            "description": "can also be accessed via :class:`Backend` attributes (e.g.,\n``Backend.GLOO``)."
          },
          "process": {
            "type": "",
            "description": "group. Default is \"env://\" if no\n``init_method`` or ``store`` is specified."
          },
          "GPUs": {
            "type": "",
            "description": "between processes can result in deadlock or NCCL invalid usage.\n``ucc`` backend is experimental."
          },
          "init_method": {
            "type": "str, optional",
            "description": "URL specifying how to initialize the"
          },
          "Mutually": {
            "type": "",
            "description": "exclusive with ``init_method``."
          },
          "world_size": {
            "type": "int, optional",
            "description": "Number of processes participating in"
          },
          "the": {
            "type": "",
            "description": "nccl backend can pick up high priority cuda streams when"
          },
          "rank": {
            "type": "int, optional",
            "description": "Rank of the current process (it should be a"
          },
          "number": {
            "type": "",
            "description": "between 0 and ``world_size``-1)."
          },
          "Required": {
            "type": "",
            "description": "if ``store`` is specified."
          },
          "store": {
            "type": "Store, optional",
            "description": "Key/value store accessible to all workers, used"
          },
          "to": {
            "type": "",
            "description": "\"bind\" this process to, allowing for backend-specific"
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "Timeout for operations executed against"
          },
          "failed": {
            "type": "",
            "description": "async NCCL operations might result in subsequent CUDA operations running on corrupted data."
          },
          "When": {
            "type": "",
            "description": "TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout."
          },
          "group_name": {
            "type": "str, optional, deprecated",
            "description": "Group name. This argument is ignored"
          },
          "pg_options": {
            "type": "ProcessGroupOptions, optional",
            "description": "process group options"
          },
          "specifying": {
            "type": "",
            "description": "what additional options need to be passed in during"
          },
          "options": {
            "type": "",
            "description": "we support is ``ProcessGroupNCCL.Options`` for the ``nccl``"
          },
          "there": {
            "type": "",
            "description": "'re compute kernels waiting. For other availble options to config nccl,"
          },
          "See": {
            "type": "",
            "description": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t"
          },
          "device_id": {
            "type": "torch.device, optional",
            "description": "a single, specific device"
          },
          "optimizations": {
            "type": "",
            "description": ".  Currently this has two effects, only under"
          },
          "NCCL": {
            "type": "",
            "description": "the communicator is immediately formed (calling\n``ncclCommInit*`` immediately rather than the normal lazy"
          },
          "call": {
            "type": "",
            "description": ") and sub-groups will use ``ncclCommSplit`` when"
          },
          "possible": {
            "type": "",
            "description": "to avoid unnecessary overhead of group creation. If you"
          },
          "want": {
            "type": "",
            "description": "to know NCCL initialization error early, you can also use this"
          },
          "field": {
            "type": "",
            "description": ".\n.. note:: To enable ``backend == Backend.MPI``, PyTorch needs to be built from source"
          },
          "on": {
            "type": "",
            "description": "a system that supports MPI.\n.. note:: Support for multiple backends is experimental. Currently when no backend is"
          },
          "specified": {
            "type": "",
            "description": ", both ``gloo`` and ``nccl`` backends will be created. The ``gloo`` backend"
          },
          "will": {
            "type": "",
            "description": "be used for collectives with CPU tensors and the ``nccl`` backend will be used"
          },
          "a": {
            "type": "",
            "description": "string with format \"<device_type>:<backend_name>,<device_type>:<backend_name>\", e.g.\n\"cpu:gloo,cuda:custom_backend\"."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "irecv",
      "signature": "irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_src: Optional[int] = None) -> Optional[torch.distributed.distributed_c10d.Work]",
      "documentation": {
        "description": "Receives a tensor asynchronously.\n\n.. warning::\n    ``tag`` is not supported with the NCCL backend.\n\nUnlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to fill with received data."
          },
          "src": {
            "type": "int, optional",
            "description": "Source rank on global process group (regardless of ``group`` argument)."
          },
          "Will": {
            "type": "",
            "description": "receive from any process if unspecified."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "tag": {
            "type": "int, optional",
            "description": "Tag to match recv with remote send"
          },
          "group_src": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``."
          }
        },
        "returns": "A distributed request object.\n    None, if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_available",
      "signature": "is_available() -> bool",
      "documentation": {
        "description": "Return ``True`` if the distributed package is available.\n\nOtherwise,\n``torch.distributed`` does not expose any other APIs. Currently,\n``torch.distributed`` is available on Linux, MacOS and Windows. Set\n``USE_DISTRIBUTED=1`` to enable it when building PyTorch from source.\nCurrently, the default value is ``USE_DISTRIBUTED=1`` for Linux and Windows,\n``USE_DISTRIBUTED=0`` for MacOS.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_backend_available",
      "signature": "is_backend_available(backend: str) -> bool",
      "documentation": {
        "description": "Check backend availability.\n\nChecks if the given backend is available and supports the built-in backends or\nthird-party backends through function ``Backend.register_backend``.",
        "parameters": {
          "backend": {
            "type": "str",
            "description": "Backend name."
          }
        },
        "returns": "bool: Returns true if the backend is available otherwise false.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_gloo_available",
      "signature": "is_gloo_available() -> bool",
      "documentation": {
        "description": "Check if the Gloo backend is available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_initialized",
      "signature": "is_initialized() -> bool",
      "documentation": {
        "description": "Check if the default process group has been initialized.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_mpi_available",
      "signature": "is_mpi_available() -> bool",
      "documentation": {
        "description": "Check if the MPI backend is available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_nccl_available",
      "signature": "is_nccl_available() -> bool",
      "documentation": {
        "description": "Check if the NCCL backend is available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_torchelastic_launched",
      "signature": "is_torchelastic_launched() -> bool",
      "documentation": {
        "description": "Check whether this process was launched with ``torch.distributed.elastic`` (aka torchelastic).\n\nThe existence of ``TORCHELASTIC_RUN_ID`` environment\nvariable is used as a proxy to determine whether the current process\nwas launched with torchelastic. This is a reasonable proxy since\n``TORCHELASTIC_RUN_ID`` maps to the rendezvous id which is always a\nnon-null value indicating the job id for peer discovery purposes..",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_ucc_available",
      "signature": "is_ucc_available() -> bool",
      "documentation": {
        "description": "Check if the UCC backend is available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_xccl_available",
      "signature": "is_xccl_available() -> bool",
      "documentation": {
        "description": "Check if the XCCL backend is available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "isend",
      "signature": "isend(tensor: torch.Tensor, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_dst: Optional[int] = None) -> Optional[torch.distributed.distributed_c10d.Work]",
      "documentation": {
        "description": "Send a tensor asynchronously.\n\n.. warning::\n    Modifying ``tensor`` before the request completes causes undefined\n    behavior.\n\n.. warning::\n    ``tag`` is not supported with the NCCL backend.\n\nUnlike send, which is blocking, isend allows src == dst rank, i.e. send to self.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to send."
          },
          "dst": {
            "type": "int",
            "description": "Destination rank on global process group (regardless of ``group`` argument)"
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "tag": {
            "type": "int, optional",
            "description": "Tag to match send with remote recv"
          },
          "group_dst": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``"
          }
        },
        "returns": "A distributed request object.\n    None, if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "monitored_barrier",
      "signature": "monitored_barrier(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, timeout=None, wait_all_ranks=False)",
      "documentation": {
        "description": "Synchronize processes similar to ``torch.distributed.barrier``, but consider a configurable timeout.\n\nIt is able to report ranks that did not pass this barrier within the provided timeout.\nSpecifically, for non-zero ranks, will block until a send/recv is processed from rank 0.\nRank 0 will block until all send /recv from other ranks are processed, and will report\nfailures for ranks that failed to respond in time. Note that if one rank does not reach the\nmonitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\n\nThis collective will block all processes/ranks in the group, until the\nwhole group exits the function successfully, making it useful for debugging\nand synchronizing. However, it can have a performance impact and should only\nbe used for debugging or scenarios that require full synchronization points\non the host-side. For debugging purposes, this barrier can be inserted\nbefore the application's collective calls to check if any ranks are\ndesynchronized.\n\n.. note:: Note that this collective is only supported with the GLOO backend.",
        "parameters": {
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If\n``None``, the default process group will be used."
          },
          "timeout": {
            "type": "datetime.timedelta, optional",
            "description": "Timeout for monitored_barrier."
          },
          "If": {
            "type": "",
            "description": "``None``, the default process group timeout will be used."
          },
          "wait_all_ranks": {
            "type": "bool, optional",
            "description": "Whether to collect all failed ranks or"
          },
          "not": {
            "type": "",
            "description": ". By default, this is ``False`` and ``monitored_barrier`` on rank 0"
          },
          "will": {
            "type": "",
            "description": "throw on the first failed rank it encounters in order to fail"
          },
          "fast": {
            "type": "",
            "description": ". By setting ``wait_all_ranks=True`` ``monitored_barrier`` will"
          },
          "collect": {
            "type": "",
            "description": "all failed ranks and throw an error containing information"
          },
          "about": {
            "type": "",
            "description": "all failed ranks."
          }
        },
        "returns": "``None``.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> if dist.get_rank() != 1:\n    >>>     dist.monitored_barrier() # Raises exception indicating that\n    >>> # rank 1 did not call into monitored_barrier.\n    >>> # Example with wait_all_ranks=True\n    >>> if dist.get_rank() == 0:\n    >>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n    >>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n    >>> # monitored_barrier.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "new_group",
      "signature": "new_group(ranks=None, timeout=None, backend=None, pg_options=None, use_local_synchronization=False, group_desc=None, device_id: Optional[torch.device] = None)",
      "documentation": {
        "description": "Create a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even\nif they are not going to be members of the group. Additionally, groups\nshould be created in the same order in all processes.\n\n.. warning::\n    Safe concurrent usage:\n    When using multiple process groups with the ``NCCL`` backend, the user\n    must ensure a globally consistent execution order of collectives across\n    ranks.\n\n    If multiple threads within a process issue collectives, explicit\n    synchronization is necessary to ensure consistent ordering.\n\n    When using async variants of torch.distributed communication APIs,\n    a work object is returned and the communication kernel is\n    enqueued on a separate CUDA stream, allowing overlap of communication\n    and computation. Once one or more async ops have been issued on one process\n    group, they must be synchronized with other cuda streams by calling `work.wait()`\n    before using another process group.\n\n    See `Using multiple NCCL communicators concurrently <https://docs.nvid\n    ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using\n    -multiple-nccl-communicators-concurrently>`_ for more details.",
        "parameters": {
          "ranks": {
            "type": "list[int]",
            "description": "List of ranks of group members. If ``None``, will be"
          },
          "set": {
            "type": "",
            "description": "to all ranks. Default is ``None``."
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "see `init_process_group` for details and default value."
          },
          "backend": {
            "type": "",
            "description": ", ``is_high_priority_stream`` can be specified so that"
          },
          "build": {
            "type": "",
            "description": "-time configurations, valid values are ``gloo`` and ``nccl``."
          },
          "By": {
            "type": "",
            "description": "default uses the same backend as the global group. This field"
          },
          "should": {
            "type": "",
            "description": "be given as a lowercase string (e.g., ``\"gloo\"``), which can"
          },
          "also": {
            "type": "",
            "description": "be accessed via :class:`Backend` attributes (e.g.,\n``Backend.GLOO``). If ``None`` is passed in, the backend"
          },
          "corresponding": {
            "type": "",
            "description": "to the default process group will be used. Default is\n``None``."
          },
          "pg_options": {
            "type": "ProcessGroupOptions, optional",
            "description": "process group options"
          },
          "specifying": {
            "type": "",
            "description": "what additional options need to be passed in during"
          },
          "the": {
            "type": "",
            "description": "construction of specific process groups. i.e. for the ``nccl``"
          },
          "process": {
            "type": "",
            "description": "group can pick up high priority cuda streams. For other availble options to config nccl,"
          },
          "See": {
            "type": "",
            "description": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t"
          },
          "use_local_synchronization": {
            "type": "bool, optional",
            "description": "perform a group-local"
          },
          "barrier": {
            "type": "",
            "description": "at the end of the process group creation. This is different"
          },
          "in": {
            "type": "",
            "description": "that non-member ranks don't need to call into API and don't"
          },
          "join": {
            "type": "",
            "description": "the barrier."
          },
          "group_desc": {
            "type": "str, optional",
            "description": "a string to describe the process group."
          },
          "device_id": {
            "type": "torch.device, optional",
            "description": "a single, specific device"
          },
          "to": {
            "type": "",
            "description": "\"bind\" this process to,  The `new_group` call will try to initialize"
          },
          "a": {
            "type": "",
            "description": "communication backend immediately for the device if this field is given."
          }
        },
        "returns": "A handle of distributed group that can be given to collective calls or\n    GroupMember.NON_GROUP_MEMBER if the rank is not part of ``ranks``.\n\nN.B. use_local_synchronization doesn't work with MPI.\n\nN.B. While use_local_synchronization=True can be significantly faster with larger\nclusters and small process groups, care must be taken since it changes cluster behavior\nas non-member ranks don't join the group barrier().\n\nN.B. use_local_synchronization=True can lead to deadlocks when each rank creates\nmultiple overlaping process groups. To avoid that, make sure all ranks follow the\nsame global creation order.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "new_subgroups",
      "signature": "new_subgroups(group_size=None, group=None, timeout=None, backend=None, pg_options=None, group_desc=None)",
      "documentation": {
        "description": "Create subgroups of equal size.\n\nBy default, it creates intra-machine subgroups,\nwhere each of which contains all the ranks of a machine, based on the assumption\nthat each machine has the same number of devices.\n\nThis is a convenience API that calls ``new_group`` to generate multiple subgroups.\nIt requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even\nif they are not going to be members of the group.\n\n.. warning::\n    If ``group_size`` is passed in, the world size must be divisible by ``group_size``.\n    If no ``group_size`` is passed in, it believe that you are creating a group based\n    on CUDA and determining the group size by number of CUDA devices, and if not all\n    the machines have the same number of devices, the subgroup division will be\n    different across nodes and can cause unexpected behaviors. Therefore, if you are\n    creating a subgroup that does not depend on CUDA (such as Gloo on CPU), please\n    pass in ``group_size`` correctly.\n\n.. warning::\n    See warning `Safe concurrent usage` for `new_group` API for important details about\n    using multiple process groups concurrently in a safe manner.",
        "parameters": {
          "group_size": {
            "type": "int, optional",
            "description": "The size of each subgroup. If ``None``,"
          },
          "the": {
            "type": "",
            "description": "construction of specific process groups. i.e. for the ``nccl``"
          },
          "based": {
            "type": "",
            "description": "on the assumption that each machine has exactly the same"
          },
          "number": {
            "type": "",
            "description": "of devices. Default is ``None``."
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "see `init_process_group` for details and default value."
          },
          "backend": {
            "type": "",
            "description": ", ``is_high_priority_stream`` can be specified so that"
          },
          "build": {
            "type": "",
            "description": "-time configurations, valid values are ``gloo`` and ``nccl``."
          },
          "By": {
            "type": "",
            "description": "default uses the same backend as the global group. This field"
          },
          "should": {
            "type": "",
            "description": "be given as a lowercase string (e.g., ``\"gloo\"``), which can"
          },
          "also": {
            "type": "",
            "description": "be accessed via :class:`Backend` attributes (e.g.,\n``Backend.GLOO``). If ``None`` is passed in, the backend"
          },
          "corresponding": {
            "type": "",
            "description": "to the default process group will be used. Default is\n``None``."
          },
          "pg_options": {
            "type": "ProcessGroupOptions, optional",
            "description": "process group options"
          },
          "specifying": {
            "type": "",
            "description": "what additional options need to be passed in during"
          },
          "process": {
            "type": "",
            "description": "group can pick up high priority cuda streams."
          },
          "group_desc": {
            "type": "str, optional",
            "description": "A string describing the group. Each subgroup will"
          },
          "inherit": {
            "type": "",
            "description": "its group_desc"
          }
        },
        "returns": "The subgroup containing the current rank, and all the subgroups used for cleanup.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # Create intra-machine subgroups.\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> cur_subgroup, subgroups = dist.new_subgroups()\n    >>> # Allreduce within the machine.\n    >>> rank = dist.get_rank()\n    >>> tensor = torch.ones(1, device=rank) * rank\n    >>> dist.all_reduce(tensor, group=cur_subgroup)\n    >>> tensor\n    tensor([28])  # Assume 8 CUDA devices per machine.  28 is sum(range(8)).\n    >>> # Cleanup.\n    >>> for subgroup in subgroups:\n    >>>     dist.destroy_process_group(subgroup)"
      }
    },
    {
      "name": "new_subgroups_by_enumeration",
      "signature": "new_subgroups_by_enumeration(ranks_per_subgroup_list, timeout=None, backend=None, pg_options=None, group_desc=None)",
      "documentation": {
        "description": "Create subgroups by dividing the global world.\n\nThe division is specified by a nested list of ranks. The subgroups cannot have\noverlap, and some ranks may not have to be in any subgroup.\n\nThis is a convenience API that calls ``new_group`` to generate multiple subgroups.\nIt requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even\nif they are not going to be members of the group.\n\n.. warning::\n    See warning `Safe concurrent usage` for `new_group` API for important details about\n    using multiple process groups concurrently in a safe manner.",
        "parameters": {
          "ranks_per_subgroup_list": {
            "type": "list[list[int]]",
            "description": "A nested list of ranks of"
          },
          "group": {
            "type": "",
            "description": "members."
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "see `init_process_group` for details and default value."
          },
          "backend": {
            "type": "",
            "description": ", ``is_high_priority_stream`` can be specified so that"
          },
          "build": {
            "type": "",
            "description": "-time configurations, valid values are ``gloo`` and ``nccl``."
          },
          "By": {
            "type": "",
            "description": "default uses the same backend as the global group. This field"
          },
          "should": {
            "type": "",
            "description": "be given as a lowercase string (e.g., ``\"gloo\"``), which can"
          },
          "also": {
            "type": "",
            "description": "be accessed via :class:`Backend` attributes (e.g.,\n``Backend.GLOO``). If ``None`` is passed in, the backend"
          },
          "corresponding": {
            "type": "",
            "description": "to the default process group will be used. Default is\n``None``."
          },
          "pg_options": {
            "type": "ProcessGroupOptions, optional",
            "description": "process group options"
          },
          "specifying": {
            "type": "",
            "description": "what additional options need to be passed in during"
          },
          "the": {
            "type": "",
            "description": "construction of specific process groups. i.e. for the ``nccl``"
          },
          "process": {
            "type": "",
            "description": "group can pick up high priority cuda streams."
          },
          "group_desc": {
            "type": "str, optional",
            "description": "A string describing the group. Each subgroup will"
          },
          "inherit": {
            "type": "",
            "description": "its group_desc."
          }
        },
        "returns": "The subgroup containing the current rank, and all the subgroups used for cleanup.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # Create two subgroups, where each has 2 processes.\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> cur_subgroup, subgroups = dist.new_subgroups(ranks=[[0, 2], [1, 3]])\n    >>> rank = dist.get_rank()\n    >>> tensor = torch.ones(1, device=rank) * rank\n    >>> dist.all_reduce(tensor, group=cur_subgroup)\n    >>> tensor\n    tensor([2])     # Subgroup 0: ranks 0 and 2\n    tensor([4])     # Subgroup 1: ranks 1 and 3"
      }
    },
    {
      "name": "recv",
      "signature": "recv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_src: Optional[int] = None) -> int",
      "documentation": {
        "description": "Receives a tensor synchronously.\n\n.. warning::\n    ``tag`` is not supported with the NCCL backend.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to fill with received data."
          },
          "src": {
            "type": "int, optional",
            "description": "Source rank on global process group (regardless of ``group`` argument)."
          },
          "Will": {
            "type": "",
            "description": "receive from any process if unspecified."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "tag": {
            "type": "int, optional",
            "description": "Tag to match recv with remote send"
          },
          "group_src": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``."
          }
        },
        "returns": "Sender rank\n    -1, if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "recv_object_list",
      "signature": "recv_object_list(object_list: List[Any], src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_src: Optional[int] = None)",
      "documentation": {
        "description": "Receives picklable objects in ``object_list`` synchronously.\n\nSimilar to :func:`recv`, but can receive Python objects.",
        "parameters": {
          "object_list": {
            "type": "List[Any]",
            "description": "List of objects to receive into."
          },
          "Must": {
            "type": "",
            "description": "provide a list of sizes equal to the size of the list being sent."
          },
          "src": {
            "type": "int, optional",
            "description": "Source rank from which to recv ``object_list``."
          },
          "Source": {
            "type": "",
            "description": "rank is based on global process group (regardless of ``group`` argument)"
          },
          "Will": {
            "type": "",
            "description": "receive from any rank if set to None. Default is ``None``."
          },
          "group": {
            "type": "",
            "description": "(ProcessGroup, optional): The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          },
          "device": {
            "type": "``torch.device``, optional",
            "description": "If not None, receives on this device."
          },
          "Default": {
            "type": "",
            "description": "is ``None``."
          },
          "group_src": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``."
          }
        },
        "returns": "Sender rank. -1 if rank is not part of the group. If rank is part of the group,\n    ``object_list`` will contain the sent objects from ``src`` rank.\n\n.. note:: For NCCL-based process groups, internal tensor representations\n    of objects must be moved to the GPU device before communication takes\n    place. In this case, the device used is given by\n    ``torch.cuda.current_device()`` and it is the user's responsibility to\n    ensure that this is set so that each rank has an individual GPU, via\n    ``torch.cuda.set_device()``.\n\n.. warning::\n    :func:`recv_object_list` uses ``pickle`` module implicitly, which\n    is known to be insecure. It is possible to construct malicious pickle\n    data which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`recv_object_list` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`recv` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> # Assumes backend is not NCCL\n    >>> device = torch.device(\"cpu\")\n    >>> if dist.get_rank() == 0:\n    >>>     # Assumes world_size of 2.\n    >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>>     dist.send_object_list(objects, dst=1, device=device)\n    >>> else:\n    >>>     objects = [None, None, None]\n    >>>     dist.recv_object_list(objects, src=0, device=device)\n    >>> objects\n    ['foo', 12, {1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce",
      "signature": "reduce(tensor: torch.Tensor, dst: Optional[int] = None, op=<RedOpType.SUM: 0>, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_dst: Optional[int] = None)",
      "documentation": {
        "description": "Reduces the tensor data across all machines.\n\nOnly the process with rank ``dst`` is going to receive the final result.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Input and output of the collective. The function"
          },
          "operates": {
            "type": "",
            "description": "in-place."
          },
          "dst": {
            "type": "int",
            "description": "Destination rank on global process group (regardless of ``group`` argument)"
          },
          "op": {
            "type": "optional",
            "description": "One of the values from\n``torch.distributed.ReduceOp``"
          },
          "enum": {
            "type": "",
            "description": ".  Specifies an operation used for element-wise reductions."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          },
          "group_dst": {
            "type": "int",
            "description": "Destination rank on ``group``.  Must specify one of ``group_dst``"
          },
          "and": {
            "type": "",
            "description": "``dst`` but not both."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_scatter",
      "signature": "reduce_scatter(output, input_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "documentation": {
        "description": "Reduces, then scatters a list of tensors to all processes in a group.",
        "parameters": {
          "output": {
            "type": "Tensor",
            "description": "Output tensor."
          },
          "input_list": {
            "type": "list[Tensor]",
            "description": "List of tensors to reduce and scatter."
          },
          "op": {
            "type": "optional",
            "description": "One of the values from\n``torch.distributed.ReduceOp``"
          },
          "enum": {
            "type": "",
            "description": ".  Specifies an operation used for element-wise reductions."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_scatter_tensor",
      "signature": "reduce_scatter_tensor(output, input, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "documentation": {
        "description": "Reduces, then scatters a tensor to all ranks in a group.",
        "parameters": {
          "output": {
            "type": "Tensor",
            "description": "Output tensor. It should have the same size across all"
          },
          "ranks": {
            "type": "",
            "description": "."
          },
          "input": {
            "type": "Tensor",
            "description": "Input tensor to be reduced and scattered. Its size"
          },
          "should": {
            "type": "",
            "description": "be output tensor size times the world size. The input tensor"
          },
          "can": {
            "type": "",
            "description": "have one of the following shapes:\n(i) a concatenation of the output tensors along the primary"
          },
          "dimension": {
            "type": "",
            "description": ", or\n(ii) a stack of the output tensors along the primary dimension."
          },
          "For": {
            "type": "",
            "description": "definition of \"stack\", see ``torch.stack()``."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op."
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n    >>> # We have two ranks.\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n    >>> # Input in concatenation form\n    >>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n    >>> tensor_in\n    tensor([0, 1, 2, 3], device='cuda:0') # Rank 0\n    tensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n    >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n    >>> tensor_out\n    tensor([0, 2], device='cuda:0') # Rank 0\n    tensor([4, 6], device='cuda:1') # Rank 1\n    >>> # Input in stack form\n    >>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n    >>> tensor_in\n    tensor([[0, 1],\n            [2, 3]], device='cuda:0') # Rank 0\n    tensor([[0, 1],\n            [2, 3]], device='cuda:1') # Rank 1\n    >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n    >>> tensor_out\n    tensor([0, 2], device='cuda:0') # Rank 0\n    tensor([4, 6], device='cuda:1') # Rank 1\n\n.. warning::\n    The Gloo backend does not support this API."
      }
    },
    {
      "name": "register_rendezvous_handler",
      "signature": "register_rendezvous_handler(scheme, handler)",
      "documentation": {
        "description": "Register a new rendezvous handler.\n\nBefore we can run collective algorithms, participating processes\nneed to find each other and exchange information to be able to\ncommunicate. We call this process rendezvous.\n\nThe outcome of the rendezvous process is a triplet containing a\nshared key/value store, the rank of the process, and the total\nnumber of participating processes.\n\nIf none of the bundled rendezvous methods apply to your execution\nenvironment you can opt to register your own rendezvous handler.\nPick a unique name and use the URL scheme to identify it when\ncalling the `rendezvous()` function.",
        "parameters": {
          "scheme": {
            "type": "str",
            "description": "URL scheme to identify your rendezvous handler."
          },
          "handler": {
            "type": "function",
            "description": "Handler that is invoked when the\n`rendezvous()` function is called with a URL that uses"
          },
          "the": {
            "type": "",
            "description": "corresponding scheme. It must be a generator function"
          },
          "that": {
            "type": "",
            "description": "yields the triplet."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "rendezvous",
      "signature": "rendezvous(url: str, rank: int = -1, world_size: int = -1, **kwargs)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter",
      "signature": "scatter(tensor: torch.Tensor, scatter_list: Optional[List[torch.Tensor]] = None, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_src: Optional[int] = None)",
      "documentation": {
        "description": "Scatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the\n``tensor`` argument.\n\nComplex tensors are supported.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Output tensor."
          },
          "scatter_list": {
            "type": "list[Tensor]",
            "description": "List of tensors to scatter (default is"
          },
          "None": {
            "type": "",
            "description": ", must be specified on the source rank)"
          },
          "src": {
            "type": "int",
            "description": "Source rank on global process group (regardless of ``group`` argument).\n(If both ``src`` and ``group_src`` are None, default is global rank 0)"
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "async_op": {
            "type": "bool, optional",
            "description": "Whether this op should be an async op"
          },
          "group_src": {
            "type": "int, optional",
            "description": "Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``"
          }
        },
        "returns": "Async work handle, if async_op is set to True.\n    None, if not async_op or if not part of the group\n\n.. note:: Note that all Tensors in scatter_list must have the same size.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> tensor_size = 2\n    >>> device = torch.device(f'cuda:{rank}')\n    >>> output_tensor = torch.zeros(tensor_size, device=device)\n    >>> if dist.get_rank() == 0:\n    >>>     # Assumes world_size of 2.\n    >>>     # Only tensors, all of which must be the same size.\n    >>>     t_ones = torch.ones(tensor_size, device=device)\n    >>>     t_fives = torch.ones(tensor_size, device=device) * 5\n    >>>     scatter_list = [t_ones, t_fives]\n    >>> else:\n    >>>     scatter_list = None\n    >>> dist.scatter(output_tensor, scatter_list, src=0)\n    >>> # Rank i gets scatter_list[i].\n    >>> output_tensor\n    tensor([1., 1.], device='cuda:0') # Rank 0\n    tensor([5., 5.], device='cuda:1') # Rank 1",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_object_list",
      "signature": "scatter_object_list(scatter_object_output_list: List[Any], scatter_object_input_list: Optional[List[Any]] = None, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, group_src: Optional[int] = None)",
      "documentation": {
        "description": "Scatters picklable objects in ``scatter_object_input_list`` to the whole group.\n\nSimilar to :func:`scatter`, but Python objects can be passed in. On\neach rank, the scattered object will be stored as the first element of\n``scatter_object_output_list``. Note that all objects in\n``scatter_object_input_list`` must be picklable in order to be scattered.",
        "parameters": {
          "scatter_object_output_list": {
            "type": "List[Any]",
            "description": "Non-empty list whose first"
          },
          "element": {
            "type": "",
            "description": "will store the object scattered to this rank."
          },
          "scatter_object_input_list": {
            "type": "List[Any], optional",
            "description": "List of input objects to scatter."
          },
          "Each": {
            "type": "",
            "description": "object must be picklable. Only objects on the ``src`` rank will"
          },
          "be": {
            "type": "",
            "description": "scattered, and the argument can be ``None`` for non-src ranks."
          },
          "src": {
            "type": "int",
            "description": "Source rank from which to scatter ``scatter_object_input_list``."
          },
          "Source": {
            "type": "",
            "description": "rank is based on global process group (regardless of ``group`` argument).\n(If both ``src`` and ``group_src`` are None, default is global rank 0)"
          },
          "group": {
            "type": "",
            "description": "(ProcessGroup, optional): The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          },
          "group_src": {
            "type": "int, optional",
            "description": "Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``"
          }
        },
        "returns": "``None``. If rank is part of the group, ``scatter_object_output_list``\n    will have its first element set to the scattered object for this rank.\n\n.. note:: Note that this API differs slightly from the scatter collective\n    since it does not provide an ``async_op`` handle and thus will be a\n    blocking call.\n\n.. warning::\n    :func:`scatter_object_list` uses ``pickle`` module implicitly, which\n    is known to be insecure. It is possible to construct malicious pickle\n    data which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`scatter_object_list` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`scatter` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> if dist.get_rank() == 0:\n    >>>     # Assumes world_size of 3.\n    >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>> else:\n    >>>     # Can be any list on non-src ranks, elements are not used.\n    >>>     objects = [None, None, None]\n    >>> output_list = [None]\n    >>> dist.scatter_object_list(output_list, objects, src=0)\n    >>> # Rank i gets objects[i]. For example, on rank 2:\n    >>> output_list\n    [{1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "send",
      "signature": "send(tensor: torch.Tensor, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_dst: Optional[int] = None) -> None",
      "documentation": {
        "description": "Send a tensor synchronously.\n\n.. warning::\n    ``tag`` is not supported with the NCCL backend.",
        "parameters": {
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to send."
          },
          "dst": {
            "type": "int",
            "description": "Destination rank on global process group (regardless of ``group`` argument)."
          },
          "Destination": {
            "type": "",
            "description": "rank should not be the same as the rank of the current process."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "tag": {
            "type": "int, optional",
            "description": "Tag to match send with remote recv"
          },
          "group_dst": {
            "type": "int, optional",
            "description": "Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "send_object_list",
      "signature": "send_object_list(object_list: List[Any], dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_dst: Optional[int] = None)",
      "documentation": {
        "description": "Sends picklable objects in ``object_list`` synchronously.\n\nSimilar to :func:`send`, but Python objects can be passed in.\nNote that all objects in ``object_list`` must be picklable in order to be\nsent.",
        "parameters": {
          "object_list": {
            "type": "List[Any]",
            "description": "List of input objects to sent."
          },
          "Each": {
            "type": "",
            "description": "object must be picklable. Receiver must provide lists of equal sizes."
          },
          "dst": {
            "type": "int",
            "description": "Destination rank to send ``object_list`` to."
          },
          "Destination": {
            "type": "",
            "description": "rank is based on global process group (regardless of ``group`` argument)"
          },
          "group": {
            "type": "",
            "description": "(ProcessGroup, optional): The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used. Default is ``None``."
          },
          "device": {
            "type": "``torch.device``, optional",
            "description": "If not None, the objects are"
          },
          "serialized": {
            "type": "",
            "description": "and converted to tensors which are moved to the\n``device`` before sending. Default is ``None``."
          },
          "group_dst": {
            "type": "int, optional",
            "description": "Destination rank on ``group``."
          },
          "Must": {
            "type": "",
            "description": "specify one of ``dst`` and ``group_dst`` but not both"
          }
        },
        "returns": "``None``.\n\n.. note:: For NCCL-based process groups, internal tensor representations\n    of objects must be moved to the GPU device before communication takes\n    place. In this case, the device used is given by\n    ``torch.cuda.current_device()`` and it is the user's responsibility to\n    ensure that this is set so that each rank has an individual GPU, via\n    ``torch.cuda.set_device()``.\n\n.. warning::\n    :func:`send_object_list` uses ``pickle`` module implicitly, which\n    is known to be insecure. It is possible to construct malicious pickle\n    data which will execute arbitrary code during unpickling. Only call this\n    function with data you trust.\n\n.. warning::\n    Calling :func:`send_object_list` with GPU tensors is not well supported\n    and inefficient as it incurs GPU -> CPU transfer since tensors would be\n    pickled. Please consider using :func:`send` instead.\n\nExample::\n    >>> # xdoctest: +SKIP(\"need process group init\")\n    >>> # Note: Process group initialization omitted on each rank.\n    >>> import torch.distributed as dist\n    >>> # Assumes backend is not NCCL\n    >>> device = torch.device(\"cpu\")\n    >>> if dist.get_rank() == 0:\n    >>>     # Assumes world_size of 2.\n    >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n    >>>     dist.send_object_list(objects, dst=1, device=device)\n    >>> else:\n    >>>     objects = [None, None, None]\n    >>>     dist.recv_object_list(objects, src=0, device=device)\n    >>> objects\n    ['foo', 12, {1: 2}]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_debug_level",
      "signature": "set_debug_level(arg0: torch._C._distributed_c10d.DebugLevel)",
      "documentation": {
        "description": "set_debug_level(arg0: torch._C._distributed_c10d.DebugLevel) -> None\n\nSets the debug level of the torch.distributed package.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_debug_level_from_env",
      "signature": "set_debug_level_from_env()",
      "documentation": {
        "description": "set_debug_level_from_env() -> None\n\nSets the debug level of the torch.distributed package from the\n          ``TORCH_DISTRIBUTED_DEBUG`` environment variable.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "split_group",
      "signature": "split_group(parent_pg: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, split_ranks: Optional[list] = None, timeout: Optional[datetime.timedelta] = None, pg_options: Optional[Any] = None, group_desc: Optional[str] = None) -> Optional[torch.distributed.distributed_c10d.ProcessGroup]",
      "documentation": {
        "description": "Create a new process group splitted from the given parent process group.\n\nwarning:: This is an experimental API and only the ``NCCL`` backend supports this API.\nOther backends will raise an error.\nUsers of this API must gurantee that all ranks in the parent group enter this API call,\nand the split of the sub groups is the same accross all ranks in the parent group.",
        "parameters": {
          "parent_pg": {
            "type": "ProcessGroup, optional",
            "description": "The parent process group. If None,"
          },
          "the": {
            "type": "",
            "description": "construction of specific process groups. i.e.``is_high_priority_stream``"
          },
          "split_ranks": {
            "type": "list[list[int]]",
            "description": "the split ranks, which is a list of list of ranks."
          },
          "Users": {
            "type": "",
            "description": "need to make sure the validity of the split ranks such that one"
          },
          "split": {
            "type": "represented by one inner list of ints",
            "description": "does not overlap with any other split."
          },
          "Note": {
            "type": "",
            "description": "that the ranks in each split is the group rank (instead of global rank)"
          },
          "in": {
            "type": "",
            "description": "the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n[[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would"
          },
          "return": {
            "type": "",
            "description": "a non-group member."
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "see `init_process_group` for details and default value."
          },
          "pg_options": {
            "type": "ProcessGroupOptions, optional",
            "description": "only ProcessGroupNCCLOptions is supported now."
          },
          "specifying": {
            "type": "",
            "description": "what additional options need to be passed in during"
          },
          "can": {
            "type": "",
            "description": "be specified so that process group can pick up high priority cuda streams."
          },
          "For": {
            "type": "",
            "description": "other availble options to config nccl,"
          },
          "See": {
            "type": "",
            "description": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t"
          },
          "group_desc": {
            "type": "str, optional",
            "description": "a string to describe the process group."
          }
        },
        "returns": "ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n    or None if the current rank is not part of any split_ranks`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "supports_complex",
      "signature": "supports_complex(reduceOp: torch.distributed.distributed_c10d.ReduceOp) -> bool",
      "documentation": {
        "description": "Return true if reduce ops is supported. False otherwise.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "AllToAllOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "AllreduceCoalescedOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "AllreduceOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Backend",
      "documentation": {
        "description": "An enum-like class for backends.\n\nAvailable backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.\n\nThe values of this class are lowercase strings, e.g., ``\"gloo\"``. They can\nbe accessed as attributes, e.g., ``Backend.NCCL``.\n\nThis class can be directly called to parse the string, e.g.,\n``Backend(backend_str)`` will check if ``backend_str`` is valid, and\nreturn the parsed lowercase string if so. It also accepts uppercase strings,\ne.g., ``Backend(\"GLOO\")`` returns ``\"gloo\"``.\n\n.. note:: The entry ``Backend.UNDEFINED`` is present but only used as\n          initial value of some fields. Users should neither use it directly\n          nor assume its existence.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "capitalize",
          "signature": "capitalize(self, /)",
          "documentation": {
            "description": "Return a capitalized version of the string.\n\nMore specifically, make the first character have upper case and the rest lower\ncase.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "casefold",
          "signature": "casefold(self, /)",
          "documentation": {
            "description": "Return a version of the string suitable for caseless comparisons.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "center",
          "signature": "center(self, width, fillchar=' ', /)",
          "documentation": {
            "description": "Return a centered string of length width.\n\nPadding is done using the specified fill character (default is a space).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "count",
          "signature": "count(sub[, start[, end]])",
          "documentation": {
            "description": "S.count(sub[, start[, end]]) -> int\n\nReturn the number of non-overlapping occurrences of substring sub in\nstring S[start:end].  Optional arguments start and end are\ninterpreted as in slice notation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "encode",
          "signature": "encode(self, /, encoding='utf-8', errors='strict')",
          "documentation": {
            "description": "Encode the string using the codec registered for encoding.\n\nencoding\n  The encoding in which to encode the string.\nerrors\n  The error handling scheme to use for encoding errors.\n  The default is 'strict' meaning that encoding errors raise a\n  UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n  'xmlcharrefreplace' as well as any other name registered with\n  codecs.register_error that can handle UnicodeEncodeErrors.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "endswith",
          "signature": "endswith(suffix[, start[, end]])",
          "documentation": {
            "description": "S.endswith(suffix[, start[, end]]) -> bool\n\nReturn True if S ends with the specified suffix, False otherwise.\nWith optional start, test S beginning at that position.\nWith optional end, stop comparing S at that position.\nsuffix can also be a tuple of strings to try.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "expandtabs",
          "signature": "expandtabs(self, /, tabsize=8)",
          "documentation": {
            "description": "Return a copy where all tab characters are expanded using spaces.\n\nIf tabsize is not given, a tab size of 8 characters is assumed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "find",
          "signature": "find(sub[, start[, end]])",
          "documentation": {
            "description": "S.find(sub[, start[, end]]) -> int\n\nReturn the lowest index in S where substring sub is found,\nsuch that sub is contained within S[start:end].  Optional\narguments start and end are interpreted as in slice notation.\n\nReturn -1 on failure.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "format",
          "signature": "format(*args, **kwargs)",
          "documentation": {
            "description": "S.format(*args, **kwargs) -> str\n\nReturn a formatted version of S, using substitutions from args and kwargs.\nThe substitutions are identified by braces ('{' and '}').",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "format_map",
          "signature": "format_map(mapping)",
          "documentation": {
            "description": "S.format_map(mapping) -> str\n\nReturn a formatted version of S, using substitutions from mapping.\nThe substitutions are identified by braces ('{' and '}').",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(sub[, start[, end]])",
          "documentation": {
            "description": "S.index(sub[, start[, end]]) -> int\n\nReturn the lowest index in S where substring sub is found,\nsuch that sub is contained within S[start:end].  Optional\narguments start and end are interpreted as in slice notation.\n\nRaises ValueError when the substring is not found.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isalnum",
          "signature": "isalnum(self, /)",
          "documentation": {
            "description": "Return True if the string is an alpha-numeric string, False otherwise.\n\nA string is alpha-numeric if all characters in the string are alpha-numeric and\nthere is at least one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isalpha",
          "signature": "isalpha(self, /)",
          "documentation": {
            "description": "Return True if the string is an alphabetic string, False otherwise.\n\nA string is alphabetic if all characters in the string are alphabetic and there\nis at least one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isascii",
          "signature": "isascii(self, /)",
          "documentation": {
            "description": "Return True if all characters in the string are ASCII, False otherwise.\n\nASCII characters have code points in the range U+0000-U+007F.\nEmpty string is ASCII too.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isdecimal",
          "signature": "isdecimal(self, /)",
          "documentation": {
            "description": "Return True if the string is a decimal string, False otherwise.\n\nA string is a decimal string if all characters in the string are decimal and\nthere is at least one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isdigit",
          "signature": "isdigit(self, /)",
          "documentation": {
            "description": "Return True if the string is a digit string, False otherwise.\n\nA string is a digit string if all characters in the string are digits and there\nis at least one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isidentifier",
          "signature": "isidentifier(self, /)",
          "documentation": {
            "description": "Return True if the string is a valid Python identifier, False otherwise.\n\nCall keyword.iskeyword(s) to test whether string s is a reserved identifier,\nsuch as \"def\" or \"class\".",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "islower",
          "signature": "islower(self, /)",
          "documentation": {
            "description": "Return True if the string is a lowercase string, False otherwise.\n\nA string is lowercase if all cased characters in the string are lowercase and\nthere is at least one cased character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isnumeric",
          "signature": "isnumeric(self, /)",
          "documentation": {
            "description": "Return True if the string is a numeric string, False otherwise.\n\nA string is numeric if all characters in the string are numeric and there is at\nleast one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isprintable",
          "signature": "isprintable(self, /)",
          "documentation": {
            "description": "Return True if the string is printable, False otherwise.\n\nA string is printable if all of its characters are considered printable in\nrepr() or if it is empty.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isspace",
          "signature": "isspace(self, /)",
          "documentation": {
            "description": "Return True if the string is a whitespace string, False otherwise.\n\nA string is whitespace if all characters in the string are whitespace and there\nis at least one character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "istitle",
          "signature": "istitle(self, /)",
          "documentation": {
            "description": "Return True if the string is a title-cased string, False otherwise.\n\nIn a title-cased string, upper- and title-case characters may only\nfollow uncased characters and lowercase characters only cased ones.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "isupper",
          "signature": "isupper(self, /)",
          "documentation": {
            "description": "Return True if the string is an uppercase string, False otherwise.\n\nA string is uppercase if all cased characters in the string are uppercase and\nthere is at least one cased character in the string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "join",
          "signature": "join(self, iterable, /)",
          "documentation": {
            "description": "Concatenate any number of strings.\n\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\n\nExample: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ljust",
          "signature": "ljust(self, width, fillchar=' ', /)",
          "documentation": {
            "description": "Return a left-justified string of length width.\n\nPadding is done using the specified fill character (default is a space).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "lower",
          "signature": "lower(self, /)",
          "documentation": {
            "description": "Return a copy of the string converted to lowercase.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "lstrip",
          "signature": "lstrip(self, chars=None, /)",
          "documentation": {
            "description": "Return a copy of the string with leading whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "maketrans",
          "signature": "maketrans(...)",
          "documentation": {
            "description": "Return a translation table usable for str.translate().\n\nIf there is only one argument, it must be a dictionary mapping Unicode\nordinals (integers) or characters to Unicode ordinals, strings or None.\nCharacter keys will be then converted to ordinals.\nIf there are two arguments, they must be strings of equal length, and\nin the resulting dictionary, each character in x will be mapped to the\ncharacter at the same position in y. If there is a third argument, it\nmust be a string, whose characters will be mapped to None in the result.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partition",
          "signature": "partition(self, sep, /)",
          "documentation": {
            "description": "Partition the string into three parts using the given separator.\n\nThis will search for the separator in the string.  If the separator is found,\nreturns a 3-tuple containing the part before the separator, the separator\nitself, and the part after it.\n\nIf the separator is not found, returns a 3-tuple containing the original string\nand two empty strings.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backend",
          "signature": "register_backend(name, func, extended_api=False, devices: Union[str, List[str], NoneType] = None) -> None",
          "documentation": {
            "description": "Register a new backend with the given name and instantiating function.\n\nThis class method is used by 3rd party ``ProcessGroup`` extension to\nregister new backends.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "Backend name of the ``ProcessGroup`` extension. It"
              },
              "should": {
                "type": "",
                "description": "match the one in ``init_process_group()``."
              },
              "func": {
                "type": "function",
                "description": "Function handler that instantiates the backend."
              },
              "The": {
                "type": "",
                "description": "function should be implemented in the backend"
              },
              "extension": {
                "type": "",
                "description": "and takes four arguments, including\n``store``, ``rank``, ``world_size``, and ``timeout``."
              },
              "extended_api": {
                "type": "bool, optional",
                "description": "Whether the backend supports extended argument structure."
              },
              "Default": {
                "type": "",
                "description": "``False``. If set to ``True``, the backend"
              },
              "will": {
                "type": "",
                "description": "get an instance of ``c10d::DistributedBackendOptions``, and"
              },
              "a": {
                "type": "",
                "description": "process group options object as defined by the backend implementation."
              },
              "device": {
                "type": "str or list of str, optional",
                "description": "device type this backend"
              },
              "supports": {
                "type": "",
                "description": ", e.g. \"cpu\", \"cuda\", etc. If `None`,"
              },
              "assuming": {
                "type": "",
                "description": "both \"cpu\" and \"cuda\"\n.. note:: This support of 3rd party backend is experimental and subject to change."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "removeprefix",
          "signature": "removeprefix(self, prefix, /)",
          "documentation": {
            "description": "Return a str with the given prefix string removed if present.\n\nIf the string starts with the prefix string, return string[len(prefix):].\nOtherwise, return a copy of the original string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "removesuffix",
          "signature": "removesuffix(self, suffix, /)",
          "documentation": {
            "description": "Return a str with the given suffix string removed if present.\n\nIf the string ends with the suffix string and that suffix is not empty,\nreturn string[:-len(suffix)]. Otherwise, return a copy of the original\nstring.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "replace",
          "signature": "replace(self, old, new, count=-1, /)",
          "documentation": {
            "description": "Return a copy with all occurrences of substring old replaced by new.\n\n  count\n    Maximum number of occurrences to replace.\n    -1 (the default value) means replace all occurrences.\n\nIf the optional argument count is given, only the first count occurrences are\nreplaced.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rfind",
          "signature": "rfind(sub[, start[, end]])",
          "documentation": {
            "description": "S.rfind(sub[, start[, end]]) -> int\n\nReturn the highest index in S where substring sub is found,\nsuch that sub is contained within S[start:end].  Optional\narguments start and end are interpreted as in slice notation.\n\nReturn -1 on failure.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rindex",
          "signature": "rindex(sub[, start[, end]])",
          "documentation": {
            "description": "S.rindex(sub[, start[, end]]) -> int\n\nReturn the highest index in S where substring sub is found,\nsuch that sub is contained within S[start:end].  Optional\narguments start and end are interpreted as in slice notation.\n\nRaises ValueError when the substring is not found.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rjust",
          "signature": "rjust(self, width, fillchar=' ', /)",
          "documentation": {
            "description": "Return a right-justified string of length width.\n\nPadding is done using the specified fill character (default is a space).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rpartition",
          "signature": "rpartition(self, sep, /)",
          "documentation": {
            "description": "Partition the string into three parts using the given separator.\n\nThis will search for the separator in the string, starting at the end. If\nthe separator is found, returns a 3-tuple containing the part before the\nseparator, the separator itself, and the part after it.\n\nIf the separator is not found, returns a 3-tuple containing two empty strings\nand the original string.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rsplit",
          "signature": "rsplit(self, /, sep=None, maxsplit=-1)",
          "documentation": {
            "description": "Return a list of the substrings in the string, using sep as the separator string.\n\n  sep\n    The separator used to split the string.\n\n    When set to None (the default value), will split on any whitespace\n    character (including \\n \\r \\t \\f and spaces) and will discard\n    empty strings from the result.\n  maxsplit\n    Maximum number of splits.\n    -1 (the default value) means no limit.\n\nSplitting starts at the end of the string and works to the front.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rstrip",
          "signature": "rstrip(self, chars=None, /)",
          "documentation": {
            "description": "Return a copy of the string with trailing whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "split",
          "signature": "split(self, /, sep=None, maxsplit=-1)",
          "documentation": {
            "description": "Return a list of the substrings in the string, using sep as the separator string.\n\n  sep\n    The separator used to split the string.\n\n    When set to None (the default value), will split on any whitespace\n    character (including \\n \\r \\t \\f and spaces) and will discard\n    empty strings from the result.\n  maxsplit\n    Maximum number of splits.\n    -1 (the default value) means no limit.\n\nSplitting starts at the front of the string and works to the end.\n\nNote, str.split() is mainly useful for data that has been intentionally\ndelimited.  With natural text that includes punctuation, consider using\nthe regular expression module.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "splitlines",
          "signature": "splitlines(self, /, keepends=False)",
          "documentation": {
            "description": "Return a list of the lines in the string, breaking at line boundaries.\n\nLine breaks are not included in the resulting list unless keepends is given and\ntrue.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "startswith",
          "signature": "startswith(prefix[, start[, end]])",
          "documentation": {
            "description": "S.startswith(prefix[, start[, end]]) -> bool\n\nReturn True if S starts with the specified prefix, False otherwise.\nWith optional start, test S beginning at that position.\nWith optional end, stop comparing S at that position.\nprefix can also be a tuple of strings to try.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "strip",
          "signature": "strip(self, chars=None, /)",
          "documentation": {
            "description": "Return a copy of the string with leading and trailing whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "swapcase",
          "signature": "swapcase(self, /)",
          "documentation": {
            "description": "Convert uppercase characters to lowercase and lowercase characters to uppercase.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "title",
          "signature": "title(self, /)",
          "documentation": {
            "description": "Return a version of the string where each word is titlecased.\n\nMore specifically, words start with uppercased characters and all remaining\ncased characters have lower case.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "translate",
          "signature": "translate(self, table, /)",
          "documentation": {
            "description": "Replace each character in the string using the given translation table.\n\n  table\n    Translation table, which must be a mapping of Unicode ordinals to\n    Unicode ordinals, strings, or None.\n\nThe table must implement lookup/indexing via __getitem__, for instance a\ndictionary or list.  If this operation raises LookupError, the character is\nleft untouched.  Characters mapped to None are deleted.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "upper",
          "signature": "upper(self, /)",
          "documentation": {
            "description": "Return a copy of the string converted to uppercase.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zfill",
          "signature": "zfill(self, width, /)",
          "documentation": {
            "description": "Pad a numeric string with zeros on the left, to fill a field of the given width.\n\nThe string is never truncated.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BackendConfig",
      "documentation": {
        "description": "Backend configuration class.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "get_device_backend_map",
          "signature": "get_device_backend_map(self) -> Dict[str, torch.distributed.distributed_c10d.Backend]",
          "documentation": {
            "description": "Return backend map of the device.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BarrierOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "BroadcastOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "BuiltinCommHookType",
      "documentation": {
        "description": "An enum-like class for built-in communication hooks: ``ALLREDUCE`` and ``FP16_COMPRESS``.\n\nMembers:\n\n  ALLREDUCE\n\n  FP16_COMPRESS",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "DebugLevel",
      "documentation": {
        "description": "An enum whose values correspond to different debug levels of the\n      torch.distributed package. Currently supporting OFF, INFO, and DETAIL,\n      which can be set via the TORCH_DISTRIBUTED_DEBUG environment variable\n      or via ``set_debug_level()`` function.\n  \n\nMembers:\n\n  OFF\n\n  INFO\n\n  DETAIL",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "DeviceMesh",
      "documentation": {
        "description": "DeviceMesh represents a mesh of devices, where layout of devices could be\nrepresented as a n-d dimension array, and each value of the n-d dimensional\narray is the global id of the default process group ranks.\n\nDeviceMesh could be used to describe the layout of devices across the cluster,\nand serves as a proxy for communication among the device lists within the cluster.\n\nDeviceMesh can be used as a context manager.\n\n.. note::\n    DeviceMesh follows SPMD programming model, which means the same PyTorch Python program\n    is running on all processes/ranks in the cluster. Therefore, users need to make sure the\n    `mesh` array (which describes the layout of devices) should be identical across all ranks.\n    Inconsistent `mesh` will lead to silent hang.",
        "parameters": {
          "device_type": {
            "type": "str",
            "description": "The device type of the mesh. Currently supports: \"cpu\", \"cuda/cuda-like\"."
          },
          "mesh": {
            "type": "ndarray",
            "description": "A multi-dimensional array or an integer tensor describing the layout"
          },
          "of": {
            "type": "",
            "description": "devices, where the IDs are global IDs of the default process group."
          }
        },
        "returns": "DeviceMesh: A :class:`DeviceMesh` object representing the device layout.\n\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2\nhosts with 4 GPUs each.\nA reduction over the first dimension of mesh will reduce across\ncolumns (0, 4), .. and (3, 7), a reduction over the second dimension\nof mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).\n\nExample::\n    >>> # xdoctest: +SKIP(\"no rank\")\n    >>> from torch.distributed.device_mesh import DeviceMesh\n    >>>\n    >>> # Initialize device mesh as (2, 4) to represent the topology\n    >>> # of cross-host(dim 0), and within-host (dim 1).\n    >>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "from_group",
          "signature": "from_group(group: Union[torch.distributed.distributed_c10d.ProcessGroup, List[torch.distributed.distributed_c10d.ProcessGroup]], device_type: str, mesh: Union[torch.Tensor, ForwardRef('ArrayLike'), NoneType] = None, *, mesh_dim_names: Optional[Tuple[str, ...]] = None) -> 'DeviceMesh'",
          "documentation": {
            "description": "Constructs a :class:`DeviceMesh` with ``device_type`` from an\nexisting :class:`ProcessGroup`.\n\nThe constructed device mesh has number of dimensions equal to the\nnumber of groups passed. If more than one group is passed, then the\n``mesh`` argument is required.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_all_groups",
          "signature": "get_all_groups(self) -> List[torch.distributed.distributed_c10d.ProcessGroup]",
          "documentation": {
            "description": "Returns a list of ProcessGroups for all mesh dimensions.",
            "parameters": {},
            "returns": "A list of :class:`ProcessGroup` object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_coordinate",
          "signature": "get_coordinate(self) -> Optional[List[int]]",
          "documentation": {
            "description": "Return the relative indices of this rank relative to all\ndimensions of the mesh. If this rank is not part of the mesh, return None.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_group",
          "signature": "get_group(self, mesh_dim: Union[int, str, NoneType] = None) -> torch.distributed.distributed_c10d.ProcessGroup",
          "documentation": {
            "description": "Returns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the\nDeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.",
            "parameters": {
              "mesh_dim": {
                "type": "str/int, optional",
                "description": "it can be the name of the mesh dimension or the index"
              },
              "of": {
                "type": "",
                "description": "the mesh dimension. Default is None."
              }
            },
            "returns": "A :class:`ProcessGroup` object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_local_rank",
          "signature": "get_local_rank(self, mesh_dim: Union[int, str, NoneType] = None) -> int",
          "documentation": {
            "description": "Returns the local rank of the given mesh_dim of the DeviceMesh.",
            "parameters": {
              "mesh_dim": {
                "type": "str/int, optional",
                "description": "it can be the name of the mesh dimension or the index"
              },
              "of": {
                "type": "",
                "description": "the mesh dimension. Default is None."
              }
            },
            "returns": "An integer denotes the local rank.\n\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2\nhosts with 4 GPUs each.\nCalling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0.\nCalling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.\n\nExample::\n    >>> # xdoctest: +SKIP(\"no rank\")\n    >>> from torch.distributed.device_mesh import DeviceMesh\n    >>>\n    >>> # Initialize device mesh as (2, 4) to represent the topology\n    >>> # of cross-host(dim 0), and within-host (dim 1).\n    >>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_rank",
          "signature": "get_rank(self) -> int",
          "documentation": {
            "description": "Returns the current global rank.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "size",
          "signature": "size(self, mesh_dim: Optional[int] = None) -> int",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DistBackendError",
      "documentation": {
        "description": "Exception raised when a backend error occurs in distributed",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DistError",
      "documentation": {
        "description": "Exception raised when an error occurs in the distributed library",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DistNetworkError",
      "documentation": {
        "description": "Exception raised when a network error occurs in distributed",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DistStoreError",
      "documentation": {
        "description": "Exception raised when an error occurs in the distributed store",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FileStore",
      "documentation": {
        "description": "A store implementation that uses a file to store the underlying key-value pairs.",
        "parameters": {
          "file_name": {
            "type": "str",
            "description": "path of the file in which to store the key-value pairs"
          },
          "world_size": {
            "type": "int, optional",
            "description": "The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users)."
          },
          "Example": {
            "type": "",
            "description": ":\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
          "documentation": {
            "description": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) -> int\n\n\nThe first call to add for a given ``key`` creates a counter associated\nwith ``key`` in the store, initialized to ``amount``. Subsequent calls to add\nwith the same ``key`` increment the counter by the specified ``amount``.\nCalling :meth:`~torch.distributed.store.add` with a key that has already\nbeen set in the store by :meth:`~torch.distributed.store.set` will result\nin an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in the store whose counter will be incremented."
              },
              "amount": {
                "type": "int",
                "description": "The quantity by which the counter will be incremented."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "append",
          "signature": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nAppend the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` does not exists in the store, it will be created.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be appended to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check",
          "signature": "check(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "check(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> bool\n\n\nThe call to check whether a given list of ``keys`` have value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCalling :meth:`~torch.distributed.store.check` with a list of keys that\none wants to check whether stored in the store or not.",
            "parameters": {
              "keys": {
                "type": "lisr[str]",
                "description": "The keys to query whether stored in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compare_set",
          "signature": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
          "documentation": {
            "description": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) -> bytes\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\nperforms comparison between ``expected_value`` and ``desired_value`` before inserting. ``desired_value``\nwill only be set if ``expected_value`` for the ``key`` already exists in the store or if ``expected_value``\nis an empty string.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be checked in the store."
              },
              "expected_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be checked before insertion."
              },
              "desired_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_key",
          "signature": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str) -> bool\n\n\nDeletes the key-value pair associated with ``key`` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n.. warning::\n    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API\n    with the :class:`~torch.distributed.FileStore` will result in an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be deleted from the store"
              }
            },
            "returns": "`True` if ``key`` was deleted, otherwise `False`.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, HashStore can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\")\n    >>> # This should return true\n    >>> store.delete_key(\"first_key\")\n    >>> # This should return false\n    >>> store.delete_key(\"bad_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "get(self: torch._C._distributed_c10d.Store, arg0: str) -> bytes\n\n\nRetrieves the value associated with the given ``key`` in the store. If ``key`` is not\npresent in the store, the function will wait for ``timeout``, which is defined\nwhen initializing the store, before throwing an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The function will return the value associated with this key."
              }
            },
            "returns": "Value associated with ``key`` if ``key`` is in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # Should return \"first_value\"\n    >>> store.get(\"first_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_extended_api",
          "signature": "has_extended_api(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "has_extended_api(self: torch._C._distributed_c10d.Store) -> bool\n\nReturns true if the store supports extended operations.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_get",
          "signature": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> list[bytes]\n\n\nRetrieve all values in ``keys``. If any key in ``keys`` is not\npresent in the store, the function will wait for ``timeout``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to be retrieved from the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_set",
          "signature": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str])",
          "documentation": {
            "description": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str]) -> None\n\n\nInserts a list key-value pair into the store based on the supplied ``keys`` and ``values``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to insert."
              },
              "values": {
                "type": "List[str]",
                "description": "The values to insert."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "num_keys",
          "signature": "num_keys(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "num_keys(self: torch._C._distributed_c10d.Store) -> int\n\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by :meth:`~torch.distributed.store.set`\nand :meth:`~torch.distributed.store.add` since one key is used to coordinate all\nthe workers using the store.\n\n.. warning::\n    When used with the :class:`~torch.distributed.TCPStore`, ``num_keys`` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.",
            "parameters": {},
            "returns": "The number of keys present in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, other store types can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # This should return 2\n    >>> store.num_keys()",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set",
          "signature": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` already exists in the store, it will overwrite the old\nvalue with the new supplied ``value``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be added to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_timeout",
          "signature": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
          "documentation": {
            "description": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) -> None\n\n\nSets the store's default timeout. This timeout is used during initialization and in\n:meth:`~torch.distributed.store.wait` and :meth:`~torch.distributed.store.get`.",
            "parameters": {
              "timeout": {
                "type": "timedelta",
                "description": "timeout to be set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(*args, **kwargs)",
          "documentation": {
            "description": "wait(*args, **kwargs)\nOverloaded function.\n\n1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None\n\n\nWaits for each key in ``keys`` to be added to the store. If not all keys are\nset before the ``timeout`` (set during store initialization), then ``wait``\nwill throw an exception.",
            "parameters": {
              "keys": {
                "type": "list",
                "description": "List of keys on which to wait until they are set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))"
              },
              "2": {
                "type": "",
                "description": ". wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None"
              },
              "Waits": {
                "type": "",
                "description": "for each key in ``keys`` to be added to the store, and throws an exception"
              },
              "if": {
                "type": "",
                "description": "the keys have not been set by the supplied ``timeout``."
              },
              "Arguments": {
                "type": "",
                "description": ""
              },
              "timeout": {
                "type": "timedelta",
                "description": "Time to wait for the keys to be added before throwing an exception."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GatherOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "GradBucket",
      "documentation": {
        "description": "This class mainly passes a flattened gradient tensor\n(returned by :meth:`~torch.distributed.GradBucket.buffer`)\nto DDP communication hook.\nThis tensor can be further decomposed into a list of per-parameter tensors within this bucket\n(returned by :meth:`~torch.distributed.GradBucket.get_per_parameter_tensors`)\nto apply layer-wise operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "buffer",
          "signature": "buffer(self: torch._C._distributed_c10d.GradBucket)",
          "documentation": {
            "description": "buffer(self: torch._C._distributed_c10d.GradBucket) -> torch.Tensor",
            "parameters": {},
            "returns": "A flattened 1D ``torch.Tensor`` buffer,\n    which can be further decomposed into a list of per-parameter tensors within this bucket.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gradients",
          "signature": "gradients(self: torch._C._distributed_c10d.GradBucket)",
          "documentation": {
            "description": "gradients(self: torch._C._distributed_c10d.GradBucket) -> list[torch.Tensor]",
            "parameters": {},
            "returns": "A list of ``torch.Tensor``. Each tensor in the list corresponds to a gradient.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self: torch._C._distributed_c10d.GradBucket)",
          "documentation": {
            "description": "index(self: torch._C._distributed_c10d.GradBucket) -> int\n\n\n.. warning::\n    Since the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training.",
            "parameters": {},
            "returns": "The index of a bucket that stores gradients of a few contiguous layers.\n    All the gradients are bucketized.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_last",
          "signature": "is_last(self: torch._C._distributed_c10d.GradBucket)",
          "documentation": {
            "description": "is_last(self: torch._C._distributed_c10d.GradBucket) -> bool",
            "parameters": {},
            "returns": "Whether this bucket is the last bucket to allreduce in an iteration.\n    This also means that this bucket corresponds to the first few layers in the forward pass.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self: torch._C._distributed_c10d.GradBucket)",
          "documentation": {
            "description": "parameters(self: torch._C._distributed_c10d.GradBucket) -> list[torch.Tensor]",
            "parameters": {},
            "returns": "A list of ``torch.Tensor``. Each tensor in the list corresponds to a model\n    parameter.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_buffer",
          "signature": "set_buffer(self: torch._C._distributed_c10d.GradBucket, buffer: torch.Tensor)",
          "documentation": {
            "description": "set_buffer(self: torch._C._distributed_c10d.GradBucket, buffer: torch.Tensor) -> None\n\n\nReplaces the tensor in the bucket with the input tensor buffer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GroupMember",
      "documentation": {
        "description": "Group member class.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "HashStore",
      "documentation": {
        "description": "A thread-safe store implementation based on an underlying hashmap. This store can be used\nwithin the same process (for example, by other threads), but cannot be used across processes.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> store = dist.HashStore()\n    >>> # store can be used from other threads\n    >>> # Use any of the store methods after initialization\n    >>> store.set(\"first_key\", \"first_value\")",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
          "documentation": {
            "description": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) -> int\n\n\nThe first call to add for a given ``key`` creates a counter associated\nwith ``key`` in the store, initialized to ``amount``. Subsequent calls to add\nwith the same ``key`` increment the counter by the specified ``amount``.\nCalling :meth:`~torch.distributed.store.add` with a key that has already\nbeen set in the store by :meth:`~torch.distributed.store.set` will result\nin an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in the store whose counter will be incremented."
              },
              "amount": {
                "type": "int",
                "description": "The quantity by which the counter will be incremented."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "append",
          "signature": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nAppend the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` does not exists in the store, it will be created.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be appended to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check",
          "signature": "check(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "check(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> bool\n\n\nThe call to check whether a given list of ``keys`` have value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCalling :meth:`~torch.distributed.store.check` with a list of keys that\none wants to check whether stored in the store or not.",
            "parameters": {
              "keys": {
                "type": "lisr[str]",
                "description": "The keys to query whether stored in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compare_set",
          "signature": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
          "documentation": {
            "description": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) -> bytes\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\nperforms comparison between ``expected_value`` and ``desired_value`` before inserting. ``desired_value``\nwill only be set if ``expected_value`` for the ``key`` already exists in the store or if ``expected_value``\nis an empty string.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be checked in the store."
              },
              "expected_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be checked before insertion."
              },
              "desired_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_key",
          "signature": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str) -> bool\n\n\nDeletes the key-value pair associated with ``key`` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n.. warning::\n    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API\n    with the :class:`~torch.distributed.FileStore` will result in an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be deleted from the store"
              }
            },
            "returns": "`True` if ``key`` was deleted, otherwise `False`.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, HashStore can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\")\n    >>> # This should return true\n    >>> store.delete_key(\"first_key\")\n    >>> # This should return false\n    >>> store.delete_key(\"bad_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "get(self: torch._C._distributed_c10d.Store, arg0: str) -> bytes\n\n\nRetrieves the value associated with the given ``key`` in the store. If ``key`` is not\npresent in the store, the function will wait for ``timeout``, which is defined\nwhen initializing the store, before throwing an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The function will return the value associated with this key."
              }
            },
            "returns": "Value associated with ``key`` if ``key`` is in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # Should return \"first_value\"\n    >>> store.get(\"first_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_extended_api",
          "signature": "has_extended_api(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "has_extended_api(self: torch._C._distributed_c10d.Store) -> bool\n\nReturns true if the store supports extended operations.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_get",
          "signature": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> list[bytes]\n\n\nRetrieve all values in ``keys``. If any key in ``keys`` is not\npresent in the store, the function will wait for ``timeout``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to be retrieved from the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_set",
          "signature": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str])",
          "documentation": {
            "description": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str]) -> None\n\n\nInserts a list key-value pair into the store based on the supplied ``keys`` and ``values``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to insert."
              },
              "values": {
                "type": "List[str]",
                "description": "The values to insert."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "num_keys",
          "signature": "num_keys(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "num_keys(self: torch._C._distributed_c10d.Store) -> int\n\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by :meth:`~torch.distributed.store.set`\nand :meth:`~torch.distributed.store.add` since one key is used to coordinate all\nthe workers using the store.\n\n.. warning::\n    When used with the :class:`~torch.distributed.TCPStore`, ``num_keys`` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.",
            "parameters": {},
            "returns": "The number of keys present in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, other store types can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # This should return 2\n    >>> store.num_keys()",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set",
          "signature": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` already exists in the store, it will overwrite the old\nvalue with the new supplied ``value``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be added to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_timeout",
          "signature": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
          "documentation": {
            "description": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) -> None\n\n\nSets the store's default timeout. This timeout is used during initialization and in\n:meth:`~torch.distributed.store.wait` and :meth:`~torch.distributed.store.get`.",
            "parameters": {
              "timeout": {
                "type": "timedelta",
                "description": "timeout to be set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(*args, **kwargs)",
          "documentation": {
            "description": "wait(*args, **kwargs)\nOverloaded function.\n\n1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None\n\n\nWaits for each key in ``keys`` to be added to the store. If not all keys are\nset before the ``timeout`` (set during store initialization), then ``wait``\nwill throw an exception.",
            "parameters": {
              "keys": {
                "type": "list",
                "description": "List of keys on which to wait until they are set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))"
              },
              "2": {
                "type": "",
                "description": ". wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None"
              },
              "Waits": {
                "type": "",
                "description": "for each key in ``keys`` to be added to the store, and throws an exception"
              },
              "if": {
                "type": "",
                "description": "the keys have not been set by the supplied ``timeout``."
              },
              "Arguments": {
                "type": "",
                "description": ""
              },
              "timeout": {
                "type": "timedelta",
                "description": "Time to wait for the keys to be added before throwing an exception."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Logger",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "set_construction_data_and_log",
          "signature": "set_construction_data_and_log(self: torch._C._distributed_c10d.Logger, module_name: str, device_ids: list[int], output_device: int, broadcast_buffers: bool, has_sync_bn: bool, static_graph: bool)",
          "documentation": {
            "description": "set_construction_data_and_log(self: torch._C._distributed_c10d.Logger, module_name: str, device_ids: list[int], output_device: int, broadcast_buffers: bool, has_sync_bn: bool, static_graph: bool) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_error_and_log",
          "signature": "set_error_and_log(self: torch._C._distributed_c10d.Logger, arg0: str)",
          "documentation": {
            "description": "set_error_and_log(self: torch._C._distributed_c10d.Logger, arg0: str) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_runtime_stats_and_log",
          "signature": "set_runtime_stats_and_log(self: torch._C._distributed_c10d.Logger)",
          "documentation": {
            "description": "set_runtime_stats_and_log(self: torch._C._distributed_c10d.Logger) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "P2POp",
      "documentation": {
        "description": "A class to build point-to-point operations for ``batch_isend_irecv``.\n\nThis class builds the type of P2P operation, communication buffer, peer rank,\nProcess Group, and tag. Instances of this class will be passed to\n``batch_isend_irecv`` for point-to-point communications.",
        "parameters": {
          "op": {
            "type": "Callable",
            "description": "A function to send data to or receive data from a peer process."
          },
          "The": {
            "type": "",
            "description": "type of ``op`` is either ``torch.distributed.isend`` or\n``torch.distributed.irecv``."
          },
          "tensor": {
            "type": "Tensor",
            "description": "Tensor to send or receive."
          },
          "peer": {
            "type": "int, optional",
            "description": "Destination or source rank."
          },
          "group": {
            "type": "ProcessGroup, optional",
            "description": "The process group to work on. If None,"
          },
          "the": {
            "type": "",
            "description": "default process group will be used."
          },
          "tag": {
            "type": "int, optional",
            "description": "Tag to match send with recv."
          },
          "group_peer": {
            "type": "int, optional",
            "description": "Destination or source rank."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "PrefixStore",
      "documentation": {
        "description": "A wrapper around any of the 3 key-value stores (:class:`~torch.distributed.TCPStore`,\n:class:`~torch.distributed.FileStore`, and :class:`~torch.distributed.HashStore`)\nthat adds a prefix to each key inserted to the store.",
        "parameters": {
          "prefix": {
            "type": "str",
            "description": "The prefix string that is prepended to each key before being inserted into the store."
          },
          "store": {
            "type": "torch.distributed.store",
            "description": "A store object that forms the underlying key-value store."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
          "documentation": {
            "description": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) -> int\n\n\nThe first call to add for a given ``key`` creates a counter associated\nwith ``key`` in the store, initialized to ``amount``. Subsequent calls to add\nwith the same ``key`` increment the counter by the specified ``amount``.\nCalling :meth:`~torch.distributed.store.add` with a key that has already\nbeen set in the store by :meth:`~torch.distributed.store.set` will result\nin an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in the store whose counter will be incremented."
              },
              "amount": {
                "type": "int",
                "description": "The quantity by which the counter will be incremented."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "append",
          "signature": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nAppend the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` does not exists in the store, it will be created.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be appended to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check",
          "signature": "check(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "check(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> bool\n\n\nThe call to check whether a given list of ``keys`` have value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCalling :meth:`~torch.distributed.store.check` with a list of keys that\none wants to check whether stored in the store or not.",
            "parameters": {
              "keys": {
                "type": "lisr[str]",
                "description": "The keys to query whether stored in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compare_set",
          "signature": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
          "documentation": {
            "description": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) -> bytes\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\nperforms comparison between ``expected_value`` and ``desired_value`` before inserting. ``desired_value``\nwill only be set if ``expected_value`` for the ``key`` already exists in the store or if ``expected_value``\nis an empty string.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be checked in the store."
              },
              "expected_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be checked before insertion."
              },
              "desired_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_key",
          "signature": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str) -> bool\n\n\nDeletes the key-value pair associated with ``key`` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n.. warning::\n    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API\n    with the :class:`~torch.distributed.FileStore` will result in an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be deleted from the store"
              }
            },
            "returns": "`True` if ``key`` was deleted, otherwise `False`.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, HashStore can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\")\n    >>> # This should return true\n    >>> store.delete_key(\"first_key\")\n    >>> # This should return false\n    >>> store.delete_key(\"bad_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "get(self: torch._C._distributed_c10d.Store, arg0: str) -> bytes\n\n\nRetrieves the value associated with the given ``key`` in the store. If ``key`` is not\npresent in the store, the function will wait for ``timeout``, which is defined\nwhen initializing the store, before throwing an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The function will return the value associated with this key."
              }
            },
            "returns": "Value associated with ``key`` if ``key`` is in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # Should return \"first_value\"\n    >>> store.get(\"first_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_extended_api",
          "signature": "has_extended_api(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "has_extended_api(self: torch._C._distributed_c10d.Store) -> bool\n\nReturns true if the store supports extended operations.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_get",
          "signature": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> list[bytes]\n\n\nRetrieve all values in ``keys``. If any key in ``keys`` is not\npresent in the store, the function will wait for ``timeout``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to be retrieved from the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_set",
          "signature": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str])",
          "documentation": {
            "description": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str]) -> None\n\n\nInserts a list key-value pair into the store based on the supplied ``keys`` and ``values``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to insert."
              },
              "values": {
                "type": "List[str]",
                "description": "The values to insert."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "num_keys",
          "signature": "num_keys(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "num_keys(self: torch._C._distributed_c10d.Store) -> int\n\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by :meth:`~torch.distributed.store.set`\nand :meth:`~torch.distributed.store.add` since one key is used to coordinate all\nthe workers using the store.\n\n.. warning::\n    When used with the :class:`~torch.distributed.TCPStore`, ``num_keys`` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.",
            "parameters": {},
            "returns": "The number of keys present in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, other store types can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # This should return 2\n    >>> store.num_keys()",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set",
          "signature": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` already exists in the store, it will overwrite the old\nvalue with the new supplied ``value``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be added to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_timeout",
          "signature": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
          "documentation": {
            "description": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) -> None\n\n\nSets the store's default timeout. This timeout is used during initialization and in\n:meth:`~torch.distributed.store.wait` and :meth:`~torch.distributed.store.get`.",
            "parameters": {
              "timeout": {
                "type": "timedelta",
                "description": "timeout to be set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(*args, **kwargs)",
          "documentation": {
            "description": "wait(*args, **kwargs)\nOverloaded function.\n\n1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None\n\n\nWaits for each key in ``keys`` to be added to the store. If not all keys are\nset before the ``timeout`` (set during store initialization), then ``wait``\nwill throw an exception.",
            "parameters": {
              "keys": {
                "type": "list",
                "description": "List of keys on which to wait until they are set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))"
              },
              "2": {
                "type": "",
                "description": ". wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None"
              },
              "Waits": {
                "type": "",
                "description": "for each key in ``keys`` to be added to the store, and throws an exception"
              },
              "if": {
                "type": "",
                "description": "the keys have not been set by the supplied ``timeout``."
              },
              "Arguments": {
                "type": "",
                "description": ""
              },
              "timeout": {
                "type": "timedelta",
                "description": "Time to wait for the keys to be added before throwing an exception."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProcessGroup",
      "documentation": {
        "description": "A ProcessGroup is a communication primitive that allows for\ncollective operations across a group of processes.\n\nThis is a base class that provides the interface for all\nProcessGroups. It is not meant to be used directly, but rather\nextended by subclasses.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "BackendType",
          "signature": "BackendType(...)",
          "documentation": {
            "description": "The type of the backend used for the process group.\n\nMembers:\n\n  UNDEFINED\n\n  GLOO\n\n  NCCL\n\n  XCCL\n\n  UCC\n\n  MPI\n\n  CUSTOM",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather",
          "signature": "allgather(*args, **kwargs)",
          "documentation": {
            "description": "allgather(*args, **kwargs)\nOverloaded function.\n\n1. allgather(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e50f0>) -> c10d::Work\n\nAllgathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_gather` for more details.\n\n2. allgather(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor) -> c10d::Work\n\nAllgathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_gather: for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather_coalesced",
          "signature": "allgather_coalesced(self: torch._C._distributed_c10d.ProcessGroup, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e54f0>)",
          "documentation": {
            "description": "allgather_coalesced(self: torch._C._distributed_c10d.ProcessGroup, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e54f0>) -> c10d::Work\n\nAllgathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_gather` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather_into_tensor_coalesced",
          "signature": "allgather_into_tensor_coalesced(self: torch._C._distributed_c10d.ProcessGroup, outputs: list[torch.Tensor], inputs: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e5670>)",
          "documentation": {
            "description": "allgather_into_tensor_coalesced(self: torch._C._distributed_c10d.ProcessGroup, outputs: list[torch.Tensor], inputs: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e5670>) -> c10d::Work\n\nAllgathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_gather` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce",
          "signature": "allreduce(*args, **kwargs)",
          "documentation": {
            "description": "allreduce(*args, **kwargs)\nOverloaded function.\n\n1. allreduce(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceOptions = <torch._C._distributed_c10d.AllreduceOptions object at 0x76e292f4b970>) -> c10d::Work\n\nAllreduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.all_reduce` for more details.\n\n2. allreduce(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\nAllreduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.all_reduce` for more details.\n\n3. allreduce(self: torch._C._distributed_c10d.ProcessGroup, tensor: torch.Tensor, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\nAllreduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.all_reduce` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce_coalesced",
          "signature": "allreduce_coalesced(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919e1d70>)",
          "documentation": {
            "description": "allreduce_coalesced(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919e1d70>) -> c10d::Work\n\nAllreduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.all_reduce` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall",
          "signature": "alltoall(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e2919e5db0>)",
          "documentation": {
            "description": "alltoall(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e2919e5db0>) -> c10d::Work\n\nAlltoalls the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_to_all` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall_base",
          "signature": "alltoall_base(self: torch._C._distributed_c10d.ProcessGroup, output: torch.Tensor, input: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f1b330>)",
          "documentation": {
            "description": "alltoall_base(self: torch._C._distributed_c10d.ProcessGroup, output: torch.Tensor, input: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f1b330>) -> c10d::Work\n\nAlltoalls the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.all_to_all for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "barrier",
          "signature": "barrier(self: torch._C._distributed_c10d.ProcessGroup, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e292f259f0>)",
          "documentation": {
            "description": "barrier(self: torch._C._distributed_c10d.ProcessGroup, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e292f259f0>) -> c10d::Work\n\nBlocks until all processes in the group enter the call, and\n              then all leave the call together.\n\n              See :func:`torch.distributed.barrier` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "boxed",
          "signature": "boxed(self: torch._C._distributed_c10d.ProcessGroup)",
          "documentation": {
            "description": "boxed(self: torch._C._distributed_c10d.ProcessGroup) -> object",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "broadcast",
          "signature": "broadcast(*args, **kwargs)",
          "documentation": {
            "description": "broadcast(*args, **kwargs)\nOverloaded function.\n\n1. broadcast(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.BroadcastOptions = <torch._C._distributed_c10d.BroadcastOptions object at 0x76e2919e46b0>) -> c10d::Work\n\nBroadcasts the tensor to all processes in the process group.\n\n              See :func:`torch.distributed.broadcast for more details.\n\n2. broadcast(self: torch._C._distributed_c10d.ProcessGroup, tensor: torch.Tensor, root: int) -> c10d::Work\n\nBroadcasts the tensor to all processes in the process group.\n\n              See :func:`torch.distributed.broadcast` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gather",
          "signature": "gather(*args, **kwargs)",
          "documentation": {
            "description": "gather(*args, **kwargs)\nOverloaded function.\n\n1. gather(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.GatherOptions = <torch._C._distributed_c10d.GatherOptions object at 0x76e2919bf2b0>) -> c10d::Work\n\nGathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.gather` for more details.\n\n2. gather(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor, root: int) -> c10d::Work\n\nGathers the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.gather` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "monitored_barrier",
          "signature": "monitored_barrier(self: torch._C._distributed_c10d.ProcessGroup, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000)",
          "documentation": {
            "description": "monitored_barrier(self: torch._C._distributed_c10d.ProcessGroup, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000), wait_all_ranks: bool = False) -> None\n\nBlocks until all processes in the group enter the call, and\n              then all leave the call together.\n\n              See :func:`torch.distributed.monitored_barrier` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(self: torch._C._distributed_c10d.ProcessGroup)",
          "documentation": {
            "description": "name(self: torch._C._distributed_c10d.ProcessGroup) -> str\n\nGet the name of this process group.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rank",
          "signature": "rank(self: torch._C._distributed_c10d.ProcessGroup)",
          "documentation": {
            "description": "rank(self: torch._C._distributed_c10d.ProcessGroup) -> int\n\nGet the rank of this process group.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv",
          "signature": "recv(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], srcRank: int, tag: int)",
          "documentation": {
            "description": "recv(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], srcRank: int, tag: int) -> c10d::Work\n\nReceives the tensor from the specified rank.\n\n              See :func:`torch.distributed.recv` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv_anysource",
          "signature": "recv_anysource(self: torch._C._distributed_c10d.ProcessGroup, arg0: list[torch.Tensor], arg1: int)",
          "documentation": {
            "description": "recv_anysource(self: torch._C._distributed_c10d.ProcessGroup, arg0: list[torch.Tensor], arg1: int) -> c10d::Work\n\nReceives the tensor from any source.\n\n              See :func:`torch.distributed.recv` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce",
          "signature": "reduce(*args, **kwargs)",
          "documentation": {
            "description": "reduce(*args, **kwargs)\nOverloaded function.\n\n1. reduce(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.ReduceOptions = <torch._C._distributed_c10d.ReduceOptions object at 0x76e2919e4ff0>) -> c10d::Work\n\nReduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.reduce` for more details.\n\n2. reduce(self: torch._C._distributed_c10d.ProcessGroup, tensor: torch.Tensor, root: int, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\nReduces the provided tensors across all processes in the process group.\n\n              See :func:`torch.distributed.reduce` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce_scatter",
          "signature": "reduce_scatter(*args, **kwargs)",
          "documentation": {
            "description": "reduce_scatter(*args, **kwargs)\nOverloaded function.\n\n1. reduce_scatter(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ReduceScatterOptions = <torch._C._distributed_c10d.ReduceScatterOptions object at 0x76e292f15970>) -> c10d::Work\n\nReduces and scatters the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.reduce_scatter` for more details.\n\n2. reduce_scatter(self: torch._C._distributed_c10d.ProcessGroup, output: torch.Tensor, input: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\nReduces and scatters the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.reduce_scatter` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce_scatter_tensor_coalesced",
          "signature": "reduce_scatter_tensor_coalesced(self: torch._C._distributed_c10d.ProcessGroup, outputs: list[torch.Tensor], inputs: list[torch.Tensor], opts: torch._C._distributed_c10d.ReduceScatterOptions = <torch._C._distributed_c10d.ReduceScatterOptions object at 0x76e292f25d30>)",
          "documentation": {
            "description": "reduce_scatter_tensor_coalesced(self: torch._C._distributed_c10d.ProcessGroup, outputs: list[torch.Tensor], inputs: list[torch.Tensor], opts: torch._C._distributed_c10d.ReduceScatterOptions = <torch._C._distributed_c10d.ReduceScatterOptions object at 0x76e292f25d30>) -> c10d::Work\n\nReduces and scatters the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.reduce_scatter` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "scatter",
          "signature": "scatter(*args, **kwargs)",
          "documentation": {
            "description": "scatter(*args, **kwargs)\nOverloaded function.\n\n1. scatter(self: torch._C._distributed_c10d.ProcessGroup, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ScatterOptions = <torch._C._distributed_c10d.ScatterOptions object at 0x76e292f27e30>) -> c10d::Work\n\nScatters the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.scatter` for more details.\n\n2. scatter(self: torch._C._distributed_c10d.ProcessGroup, output_tensor: torch.Tensor, input_tensors: list[torch.Tensor], root: int) -> c10d::Work\n\nScatters the input tensors from all processes across the process group.\n\n              See :func:`torch.distributed.scatter` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "send",
          "signature": "send(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], dstRank: int, tag: int)",
          "documentation": {
            "description": "send(self: torch._C._distributed_c10d.ProcessGroup, tensors: list[torch.Tensor], dstRank: int, tag: int) -> c10d::Work\n\nSends the tensor to the specified rank.\n\n              See :func:`torch.distributed.send` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "size",
          "signature": "size(self: torch._C._distributed_c10d.ProcessGroup)",
          "documentation": {
            "description": "size(self: torch._C._distributed_c10d.ProcessGroup) -> int\n\nGet the size of this process group.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "unbox",
          "signature": "unbox(arg0: object)",
          "documentation": {
            "description": "unbox(arg0: object) -> torch._C._distributed_c10d.ProcessGroup",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProcessGroupGloo",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "Device",
          "signature": "Device(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "Options",
          "signature": "Options(...)",
          "documentation": {
            "description": "Base class for all backend options implementations, such as the nccl\noptions :class:`~torch.distributed.ProcessGroupNCCL.Options`).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather",
          "signature": "allgather(*args, **kwargs)",
          "documentation": {
            "description": "allgather(*args, **kwargs)\nOverloaded function.\n\n1. allgather(self: torch._C._distributed_c10d.Backend, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e1630>) -> c10d::Work\n\n2. allgather(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather_coalesced",
          "signature": "allgather_coalesced(self: torch._C._distributed_c10d.Backend, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2eccef7f0>)",
          "documentation": {
            "description": "allgather_coalesced(self: torch._C._distributed_c10d.Backend, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2eccef7f0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce",
          "signature": "allreduce(*args, **kwargs)",
          "documentation": {
            "description": "allreduce(*args, **kwargs)\nOverloaded function.\n\n1. allreduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceOptions = <torch._C._distributed_c10d.AllreduceOptions object at 0x76e2919d1f70>) -> c10d::Work\n\n2. allreduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\n3. allreduce(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce_coalesced",
          "signature": "allreduce_coalesced(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919d37b0>)",
          "documentation": {
            "description": "allreduce_coalesced(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919d37b0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall",
          "signature": "alltoall(self: torch._C._distributed_c10d.Backend, output_tensor: list[torch.Tensor], input_tensor: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f28fb0>)",
          "documentation": {
            "description": "alltoall(self: torch._C._distributed_c10d.Backend, output_tensor: list[torch.Tensor], input_tensor: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f28fb0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall_base",
          "signature": "alltoall_base(*args, **kwargs)",
          "documentation": {
            "description": "alltoall_base(*args, **kwargs)\nOverloaded function.\n\n1. alltoall_base(self: torch._C._distributed_c10d.Backend, output_tensor: torch.Tensor, input_tensor: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e2919d01b0>) -> c10d::Work\n\n2. alltoall_base(self: torch._C._distributed_c10d.Backend, output: torch.Tensor, input: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int]) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "barrier",
          "signature": "barrier(self: torch._C._distributed_c10d.Backend, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e2919e03b0>)",
          "documentation": {
            "description": "barrier(self: torch._C._distributed_c10d.Backend, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e2919e03b0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "broadcast",
          "signature": "broadcast(*args, **kwargs)",
          "documentation": {
            "description": "broadcast(*args, **kwargs)\nOverloaded function.\n\n1. broadcast(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.BroadcastOptions = <torch._C._distributed_c10d.BroadcastOptions object at 0x76e2919d7370>) -> c10d::Work\n\n2. broadcast(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_default_device",
          "signature": "create_default_device()",
          "documentation": {
            "description": "create_default_device() -> torch._C._distributed_c10d.ProcessGroupGloo.Device",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_device",
          "signature": "create_device(hostname: str = '', interface: str = '')",
          "documentation": {
            "description": "create_device(hostname: str = '', interface: str = '') -> torch._C._distributed_c10d.ProcessGroupGloo.Device",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eager_connect_single_device",
          "signature": "eager_connect_single_device(self: torch._C._distributed_c10d.Backend, arg0: torch.device)",
          "documentation": {
            "description": "eager_connect_single_device(self: torch._C._distributed_c10d.Backend, arg0: torch.device) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gather",
          "signature": "gather(*args, **kwargs)",
          "documentation": {
            "description": "gather(*args, **kwargs)\nOverloaded function.\n\n1. gather(self: torch._C._distributed_c10d.Backend, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.GatherOptions = <torch._C._distributed_c10d.GatherOptions object at 0x76e292f36070>) -> c10d::Work\n\n2. gather(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor, root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "monitored_barrier",
          "signature": "monitored_barrier(self: torch._C._distributed_c10d.Backend, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000)",
          "documentation": {
            "description": "monitored_barrier(self: torch._C._distributed_c10d.Backend, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000), wait_all_ranks: bool = False) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "name(self: torch._C._distributed_c10d.Backend) -> str",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rank",
          "signature": "rank(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "rank(self: torch._C._distributed_c10d.Backend) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv",
          "signature": "recv(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], srcRank: int, tag: int)",
          "documentation": {
            "description": "recv(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], srcRank: int, tag: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv_anysource",
          "signature": "recv_anysource(self: torch._C._distributed_c10d.Backend, arg0: list[torch.Tensor], arg1: int)",
          "documentation": {
            "description": "recv_anysource(self: torch._C._distributed_c10d.Backend, arg0: list[torch.Tensor], arg1: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce",
          "signature": "reduce(*args, **kwargs)",
          "documentation": {
            "description": "reduce(*args, **kwargs)\nOverloaded function.\n\n1. reduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.ReduceOptions = <torch._C._distributed_c10d.ReduceOptions object at 0x76e2ed127cf0>) -> c10d::Work\n\n2. reduce(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, root: int, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce_scatter",
          "signature": "reduce_scatter(*args, **kwargs)",
          "documentation": {
            "description": "reduce_scatter(*args, **kwargs)\nOverloaded function.\n\n1. reduce_scatter(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ReduceScatterOptions = <torch._C._distributed_c10d.ReduceScatterOptions object at 0x76e292f249b0>) -> c10d::Work\n\n2. reduce_scatter(self: torch._C._distributed_c10d.Backend, output_tensors: torch.Tensor, input_tensor: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "scatter",
          "signature": "scatter(*args, **kwargs)",
          "documentation": {
            "description": "scatter(*args, **kwargs)\nOverloaded function.\n\n1. scatter(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ScatterOptions = <torch._C._distributed_c10d.ScatterOptions object at 0x76e2919bf670>) -> c10d::Work\n\n2. scatter(self: torch._C._distributed_c10d.Backend, output_tensor: torch.Tensor, input_tensors: list[torch.Tensor], root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "send",
          "signature": "send(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], dstRank: int, tag: int)",
          "documentation": {
            "description": "send(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], dstRank: int, tag: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "size",
          "signature": "size(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "size(self: torch._C._distributed_c10d.Backend) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProcessGroupNCCL",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "NCCLConfig",
          "signature": "NCCLConfig(...)",
          "documentation": {
            "description": "ncclConfig_t data type for configuring NCCL communicators.\nSee https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\nfor details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "Options",
          "signature": "Options(...)",
          "documentation": {
            "description": "ProcessGroup options for the NCCL backend",
            "parameters": {
              "is_high_priority_stream": {
                "type": "bool, optional",
                "description": "flag to enable/disable process"
              },
              "group": {
                "type": "",
                "description": "to pick up high priority cuda streams. It lets CUDA driver"
              },
              "to": {
                "type": "",
                "description": "prioritize NCCL kernels when there are compute kernels waiting."
              },
              "Default": {
                "type": "",
                "description": "is False."
              },
              "Attributes": {
                "type": "",
                "description": ""
              },
              "config": {
                "type": "NCCLConfig",
                "description": "configures NCCL communicators (only avaiable for"
              },
              "builds": {
                "type": "",
                "description": "using NCCL 2.17+). This can be used to improve"
              },
              "communication": {
                "type": "",
                "description": "-computation overlap for NCCL kernels by tuning"
              },
              "available": {
                "type": "",
                "description": "parameters in the config. See"
              },
              "https": {
                "type": "",
                "description": "//docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t"
              },
              "for": {
                "type": "",
                "description": "details."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>>\n>>> nccl_options = dist.ProcessGroupNCCL.Options(is_high_priority_stream=True)\n>>> # For builds using NCCL 2.17+, configure communicators\n>>> nccl_options.config.cga_cluster_size = 2\n>>> nccl_options.config.max_ctas = 4\n>>> nccl_options.config.min_ctas = 2\n>>> nccl_options.config.split_share = 1\n>>> # initialize a nccl process group with the options just created\n>>> dist.init_process_group(\"nccl\", pg_options=nccl_options)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "abort",
          "signature": "abort(self: torch._C._distributed_c10d.ProcessGroupNCCL)",
          "documentation": {
            "description": "abort(self: torch._C._distributed_c10d.ProcessGroupNCCL) -> None\n\nAbort the process group.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather",
          "signature": "allgather(*args, **kwargs)",
          "documentation": {
            "description": "allgather(*args, **kwargs)\nOverloaded function.\n\n1. allgather(self: torch._C._distributed_c10d.Backend, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2919e1630>) -> c10d::Work\n\n2. allgather(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allgather_coalesced",
          "signature": "allgather_coalesced(self: torch._C._distributed_c10d.Backend, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2eccef7f0>)",
          "documentation": {
            "description": "allgather_coalesced(self: torch._C._distributed_c10d.Backend, output_lists: list[list[torch.Tensor]], input_list: list[torch.Tensor], opts: torch._C._distributed_c10d.AllgatherOptions = <torch._C._distributed_c10d.AllgatherOptions object at 0x76e2eccef7f0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce",
          "signature": "allreduce(*args, **kwargs)",
          "documentation": {
            "description": "allreduce(*args, **kwargs)\nOverloaded function.\n\n1. allreduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceOptions = <torch._C._distributed_c10d.AllreduceOptions object at 0x76e2919d1f70>) -> c10d::Work\n\n2. allreduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work\n\n3. allreduce(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "allreduce_coalesced",
          "signature": "allreduce_coalesced(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919d37b0>)",
          "documentation": {
            "description": "allreduce_coalesced(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.AllreduceCoalescedOptions = <torch._C._distributed_c10d.AllreduceCoalescedOptions object at 0x76e2919d37b0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall",
          "signature": "alltoall(self: torch._C._distributed_c10d.Backend, output_tensor: list[torch.Tensor], input_tensor: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f28fb0>)",
          "documentation": {
            "description": "alltoall(self: torch._C._distributed_c10d.Backend, output_tensor: list[torch.Tensor], input_tensor: list[torch.Tensor], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e292f28fb0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "alltoall_base",
          "signature": "alltoall_base(*args, **kwargs)",
          "documentation": {
            "description": "alltoall_base(*args, **kwargs)\nOverloaded function.\n\n1. alltoall_base(self: torch._C._distributed_c10d.Backend, output_tensor: torch.Tensor, input_tensor: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int], opts: torch._C._distributed_c10d.AllToAllOptions = <torch._C._distributed_c10d.AllToAllOptions object at 0x76e2919d01b0>) -> c10d::Work\n\n2. alltoall_base(self: torch._C._distributed_c10d.Backend, output: torch.Tensor, input: torch.Tensor, output_split_sizes: list[int], input_split_sizes: list[int]) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "barrier",
          "signature": "barrier(self: torch._C._distributed_c10d.Backend, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e2919e03b0>)",
          "documentation": {
            "description": "barrier(self: torch._C._distributed_c10d.Backend, opts: torch._C._distributed_c10d.BarrierOptions = <torch._C._distributed_c10d.BarrierOptions object at 0x76e2919e03b0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "broadcast",
          "signature": "broadcast(*args, **kwargs)",
          "documentation": {
            "description": "broadcast(*args, **kwargs)\nOverloaded function.\n\n1. broadcast(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.BroadcastOptions = <torch._C._distributed_c10d.BroadcastOptions object at 0x76e2919d7370>) -> c10d::Work\n\n2. broadcast(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "comm_split_count",
          "signature": "comm_split_count(self: torch._C._distributed_c10d.ProcessGroupNCCL)",
          "documentation": {
            "description": "comm_split_count(self: torch._C._distributed_c10d.ProcessGroupNCCL) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "deregister_mem_pool",
          "signature": "deregister_mem_pool(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch._C._MemPool)",
          "documentation": {
            "description": "deregister_mem_pool(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch._C._MemPool) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eager_connect_single_device",
          "signature": "eager_connect_single_device(self: torch._C._distributed_c10d.Backend, arg0: torch.device)",
          "documentation": {
            "description": "eager_connect_single_device(self: torch._C._distributed_c10d.Backend, arg0: torch.device) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gather",
          "signature": "gather(*args, **kwargs)",
          "documentation": {
            "description": "gather(*args, **kwargs)\nOverloaded function.\n\n1. gather(self: torch._C._distributed_c10d.Backend, output_tensors: list[list[torch.Tensor]], input_tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.GatherOptions = <torch._C._distributed_c10d.GatherOptions object at 0x76e292f36070>) -> c10d::Work\n\n2. gather(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensor: torch.Tensor, root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "monitored_barrier",
          "signature": "monitored_barrier(self: torch._C._distributed_c10d.Backend, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000)",
          "documentation": {
            "description": "monitored_barrier(self: torch._C._distributed_c10d.Backend, timeout: datetime.timedelta = datetime.timedelta(days=-1, seconds=86399, microseconds=999000), wait_all_ranks: bool = False) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "name(self: torch._C._distributed_c10d.Backend) -> str",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "perform_nocolor_split",
          "signature": "perform_nocolor_split(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch.device)",
          "documentation": {
            "description": "perform_nocolor_split(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch.device) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "rank",
          "signature": "rank(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "rank(self: torch._C._distributed_c10d.Backend) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv",
          "signature": "recv(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], srcRank: int, tag: int)",
          "documentation": {
            "description": "recv(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], srcRank: int, tag: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recv_anysource",
          "signature": "recv_anysource(self: torch._C._distributed_c10d.Backend, arg0: list[torch.Tensor], arg1: int)",
          "documentation": {
            "description": "recv_anysource(self: torch._C._distributed_c10d.Backend, arg0: list[torch.Tensor], arg1: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce",
          "signature": "reduce(*args, **kwargs)",
          "documentation": {
            "description": "reduce(*args, **kwargs)\nOverloaded function.\n\n1. reduce(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], opts: torch._C._distributed_c10d.ReduceOptions = <torch._C._distributed_c10d.ReduceOptions object at 0x76e2ed127cf0>) -> c10d::Work\n\n2. reduce(self: torch._C._distributed_c10d.Backend, tensor: torch.Tensor, root: int, op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reduce_scatter",
          "signature": "reduce_scatter(*args, **kwargs)",
          "documentation": {
            "description": "reduce_scatter(*args, **kwargs)\nOverloaded function.\n\n1. reduce_scatter(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ReduceScatterOptions = <torch._C._distributed_c10d.ReduceScatterOptions object at 0x76e292f249b0>) -> c10d::Work\n\n2. reduce_scatter(self: torch._C._distributed_c10d.Backend, output_tensors: torch.Tensor, input_tensor: list[torch.Tensor], op: torch._C._distributed_c10d.ReduceOp = <RedOpType.SUM: 0>) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_mem_pool",
          "signature": "register_mem_pool(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch._C._MemPool)",
          "documentation": {
            "description": "register_mem_pool(self: torch._C._distributed_c10d.ProcessGroupNCCL, arg0: torch._C._MemPool) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "scatter",
          "signature": "scatter(*args, **kwargs)",
          "documentation": {
            "description": "scatter(*args, **kwargs)\nOverloaded function.\n\n1. scatter(self: torch._C._distributed_c10d.Backend, output_tensors: list[torch.Tensor], input_tensors: list[list[torch.Tensor]], opts: torch._C._distributed_c10d.ScatterOptions = <torch._C._distributed_c10d.ScatterOptions object at 0x76e2919bf670>) -> c10d::Work\n\n2. scatter(self: torch._C._distributed_c10d.Backend, output_tensor: torch.Tensor, input_tensors: list[torch.Tensor], root: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "send",
          "signature": "send(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], dstRank: int, tag: int)",
          "documentation": {
            "description": "send(self: torch._C._distributed_c10d.Backend, tensors: list[torch.Tensor], dstRank: int, tag: int) -> c10d::Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "size",
          "signature": "size(self: torch._C._distributed_c10d.Backend)",
          "documentation": {
            "description": "size(self: torch._C._distributed_c10d.Backend) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ReduceOp",
      "documentation": {
        "description": "An enum-like class for available reduction operations: ``SUM``, ``PRODUCT``,\n``MIN``, ``MAX``, ``BAND``, ``BOR``, ``BXOR``, and ``PREMUL_SUM``.\n\n``BAND``, ``BOR``, and ``BXOR`` reductions are not available when\nusing the ``NCCL`` backend.\n\n``AVG`` divides values by the world size before summing across ranks.\n``AVG`` is only available with the ``NCCL`` backend,\nand only for NCCL versions 2.10 or later.\n\n``PREMUL_SUM`` multiplies inputs by a given scalar locally before reduction.\n``PREMUL_SUM`` is only available with the ``NCCL`` backend,\nand only available for NCCL versions 2.11 or later. Users are supposed to\nuse ``torch.distributed._make_nccl_premul_sum``.\n\nAdditionally, ``MAX``, ``MIN`` and ``PRODUCT`` are not supported for complex tensors.\n\nThe values of this class can be accessed as attributes, e.g., ``ReduceOp.SUM``.\nThey are used in specifying strategies for reduction collectives, e.g.,\n:func:`reduce`.\n\nThis class does not support ``__members__`` property.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "RedOpType",
          "signature": "RedOpType(...)",
          "documentation": {
            "description": "Members:\n\nSUM\n\nAVG\n\nPRODUCT\n\nMIN\n\nMAX\n\nBAND\n\nBOR\n\nBXOR\n\nPREMUL_SUM",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ReduceOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "ReduceScatterOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Reducer",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "get_backward_stats",
          "signature": "get_backward_stats(self: torch._C._distributed_c10d.Reducer)",
          "documentation": {
            "description": "get_backward_stats(self: torch._C._distributed_c10d.Reducer) -> list[int]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "prepare_for_backward",
          "signature": "prepare_for_backward(*args, **kwargs)",
          "documentation": {
            "description": "prepare_for_backward(*args, **kwargs)\nOverloaded function.\n\n1. prepare_for_backward(self: torch._C._distributed_c10d.Reducer, arg0: list[torch.Tensor]) -> None\n\n2. prepare_for_backward(self: torch._C._distributed_c10d.Reducer, arg0: torch.Tensor) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "prepare_for_forward",
          "signature": "prepare_for_forward(self: torch._C._distributed_c10d.Reducer)",
          "documentation": {
            "description": "prepare_for_forward(self: torch._C._distributed_c10d.Reducer) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_logger",
          "signature": "set_logger(self: torch._C._distributed_c10d.Reducer, arg0: c10d::Logger)",
          "documentation": {
            "description": "set_logger(self: torch._C._distributed_c10d.Reducer, arg0: c10d::Logger) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ScatterOptions",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Store",
      "documentation": {
        "description": "Base class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (:class:`~torch.distributed.TCPStore`, :class:`~torch.distributed.FileStore`,\nand :class:`~torch.distributed.HashStore`).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
          "documentation": {
            "description": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) -> int\n\n\nThe first call to add for a given ``key`` creates a counter associated\nwith ``key`` in the store, initialized to ``amount``. Subsequent calls to add\nwith the same ``key`` increment the counter by the specified ``amount``.\nCalling :meth:`~torch.distributed.store.add` with a key that has already\nbeen set in the store by :meth:`~torch.distributed.store.set` will result\nin an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in the store whose counter will be incremented."
              },
              "amount": {
                "type": "int",
                "description": "The quantity by which the counter will be incremented."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "append",
          "signature": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nAppend the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` does not exists in the store, it will be created.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be appended to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check",
          "signature": "check(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "check(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> bool\n\n\nThe call to check whether a given list of ``keys`` have value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCalling :meth:`~torch.distributed.store.check` with a list of keys that\none wants to check whether stored in the store or not.",
            "parameters": {
              "keys": {
                "type": "lisr[str]",
                "description": "The keys to query whether stored in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compare_set",
          "signature": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
          "documentation": {
            "description": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) -> bytes\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\nperforms comparison between ``expected_value`` and ``desired_value`` before inserting. ``desired_value``\nwill only be set if ``expected_value`` for the ``key`` already exists in the store or if ``expected_value``\nis an empty string.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be checked in the store."
              },
              "expected_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be checked before insertion."
              },
              "desired_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_key",
          "signature": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str) -> bool\n\n\nDeletes the key-value pair associated with ``key`` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n.. warning::\n    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API\n    with the :class:`~torch.distributed.FileStore` will result in an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be deleted from the store"
              }
            },
            "returns": "`True` if ``key`` was deleted, otherwise `False`.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, HashStore can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\")\n    >>> # This should return true\n    >>> store.delete_key(\"first_key\")\n    >>> # This should return false\n    >>> store.delete_key(\"bad_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "get(self: torch._C._distributed_c10d.Store, arg0: str) -> bytes\n\n\nRetrieves the value associated with the given ``key`` in the store. If ``key`` is not\npresent in the store, the function will wait for ``timeout``, which is defined\nwhen initializing the store, before throwing an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The function will return the value associated with this key."
              }
            },
            "returns": "Value associated with ``key`` if ``key`` is in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # Should return \"first_value\"\n    >>> store.get(\"first_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_extended_api",
          "signature": "has_extended_api(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "has_extended_api(self: torch._C._distributed_c10d.Store) -> bool\n\nReturns true if the store supports extended operations.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_get",
          "signature": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> list[bytes]\n\n\nRetrieve all values in ``keys``. If any key in ``keys`` is not\npresent in the store, the function will wait for ``timeout``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to be retrieved from the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_set",
          "signature": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str])",
          "documentation": {
            "description": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str]) -> None\n\n\nInserts a list key-value pair into the store based on the supplied ``keys`` and ``values``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to insert."
              },
              "values": {
                "type": "List[str]",
                "description": "The values to insert."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "num_keys",
          "signature": "num_keys(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "num_keys(self: torch._C._distributed_c10d.Store) -> int\n\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by :meth:`~torch.distributed.store.set`\nand :meth:`~torch.distributed.store.add` since one key is used to coordinate all\nthe workers using the store.\n\n.. warning::\n    When used with the :class:`~torch.distributed.TCPStore`, ``num_keys`` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.",
            "parameters": {},
            "returns": "The number of keys present in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, other store types can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # This should return 2\n    >>> store.num_keys()",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set",
          "signature": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` already exists in the store, it will overwrite the old\nvalue with the new supplied ``value``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be added to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_timeout",
          "signature": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
          "documentation": {
            "description": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) -> None\n\n\nSets the store's default timeout. This timeout is used during initialization and in\n:meth:`~torch.distributed.store.wait` and :meth:`~torch.distributed.store.get`.",
            "parameters": {
              "timeout": {
                "type": "timedelta",
                "description": "timeout to be set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(*args, **kwargs)",
          "documentation": {
            "description": "wait(*args, **kwargs)\nOverloaded function.\n\n1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None\n\n\nWaits for each key in ``keys`` to be added to the store. If not all keys are\nset before the ``timeout`` (set during store initialization), then ``wait``\nwill throw an exception.",
            "parameters": {
              "keys": {
                "type": "list",
                "description": "List of keys on which to wait until they are set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))"
              },
              "2": {
                "type": "",
                "description": ". wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None"
              },
              "Waits": {
                "type": "",
                "description": "for each key in ``keys`` to be added to the store, and throws an exception"
              },
              "if": {
                "type": "",
                "description": "the keys have not been set by the supplied ``timeout``."
              },
              "Arguments": {
                "type": "",
                "description": ""
              },
              "timeout": {
                "type": "timedelta",
                "description": "Time to wait for the keys to be added before throwing an exception."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TCPStore",
      "documentation": {
        "description": "A TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as :meth:`~torch.distributed.store.set` to insert a key-value\npair, :meth:`~torch.distributed.store.get` to retrieve a key-value pair, etc. There\nshould always be one server store initialized because the client store(s) will wait for\nthe server to establish a connection.",
        "parameters": {
          "host_name": {
            "type": "str",
            "description": "The hostname or IP Address the server store should run on."
          },
          "port": {
            "type": "int",
            "description": "The port on which the server store should listen for incoming requests."
          },
          "world_size": {
            "type": "int, optional",
            "description": "The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users)."
          },
          "is_master": {
            "type": "bool, optional",
            "description": "True when initializing the server store and False for client stores. Default is False."
          },
          "timeout": {
            "type": "timedelta, optional",
            "description": "Timeout used by the store during initialization and for methods such as :meth:`~torch.distributed.store.get` and :meth:`~torch.distributed.store.wait`. Default is timedelta(seconds=300)"
          },
          "wait_for_workers": {
            "type": "bool, optional",
            "description": "Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True."
          },
          "multi_tenant": {
            "type": "bool, optional",
            "description": "If True, all ``TCPStore`` instances in the current process with the same host/port will use the same underlying ``TCPServer``. Default is False."
          },
          "master_listen_fd": {
            "type": "int, optional",
            "description": "If specified, the underlying ``TCPServer`` will listen on this file descriptor, which must be a socket already bound to ``port``. Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to ``port``)."
          },
          "use_libuv": {
            "type": "bool, optional",
            "description": "If True, use libuv for ``TCPServer`` backend. Default is True."
          },
          "Example": {
            "type": "",
            "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)",
          "documentation": {
            "description": "add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) -> int\n\n\nThe first call to add for a given ``key`` creates a counter associated\nwith ``key`` in the store, initialized to ``amount``. Subsequent calls to add\nwith the same ``key`` increment the counter by the specified ``amount``.\nCalling :meth:`~torch.distributed.store.add` with a key that has already\nbeen set in the store by :meth:`~torch.distributed.store.set` will result\nin an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in the store whose counter will be incremented."
              },
              "amount": {
                "type": "int",
                "description": "The quantity by which the counter will be incremented."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "append",
          "signature": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "append(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nAppend the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` does not exists in the store, it will be created.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be appended to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check",
          "signature": "check(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "check(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> bool\n\n\nThe call to check whether a given list of ``keys`` have value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCalling :meth:`~torch.distributed.store.check` with a list of keys that\none wants to check whether stored in the store or not.",
            "parameters": {
              "keys": {
                "type": "lisr[str]",
                "description": "The keys to query whether stored in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compare_set",
          "signature": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)",
          "documentation": {
            "description": "compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) -> bytes\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\nperforms comparison between ``expected_value`` and ``desired_value`` before inserting. ``desired_value``\nwill only be set if ``expected_value`` for the ``key`` already exists in the store or if ``expected_value``\nis an empty string.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be checked in the store."
              },
              "expected_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be checked before insertion."
              },
              "desired_value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_key",
          "signature": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "delete_key(self: torch._C._distributed_c10d.Store, arg0: str) -> bool\n\n\nDeletes the key-value pair associated with ``key`` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n.. warning::\n    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API\n    with the :class:`~torch.distributed.FileStore` will result in an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be deleted from the store"
              }
            },
            "returns": "`True` if ``key`` was deleted, otherwise `False`.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, HashStore can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\")\n    >>> # This should return true\n    >>> store.delete_key(\"first_key\")\n    >>> # This should return false\n    >>> store.delete_key(\"bad_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self: torch._C._distributed_c10d.Store, arg0: str)",
          "documentation": {
            "description": "get(self: torch._C._distributed_c10d.Store, arg0: str) -> bytes\n\n\nRetrieves the value associated with the given ``key`` in the store. If ``key`` is not\npresent in the store, the function will wait for ``timeout``, which is defined\nwhen initializing the store, before throwing an exception.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The function will return the value associated with this key."
              }
            },
            "returns": "Value associated with ``key`` if ``key`` is in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # Should return \"first_value\"\n    >>> store.get(\"first_key\")",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_extended_api",
          "signature": "has_extended_api(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "has_extended_api(self: torch._C._distributed_c10d.Store) -> bool\n\nReturns true if the store supports extended operations.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_get",
          "signature": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str])",
          "documentation": {
            "description": "multi_get(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> list[bytes]\n\n\nRetrieve all values in ``keys``. If any key in ``keys`` is not\npresent in the store, the function will wait for ``timeout``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to be retrieved from the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "multi_set",
          "signature": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str])",
          "documentation": {
            "description": "multi_set(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: list[str]) -> None\n\n\nInserts a list key-value pair into the store based on the supplied ``keys`` and ``values``",
            "parameters": {
              "keys": {
                "type": "List[str]",
                "description": "The keys to insert."
              },
              "values": {
                "type": "List[str]",
                "description": "The values to insert."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "num_keys",
          "signature": "num_keys(self: torch._C._distributed_c10d.Store)",
          "documentation": {
            "description": "num_keys(self: torch._C._distributed_c10d.Store) -> int\n\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added by :meth:`~torch.distributed.store.set`\nand :meth:`~torch.distributed.store.add` since one key is used to coordinate all\nthe workers using the store.\n\n.. warning::\n    When used with the :class:`~torch.distributed.TCPStore`, ``num_keys`` returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.",
            "parameters": {},
            "returns": "The number of keys present in the store.\n\nExample::\n    >>> import torch.distributed as dist\n    >>> from datetime import timedelta\n    >>> # Using TCPStore as an example, other store types can also be used\n    >>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n    >>> store.set(\"first_key\", \"first_value\")\n    >>> # This should return 2\n    >>> store.num_keys()",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set",
          "signature": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)",
          "documentation": {
            "description": "set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) -> None\n\n\nInserts the key-value pair into the store based on the supplied ``key`` and\n``value``. If ``key`` already exists in the store, it will overwrite the old\nvalue with the new supplied ``value``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key to be added to the store."
              },
              "value": {
                "type": "str",
                "description": "The value associated with ``key`` to be added to the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_timeout",
          "signature": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)",
          "documentation": {
            "description": "set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) -> None\n\n\nSets the store's default timeout. This timeout is used during initialization and in\n:meth:`~torch.distributed.store.wait` and :meth:`~torch.distributed.store.get`.",
            "parameters": {
              "timeout": {
                "type": "timedelta",
                "description": "timeout to be set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(*args, **kwargs)",
          "documentation": {
            "description": "wait(*args, **kwargs)\nOverloaded function.\n\n1. wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None\n\n\nWaits for each key in ``keys`` to be added to the store. If not all keys are\nset before the ``timeout`` (set during store initialization), then ``wait``\nwill throw an exception.",
            "parameters": {
              "keys": {
                "type": "list",
                "description": "List of keys on which to wait until they are set in the store."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))"
              },
              "2": {
                "type": "",
                "description": ". wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None"
              },
              "Waits": {
                "type": "",
                "description": "for each key in ``keys`` to be added to the store, and throws an exception"
              },
              "if": {
                "type": "",
                "description": "the keys have not been set by the supplied ``timeout``."
              },
              "Arguments": {
                "type": "",
                "description": ""
              },
              "timeout": {
                "type": "timedelta",
                "description": "Time to wait for the keys to be added before throwing an exception."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Work",
      "documentation": {
        "description": "A `Work` object represents the handle to a pending asynchronous operation in\nPyTorch's distributed package. It is returned by non-blocking collective operations,\nsuch as `dist.all_reduce(tensor, async_op=True)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "boxed",
          "signature": "boxed(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "boxed(self: torch._C._distributed_c10d.Work) -> object",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "exception",
          "signature": "exception(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "exception(self: torch._C._distributed_c10d.Work) -> std::__exception_ptr::exception_ptr",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_future",
          "signature": "get_future(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "get_future(self: torch._C._distributed_c10d.Work) -> torch.Future",
            "parameters": {},
            "returns": "A ``torch.futures.Future`` object which is associated with the completion of\n    the ``Work``. As an example, a future object can be retrieved\n    by ``fut = process_group.allreduce(tensors).get_future()``.\n\nExample::\n    Below is an example of a simple allreduce DDP communication hook that uses\n    ``get_future` API to retrieve a Future associated with the completion of\n    ``allreduce``.\n\n    >>> def allreduce(process_group: dist.ProcessGroup, bucket: dist.GradBucket): -> torch.futures.Future\n    >>>     group_to_use = process_group if process_group is not None else torch.distributed.group.WORLD\n    >>>     tensor = bucket.buffer().div_(group_to_use.size())\n    >>>     return torch.distributed.all_reduce(tensor, group=group_to_use, async_op=True).get_future()\n    >>> ddp_model.register_comm_hook(state=None, hook=allreduce)\n\n.. warning ::\n    ``get_future`` API supports NCCL, and partially GLOO and MPI backends\n    (no support for peer-to-peer operations like send/recv) and will return a ``torch.futures.Future``.\n\n    In the example above, ``allreduce`` work will be done on GPU using NCCL backend,\n    ``fut.wait()`` will return after synchronizing the appropriate NCCL streams\n    with PyTorch's current device streams to ensure we can have asynchronous CUDA\n    execution and it does not wait for the entire operation to complete on GPU. Note that\n    ``CUDAFuture``  does not support ``TORCH_NCCL_BLOCKING_WAIT`` flag or NCCL's ``barrier()``.\n    In addition, if a callback function was added by ``fut.then()``, it will wait until\n    ``WorkNCCL``'s NCCL streams synchronize with ``ProcessGroupNCCL``'s dedicated callback\n    stream and invoke the callback inline after running the callback on the callback stream.\n    ``fut.then()`` will return another ``CUDAFuture`` that holds the return value of the\n    callback and a ``CUDAEvent`` that recorded the callback stream.\n\n        1. For CPU work, ``fut.done()`` returns true when work has been completed and value()\n           tensors are ready.\n        2. For GPU work, ``fut.done()`` returns true only whether the operation has been enqueued.\n        3. For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), ``fut.done()`` returns\n           true when tensors have arrived on respective nodes, but not yet necessarily synched on\n           respective GPUs (similarly to GPU work).",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_future_result",
          "signature": "get_future_result(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "get_future_result(self: torch._C._distributed_c10d.Work) -> torch.Future",
            "parameters": {},
            "returns": "A ``torch.futures.Future`` object of int type which maps to the enum type of WorkResult\n    As an example, a future object can be retrieved\n    by ``fut = process_group.allreduce(tensor).get_future_result()``.\n\nExample::\n    users can use ``fut.wait()`` to blocking wait for the completion of the work and\n    get the WorkResult by ``fut.value()``.\n    Also, users can use ``fut.then(call_back_func)`` to register a callback function to be called\n    when the work is completed, without blocking the current thread.\n\n.. warning ::\n    ``get_future_result`` API supports NCCL",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_completed",
          "signature": "is_completed(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "is_completed(self: torch._C._distributed_c10d.Work) -> bool",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_success",
          "signature": "is_success(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "is_success(self: torch._C._distributed_c10d.Work) -> bool",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "result",
          "signature": "result(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "result(self: torch._C._distributed_c10d.Work) -> list[torch.Tensor]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "source_rank",
          "signature": "source_rank(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "source_rank(self: torch._C._distributed_c10d.Work) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "synchronize",
          "signature": "synchronize(self: torch._C._distributed_c10d.Work)",
          "documentation": {
            "description": "synchronize(self: torch._C._distributed_c10d.Work) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "unbox",
          "signature": "unbox(arg0: object)",
          "documentation": {
            "description": "unbox(arg0: object) -> torch._C._distributed_c10d.Work",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(self: torch._C._distributed_c10d.Work, timeout: datetime.timedelta = datetime.timedelta(0)",
          "documentation": {
            "description": "wait(self: torch._C._distributed_c10d.Work, timeout: datetime.timedelta = datetime.timedelta(0)) -> bool",
            "parameters": {},
            "returns": "true/false.\n\nExample::\n   try:\n       work.wait(timeout)\n   except:\n       # some handling\n\n.. warning ::\n    In normal cases, users do not need to set the timeout.\n    calling wait() is the same as calling synchronize():\n    Letting the current stream block on the completion of the NCCL work.\n    However, if timeout is set, it will block the CPU thread until the NCCL work is completed\n    or timed out. If timeout, exception will be thrown.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "group",
      "documentation": {
        "description": "Group class. Placeholder.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    }
  ]
}