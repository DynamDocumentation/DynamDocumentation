{
  "description": "No description available",
  "functions": [
    {
      "name": "generate_methods_for_privateuse1_backend",
      "signature": "generate_methods_for_privateuse1_backend(for_tensor: bool = True, for_module: bool = True, for_packed_sequence: bool = True, for_storage: bool = False, unsupported_dtype: Optional[List[torch.dtype]] = None) -> None",
      "documentation": {
        "description": "Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.\n\nIn the default scenario, storage-related methods will not be generated automatically.\n\nWhen you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key.\nAnd call the function torch.rename_privateuse1_backend(\"foo\") to rename your backend name.\nAt this point, you can easily register specific methods and attributes by calling this function.\nJust like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.",
        "parameters": {
          "for_tensor": {
            "type": "bool",
            "description": "whether register related methods for torch.Tensor class."
          },
          "for_module": {
            "type": "bool",
            "description": "whether register related methods for torch.nn.Module class."
          },
          "for_storage": {
            "type": "bool",
            "description": "whether register related methods for torch.Storage class."
          },
          "unsupported_dtype": {
            "type": "List[torch.dtype]",
            "description": "takes effect only when the storage method needs to be generated,"
          },
          "indicating": {
            "type": "",
            "description": "that the storage does not support the torch.dtype type."
          },
          "Example": {
            "type": "",
            "description": ":\n>>> # xdoctest: +SKIP(\"failing\")\n>>> torch.utils.rename_privateuse1_backend(\"foo\")\n>>> torch.utils.generate_methods_for_privateuse1_backend()\n# Then automatically generate backend-related attributes and methods.\n>>> a = torch.tensor(2).foo()\n>>> a.is_foo\n>>> hasattr(torch.nn.Module, 'foo')"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_cpp_backtrace",
      "signature": "get_cpp_backtrace(frames_to_skip=0, maximum_number_of_frames=64) -> str",
      "documentation": {
        "description": "Return a string containing the C++ stack trace of the current thread.",
        "parameters": {
          "frames_to_skip": {
            "type": "int",
            "description": "the number of frames to skip from the top of the stack"
          },
          "maximum_number_of_frames": {
            "type": "int",
            "description": "the maximum number of frames to return"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "rename_privateuse1_backend",
      "signature": "rename_privateuse1_backend(backend_name: str) -> None",
      "documentation": {
        "description": "Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.\n\nThe steps are:\n\n(1) (In C++) implement kernels for various torch operations, and register them\n    to the PrivateUse1 dispatch key.\n(2) (In python) call torch.utils.rename_privateuse1_backend(\"foo\")\n\nYou can now use \"foo\" as an ordinary device string in python.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_module",
      "signature": "set_module(obj, mod)",
      "documentation": {
        "description": "Set the module attribute on a python object for a given object for nicer printing",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "swap_tensors",
      "signature": "swap_tensors(t1, t2)",
      "documentation": {
        "description": "This function swaps the content of the two Tensor objects.\nAt a high level, this will make t1 have the content of t2 while preserving\nits identity.\n\nThis will not work if t1 and t2 have different slots.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "ThroughputBenchmark",
      "documentation": {
        "description": "This class is a wrapper around a c++ component throughput_benchmark::ThroughputBenchmark.\n\nThis wrapper on the throughput_benchmark::ThroughputBenchmark component is responsible\nfor executing a PyTorch module (nn.Module or ScriptModule) under an inference\nserver like load. It can emulate multiple calling threads to a single module\nprovided. In the future we plan to enhance this component to support inter and\nintra-op parallelism as well as multiple models running in a single process.\n\nPlease note that even though nn.Module is supported, it might incur an overhead\nfrom the need to hold GIL every time we execute Python code or pass around\ninputs as Python objects. As soon as you have a ScriptModule version of your\nmodel for inference deployment it is better to switch to using it in this\nbenchmark.\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> from torch.utils import ThroughputBenchmark\n    >>> bench = ThroughputBenchmark(my_module)\n    >>> # Pre-populate benchmark's data set with the inputs\n    >>> for input in inputs:\n    ...     # Both args and kwargs work, same as any PyTorch Module / ScriptModule\n    ...     bench.add_input(input[0], x2=input[1])\n    >>> # Inputs supplied above are randomly used during the execution\n    >>> stats = bench.benchmark(\n    ...     num_calling_threads=4,\n    ...     num_warmup_iters = 100,\n    ...     num_iters = 1000,\n    ... )\n    >>> print(\"Avg latency (ms): {}\".format(stats.latency_avg_ms))\n    >>> print(\"Number of iterations: {}\".format(stats.num_iters))",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_input",
          "signature": "add_input(self, *args, **kwargs)",
          "documentation": {
            "description": "Store a single input to a module into the benchmark memory and keep it there.\n\nDuring the benchmark execution every thread is going to pick up a\nrandom input from the all the inputs ever supplied to the benchmark via\nthis function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "benchmark",
          "signature": "benchmark(self, num_calling_threads=1, num_warmup_iters=10, num_iters=100, profiler_output_path='')",
          "documentation": {
            "description": "Run a benchmark on the module.",
            "parameters": {
              "num_warmup_iters": {
                "type": "int",
                "description": "Warmup iters are used to make sure we run a module"
              },
              "a": {
                "type": "",
                "description": "few times before actually measuring things. This way we avoid cold"
              },
              "caches": {
                "type": "",
                "description": "and any other similar problems. This is the number of warmup"
              },
              "iterations": {
                "type": "",
                "description": "might be slightly larger. Which is reported as"
              },
              "num_iters": {
                "type": "int",
                "description": "Number of iterations the benchmark should run with."
              },
              "This": {
                "type": "",
                "description": "function returns BenchmarkExecutionStats object which is defined via pybind11."
              },
              "shared": {
                "type": "",
                "description": "across all the threads. Once the num_iters iterations across all"
              },
              "the": {
                "type": "",
                "description": "threads is reached, we will stop execution. Though total number of"
              },
              "stats": {
                "type": "",
                "description": ".num_iters where stats is the result of this function"
              },
              "profiler_output_path": {
                "type": "str",
                "description": "Location to save Autograd Profiler trace."
              },
              "If": {
                "type": "",
                "description": "not empty, Autograd Profiler will be enabled for the main benchmark"
              },
              "execution": {
                "type": "but not the warmup phase",
                "description": ". The full trace will be saved"
              },
              "into": {
                "type": "",
                "description": "the file path provided by this argument"
              },
              "It": {
                "type": "",
                "description": "currently has two fields:\n- num_iters - number of actual iterations the benchmark have made\n- avg_latency_ms - average time it took to infer on one input example in milliseconds"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "run_once",
          "signature": "run_once(self, *args, **kwargs)",
          "documentation": {
            "description": "Given input id (input_idx) run benchmark once and return prediction.\n\nThis is useful for testing that benchmark actually runs the module you\nwant it to run. input_idx here is an index into inputs array populated\nby calling add_input() method.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}