{
  "description": "FX is a toolkit for developers to use to transform ``nn.Module``\ninstances. FX consists of three main components: a **symbolic tracer,**\nan **intermediate representation**, and **Python code generation**. A\ndemonstration of these components in action:\n\n::\n\n    import torch\n\n\n    # Simple module for demonstration\n    class MyModule(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.param = torch.nn.Parameter(torch.rand(3, 4))\n            self.linear = torch.nn.Linear(4, 5)\n\n        def forward(self, x):\n            return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\n\n    module = MyModule()\n\n    from torch.fx import symbolic_trace\n\n    # Symbolic tracing frontend - captures the semantics of the module\n    symbolic_traced: torch.fx.GraphModule = symbolic_trace(module)\n\n    # High-level intermediate representation (IR) - Graph representation\n    print(symbolic_traced.graph)\n    \"\"\"\n    graph():\n        %x : [num_users=1] = placeholder[target=x]\n        %param : [num_users=1] = get_attr[target=param]\n        %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n        %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n        %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n        return clamp\n    \"\"\"\n\n    # Code generation - valid Python code\n    print(symbolic_traced.code)\n    \"\"\"\n    def forward(self, x):\n        param = self.param\n        add = x + param;  x = param = None\n        linear = self.linear(add);  add = None\n        clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n        return clamp\n    \"\"\"\n\nThe **symbolic tracer** performs \"symbolic execution\" of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the :func:`symbolic_trace` and :class:`Tracer`\ndocumentation.\n\nThe **intermediate representation** is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor :class:`torch.nn.Module` instances), and return values. More information\nabout the IR can be found in the documentation for :class:`Graph`. The\nIR is the format on which transformations are applied.\n\n**Python code generation** is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph's semantics. This\nfunctionality is wrapped up in :class:`GraphModule`, which is a\n:class:`torch.nn.Module` instance that holds a :class:`Graph` as well as a\n``forward`` method generated from the Graph.\n\nTaken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX!\n\nSeveral example transformations can be found at the\n`examples <https://github.com/pytorch/examples/tree/master/fx>`__\nrepository.",
  "functions": [
    {
      "name": "has_side_effect",
      "signature": "has_side_effect(fn: Callable) -> Callable",
      "documentation": {
        "description": ".. warning::\n    This API is experimental and is *NOT* backward-compatible.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "map_arg",
      "signature": "map_arg(a: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], fn: Callable[[torch.fx.node.Node], Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]]) -> Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]",
      "documentation": {
        "description": "Apply fn to each Node appearing arg. arg may be a list, tuple, slice, or dict with string keys.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "replace_pattern",
      "signature": "replace_pattern(gm: torch.fx.graph_module.GraphModule, pattern: Union[Callable, torch.fx.graph_module.GraphModule], replacement: Union[Callable, torch.fx.graph_module.GraphModule]) -> List[torch.fx.subgraph_rewriter.Match]",
      "documentation": {
        "description": "Matches all possible non-overlapping sets of operators and their\ndata dependencies (``pattern``) in the Graph of a GraphModule\n(``gm``), then replaces each of these matched subgraphs with another\nsubgraph (``replacement``).",
        "parameters": {},
        "returns": "List[Match]: A list of ``Match`` objects representing the places\n    in the original graph that ``pattern`` was matched to. The list\n    is empty if there are no matches. ``Match`` is defined as:\n\n    .. code-block:: python\n\n        class Match(NamedTuple):\n            # Node from which the match was found\n            anchor: Node\n            # Maps nodes in the pattern subgraph to nodes in the larger graph\n            nodes_map: Dict[Node, Node]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. code-block:: python\n\n    import torch\n    from torch.fx import symbolic_trace, subgraph_rewriter\n\n\n    class M(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n\n        def forward(self, x, w1, w2):\n            m1 = torch.cat([w1, w2]).sum()\n            m2 = torch.cat([w1, w2]).sum()\n            return x + torch.max(m1) + torch.max(m2)\n\n\n    def pattern(w1, w2):\n        return torch.cat([w1, w2]).sum()\n\n\n    def replacement(w1, w2):\n        return torch.stack([w1, w2])\n\n\n    traced_module = symbolic_trace(M())\n\n    subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\nThe above code will first match ``pattern`` in the ``forward``\nmethod of ``traced_module``. Pattern-matching is done based on\nuse-def relationships, not node names. For example, if you had\n``p = torch.cat([a, b])`` in ``pattern``, you could match\n``m = torch.cat([a, b])`` in the original ``forward`` function,\ndespite the variable names being different (``p`` vs ``m``).\n\nThe ``return`` statement in ``pattern`` is matched based on its\nvalue only; it may or may not match to the ``return`` statement in\nthe larger graph. In other words, the pattern doesn't have to extend\nto the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger\nfunction and replaced by ``replacement``. If there are multiple\nmatches for ``pattern`` in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\"First\" here being defined as the first in a topological ordering\nof the Nodes' use-def relationships. In most cases, the first Node\nis the parameter that appears directly after ``self``, while the\nlast Node is whatever the function returns.)\n\nOne important thing to note is that the parameters of the\n``pattern`` Callable must be used in the Callable itself,\nand the parameters of the ``replacement`` Callable must match\nthe pattern. The first rule is why, in the above code block, the\n``forward`` function has parameters ``x, w1, w2``, but the\n``pattern`` function only has parameters ``w1, w2``. ``pattern``\ndoesn't use ``x``, so it shouldn't specify ``x`` as a parameter.\nAs an example of the second rule, consider replacing\n\n.. code-block:: python\n\n    def pattern(x, y):\n        return torch.neg(x) + torch.relu(y)\n\nwith\n\n.. code-block:: python\n\n    def replacement(x, y):\n        return torch.relu(x)\n\nIn this case, ``replacement`` needs the same number of parameters\nas ``pattern`` (both ``x`` and ``y``), even though the parameter\n``y`` isn't used in ``replacement``.\n\nAfter calling ``subgraph_rewriter.replace_pattern``, the generated\nPython code looks like this:\n\n.. code-block:: python\n\n    def forward(self, x, w1, w2):\n        stack_1 = torch.stack([w1, w2])\n        sum_1 = stack_1.sum()\n        stack_2 = torch.stack([w1, w2])\n        sum_2 = stack_2.sum()\n        max_1 = torch.max(sum_1)\n        add_1 = x + max_1\n        max_2 = torch.max(sum_2)\n        add_2 = add_1 + max_2\n        return add_2\n\n.. note::\n    Backwards-compatibility for this API is guaranteed."
      }
    },
    {
      "name": "symbolic_trace",
      "signature": "symbolic_trace(root: Union[torch.nn.modules.module.Module, Callable[..., Any]], concrete_args: Optional[Dict[str, Any]] = None) -> torch.fx.graph_module.GraphModule",
      "documentation": {
        "description": "Symbolic tracing API\n\nGiven an ``nn.Module`` or function instance ``root``, this function will return a ``GraphModule``\nconstructed by recording operations seen while tracing through ``root``.\n\n``concrete_args`` allows you to partially specialize your function, whether it's to remove control flow or data structures.\n\nFor example::\n\n    def f(a, b):\n        if b == True:\n            return a\n        else:\n            return a * 2\n\nFX can typically not trace through this due to the presence of control\nflow. However, we can use `concrete_args` to specialize on the value of\n`b` to trace through this::\n\n    f = fx.symbolic_trace(f, concrete_args={\"b\": False})\n    assert f(3, False) == 6\n\nNote that although you can still pass in different values of `b`, they will be ignored.\n\nWe can also use `concrete_args` to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in `fx.PH` for values that shouldn't be\nspecialized. For example::\n\n    def f(x):\n        out = 0\n        for v in x.values():\n            out += v\n        return out\n\n\n    f = fx.symbolic_trace(f, concrete_args={\"x\": {\"a\": fx.PH, \"b\": fx.PH, \"c\": fx.PH}})\n    assert f({\"a\": 1, \"b\": 2, \"c\": 4}) == 7",
        "parameters": {
          "root": {
            "type": "Union[torch.nn.Module, Callable]",
            "description": "Module or function to be traced and converted"
          },
          "into": {
            "type": "",
            "description": "a Graph representation."
          },
          "concrete_args": {
            "type": "Optional[Dict[str, any]]",
            "description": "Inputs to be partially specialized"
          }
        },
        "returns": "GraphModule: a Module created from the recorded operations from ``root``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "wrap",
      "signature": "wrap(fn_or_name: Union[str, Callable])",
      "documentation": {
        "description": "This function can be called at module-level scope to register fn_or_name as a \"leaf function\".\nA \"leaf function\" will be preserved as a CallFunction node in the FX trace instead of being\ntraced through::\n\n    # foo/bar/baz.py\n    def my_custom_function(x, y):\n        return x * x + y * y\n\n\n    torch.fx.wrap(\"my_custom_function\")\n\n\n    def fn_to_be_traced(x, y):\n        # When symbolic tracing, the below call to my_custom_function will be inserted into\n        # the graph rather than tracing it.\n        return my_custom_function(x, y)\n\nThis function can also equivalently be used as a decorator::\n\n    # foo/bar/baz.py\n    @torch.fx.wrap\n    def my_custom_function(x, y):\n        return x * x + y * y\n\nA wrapped function can be thought of a \"leaf function\", analogous to the concept of\n\"leaf modules\", that is, they are functions that are left as calls in the FX trace\nrather than traced through.",
        "parameters": {
          "fn_or_name": {
            "type": "Union[str, Callable]",
            "description": "The function or name of the global function to insert into the"
          },
          "graph": {
            "type": "",
            "description": "when it's called\n.. note::"
          },
          "Backwards": {
            "type": "",
            "description": "-compatibility for this API is guaranteed."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "CodeGen",
      "documentation": {
        "description": ".. warning::\n    This API is experimental and is *NOT* backward-compatible.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "additional_globals",
          "signature": "additional_globals(self) -> List[Tuple[str, Any]]",
          "documentation": {
            "description": "If your codegen uses extra global values, add tuples of (identifier,reference to the value) here.\nFor example, return ['List', typing.List] if you need ``List`` in the global context.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gen_fn_def",
          "signature": "gen_fn_def(self, free_vars: List[str], maybe_return_annotation: str) -> str",
          "documentation": {
            "description": "Given the free variables and a return annotation, generates the beginning of the FX function.\nBy default, `gen_fn_def(['a', 'b'], '') == 'def {self._func_name}(a, b):'`",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "generate_output",
          "signature": "generate_output(self, output_args: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]) -> str",
          "documentation": {
            "description": "Given the output arguments, generates the return statement of the FX function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "process_inputs",
          "signature": "process_inputs(self, *args: Any) -> Any",
          "documentation": {
            "description": "Transforms the inputs so that the graph can take them as arguments, as\nnon-default codegen may result in the inputs to the function being\ndifferent from the inputs to the graph.\n\nIf the graph was directly runnable, this invariant should hold true\n`f.graph.process_outputs(f.graph(*f.graph.process_inputs(*inputs))) == f(*inputs)`",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "process_outputs",
          "signature": "process_outputs(self, outputs: Any) -> Any",
          "documentation": {
            "description": "Transforms the outputs of the graph to be identical to the codegen.\n\nSee ``process_inputs`` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Graph",
      "documentation": {
        "description": "``Graph`` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of ``Node`` s, each representing callsites (or other\nsyntactic constructs). The list of ``Node`` s, taken together, constitute a\nvalid Python function.\n\nFor example, the following code\n\n.. code-block:: python\n\n    import torch\n    import torch.fx\n\n\n    class MyModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.param = torch.nn.Parameter(torch.rand(3, 4))\n            self.linear = torch.nn.Linear(4, 5)\n\n        def forward(self, x):\n            return torch.topk(\n                torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3\n            )\n\n\n    m = MyModule()\n    gm = torch.fx.symbolic_trace(m)\n\nWill produce the following Graph::\n\n    print(gm.graph)\n\n.. code-block:: text\n\n    graph(x):\n        %linear_weight : [num_users=1] = self.linear.weight\n        %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n        %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n        %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n        %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n        %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n        return topk_1\n\nFor the semantics of operations represented in the ``Graph``, please see :class:`Node`.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "call_function",
          "signature": "call_function(self, the_function: Callable[..., Any], args: Optional[Tuple[ForwardRef('Argument'), ...]] = None, kwargs: Optional[Dict[str, ForwardRef('Argument')]] = None, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Insert a ``call_function`` ``Node`` into the ``Graph``. A ``call_function`` node\nrepresents a call to a Python callable, specified by ``the_function``.",
            "parameters": {
              "the_function": {
                "type": "Callable[..., Any]",
                "description": "The function to be called. Can be any PyTorch"
              },
              "operator": {
                "type": "",
                "description": ", Python function, or member of the ``builtins`` or ``operator``"
              },
              "namespaces": {
                "type": "",
                "description": "."
              },
              "args": {
                "type": "Optional[Tuple[Argument, ...]]",
                "description": "The positional arguments to be passed"
              },
              "to": {
                "type": "",
                "description": "the called function"
              },
              "kwargs": {
                "type": "Optional[Dict[str, Argument]]",
                "description": "The keyword arguments to be passed"
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have."
              }
            },
            "returns": "The newly created and inserted ``call_function`` node.\n\n.. note::\n    The same insertion point and type expression rules apply for this method\n    as :meth:`Graph.create_node`.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_method",
          "signature": "call_method(self, method_name: str, args: Optional[Tuple[ForwardRef('Argument'), ...]] = None, kwargs: Optional[Dict[str, ForwardRef('Argument')]] = None, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Insert a ``call_method`` ``Node`` into the ``Graph``. A ``call_method`` node\nrepresents a call to a given method on the 0th element of ``args``.",
            "parameters": {
              "method_name": {
                "type": "str",
                "description": "The name of the method to apply to the self argument."
              },
              "For": {
                "type": "",
                "description": "example, if args[0] is a ``Node`` representing a ``Tensor``,"
              },
              "then": {
                "type": "",
                "description": "to call ``relu()`` on that ``Tensor``, pass ``relu`` to ``method_name``."
              },
              "args": {
                "type": "Optional[Tuple[Argument, ...]]",
                "description": "The positional arguments to be passed"
              },
              "to": {
                "type": "",
                "description": "the called method"
              },
              "kwargs": {
                "type": "Optional[Dict[str, Argument]]",
                "description": "The keyword arguments to be passed"
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have."
              }
            },
            "returns": "The newly created and inserted ``call_method`` node.\n\n.. note::\n    The same insertion point and type expression rules apply for this method\n    as :meth:`Graph.create_node`.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_module",
          "signature": "call_module(self, module_name: str, args: Optional[Tuple[ForwardRef('Argument'), ...]] = None, kwargs: Optional[Dict[str, ForwardRef('Argument')]] = None, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Insert a ``call_module`` ``Node`` into the ``Graph``. A ``call_module`` node\nrepresents a call to the forward() function of a ``Module`` in the ``Module``\nhierarchy.",
            "parameters": {
              "module_name": {
                "type": "str",
                "description": "The qualified name of the ``Module`` in the ``Module``"
              },
              "hierarchy": {
                "type": "",
                "description": "to be called. For example, if the traced ``Module`` has a"
              },
              "submodule": {
                "type": "",
                "description": "named ``foo``, which has a submodule named ``bar``, the"
              },
              "qualified": {
                "type": "",
                "description": "name ``foo.bar`` should be passed as ``module_name`` to"
              },
              "call": {
                "type": "",
                "description": "that module."
              },
              "args": {
                "type": "Optional[Tuple[Argument, ...]]",
                "description": "The positional arguments to be passed"
              },
              "to": {
                "type": "",
                "description": "the called method"
              },
              "kwargs": {
                "type": "Optional[Dict[str, Argument]]",
                "description": "The keyword arguments to be passed"
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have."
              }
            },
            "returns": "The newly-created and inserted ``call_module`` node.\n\n.. note::\n    The same insertion point and type expression rules apply for this method\n    as :meth:`Graph.create_node`.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_node",
          "signature": "create_node(self, op: str, target: 'Target', args: Optional[Tuple[ForwardRef('Argument'), ...]] = None, kwargs: Optional[Dict[str, ForwardRef('Argument')]] = None, name: Optional[str] = None, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Create a ``Node`` and add it to the ``Graph`` at the current insert-point.\nNote that the current insert-point can be set via :meth:`Graph.inserting_before`\nand :meth:`Graph.inserting_after`.",
            "parameters": {
              "op": {
                "type": "str",
                "description": "the opcode for this Node. One of 'call_function', 'call_method', 'get_attr',\n'call_module', 'placeholder', or 'output'. The semantics of these opcodes are"
              },
              "described": {
                "type": "",
                "description": "in the ``Graph`` docstring."
              },
              "args": {
                "type": "Optional[Tuple[Argument, ...]]",
                "description": "is a tuple of arguments to this node."
              },
              "kwargs": {
                "type": "Optional[Dict[str, Argument]]",
                "description": "the kwargs of this Node"
              },
              "name": {
                "type": "Optional[str]",
                "description": "an optional string name for the ``Node``."
              },
              "This": {
                "type": "",
                "description": "will influence the name of the value assigned to in the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have."
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              }
            },
            "returns": "The newly-created and inserted node.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eliminate_dead_code",
          "signature": "eliminate_dead_code(self, is_impure_node: Optional[Callable[[torch.fx.node.Node], bool]] = None) -> bool",
          "documentation": {
            "description": "Remove all dead code from the graph, based on each node's number of\nusers, and whether the nodes have any side effects. The graph must be\ntopologically sorted before calling.",
            "parameters": {
              "is_impure_node": {
                "type": "Optional[Callable[[Node], bool]]",
                "description": "A function that returns"
              },
              "whether": {
                "type": "",
                "description": "a node is impure. If this is None, then the default behavior is to"
              },
              "use": {
                "type": "",
                "description": "Node.is_impure."
              }
            },
            "returns": "bool: Whether the graph was changed as a result of the pass.\n\nExample:\n\nBefore dead code is eliminated, `a` from `a = x + 1` below has no users\nand thus can be eliminated from the graph without having an effect.\n\n.. code-block:: python\n\n    def forward(self, x):\n        a = x + 1\n        return x + self.attr_1\n\nAfter dead code is eliminated, `a = x + 1` has been removed, and the rest\nof `forward` remains.\n\n.. code-block:: python\n\n    def forward(self, x):\n        return x + self.attr_1\n\n.. warning::\n\n    Dead code elimination has some heuristics to avoid removing\n    side-effectful nodes (see Node.is_impure) but in general coverage\n    is very bad, so you should assume that this method is not sound\n    to call unless you know that your FX graph consists entirely\n    of functional operations or you supply your own custom\n    function for detecting side-effectful nodes.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "Before dead code is eliminated, `a` from `a = x + 1` below has no users\nand thus can be eliminated from the graph without having an effect.\n\n.. code-block:: python\n\n    def forward(self, x):\n        a = x + 1\n        return x + self.attr_1\n\nAfter dead code is eliminated, `a = x + 1` has been removed, and the rest\nof `forward` remains.\n\n.. code-block:: python\n\n    def forward(self, x):\n        return x + self.attr_1\n\n.. warning::\n\n    Dead code elimination has some heuristics to avoid removing\n    side-effectful nodes (see Node.is_impure) but in general coverage\n    is very bad, so you should assume that this method is not sound\n    to call unless you know that your FX graph consists entirely\n    of functional operations or you supply your own custom\n    function for detecting side-effectful nodes.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed."
          }
        },
        {
          "name": "erase_node",
          "signature": "erase_node(self, to_erase: torch.fx.node.Node) -> None",
          "documentation": {
            "description": "Erases a ``Node`` from the ``Graph``. Throws an exception if\nthere are still users of that node in the ``Graph``.",
            "parameters": {
              "to_erase": {
                "type": "Node",
                "description": "The ``Node`` to erase from the ``Graph``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "find_nodes",
          "signature": "find_nodes(self, *, op: str, target: Optional[ForwardRef('Target')] = None, sort: bool = True)",
          "documentation": {
            "description": "Allows for fast query of nodes",
            "parameters": {
              "op": {
                "type": "str",
                "description": "the name of the operation"
              },
              "target": {
                "type": "Optional[Target]",
                "description": "the target of the node. For call_function,"
              },
              "the": {
                "type": "",
                "description": "target is required. For other ops, the target is optional."
              },
              "sort": {
                "type": "bool",
                "description": "whether to return nodes in the order they appear on"
              },
              "on": {
                "type": "",
                "description": "the graph."
              }
            },
            "returns": "Iteratable of nodes with the requested op and target.\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_attr",
          "signature": "get_attr(self, qualified_name: str, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Insert a ``get_attr`` node into the Graph. A ``get_attr`` ``Node`` represents the\nfetch of an attribute from the ``Module`` hierarchy.",
            "parameters": {
              "qualified_name": {
                "type": "str",
                "description": "the fully-qualified name of the attribute to be retrieved."
              },
              "For": {
                "type": "",
                "description": "example, if the traced Module has a submodule named ``foo``, which has a"
              },
              "submodule": {
                "type": "",
                "description": "named ``bar``, which has an attribute named ``baz``, the qualified"
              },
              "name": {
                "type": "",
                "description": "``foo.bar.baz`` should be passed as ``qualified_name``."
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have."
              }
            },
            "returns": "The newly-created and inserted ``get_attr`` node.\n\n.. note::\n    The same insertion point and type expression rules apply for this method\n    as ``Graph.create_node``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_copy",
          "signature": "graph_copy(self, g: 'Graph', val_map: Dict[torch.fx.node.Node, torch.fx.node.Node], return_output_node=False) -> 'Optional[Argument]'",
          "documentation": {
            "description": "Copy all nodes from a given graph into ``self``.",
            "parameters": {
              "g": {
                "type": "Graph",
                "description": "The source graph from which to copy Nodes."
              },
              "val_map": {
                "type": "Dict[Node, Node]",
                "description": "a dictionary that will be populated with a mapping"
              },
              "from": {
                "type": "",
                "description": "nodes in ``g`` to nodes in ``self``. Note that ``val_map`` can be passed"
              },
              "in": {
                "type": "",
                "description": "with values in it already to override copying of certain values."
              }
            },
            "returns": "The value in ``self`` that is now equivalent to the output value in ``g``,\n    if ``g`` had an ``output`` node. ``None`` otherwise.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inserting_after",
          "signature": "inserting_after(self, n: Optional[torch.fx.node.Node] = None)",
          "documentation": {
            "description": "Set the point at which create_node and companion methods will insert into the graph.\n        When used within a 'with' statement, this will temporary set the insert point and\n        then restore it when the with statement exits::\n\n            with g.inserting_after(n):\n                ...  # inserting after node n\n            ...  # insert point restored to what it was previously\n            g.inserting_after(n)  #  set the insert point permanently",
            "parameters": {
              "n": {
                "type": "Optional[Node]",
                "description": "The node before which to insert. If None this will insert after"
              },
              "the": {
                "type": "",
                "description": "beginning of the entire graph."
              }
            },
            "returns": "A resource manager that will restore the insert point on ``__exit__``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inserting_before",
          "signature": "inserting_before(self, n: Optional[torch.fx.node.Node] = None)",
          "documentation": {
            "description": "Set the point at which create_node and companion methods will insert into the graph.\n        When used within a 'with' statement, this will temporary set the insert point and\n        then restore it when the with statement exits::\n\n            with g.inserting_before(n):\n                ...  # inserting before node n\n            ...  # insert point restored to what it was previously\n            g.inserting_before(n)  #  set the insert point permanently",
            "parameters": {
              "n": {
                "type": "Optional[Node]",
                "description": "The node before which to insert. If None this will insert before"
              },
              "the": {
                "type": "",
                "description": "beginning of the entire graph."
              }
            },
            "returns": "A resource manager that will restore the insert point on ``__exit__``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "lint",
          "signature": "lint(self)",
          "documentation": {
            "description": "Runs various checks on this Graph to make sure it is well-formed. In\nparticular:\n- Checks Nodes have correct ownership (owned by this graph)\n- Checks Nodes appear in topological order\n- If this Graph has an owning GraphModule, checks that targets\nexist in that GraphModule\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "node_copy",
          "signature": "node_copy(self, node: torch.fx.node.Node, arg_transform: Callable[[torch.fx.node.Node], ForwardRef('Argument')] = <function Graph.<lambda> at 0x76e266453240>) -> torch.fx.node.Node",
          "documentation": {
            "description": "Copy a node from one graph into another. ``arg_transform`` needs to transform arguments from\nthe graph of node to the graph of self. Example::\n\n    # Copying all the nodes in `g` into `new_graph`\n    g: torch.fx.Graph = ...\n    new_graph = torch.fx.graph()\n    value_remap = {}\n    for node in g.nodes:\n        value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])",
            "parameters": {
              "node": {
                "type": "Node",
                "description": "The node to copy into ``self``."
              },
              "arg_transform": {
                "type": "Callable[[Node], Argument]",
                "description": "A function that transforms\n``Node`` arguments in node's ``args`` and ``kwargs`` into the"
              },
              "equivalent": {
                "type": "",
                "description": "argument in ``self``. In the simplest case, this should"
              },
              "retrieve": {
                "type": "",
                "description": "a value out of a table mapping Nodes in the original"
              },
              "graph": {
                "type": "",
                "description": "to ``self``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "on_generate_code",
          "signature": "on_generate_code(self, make_transformer: Callable[[Optional[Callable[[List[str]], List[str]]]], Callable[[List[str]], List[str]]])",
          "documentation": {
            "description": "Register a transformer function when python code is generated",
            "parameters": {
              "make_transformer": {
                "type": "Callable[[Optional[TransformCodeFunc]], TransformCodeFunc]",
                "description": ""
              },
              "a": {
                "type": "",
                "description": "function that returns a code transformer to be registered."
              },
              "This": {
                "type": "",
                "description": "function is also given as its input the currently"
              },
              "code": {
                "type": "",
                "description": "transformer."
              },
              "registered": {
                "type": "",
                "description": "code transformer (or None if nothing is registered),"
              },
              "in": {
                "type": "",
                "description": "case it is not desirable to overwrite it. This is useful to"
              },
              "chain": {
                "type": "",
                "description": "code transformers together."
              }
            },
            "returns": "a context manager that when used in a `with` statement, to automatically\n            restore the previously registered code transformer.\n\n        Example:\n\n        .. code-block:: python\n\n\n            gm: fx.GraphModule = ...\n\n\n            # This is a code transformer we want to register. This code\n            # transformer prepends a pdb import and trace statement at the very\n            # beginning of the generated torch.fx code to allow for manual\n            # debugging with the PDB library.\n            def insert_pdb(body):\n                return [\"import pdb; pdb.set_trace()\\n\", *body]\n\n\n            # Registers `insert_pdb`, and overwrites the current registered\n            # code transformer (given by `_` to the lambda):\n            gm.graph.on_generate_code(lambda _: insert_pdb)\n\n            # Or alternatively, registers a code transformer which first\n            # runs `body` through existing registered transformer, then\n            # through `insert_pdb`:\n            gm.graph.on_generate_code(\n                lambda current_trans: (\n                    lambda body: insert_pdb(current_trans(body) if current_trans else body)\n                )\n            )\n\n            gm.recompile()\n            gm(*inputs)  # drops into pdb\n\n\n        This function can also be used as a context manager, with the benefit to\n        automatically restores the previously registered code transformer:\n\n        .. code-block:: python\n\n            # ... continue from previous example\n\n            with gm.graph.on_generate_code(lambda _: insert_pdb):\n                # do more stuff with `gm`...\n                gm.recompile()\n                gm(*inputs)  # drops into pdb\n\n            # now previous code transformer is restored (but `gm`'s code with pdb\n            # remains - that means you can run `gm` with pdb here too, until you\n            # run next `recompile()`).\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ".. code-block:: python\n\n\n            gm: fx.GraphModule = ...\n\n\n            # This is a code transformer we want to register. This code\n            # transformer prepends a pdb import and trace statement at the very\n            # beginning of the generated torch.fx code to allow for manual\n            # debugging with the PDB library.\n            def insert_pdb(body):\n                return [\"import pdb; pdb.set_trace()\\n\", *body]\n\n\n            # Registers `insert_pdb`, and overwrites the current registered\n            # code transformer (given by `_` to the lambda):\n            gm.graph.on_generate_code(lambda _: insert_pdb)\n\n            # Or alternatively, registers a code transformer which first\n            # runs `body` through existing registered transformer, then\n            # through `insert_pdb`:\n            gm.graph.on_generate_code(\n                lambda current_trans: (\n                    lambda body: insert_pdb(current_trans(body) if current_trans else body)\n                )\n            )\n\n            gm.recompile()\n            gm(*inputs)  # drops into pdb\n\n\n        This function can also be used as a context manager, with the benefit to\n        automatically restores the previously registered code transformer:\n\n        .. code-block:: python\n\n            # ... continue from previous example\n\n            with gm.graph.on_generate_code(lambda _: insert_pdb):\n                # do more stuff with `gm`...\n                gm.recompile()\n                gm(*inputs)  # drops into pdb\n\n            # now previous code transformer is restored (but `gm`'s code with pdb\n            # remains - that means you can run `gm` with pdb here too, until you\n            # run next `recompile()`).\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible."
          }
        },
        {
          "name": "output",
          "signature": "output(self, result: 'Argument', type_expr: Optional[Any] = None)",
          "documentation": {
            "description": "Insert an ``output`` ``Node`` into the ``Graph``. An ``output`` node represents\na ``return`` statement in Python code. ``result`` is the value that should\nbe returned.",
            "parameters": {
              "result": {
                "type": "Argument",
                "description": "The value to be returned."
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "same insertion point and type expression rules apply for this method"
              },
              "as": {
                "type": "",
                "description": "``Graph.create_node``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "output_node",
          "signature": "output_node(self) -> torch.fx.node.Node",
          "documentation": {
            "description": ".. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "placeholder",
          "signature": "placeholder(self, name: str, type_expr: Optional[Any] = None, default_value: Any) -> torch.fx.node.Node",
          "documentation": {
            "description": "Insert a ``placeholder`` node into the Graph. A ``placeholder`` represents\na function input.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "A name for the input value. This corresponds to the name"
              },
              "of": {
                "type": "",
                "description": "the positional argument to the function this ``Graph`` represents."
              },
              "type_expr": {
                "type": "Optional[Any]",
                "description": "an optional type annotation representing the"
              },
              "Python": {
                "type": "",
                "description": "type the output of this node will have. This is needed in some"
              },
              "cases": {
                "type": "",
                "description": "for proper code generation (e.g. when the function is used"
              },
              "subsequently": {
                "type": "",
                "description": "in TorchScript compilation)."
              },
              "default_value": {
                "type": "Any",
                "description": "The default value this function argument should take"
              },
              "on": {
                "type": "",
                "description": ". NOTE: to allow for `None` as a default value, `inspect.Signature.empty`"
              },
              "should": {
                "type": "",
                "description": "be passed as this argument to specify that the parameter does _not_"
              },
              "have": {
                "type": "",
                "description": "a default value.\n.. note::"
              },
              "The": {
                "type": "",
                "description": "same insertion point and type expression rules apply for this method"
              },
              "as": {
                "type": "",
                "description": "``Graph.create_node``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "print_tabular",
          "signature": "print_tabular(self)",
          "documentation": {
            "description": "Prints the intermediate representation of the graph in tabular\nformat. Note that this API requires the ``tabulate`` module to be\ninstalled.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "process_inputs",
          "signature": "process_inputs(self, *args)",
          "documentation": {
            "description": "Processes args so that they can be passed to the FX graph.\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "process_outputs",
          "signature": "process_outputs(self, out)",
          "documentation": {
            "description": ".. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "python_code",
          "signature": "python_code(self, root_module: str, *, verbose: bool = False, include_stride: bool = False, include_device: bool = False, colored: bool = False) -> torch.fx.graph.PythonCode",
          "documentation": {
            "description": "Turn this ``Graph`` into valid Python code.",
            "parameters": {
              "root_module": {
                "type": "str",
                "description": "The name of the root module on which to look-up"
              },
              "qualified": {
                "type": "",
                "description": "name targets. This is usually 'self'."
              }
            },
            "returns": "A PythonCode object, consisting of two fields:\n        src: the Python source code representing the object\n        globals: a dictionary of global names in `src` -> the objects that they reference.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_codegen",
          "signature": "set_codegen(self, codegen: torch.fx.graph.CodeGen)",
          "documentation": {
            "description": ".. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GraphModule",
      "documentation": {
        "description": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n``graph`` attribute, as well as ``code`` and ``forward`` attributes generated\nfrom that ``graph``.\n\n.. warning::\n\n    When ``graph`` is reassigned, ``code`` and ``forward`` will be automatically\n    regenerated. However, if you edit the contents of the ``graph`` without reassigning\n    the ``graph`` attribute itself, you must call ``recompile()`` to update the generated\n    code.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_submodule",
          "signature": "add_submodule(self, target: str, m: torch.nn.modules.module.Module) -> bool",
          "documentation": {
            "description": "Adds the given submodule to ``self``.\n\nThis installs empty Modules where none exist yet if they are\nsubpaths of ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the new submodule\n(See example in ``nn.Module.get_submodule`` for how to"
              },
              "specify": {
                "type": "",
                "description": "a fully-qualified string.)"
              },
              "m": {
                "type": "",
                "description": "The submodule itself; the actual object we want to"
              },
              "install": {
                "type": "",
                "description": "in the current Module"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "bool": {
                "type": "",
                "description": "Whether or not the submodule could be inserted. For"
              },
              "this": {
                "type": "",
                "description": "method to return True, each object in the chain"
              },
              "denoted": {
                "type": "",
                "description": "by ``target`` must either a) not exist yet,"
              },
              "or": {
                "type": "",
                "description": "b) reference an ``nn.Module`` (not a parameter or"
              },
              "other": {
                "type": "",
                "description": "attribute)\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_all_unused_submodules",
          "signature": "delete_all_unused_submodules(self) -> None",
          "documentation": {
            "description": "Deletes all unused submodules from ``self``.\n\nA Module is considered \"used\" if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a ``call_module`` node\n3. It has a non-Module attribute that is used from a\n``get_attr`` node\n\nThis method can be called to clean up an ``nn.Module`` without\nmanually calling ``delete_submodule`` on each unused submodule.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "delete_submodule",
          "signature": "delete_submodule(self, target: str) -> bool",
          "documentation": {
            "description": "Deletes the given submodule from ``self``.\n\nThe module will not be deleted if ``target`` is not a valid\ntarget.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the new submodule\n(See example in ``nn.Module.get_submodule`` for how to"
              },
              "specify": {
                "type": "",
                "description": "a fully-qualified string.)"
              }
            },
            "returns": "bool: Whether or not the target string referenced a\n        submodule we want to delete. A return value of ``False``\n        means that the ``target`` was not a valid reference to\n        a submodule.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "_forward_unimplemented(self, *input: Any) -> None",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "print_readable",
          "signature": "print_readable(self, print_output=True, include_stride=False, include_device=False, colored=False)",
          "documentation": {
            "description": "Return the Python code generated for current GraphModule and its children GraphModules\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "recompile",
          "signature": "recompile(self) -> torch.fx.graph.PythonCode",
          "documentation": {
            "description": "Recompile this GraphModule from its ``graph`` attribute. This should be\ncalled after editing the contained ``graph``, otherwise the generated\ncode of this ``GraphModule`` will be out of date.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_folder",
          "signature": "to_folder(self, folder: Union[str, os.PathLike], module_name: str = 'FxModule')",
          "documentation": {
            "description": "Dumps out module to ``folder`` with ``module_name`` so that it can be\n        imported with ``from <folder> import <module_name>``",
            "parameters": {
              "folder": {
                "type": "Union[str, os.PathLike]",
                "description": "The folder to write the code out to"
              },
              "module_name": {
                "type": "str",
                "description": "Top-level name to use for the ``Module`` while"
              },
              "writing": {
                "type": "",
                "description": "out the code\n.. warning::"
              },
              "This": {
                "type": "",
                "description": "API is experimental and is *NOT* backward-compatible."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interpreter",
      "documentation": {
        "description": "An Interpreter executes an FX graph Node-by-Node. This pattern\ncan be useful for many things, including writing code\ntransformations as well as analysis passes.\n\nMethods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy::\n\n    run()\n        +-- run_node\n            +-- placeholder()\n            +-- get_attr()\n            +-- call_function()\n            +-- call_method()\n            +-- call_module()\n            +-- output()\n\nExample:\n\n    Suppose we want to swap all instances of ``torch.neg`` with\n    ``torch.sigmoid`` and vice versa (including their ``Tensor``\n    method equivalents). We could subclass Interpreter like so::\n\n        class NegSigmSwapInterpreter(Interpreter):\n            def call_function(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n                if target == torch.sigmoid:\n                    return torch.neg(*args, **kwargs)\n                return super().call_function(target, args, kwargs)\n\n            def call_method(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n                if target == \"neg\":\n                    call_self, *args_tail = args\n                    return call_self.sigmoid(*args_tail, **kwargs)\n                return super().call_method(target, args, kwargs)\n\n\n        def fn(x):\n            return torch.sigmoid(x).neg()\n\n\n        gm = torch.fx.symbolic_trace(fn)\n        input = torch.randn(3, 4)\n        result = NegSigmSwapInterpreter(gm).run(input)\n        torch.testing.assert_close(result, torch.neg(input).sigmoid())",
        "parameters": {
          "module": {
            "type": "torch.nn.Module",
            "description": "The module to be executed"
          },
          "garbage_collect_values": {
            "type": "bool",
            "description": "Whether to delete values after their last"
          },
          "use": {
            "type": "",
            "description": "within the Module's execution. This ensures optimal memory usage during"
          },
          "execution": {
            "type": "",
            "description": ". This can be disabled to, for example, examine all of the intermediate"
          },
          "values": {
            "type": "",
            "description": "in the execution by looking at the ``Interpreter.env`` attribute."
          },
          "graph": {
            "type": "",
            "description": "instead of `module.graph`, using the provided `module`"
          },
          "argument": {
            "type": "",
            "description": "to satisfy any requests for state.\n.. note::"
          },
          "Backwards": {
            "type": "",
            "description": "-compatibility for this API is guaranteed."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Suppose we want to swap all instances of ``torch.neg`` with\n    ``torch.sigmoid`` and vice versa (including their ``Tensor``\n    method equivalents). We could subclass Interpreter like so::\n\n        class NegSigmSwapInterpreter(Interpreter):\n            def call_function(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n                if target == torch.sigmoid:\n                    return torch.neg(*args, **kwargs)\n                return super().call_function(target, args, kwargs)\n\n            def call_method(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n                if target == \"neg\":\n                    call_self, *args_tail = args\n                    return call_self.sigmoid(*args_tail, **kwargs)\n                return super().call_method(target, args, kwargs)\n\n\n        def fn(x):\n            return torch.sigmoid(x).neg()\n\n\n        gm = torch.fx.symbolic_trace(fn)\n        input = torch.randn(3, 4)\n        result = NegSigmSwapInterpreter(gm).run(input)\n        torch.testing.assert_close(result, torch.neg(input).sigmoid())\n\nArgs:\n    module (torch.nn.Module): The module to be executed\n    garbage_collect_values (bool): Whether to delete values after their last\n        use within the Module's execution. This ensures optimal memory usage during\n        execution. This can be disabled to, for example, examine all of the intermediate\n        values in the execution by looking at the ``Interpreter.env`` attribute.\n    graph (Optional[Graph]): If passed, the interpreter will execute this\n        graph instead of `module.graph`, using the provided `module`\n        argument to satisfy any requests for state.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed."
      },
      "methods": [
        {
          "name": "boxed_run",
          "signature": "boxed_run(self, args_list)",
          "documentation": {
            "description": "Run `module` via interpretation and return the result.  This uses the \"boxed\"\ncalling convention, where you pass a list of arguments, which will be cleared\nby the interpreter.  This ensures that input tensors are promptly deallocated.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_function",
          "signature": "call_function(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``call_function`` node and return the result.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value returned by the function invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_method",
          "signature": "call_method(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``call_method`` node and return the result.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value returned by the method invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_module",
          "signature": "call_module(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``call_module`` node and return the result.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value returned by the module invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fetch_args_kwargs_from_env",
          "signature": "fetch_args_kwargs_from_env(self, n: torch.fx.node.Node) -> Tuple[Tuple, Dict]",
          "documentation": {
            "description": "Fetch the concrete values of ``args`` and ``kwargs`` of node ``n``\nfrom the current execution environment.",
            "parameters": {
              "n": {
                "type": "Node",
                "description": "The node for which ``args`` and ``kwargs`` should be fetched."
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Tuple": {
                "type": "",
                "description": "[Tuple, Dict]: ``args`` and ``kwargs`` with concrete values for ``n``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fetch_attr",
          "signature": "fetch_attr(self, target: str)",
          "documentation": {
            "description": "Fetch an attribute from the ``Module`` hierarchy of ``self.module``.",
            "parameters": {
              "target": {
                "type": "str",
                "description": "The fully-qualified name of the attribute to fetch"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value of the attribute.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_attr",
          "signature": "get_attr(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``get_attr`` node. Will retrieve an attribute\nvalue from the ``Module`` hierarchy of ``self.module``.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value of the attribute that was retrieved\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "map_nodes_to_values",
          "signature": "map_nodes_to_values(self, args: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], n: torch.fx.node.Node) -> Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]",
          "documentation": {
            "description": "Recursively descend through ``args`` and look up the concrete value\nfor each ``Node`` in the current execution environment.",
            "parameters": {
              "args": {
                "type": "Argument",
                "description": "Data structure within which to look up concrete values"
              },
              "n": {
                "type": "Node",
                "description": "Node to which ``args`` belongs. This is only used for error reporting.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "output",
          "signature": "output(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute an ``output`` node. This really just retrieves\nthe value referenced by the ``output`` node and returns it.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The return value referenced by the output node\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "placeholder",
          "signature": "placeholder(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``placeholder`` node. Note that this is stateful:\n``Interpreter`` maintains an internal iterator over\narguments passed to ``run`` and this method returns\nnext() on that iterator.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              }
            },
            "returns": "Any: The argument value that was retrieved.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "run",
          "signature": "run(self, *args, initial_env: Optional[Dict[torch.fx.node.Node, Any]] = None, enable_io_processing: bool = True) -> Any",
          "documentation": {
            "description": "Run `module` via interpretation and return the result.",
            "parameters": {
              "initial_env": {
                "type": "Optional[Dict[Node, Any]]",
                "description": "An optional starting environment for execution."
              },
              "This": {
                "type": "",
                "description": "is a dict mapping `Node` to any value. This can be used, for example, to"
              },
              "pre": {
                "type": "",
                "description": "-populate results for certain `Nodes` so as to do only partial evaluation within"
              },
              "the": {
                "type": "",
                "description": "interpreter."
              },
              "enable_io_processing": {
                "type": "bool",
                "description": "If true, we process the inputs and outputs with graph's process_inputs and"
              },
              "process_outputs": {
                "type": "",
                "description": "function first before using them."
              }
            },
            "returns": "Any: The value returned from executing the Module\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "run_node",
          "signature": "run_node(self, n: torch.fx.node.Node) -> Any",
          "documentation": {
            "description": "Run a specific node ``n`` and return the result.\nCalls into placeholder, get_attr, call_function,\ncall_method, call_module, or output depending\non ``node.op``",
            "parameters": {
              "n": {
                "type": "Node",
                "description": "The Node to execute"
              }
            },
            "returns": "Any: The result of executing ``n``\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Node",
      "documentation": {
        "description": "``Node`` is the data structure that represents individual operations within\na ``Graph``. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each ``Node`` has a function specified\nby its ``op`` property. The ``Node`` semantics for each value of ``op`` are as follows:\n\n- ``placeholder`` represents a function input. The ``name`` attribute specifies the name this value will take on.\n  ``target`` is similarly the name of the argument. ``args`` holds either: 1) nothing, or 2) a single argument\n  denoting the default parameter of the function input. ``kwargs`` is don't-care. Placeholders correspond to\n  the function parameters (e.g. ``x``) in the graph printout.\n- ``get_attr`` retrieves a parameter from the module hierarchy. ``name`` is similarly the name the result of the\n  fetch is assigned to. ``target`` is the fully-qualified name of the parameter's position in the module hierarchy.\n  ``args`` and ``kwargs`` are don't-care\n- ``call_function`` applies a free function to some values. ``name`` is similarly the name of the value to assign\n  to. ``target`` is the function to be applied. ``args`` and ``kwargs`` represent the arguments to the function,\n  following the Python calling convention\n- ``call_module`` applies a module in the module hierarchy's ``forward()`` method to given arguments. ``name`` is\n  as previous. ``target`` is the fully-qualified name of the module in the module hierarchy to call.\n  ``args`` and ``kwargs`` represent the arguments to invoke the module on, *excluding the self argument*.\n- ``call_method`` calls a method on a value. ``name`` is as similar. ``target`` is the string name of the method\n  to apply to the ``self`` argument. ``args`` and ``kwargs`` represent the arguments to invoke the module on,\n  *including the self argument*\n- ``output`` contains the output of the traced function in its ``args[0]`` attribute. This corresponds to the \"return\" statement\n  in the Graph printout.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "append",
          "signature": "append(self, x: 'Node') -> None",
          "documentation": {
            "description": "Insert ``x`` after this node in the list of nodes in the graph.\nEquivalent to ``self.next.prepend(x)``",
            "parameters": {
              "x": {
                "type": "Node",
                "description": "The node to put after this node. Must be a member of the same graph.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "format_node",
          "signature": "format_node(self, placeholder_names: Optional[List[str]] = None, maybe_return_typename: Optional[List[str]] = None) -> Optional[str]",
          "documentation": {
            "description": "Return a descriptive string representation of ``self``.\n\nThis method can be used with no arguments as a debugging\nutility.\n\nThis function is also used internally in the ``__str__`` method\nof ``Graph``. Together, the strings in ``placeholder_names``\nand ``maybe_return_typename`` make up the signature of the\nautogenerated ``forward`` function in this Graph's surrounding\nGraphModule. ``placeholder_names`` and ``maybe_return_typename``\nshould not be used otherwise.",
            "parameters": {
              "placeholder_names": {
                "type": "",
                "description": "A list that will store formatted strings"
              },
              "representing": {
                "type": "",
                "description": "the placeholders in the generated\n``forward`` function. Internal use only."
              },
              "maybe_return_typename": {
                "type": "",
                "description": "A single-element list that will store"
              },
              "a": {
                "type": "",
                "description": "formatted string representing the output of the"
              },
              "generated": {
                "type": "",
                "description": "``forward`` function. Internal use only."
              }
            },
            "returns": "str: If 1) we're using ``format_node`` as an internal helper\n        in the ``__str__`` method of ``Graph``, and 2) ``self``\n        is a placeholder Node, return ``None``. Otherwise,\n        return a  descriptive string representation of the\n        current Node.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "insert_arg",
          "signature": "insert_arg(self, idx: int, arg: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]) -> None",
          "documentation": {
            "description": "Insert an positional argument to the argument list with given index.",
            "parameters": {
              "idx": {
                "type": "int",
                "description": "The index of the element in ``self.args`` to be inserted before."
              },
              "arg": {
                "type": "Argument",
                "description": "The new argument value to insert into ``args``\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_impure",
          "signature": "is_impure(self) -> bool",
          "documentation": {
            "description": "Returns whether this op is impure, i.e. if its op is a placeholder or\noutput, or if a call_function or call_module which is impure.",
            "parameters": {},
            "returns": "bool: If the op is impure or not.\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "normalized_arguments",
          "signature": "normalized_arguments(self, root: torch.nn.modules.module.Module, arg_types: Optional[Tuple[Any]] = None, kwarg_types: Optional[Dict[str, Any]] = None, normalize_to_only_use_kwargs: bool = False) -> Optional[torch.fx.operator_schemas.ArgsKwargsPair]",
          "documentation": {
            "description": "Returns normalized arguments to Python targets. This means that\n`args/kwargs` will be matched up to the module/functional's\nsignature and return exclusively kwargs in positional order\nif `normalize_to_only_use_kwargs` is true.\nAlso populates default values. Does not support positional-only\nparameters or varargs parameters.\n\nSupports module calls.\n\nMay require `arg_types` and `kwarg_types` in order to disambiguate overloads.",
            "parameters": {
              "root": {
                "type": "torch.nn.Module",
                "description": "Module upon which to resolve module targets."
              },
              "arg_types": {
                "type": "Optional[Tuple[Any]]",
                "description": "Tuple of arg types for the args"
              },
              "kwarg_types": {
                "type": "Optional[Dict[str, Any]]",
                "description": "Dict of arg types for the kwargs"
              },
              "normalize_to_only_use_kwargs": {
                "type": "bool",
                "description": "Whether to normalize to only use kwargs."
              }
            },
            "returns": "Returns NamedTuple ArgsKwargsPair, or `None` if not successful.\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "prepend",
          "signature": "prepend(self, x: 'Node') -> None",
          "documentation": {
            "description": "Insert x before this node in the list of nodes in the graph. Example::\n\n    Before: p -> self\n            bx -> x -> ax\n    After:  p -> x -> self\n            bx -> ax",
            "parameters": {
              "x": {
                "type": "Node",
                "description": "The node to put before this node. Must be a member of the same graph.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "replace_all_uses_with",
          "signature": "replace_all_uses_with(self, replace_with: 'Node', delete_user_cb: Callable[[ForwardRef('Node')], bool] = <function Node.<lambda> at 0x76e266437a60>, *, propagate_meta: bool = False) -> List[ForwardRef('Node')]",
          "documentation": {
            "description": "Replace all uses of ``self`` in the Graph with the Node ``replace_with``.",
            "parameters": {
              "replace_with": {
                "type": "Node",
                "description": "The node to replace all uses of ``self`` with."
              },
              "delete_user_cb": {
                "type": "Callable",
                "description": "Callback that is called to determine"
              },
              "whether": {
                "type": "",
                "description": "a given user of the self node should be removed."
              },
              "propagate_meta": {
                "type": "bool",
                "description": "Whether or not to copy all properties"
              },
              "on": {
                "type": "",
                "description": "the .meta field of the original node onto the replacement node."
              },
              "For": {
                "type": "",
                "description": "safety, this is only valid to do if the replacement node"
              },
              "doesn": {
                "type": "",
                "description": "'t already have an existing .meta field."
              }
            },
            "returns": "The list of Nodes on which this change was made.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "replace_input_with",
          "signature": "replace_input_with(self, old_input: 'Node', new_input: 'Node') -> None",
          "documentation": {
            "description": "Loop through input nodes of ``self``, and replace all instances of\n``old_input`` with ``new_input``.",
            "parameters": {
              "old_input": {
                "type": "Node",
                "description": "The old input node to be replaced."
              },
              "new_input": {
                "type": "Node",
                "description": "The new input node to replace ``old_input``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update_arg",
          "signature": "update_arg(self, idx: int, arg: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]) -> None",
          "documentation": {
            "description": "Update an existing positional argument to contain the new value\n``arg``. After calling, ``self.args[idx] == arg``.",
            "parameters": {
              "idx": {
                "type": "int",
                "description": "The index into ``self.args`` of the element to update"
              },
              "arg": {
                "type": "Argument",
                "description": "The new argument value to write into ``args``\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update_kwarg",
          "signature": "update_kwarg(self, key: str, arg: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]) -> None",
          "documentation": {
            "description": "Update an existing keyword argument to contain the new value\n``arg``. After calling, ``self.kwargs[key] == arg``.",
            "parameters": {
              "key": {
                "type": "str",
                "description": "The key in ``self.kwargs`` of the element to update"
              },
              "arg": {
                "type": "Argument",
                "description": "The new argument value to write into ``kwargs``\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Proxy",
      "documentation": {
        "description": "``Proxy`` objects are ``Node`` wrappers that flow through the\nprogram during symbolic tracing and record all the operations\n(``torch`` function calls, method calls, operators) that they touch\ninto the growing FX Graph.\n\nIf you're doing graph transforms, you can wrap your own ``Proxy``\nmethod around a raw ``Node`` so that you can use the overloaded\noperators to add additional things to a ``Graph``.\n\n``Proxy`` objects cannot be iterated. In other words, the symbolic\ntracer will throw an error if a ``Proxy`` is used in a loop or as\nan ``*args``/``**kwargs`` function argument.\n\nThere are two main ways around this:\n1. Factor out the untraceable logic into a top-level function and\nuse ``fx.wrap`` on it.\n2. If the control flow is static (i.e. the loop trip count is\nbased on some hyperparameter), the code can be kept in its original\nposition and refactored into something like::\n\n    for i in range(self.some_hyperparameter):\n        indexed_item = proxied_value[i]\n\nFor a more detailed description into the Proxy internals, check out\nthe \"Proxy\" section in `torch/fx/README.md`\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "keys",
          "signature": "keys(self)",
          "documentation": {
            "description": ".. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProxyableClassMeta",
      "documentation": {
        "description": "ProxyableClassMeta allows you to make construction of a given Python class\nsymbolically traceable. For example::\n\n    import torch\n    import torch.fx\n\n\n    class TensorPair(metaclass=torch.fx.ProxyableClassMeta):\n        def __init__(self, left, right):\n            self.left, self.right = left, right\n\n        def add(self, other):\n            l = self.left + other.left\n            r = self.right + other.right\n            return TensorPair(l, r)\n\n        def mul(self, other):\n            l = self.left * other.left\n            r = self.right * other.right\n            return TensorPair(l, r)\n\n\n    def use_tensor_pair_ctor(x: TensorPair, y: torch.Tensor):\n        s = x.add(TensorPair(y, y))\n        return s.mul(x)\n\n\n    x = TensorPair(torch.randn(5, 3), torch.randn(5, 3))\n    y = torch.randn(5, 3)\n    ref_out = use_tensor_pair_ctor(x, y)\n\n    traced = torch.fx.symbolic_trace(use_tensor_pair_ctor)\n    print(traced.code)\n    '''\n    def forward(self, x : __main___TensorPair, y : torch.Tensor):\n        tensor_pair = __main___TensorPair(y, y);  y = None\n        add = x.add(tensor_pair);  tensor_pair = None\n        mul = add.mul(x);  add = x = None\n        return mul\n    '''\n\nFrom this example, we can see that construction of a class (``TensorPair``)\ndefined with ``ProxyableClassMeta`` as metaclass can be recorded in symbolic\ntracing.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "mro",
          "signature": "mro(self, /)",
          "documentation": {
            "description": "Return a type's method resolution order.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Tracer",
      "documentation": {
        "description": "Tracer(autowrap_modules=(math,), autowrap_functions=())\n\n    ``Tracer`` is the class that implements the symbolic tracing functionality\n    of ``torch.fx.symbolic_trace``. A call to ``symbolic_trace(m)`` is equivalent\n    to ``Tracer().trace(m)``.\n\n    Tracer can be subclassed to override various behaviors of the tracing\n    process. The different behaviors that can be overridden are described\n    in the docstrings of the methods on this class.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "call_module",
          "signature": "call_module(self, m: torch.nn.modules.module.Module, forward: Callable[..., Any], args: Tuple[Any, ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Method that specifies the behavior of this ``Tracer`` when it encounters\na call to an ``nn.Module`` instance.\n\nBy default, the behavior is to check if the called module is a leaf module\nvia ``is_leaf_module``. If it is, emit a ``call_module`` node referring to\n``m`` in the ``Graph``. Otherwise, call the ``Module`` normally, tracing through\nthe operations in its ``forward`` function.\n\nThis method can be overridden to--for example--create nested traced\nGraphModules, or any other behavior you would want while tracing across\n``Module`` boundaries.",
            "parameters": {
              "m": {
                "type": "Module",
                "description": "The module for which a call is being emitted"
              },
              "forward": {
                "type": "Callable",
                "description": "The forward() method of the ``Module`` to be invoked"
              },
              "args": {
                "type": "Tuple",
                "description": "args of the module callsite"
              },
              "kwargs": {
                "type": "Dict",
                "description": "kwargs of the module callsite"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "The": {
                "type": "",
                "description": "return value from the Module call. In the case that a ``call_module``"
              },
              "node": {
                "type": "",
                "description": "was emitted, this is a ``Proxy`` value. Otherwise, it is whatever"
              },
              "value": {
                "type": "",
                "description": "was returned from the ``Module`` invocation.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_arg",
          "signature": "create_arg(self, a: Any) -> 'Argument'",
          "documentation": {
            "description": "A method to specify the behavior of tracing when preparing values to\nbe used as arguments to nodes in the ``Graph``.\n\nBy default, the behavior includes:\n\n#. Iterate through collection types (e.g. tuple, list, dict) and recursively\n   call ``create_args`` on the elements.\n#. Given a Proxy object, return a reference to the underlying IR ``Node``\n#. Given a non-Proxy Tensor object, emit IR for various cases:\n\n    * For a Parameter, emit a ``get_attr`` node referring to that Parameter\n    * For a non-Parameter Tensor, store the Tensor away in a special\n      attribute referring to that attribute.\n\nThis method can be overridden to support more types.",
            "parameters": {
              "a": {
                "type": "Any",
                "description": "The value to be emitted as an ``Argument`` in the ``Graph``."
              }
            },
            "returns": "The value ``a`` converted into the appropriate ``Argument``\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_args_for_root",
          "signature": "create_args_for_root(self, root_fn, is_module, concrete_args=None)",
          "documentation": {
            "description": "Create ``placeholder`` nodes corresponding to the signature of the ``root``\nModule. This method introspects root's signature and emits those\nnodes accordingly, also supporting ``*args`` and ``**kwargs``.\n\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_node",
          "signature": "create_node(self, kind: str, target: Union[Callable[..., Any], str], args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]], name: Optional[str] = None, type_expr: Optional[Any] = None) -> torch.fx.node.Node",
          "documentation": {
            "description": "Inserts a graph node given target, args, kwargs, and name.\n\nThis method can be overridden to do extra checking, validation, or\nmodification of values used in node creation. For example, one might\nwant to disallow in-place operations from being recorded.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "create_proxy",
          "signature": "create_proxy(self, kind: str, target: Union[Callable[..., Any], str], args: Tuple[Any, ...], kwargs: Dict[str, Any], name: Optional[str] = None, type_expr: Optional[Any] = None, proxy_factory_fn: Callable[[torch.fx.node.Node], ForwardRef('Proxy')] = None)",
          "documentation": {
            "description": "Create a Node from the given arguments, then return the Node\nwrapped in a Proxy object.\n\nIf kind = 'placeholder', then we're creating a Node that\nrepresents the parameter of a function. If we need to encode\na default parameter, we use the ``args`` tuple. ``args`` is\notherwise empty for ``placeholder`` Nodes.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_fresh_qualname",
          "signature": "get_fresh_qualname(self, prefix: str) -> str",
          "documentation": {
            "description": "Gets a fresh name for a prefix and returns it. This function ensures\nthat it will not clash with an existing attribute on the graph.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "getattr",
          "signature": "getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: Dict[str, Any])",
          "documentation": {
            "description": "Method that specifies the behavior of this ``Tracer`` when we call getattr\non a call to an ``nn.Module`` instance.\n\nBy default, the behavior is to return a proxy value for the attribute. It\nalso stores the proxy value in the ``parameter_proxy_cache``, so that future\ncalls will reuse the proxy rather than creating a new one.\n\nThis method can be overridden to --for example-- not return proxies when\nquerying parameters.",
            "parameters": {
              "attr": {
                "type": "str",
                "description": "The name of the attribute being queried"
              },
              "attr_val": {
                "type": "Any",
                "description": "The value of the attribute"
              },
              "parameter_proxy_cache": {
                "type": "Dict[str, Any]",
                "description": "A cache of attr names to proxies"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "The": {
                "type": "",
                "description": "return value from the getattr call.\n.. warning::"
              },
              "This": {
                "type": "",
                "description": "API is experimental and is *NOT* backward-compatible."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_leaf_module",
          "signature": "is_leaf_module(self, m: torch.nn.modules.module.Module, module_qualified_name: str) -> bool",
          "documentation": {
            "description": "A method to specify whether a given ``nn.Module`` is a \"leaf\" module.\n\nLeaf modules are the atomic units that appear in\nthe IR, referenced by ``call_module`` calls. By default,\nModules in the PyTorch standard library namespace (torch.nn)\nare leaf modules. All other modules are traced through and\ntheir constituent ops are recorded, unless specified otherwise\nvia this parameter.",
            "parameters": {
              "m": {
                "type": "Module",
                "description": "The module being queried about"
              },
              "module_qualified_name": {
                "type": "str",
                "description": "The path to root of this module. For example,"
              },
              "if": {
                "type": "",
                "description": "you have a module hierarchy where submodule ``foo`` contains"
              },
              "submodule": {
                "type": "",
                "description": "``bar``, which contains submodule ``baz``, that module will"
              },
              "appear": {
                "type": "",
                "description": "with the qualified name ``foo.bar.baz`` here.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "iter",
          "signature": "iter(self, obj: 'Proxy') -> Iterator",
          "documentation": {
            "description": "Called when a proxy object is being iterated over, such as\n        when used in control flow.  Normally we don't know what to do because\n        we don't know the value of the proxy, but a custom tracer can attach more\n        information to the graph node using create_node and can choose to return an iterator.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "keys",
          "signature": "keys(self, obj: 'Proxy') -> Any",
          "documentation": {
            "description": "Called when a proxy object is has the keys() method called.\n        This is what happens when ** is called on a proxy. This should return an\n        iterator it ** is suppose to work in your custom tracer.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path_of_module",
          "signature": "path_of_module(self, mod: torch.nn.modules.module.Module) -> str",
          "documentation": {
            "description": "Helper method to find the qualified name of ``mod`` in the Module hierarchy\nof ``root``. For example, if ``root`` has a submodule named ``foo``, which has\na submodule named ``bar``, passing ``bar`` into this function will return\nthe string \"foo.bar\".",
            "parameters": {
              "mod": {
                "type": "str",
                "description": "The ``Module`` to retrieve the qualified name for.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "proxy",
          "signature": "proxy(self, node: torch.fx.node.Node) -> 'Proxy'",
          "documentation": {
            "description": ".. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bool",
          "signature": "to_bool(self, obj: 'Proxy') -> bool",
          "documentation": {
            "description": "Called when a proxy object is being converted to a boolean, such as\n        when used in control flow.  Normally we don't know what to do because\n        we don't know the value of the proxy, but a custom tracer can attach more\n        information to the graph node using create_node and can choose to return a value.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "trace",
          "signature": "trace(self, root: Union[torch.nn.modules.module.Module, Callable[..., Any]], concrete_args: Optional[Dict[str, Any]] = None) -> torch.fx.graph.Graph",
          "documentation": {
            "description": "Trace ``root`` and return the corresponding FX ``Graph`` representation. ``root``\ncan either be an ``nn.Module`` instance or a Python callable.\n\nNote that after this call, ``self.root`` may be different from the ``root`` passed\nin here. For example, when a free function is passed to ``trace()``, we will\ncreate an ``nn.Module`` instance to use as the root and add embedded constants\nto.",
            "parameters": {
              "root": {
                "type": "Union[Module, Callable]",
                "description": "Either a ``Module`` or a function to be"
              },
              "traced": {
                "type": "",
                "description": "through. Backwards-compatibility for this parameter is"
              },
              "guaranteed": {
                "type": "",
                "description": "."
              },
              "concrete_args": {
                "type": "Optional[Dict[str, any]]",
                "description": "Concrete arguments that should"
              },
              "not": {
                "type": "",
                "description": "be treated as Proxies. This parameter is experimental and"
              },
              "its": {
                "type": "",
                "description": "backwards-compatibility is *NOT* guaranteed."
              }
            },
            "returns": "A ``Graph`` representing the semantics of the passed-in ``root``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Transformer",
      "documentation": {
        "description": "``Transformer`` is a special type of interpreter that produces a\nnew ``Module``. It exposes a ``transform()`` method that returns\nthe transformed ``Module``. ``Transformer`` does not require\narguments to run, as ``Interpreter`` does. ``Transformer`` works\nentirely symbolically.\n\nExample:\n\n    Suppose we want to swap all instances of ``torch.neg`` with\n    ``torch.sigmoid`` and vice versa (including their ``Tensor``\n    method equivalents). We could subclass ``Transformer`` like so::\n\n        class NegSigmSwapXformer(Transformer):\n            def call_function(\n                self, target: \"Target\", args: Tuple[Argument, ...], kwargs: Dict[str, Any]\n            ) -> Any:\n                if target == torch.sigmoid:\n                    return torch.neg(*args, **kwargs)\n                return super().call_function(target, args, kwargs)\n\n            def call_method(\n                self, target: \"Target\", args: Tuple[Argument, ...], kwargs: Dict[str, Any]\n            ) -> Any:\n                if target == \"neg\":\n                    call_self, *args_tail = args\n                    return call_self.sigmoid(*args_tail, **kwargs)\n                return super().call_method(target, args, kwargs)\n\n\n        def fn(x):\n            return torch.sigmoid(x).neg()\n\n\n        gm = torch.fx.symbolic_trace(fn)\n\n        transformed: torch.nn.Module = NegSigmSwapXformer(gm).transform()\n        input = torch.randn(3, 4)\n        torch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())",
        "parameters": {
          "module": {
            "type": "GraphModule",
            "description": "The ``Module`` to be transformed.\n.. note::"
          },
          "Backwards": {
            "type": "",
            "description": "-compatibility for this API is guaranteed."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Suppose we want to swap all instances of ``torch.neg`` with\n    ``torch.sigmoid`` and vice versa (including their ``Tensor``\n    method equivalents). We could subclass ``Transformer`` like so::\n\n        class NegSigmSwapXformer(Transformer):\n            def call_function(\n                self, target: \"Target\", args: Tuple[Argument, ...], kwargs: Dict[str, Any]\n            ) -> Any:\n                if target == torch.sigmoid:\n                    return torch.neg(*args, **kwargs)\n                return super().call_function(target, args, kwargs)\n\n            def call_method(\n                self, target: \"Target\", args: Tuple[Argument, ...], kwargs: Dict[str, Any]\n            ) -> Any:\n                if target == \"neg\":\n                    call_self, *args_tail = args\n                    return call_self.sigmoid(*args_tail, **kwargs)\n                return super().call_method(target, args, kwargs)\n\n\n        def fn(x):\n            return torch.sigmoid(x).neg()\n\n\n        gm = torch.fx.symbolic_trace(fn)\n\n        transformed: torch.nn.Module = NegSigmSwapXformer(gm).transform()\n        input = torch.randn(3, 4)\n        torch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())\n\nArgs:\n    module (GraphModule): The ``Module`` to be transformed.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed."
      },
      "methods": [
        {
          "name": "boxed_run",
          "signature": "boxed_run(self, args_list)",
          "documentation": {
            "description": "Run `module` via interpretation and return the result.  This uses the \"boxed\"\ncalling convention, where you pass a list of arguments, which will be cleared\nby the interpreter.  This ensures that input tensors are promptly deallocated.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_function",
          "signature": "call_function(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": ".. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_method",
          "signature": "call_method(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute a ``call_method`` node and return the result.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value returned by the method invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "call_module",
          "signature": "call_module(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": ".. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fetch_args_kwargs_from_env",
          "signature": "fetch_args_kwargs_from_env(self, n: torch.fx.node.Node) -> Tuple[Tuple, Dict]",
          "documentation": {
            "description": "Fetch the concrete values of ``args`` and ``kwargs`` of node ``n``\nfrom the current execution environment.",
            "parameters": {
              "n": {
                "type": "Node",
                "description": "The node for which ``args`` and ``kwargs`` should be fetched."
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Tuple": {
                "type": "",
                "description": "[Tuple, Dict]: ``args`` and ``kwargs`` with concrete values for ``n``.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fetch_attr",
          "signature": "fetch_attr(self, target: str)",
          "documentation": {
            "description": "Fetch an attribute from the ``Module`` hierarchy of ``self.module``.",
            "parameters": {
              "target": {
                "type": "str",
                "description": "The fully-qualified name of the attribute to fetch"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The value of the attribute.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_attr",
          "signature": "get_attr(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> torch.fx.proxy.Proxy",
          "documentation": {
            "description": "Execute a ``get_attr`` node. In ``Transformer``, this is\noverridden to insert a new ``get_attr`` node into the output\ngraph.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "map_nodes_to_values",
          "signature": "map_nodes_to_values(self, args: Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], n: torch.fx.node.Node) -> Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]",
          "documentation": {
            "description": "Recursively descend through ``args`` and look up the concrete value\nfor each ``Node`` in the current execution environment.",
            "parameters": {
              "args": {
                "type": "Argument",
                "description": "Data structure within which to look up concrete values"
              },
              "n": {
                "type": "Node",
                "description": "Node to which ``args`` belongs. This is only used for error reporting.\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "output",
          "signature": "output(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> Any",
          "documentation": {
            "description": "Execute an ``output`` node. This really just retrieves\nthe value referenced by the ``output`` node and returns it.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation"
              },
              "Return": {
                "type": "",
                "description": ""
              },
              "Any": {
                "type": "",
                "description": "The return value referenced by the output node\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "placeholder",
          "signature": "placeholder(self, target: 'Target', args: Tuple[Union[Tuple[ForwardRef('Argument'), ...], Sequence[ForwardRef('Argument')], Mapping[str, ForwardRef('Argument')], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType], ...], kwargs: Dict[str, Any]) -> torch.fx.proxy.Proxy",
          "documentation": {
            "description": "Execute a ``placeholder`` node. In ``Transformer``, this is\noverridden to insert a new ``placeholder`` into the output\ngraph.",
            "parameters": {
              "target": {
                "type": "Target",
                "description": "The call target for this node. See\n`Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for"
              },
              "details": {
                "type": "",
                "description": "on semantics"
              },
              "args": {
                "type": "Tuple",
                "description": "Tuple of positional args for this invocation"
              },
              "kwargs": {
                "type": "Dict",
                "description": "Dict of keyword arguments for this invocation\n.. note::"
              },
              "Backwards": {
                "type": "",
                "description": "-compatibility for this API is guaranteed."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "run",
          "signature": "run(self, *args, initial_env: Optional[Dict[torch.fx.node.Node, Any]] = None, enable_io_processing: bool = True) -> Any",
          "documentation": {
            "description": "Run `module` via interpretation and return the result.",
            "parameters": {
              "initial_env": {
                "type": "Optional[Dict[Node, Any]]",
                "description": "An optional starting environment for execution."
              },
              "This": {
                "type": "",
                "description": "is a dict mapping `Node` to any value. This can be used, for example, to"
              },
              "pre": {
                "type": "",
                "description": "-populate results for certain `Nodes` so as to do only partial evaluation within"
              },
              "the": {
                "type": "",
                "description": "interpreter."
              },
              "enable_io_processing": {
                "type": "bool",
                "description": "If true, we process the inputs and outputs with graph's process_inputs and"
              },
              "process_outputs": {
                "type": "",
                "description": "function first before using them."
              }
            },
            "returns": "Any: The value returned from executing the Module\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "run_node",
          "signature": "run_node(self, n: torch.fx.node.Node) -> Any",
          "documentation": {
            "description": "Run a specific node ``n`` and return the result.\nCalls into placeholder, get_attr, call_function,\ncall_method, call_module, or output depending\non ``node.op``",
            "parameters": {
              "n": {
                "type": "Node",
                "description": "The Node to execute"
              }
            },
            "returns": "Any: The result of executing ``n``\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self) -> torch.fx.graph_module.GraphModule",
          "documentation": {
            "description": "Transform ``self.module`` and return the transformed\n``GraphModule``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}