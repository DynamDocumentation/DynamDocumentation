{
  "description": "This package introduces support for the XPU backend, specifically tailored for\nIntel GPU optimization.\n\nThis package is lazily initialized, so you can always import it, and use\n:func:`is_available()` to determine if your system supports XPU.",
  "functions": [
    {
      "name": "Callable",
      "signature": "Callable(*args, **kwargs)",
      "documentation": {
        "description": "Deprecated alias to collections.abc.Callable.\n\nCallable[[int], str] signifies a function that takes a single\nparameter of type int and returns a str.\n\nThe subscription syntax must always be used with exactly two\nvalues: the argument list and the return type.\nThe argument list must be a list of types, a ParamSpec,\nConcatenate or ellipsis. The return type must be a single type.\n\nThere is no syntax to indicate optional or keyword arguments;\nsuch function types are rarely used as callback types.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Dict",
      "signature": "Dict(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of dict.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "List",
      "signature": "List(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of list.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Optional",
      "signature": "Optional(*args, **kwds)",
      "documentation": {
        "description": "Optional[X] is equivalent to Union[X, None].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Tuple",
      "signature": "Tuple(*args, **kwargs)",
      "documentation": {
        "description": "Deprecated alias to builtins.tuple.\n\nTuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tuple[T1, T2] is a tuple of two elements corresponding\nto type variables T1 and T2.  Tuple[int, float, str] is a tuple\nof an int, a float and a string.\n\nTo specify a variable-length tuple of homogeneous type, use Tuple[T, ...].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Union",
      "signature": "Union(*args, **kwds)",
      "documentation": {
        "description": "Union type; Union[X, Y] means either X or Y.\n\nOn Python 3.10 and higher, the | operator\ncan also be used to denote unions;\nX | Y means the same thing to the type checker as Union[X, Y].\n\nTo define a union, use e.g. Union[int, str]. Details:\n- The arguments must be types and there must be at least one.\n- None as an argument is a special case and is replaced by\n  type(None).\n- Unions of unions are flattened, e.g.::\n\n    assert Union[Union[int, str], float] == Union[int, str, float]\n\n- Unions of a single argument vanish, e.g.::\n\n    assert Union[int] == int  # The constructor actually returns int\n\n- Redundant arguments are skipped, e.g.::\n\n    assert Union[int, str, int] == Union[int, str]\n\n- When comparing unions, the argument order is ignored, e.g.::\n\n    assert Union[int, str] == Union[str, int]\n\n- You cannot subclass or instantiate a union.\n- You can use Optional[X] as a shorthand for Union[X, None].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "current_device",
      "signature": "current_device() -> int",
      "documentation": {
        "description": "Return the index of a currently selected device.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "current_stream",
      "signature": "current_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.xpu.streams.Stream",
      "documentation": {
        "description": "Return the currently selected :class:`Stream` for a given device.",
        "parameters": {
          "device": {
            "type": "torch.device or int, optional",
            "description": "selected device. Returns"
          },
          "the": {
            "type": "",
            "description": "currently selected :class:`Stream` for the current device, given"
          },
          "by": {
            "type": "",
            "description": "func:`~torch.xpu.current_device`, if :attr:`device` is ``None``\n(default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "device_count",
      "signature": "device_count() -> int",
      "documentation": {
        "description": "Return the number of XPU device available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "empty_cache",
      "signature": "empty_cache() -> None",
      "documentation": {
        "description": "Release all unoccupied cached memory currently held by the caching\nallocator so that those can be used in other XPU application.\n\n.. note::\n    :func:`~torch.xpu.empty_cache` doesn't increase the amount of XPU\n    memory available for PyTorch. However, it may help reduce fragmentation\n    of XPU memory in certain cases.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_arch_list",
      "signature": "get_arch_list() -> List[str]",
      "documentation": {
        "description": "Return list XPU architectures this library was compiled for.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_device_capability",
      "signature": "get_device_capability(device: Union[torch.device, str, int, NoneType] = None) -> Dict[str, Any]",
      "documentation": {
        "description": "Get the xpu capability of a device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "device for which to"
          },
          "return": {
            "type": "",
            "description": "the device capability. This function is a no-op if this"
          },
          "argument": {
            "type": "",
            "description": "is a negative integer. It uses the current device, given by\n:func:`~torch.xpu.current_device`, if :attr:`device` is ``None``\n(default)."
          }
        },
        "returns": "Dict[str, Any]: the xpu capability dictionary of the device",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_device_name",
      "signature": "get_device_name(device: Union[torch.device, str, int, NoneType] = None) -> str",
      "documentation": {
        "description": "Get the name of a device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "device for which to"
          },
          "return": {
            "type": "",
            "description": "the name. This function is a no-op if this argument is a"
          },
          "negative": {
            "type": "",
            "description": "integer. It uses the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "str: the name of the device",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_device_properties",
      "signature": "get_device_properties(device: Union[torch.device, str, int, NoneType] = None) -> torch._utils._XpuDeviceProperties",
      "documentation": {
        "description": "Get the properties of a device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str",
            "description": "device for which to return the"
          },
          "properties": {
            "type": "",
            "description": "of the device."
          }
        },
        "returns": "_XpuDeviceProperties: the properties of the device",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_gencode_flags",
      "signature": "get_gencode_flags() -> str",
      "documentation": {
        "description": "Return XPU AOT(ahead-of-time) build flags this library was compiled with.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_rng_state",
      "signature": "get_rng_state(device: Union[int, str, torch.device] = 'xpu') -> torch.Tensor",
      "documentation": {
        "description": "Return the random number generator state of the specified GPU as a ByteTensor.",
        "parameters": {
          "device": {
            "type": "torch.device or int, optional",
            "description": "The device to return the RNG state of."
          },
          "Default": {
            "type": "",
            "description": "``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device).\n.. warning::"
          },
          "This": {
            "type": "",
            "description": "function eagerly initializes XPU."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_rng_state_all",
      "signature": "get_rng_state_all() -> List[torch.Tensor]",
      "documentation": {
        "description": "Return a list of ByteTensor representing the random number states of all devices.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "init",
      "signature": "init()",
      "documentation": {
        "description": "Initialize PyTorch's XPU state.\nThis is a Python API about lazy initialization that avoids initializing\nXPU until the first time it is accessed. Does nothing if the XPU state is\nalready initialized.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "initial_seed",
      "signature": "initial_seed() -> int",
      "documentation": {
        "description": "Return the current random seed of the current GPU.\n\n.. warning::\n    This function eagerly initializes XPU.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_available",
      "signature": "is_available() -> bool",
      "documentation": {
        "description": "Return a bool indicating if XPU is currently available.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_bf16_supported",
      "signature": "is_bf16_supported()",
      "documentation": {
        "description": "Return a bool indicating if the current XPU device supports dtype bfloat16.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_initialized",
      "signature": "is_initialized()",
      "documentation": {
        "description": "Return whether PyTorch's XPU state has been initialized.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "lru_cache",
      "signature": "lru_cache(maxsize=128, typed=False)",
      "documentation": {
        "description": "Least-recently-used cache decorator.\n\nIf *maxsize* is set to None, the LRU features are disabled and the cache\ncan grow without bound.\n\nIf *typed* is True, arguments of different types will be cached separately.\nFor example, f(3.0) and f(3) will be treated as distinct calls with\ndistinct results.\n\nArguments to the cached function must be hashable.\n\nView the cache statistics named tuple (hits, misses, maxsize, currsize)\nwith f.cache_info().  Clear the cache and statistics with f.cache_clear().\nAccess the underlying function with f.__wrapped__.\n\nSee:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "manual_seed",
      "signature": "manual_seed(seed: int) -> None",
      "documentation": {
        "description": "Set the seed for generating random numbers for the current GPU.\n\nIt's safe to call this function if XPU is not available; in that case, it is silently ignored.",
        "parameters": {
          "seed": {
            "type": "int",
            "description": "The desired seed.\n.. warning::"
          },
          "If": {
            "type": "",
            "description": "you are working with a multi-GPU model, this function is insufficient"
          },
          "to": {
            "type": "",
            "description": "get determinism.  To seed all GPUs, use :func:`manual_seed_all`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "manual_seed_all",
      "signature": "manual_seed_all(seed: int) -> None",
      "documentation": {
        "description": "Set the seed for generating random numbers on all GPUs.\n\nIt's safe to call this function if XPU is not available; in that case, it is silently ignored.",
        "parameters": {
          "seed": {
            "type": "int",
            "description": "The desired seed."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "max_memory_allocated",
      "signature": "max_memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "documentation": {
        "description": "Return the maximum GPU memory occupied by tensors in bytes for a given device.\n\nBy default, this returns the peak allocated memory since the beginning of\nthis program. :func:`~torch.xpu.reset_peak_memory_stats` can be used to\nreset the starting point in tracking this metric. For example, these two\nfunctions can measure the peak allocated memory usage of each iteration in a\ntraining loop.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "max_memory_reserved",
      "signature": "max_memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "documentation": {
        "description": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\nBy default, this returns the peak cached memory since the beginning of this\nprogram. :func:`~torch.xpu.reset_peak_memory_stats` can be used to reset\nthe starting point in tracking this metric. For example, these two functions\ncan measure the peak cached memory amount of each iteration in a training\nloop.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "mem_get_info",
      "signature": "mem_get_info(device: Union[torch.device, str, int, NoneType] = None) -> Tuple[int, int]",
      "documentation": {
        "description": "Return the global free and total GPU memory for a given device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "int: the memory available on the device in units of bytes.\n    int: the total memory on the device in units of bytes",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "memory_allocated",
      "signature": "memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "documentation": {
        "description": "Return the current GPU memory occupied by tensors in bytes for a given device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default).\n.. note::"
          },
          "This": {
            "type": "",
            "description": "is likely less than the amount shown in `xpu-smi` since some"
          },
          "unused": {
            "type": "",
            "description": "memory can be held by the caching allocator and some context"
          },
          "needs": {
            "type": "",
            "description": "to be created on GPU."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "memory_reserved",
      "signature": "memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "documentation": {
        "description": "Return the current GPU memory managed by the caching allocator in bytes for a given device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "memory_stats",
      "signature": "memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> Dict[str, Any]",
      "documentation": {
        "description": "Return a dictionary of XPU memory allocator statistics for a given device.\n\nThe return value of this function is a dictionary of statistics, each of\nwhich is a non-negative integer.\n\nCore statistics:\n\n- ``\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n  amount of allocated memory.\n- ``\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n  amount of reserved memory.\n- ``\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n  amount of active memory.\n- ``\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n  memory requested by client code, compare this with allocated_bytes to check if\n  allocation rounding adds too much overhead.\n\nFor these core statistics, values are broken down as follows.\n\nPool type:\n\n- ``all``: combined statistics across all memory pools.\n- ``large_pool``: statistics for the large allocation pool (for size >= 1MB allocations).\n- ``small_pool``: statistics for the small allocation pool (for size < 1MB allocations).\n\nMetric type:\n\n- ``current``: current value of this metric.\n- ``peak``: maximum value of this metric.\n- ``allocated``: historical total increase in this metric.\n- ``freed``: historical total decrease in this metric.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistics": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "memory_stats_as_nested_dict",
      "signature": "memory_stats_as_nested_dict(device: Union[torch.device, str, int, NoneType] = None) -> Dict[str, Any]",
      "documentation": {
        "description": "Return the result of :func:`~torch.xpu.memory_stats` as a nested dictionary.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reset_accumulated_memory_stats",
      "signature": "reset_accumulated_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "documentation": {
        "description": "Reset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.\n\nSee :func:`~torch.xpu.memory_stats` for details. Accumulated stats correspond to\nthe `\"allocated\"` and `\"freed\"` keys in each individual stat dict.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reset_peak_memory_stats",
      "signature": "reset_peak_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "documentation": {
        "description": "Reset the \"peak\" stats tracked by the XPU memory allocator.\n\nSee :func:`~torch.xpu.memory_stats` for details. Peak stats correspond to the\n`\"peak\"` key in each individual stat dict.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str, optional",
            "description": "selected device. Returns"
          },
          "statistic": {
            "type": "",
            "description": "for the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "seed",
      "signature": "seed() -> None",
      "documentation": {
        "description": "Set the seed for generating random numbers to a random number for the current GPU.\n\nIt's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n.. warning::\n    If you are working with a multi-GPU model, this function will only initialize\n    the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "seed_all",
      "signature": "seed_all() -> None",
      "documentation": {
        "description": "Set the seed for generating random numbers to a random number on all GPUs.\n\nIt's safe to call this function if XPU is not available; in that case, it is silently ignored.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_device",
      "signature": "set_device(device: Union[torch.device, str, int, NoneType]) -> None",
      "documentation": {
        "description": "Set the current device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str",
            "description": "selected device. This function is a"
          },
          "no": {
            "type": "",
            "description": "-op if this argument is negative."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_rng_state",
      "signature": "set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'xpu') -> None",
      "documentation": {
        "description": "Set the random number generator state of the specified GPU.",
        "parameters": {
          "new_state": {
            "type": "torch.ByteTensor",
            "description": "The desired state"
          },
          "device": {
            "type": "torch.device or int, optional",
            "description": "The device to set the RNG state."
          },
          "Default": {
            "type": "",
            "description": "``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_rng_state_all",
      "signature": "set_rng_state_all(new_states: Iterable[torch.Tensor]) -> None",
      "documentation": {
        "description": "Set the random number generator state of all devices.",
        "parameters": {
          "new_states": {
            "type": "Iterable of torch.ByteTensor",
            "description": "The desired state for each device."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_stream",
      "signature": "set_stream(stream: torch.xpu.streams.Stream)",
      "documentation": {
        "description": "Set the current stream.This is a wrapper API to set the stream.\n    Usage of this function is discouraged in favor of the ``stream``\n    context manager.",
        "parameters": {
          "stream": {
            "type": "Stream",
            "description": "selected stream. This function is a no-op"
          },
          "if": {
            "type": "",
            "description": "this argument is ``None``."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "stream",
      "signature": "stream(stream: Optional[ForwardRef('torch.xpu.Stream')]) -> torch.xpu.StreamContext",
      "documentation": {
        "description": "Wrap around the Context-manager StreamContext that selects a given stream.",
        "parameters": {
          "stream": {
            "type": "Stream",
            "description": "selected stream. This manager is a no-op if it's ``None``."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "synchronize",
      "signature": "synchronize(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "documentation": {
        "description": "Wait for all kernels in all streams on a XPU device to complete.",
        "parameters": {
          "device": {
            "type": "torch.device or int, optional",
            "description": "device for which to synchronize."
          },
          "It": {
            "type": "",
            "description": "uses the current device, given by :func:`~torch.xpu.current_device`,"
          },
          "if": {
            "type": "",
            "description": "attr:`device` is ``None`` (default)."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "Any",
      "documentation": {
        "description": "Special type indicating an unconstrained type.\n\n- Any is compatible with every type.\n- Any assumed to have all methods.\n- All values assumed to be instances of Any.\n\nNote that all the above statements are true from the point of view of\nstatic type checkers. At runtime, Any should not be used with instance\nchecks.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Event",
      "documentation": {
        "description": "Wrapper around a XPU event.\n\nXPU events are synchronization markers that can be used to monitor the\ndevice's progress, and to synchronize XPU streams.\n\nThe underlying XPU events are lazily initialized when the event is first\nrecorded. After creation, only streams on the same device may record the\nevent. However, streams on any device can wait on the event.",
        "parameters": {
          "enable_timing": {
            "type": "bool, optional",
            "description": "indicates if the event should measure time\n(default: ``False``)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "elapsed_time",
          "signature": "elapsed_time(self, end_event)",
          "documentation": {
            "description": "Return the time elapsed.\n\nTime reported in milliseconds after the event was recorded and\nbefore the end_event was recorded.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query",
          "signature": "query(self) -> bool",
          "documentation": {
            "description": "Check if all work currently captured by event has completed.",
            "parameters": {},
            "returns": "A boolean indicating if all work currently captured by event has\n    completed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "record",
          "signature": "record(self, stream=None) -> None",
          "documentation": {
            "description": "Record the event in a given stream.\n\nUses ``torch.xpu.current_stream()`` if no stream is specified. The\nstream's device must match the event's device.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "synchronize",
          "signature": "synchronize(self) -> None",
          "documentation": {
            "description": "Wait for the event to complete.\n\nWaits until the completion of all work currently captured in this event.\nThis prevents the CPU thread from proceeding until the event completes.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(self, stream=None) -> None",
          "documentation": {
            "description": "Make all future work submitted to the given stream wait for this event.\n\nUse ``torch.xpu.current_stream()`` if no stream is specified.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Stream",
      "documentation": {
        "description": "Wrapper around a XPU stream.\n\nA XPU stream is a linear sequence of execution that belongs to a specific\ndevice, independent from other streams.",
        "parameters": {
          "device": {
            "type": "torch.device or int, optional",
            "description": "a device on which to allocate"
          },
          "the": {
            "type": "",
            "description": "stream. If :attr:`device` is ``None`` (default) or a negative"
          },
          "integer": {
            "type": "",
            "description": ", this will use the current device."
          },
          "priority": {
            "type": "int, optional",
            "description": "priority of the stream, should be 0 or"
          },
          "negative": {
            "type": "",
            "description": ", where negative numbers indicate higher priority. By default,"
          },
          "streams": {
            "type": "",
            "description": "have priority 0."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "query",
          "signature": "query(self) -> bool",
          "documentation": {
            "description": "Check if all the work submitted has been completed.",
            "parameters": {},
            "returns": "A boolean indicating if all kernels in this stream are completed.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "record_event",
          "signature": "record_event(self, event=None)",
          "documentation": {
            "description": "Record an event.",
            "parameters": {
              "event": {
                "type": "torch.xpu.Event, optional",
                "description": "event to record. If not given, a new one"
              },
              "will": {
                "type": "",
                "description": "be allocated."
              }
            },
            "returns": "Recorded event.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "synchronize",
          "signature": "synchronize(self) -> None",
          "documentation": {
            "description": "Wait for all the kernels in this stream to complete.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait_event",
          "signature": "wait_event(self, event) -> None",
          "documentation": {
            "description": "Make all future work submitted to the stream wait for an event.",
            "parameters": {
              "event": {
                "type": "torch.xpu.Event",
                "description": "an event to wait for."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait_stream",
          "signature": "wait_stream(self, stream) -> None",
          "documentation": {
            "description": "Synchronize with another stream.\n\nAll future work submitted to this stream will wait until all kernels\nsubmitted to a given stream at the time of call complete.",
            "parameters": {
              "stream": {
                "type": "Stream",
                "description": "a stream to synchronize."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StreamContext",
      "documentation": {
        "description": "Context-manager that selects a given stream.\n\nAll XPU kernels queued within its context will be enqueued on a selected\nstream.",
        "parameters": {
          "Stream": {
            "type": "Stream",
            "description": "selected stream. This manager is a no-op if it's\n``None``.\n.. note:: Streams are per-device."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "device",
      "documentation": {
        "description": "Context-manager that changes the selected device.",
        "parameters": {
          "device": {
            "type": "torch.device or int or str",
            "description": "device index to select. It's a no-op if"
          },
          "this": {
            "type": "",
            "description": "argument is a negative integer or ``None``."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "device_of",
      "documentation": {
        "description": "Context-manager that changes the current device to that of given object.\n\nYou can use both tensors and storages as arguments. If a given object is\nnot allocated on a XPU, this is a no-op.",
        "parameters": {
          "obj": {
            "type": "Tensor or Storage",
            "description": "object allocated on the selected device."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    }
  ]
}