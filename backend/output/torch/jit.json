{
  "description": "No description available",
  "functions": [
    {
      "name": "Final",
      "signature": "Final(*args, **kwds)",
      "documentation": {
        "description": "Special typing construct to indicate final names to type checkers.\n\nA final name cannot be re-assigned or overridden in a subclass.\n\nFor example::\n\n    MAX_SIZE: Final = 9000\n    MAX_SIZE += 1  # Error reported by type checker\n\n    class Connection:\n        TIMEOUT: Final[int] = 10\n\n    class FastConnector(Connection):\n        TIMEOUT = 1  # Error reported by type checker\n\nThere is no runtime checking of these properties.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Iterator",
      "signature": "Iterator(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of collections.abc.Iterator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "annotate",
      "signature": "annotate(the_type, the_value)",
      "documentation": {
        "description": "Use to give type of `the_value` in TorchScript compiler.\n\nThis method is a pass-through function that returns `the_value`, used to hint TorchScript\ncompiler the type of `the_value`. It is a no-op when running outside of TorchScript.\n\nThough TorchScript can infer correct type for most Python expressions, there are some cases where\ntype inference can be wrong, including:\n\n- Empty containers like `[]` and `{}`, which TorchScript assumes to be container of `Tensor`\n- Optional types like `Optional[T]` but assigned a valid value of type `T`, TorchScript would assume\n  it is type `T` rather than `Optional[T]`\n\nNote that `annotate()` does not help in `__init__` method of `torch.nn.Module` subclasses because it\nis executed in eager mode. To annotate types of `torch.nn.Module` attributes,\nuse :meth:`~torch.jit.Attribute` instead.\n\nExample:\n\n.. testcode::\n\n    import torch\n    from typing import Dict\n\n    @torch.jit.script\n    def fn():\n        # Telling TorchScript that this empty dictionary is a (str -> int) dictionary\n        # instead of default dictionary type of (str -> Tensor).\n        d = torch.jit.annotate(Dict[str, int], {})\n\n        # Without `torch.jit.annotate` above, following statement would fail because of\n        # type mismatch.\n        d[\"name\"] = 20\n\n.. testcleanup::\n\n    del fn",
        "parameters": {
          "the_type": {
            "type": "",
            "description": "Python type that should be passed to TorchScript compiler as type hint for `the_value`"
          },
          "the_value": {
            "type": "",
            "description": "Value or expression to hint type for."
          }
        },
        "returns": "`the_value` is passed back as return value.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    from typing import Dict\n\n    @torch.jit.script\n    def fn():\n        # Telling TorchScript that this empty dictionary is a (str -> int) dictionary\n        # instead of default dictionary type of (str -> Tensor).\n        d = torch.jit.annotate(Dict[str, int], {})\n\n        # Without `torch.jit.annotate` above, following statement would fail because of\n        # type mismatch.\n        d[\"name\"] = 20\n\n.. testcleanup::\n\n    del fn\n\nArgs:\n    the_type: Python type that should be passed to TorchScript compiler as type hint for `the_value`\n    the_value: Value or expression to hint type for.\n\nReturns:\n    `the_value` is passed back as return value."
      }
    },
    {
      "name": "contextmanager",
      "signature": "contextmanager(func)",
      "documentation": {
        "description": "@contextmanager decorator.\n\nTypical usage:\n\n    @contextmanager\n    def some_generator(<arguments>):\n        <setup>\n        try:\n            yield <value>\n        finally:\n            <cleanup>\n\nThis makes this:\n\n    with some_generator(<arguments>) as <variable>:\n        <body>\n\nequivalent to this:\n\n    <setup>\n    try:\n        <variable> = <value>\n        <body>\n    finally:\n        <cleanup>",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_onednn_fusion",
      "signature": "enable_onednn_fusion(enabled: bool)",
      "documentation": {
        "description": "Enable or disables onednn JIT fusion based on the parameter `enabled`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "export",
      "signature": "export(fn)",
      "documentation": {
        "description": "This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a\n:class:`ScriptModule` and should be compiled.\n\n``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called from ``forward`` are compiled as they are seen\nby the compiler, so they do not need this decorator either.\n\nExample (using ``@torch.jit.export`` on a method):\n\n.. testcode::\n\n    import torch\n    import torch.nn as nn\n\n    class MyModule(nn.Module):\n        def implicitly_compiled_method(self, x):\n            return x + 99\n\n        # `forward` is implicitly decorated with `@torch.jit.export`,\n        # so adding it here would have no effect\n        def forward(self, x):\n            return x + 10\n\n        @torch.jit.export\n        def another_forward(self, x):\n            # When the compiler sees this call, it will compile\n            # `implicitly_compiled_method`\n            return self.implicitly_compiled_method(x)\n\n        def unused_method(self, x):\n            return x - 20\n\n    # `m` will contain compiled methods:\n    #     `forward`\n    #     `another_forward`\n    #     `implicitly_compiled_method`\n    # `unused_method` will not be compiled since it was not called from\n    # any compiled methods and wasn't decorated with `@torch.jit.export`\n    m = torch.jit.script(MyModule())",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "export_opnames",
      "signature": "export_opnames(m)",
      "documentation": {
        "description": "Generate new bytecode for a Script module.\n\nReturns what the op list would be for a Script Module based off the current code base.\n\nIf you have a LiteScriptModule and want to get the currently present\nlist of ops call _export_operator_list instead.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fork",
      "signature": "fork(func, *args, **kwargs)",
      "documentation": {
        "description": "Create an asynchronous task executing `func` and a reference to the value of the result of this execution.\n\n`fork` will return immediately, so the return value of `func` may not have been computed yet. To force completion\nof the task and access the return value invoke `torch.jit.wait` on the Future. `fork` invoked\nwith a `func` which returns `T` is typed as `torch.jit.Future[T]`. `fork` calls can be arbitrarily\nnested, and may be invoked with positional and keyword arguments.\nAsynchronous execution will only occur when run in TorchScript. If run in pure python,\n`fork` will not execute in parallel. `fork` will also not execute in parallel when invoked\nwhile tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.\n\n.. warning::\n    `fork` tasks will execute non-deterministically. We recommend only spawning\n    parallel fork tasks for pure functions that do not modify their inputs,\n    module attributes, or global state.",
        "parameters": {
          "func": {
            "type": "callable or torch.nn.Module",
            "description": "A Python function or `torch.nn.Module`"
          },
          "that": {
            "type": "",
            "description": "will be invoked. If executed in TorchScript, it will execute asynchronously,"
          },
          "otherwise": {
            "type": "",
            "description": "it will not. Traced invocations of fork will be captured in the IR.\n``*args``, ``**kwargs``: arguments to invoke `func` with."
          }
        },
        "returns": "`torch.jit.Future[T]`: a reference to the execution of `func`. The value `T`\n    can only be accessed by forcing completion of `func` through `torch.jit.wait`.\n\nExample (fork a free function):\n\n.. code-block:: python\n\n    import torch\n    from torch import Tensor\n    def foo(a : Tensor, b : int) -> Tensor:\n        return a + b\n    def bar(a):\n        fut : torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)\n        return torch.jit.wait(fut)\n    script_bar = torch.jit.script(bar)\n    input = torch.tensor(2)\n    # only the scripted version executes asynchronously\n    assert script_bar(input) == bar(input)\n    # trace is not run asynchronously, but fork is captured in IR\n    graph = torch.jit.trace(bar, (input,)).graph\n    assert \"fork\" in str(graph)\n\nExample (fork a module method):\n\n.. code-block:: python\n\n    import torch\n    from torch import Tensor\n    class AddMod(torch.nn.Module):\n        def forward(self, a: Tensor, b : int):\n            return a + b\n    class Mod(torch.nn.Module):\n        def __init__(self) -> None:\n            super(self).__init__()\n            self.mod = AddMod()\n        def forward(self, input):\n            fut = torch.jit.fork(self.mod, a, b=2)\n            return torch.jit.wait(fut)\n    input = torch.tensor(2)\n    mod = Mod()\n    assert mod(input) == torch.jit.script(mod).forward(input)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "freeze",
      "signature": "freeze(mod, preserved_attrs: Optional[List[str]] = None, optimize_numerics: bool = True)",
      "documentation": {
        "description": "Freeze ScriptModule, inline submodules, and attributes as constants.\n\nFreezing a :class:`ScriptModule` will clone it and attempt to inline the cloned\nmodule's submodules, parameters, and attributes as constants in the TorchScript IR Graph.\nBy default, `forward` will be preserved, as well as attributes & methods specified in\n`preserved_attrs`. Additionally, any attribute that is modified within a preserved\nmethod will be preserved.\n\nFreezing currently only accepts ScriptModules that are in eval mode.\n\nFreezing applies generic optimization that will speed up your model regardless of machine.\nTo further optimize using server-specific settings, run `optimize_for_inference` after\nfreezing.",
        "parameters": {
          "mod": {
            "type": ":class:`ScriptModule`",
            "description": "a module to be frozen"
          },
          "preserved_attrs": {
            "type": "Optional[List[str]]",
            "description": "a list of attributes to preserve in addition to the forward method."
          },
          "Attributes": {
            "type": "",
            "description": "modified in preserved methods will also be preserved."
          },
          "optimize_numerics": {
            "type": "bool",
            "description": "If ``True``, a set of optimization passes will be run that does not strictly"
          },
          "preserve": {
            "type": "",
            "description": "numerics. Full details of optimization can be found at `torch.jit.run_frozen_optimizations`."
          }
        },
        "returns": "Frozen :class:`ScriptModule`.\n\nExample (Freezing a simple module with a Parameter):\n\n.. testcode::\n    import torch\n    class MyModule(torch.nn.Module):\n        def __init__(self, N, M):\n            super().__init__()\n            self.weight = torch.nn.Parameter(torch.rand(N, M))\n            self.linear = torch.nn.Linear(N, M)\n\n        def forward(self, input):\n            output = self.weight.mm(input)\n            output = self.linear(output)\n            return output\n\n    scripted_module = torch.jit.script(MyModule(2, 3).eval())\n    frozen_module = torch.jit.freeze(scripted_module)\n    # parameters have been removed and inlined into the Graph as constants\n    assert len(list(frozen_module.named_parameters())) == 0\n    # See the compiled graph as Python code\n    print(frozen_module.code)\n\nExample (Freezing a module with preserved attributes)\n\n.. testcode::\n    import torch\n    class MyModule2(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.modified_tensor = torch.tensor(10.)\n            self.version = 1\n\n        def forward(self, input):\n            self.modified_tensor += 1\n            return input + self.modified_tensor\n\n    scripted_module = torch.jit.script(MyModule2().eval())\n    frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n    # we've manually preserved `version`, so it still exists on the frozen module and can be modified\n    assert frozen_module.version == 1\n    frozen_module.version = 2\n    # `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n    # it to retain model semantics\n    assert frozen_module(torch.tensor(1)) == torch.tensor(12)\n    # now that we've run it once, the next result will be incremented by one\n    assert frozen_module(torch.tensor(1)) == torch.tensor(13)",
        "raises": "",
        "see_also": "",
        "notes": "Freezing submodule attributes is also supported:\n    frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"submodule.version\"])\n\nNote:\n    If you're not sure why an attribute is not being inlined as a constant, you can run\n    `dump_alias_db` on frozen_module.forward.graph to see if freezing has detected the\n    attribute is being modified.\n\nNote:\n    Because freezing makes weights constants and removes module hierarchy, `to` and other\n    nn.Module methods to manipulate device or dtype no longer work. As a workaround,\n    You can remap devices by specifying `map_location` in `torch.jit.load`, however\n    device-specific logic may have been baked into the model.",
        "examples": ""
      }
    },
    {
      "name": "fuser",
      "signature": "fuser(name)",
      "documentation": {
        "description": "Context manager that facilitates switching between backend fusers.\n\nValid names:\n* ``fuser0`` - enables only legacy fuser\n* ``fuser1`` - enables only NNC\n* ``fuser2`` - enables only nvFuser\n* ``fuser3`` - enables oneDNN Graph",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ignore",
      "signature": "ignore(drop=False, **kwargs)",
      "documentation": {
        "description": "This decorator indicates to the compiler that a function or method should\nbe ignored and left as a Python function. This allows you to leave code in\nyour model that is not yet TorchScript compatible. If called from TorchScript,\nignored functions will dispatch the call to the Python interpreter. Models with ignored\nfunctions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.\n\nExample (using ``@torch.jit.ignore`` on a method)::\n\n    import torch\n    import torch.nn as nn\n\n\n    class MyModule(nn.Module):\n        @torch.jit.ignore\n        def debugger(self, x):\n            import pdb\n\n            pdb.set_trace()\n\n        def forward(self, x):\n            x += 10\n            # The compiler would normally try to compile `debugger`,\n            # but since it is `@ignore`d, it will be left as a call\n            # to Python\n            self.debugger(x)\n            return x\n\n\n    m = torch.jit.script(MyModule())\n\n    # Error! The call `debugger` cannot be saved since it calls into Python\n    m.save(\"m.pt\")\n\nExample (using ``@torch.jit.ignore(drop=True)`` on a method):\n\n.. testcode::\n\n    import torch\n    import torch.nn as nn\n\n    class MyModule(nn.Module):\n        @torch.jit.ignore(drop=True)\n        def training_method(self, x):\n            import pdb\n            pdb.set_trace()\n\n        def forward(self, x):\n            if self.training:\n                self.training_method(x)\n            return x\n\n    m = torch.jit.script(MyModule())\n\n    # This is OK since `training_method` is not saved, the call is replaced\n    # with a `raise`.\n    m.save(\"m.pt\")\n\n.. testcleanup::\n\n    import os\n    os.remove('m.pt')",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "interface",
      "signature": "interface(obj)",
      "documentation": {
        "description": "Decorate to annotate classes or modules of different types.\n\nThis decorator can be used to define an interface that can be used to annotate\nclasses or modules of different types. This can be used for to annotate a submodule\nor attribute class that could have different types that implement the same\ninterface, or which could be swapped at runtime; or to store a list of modules or\nclasses of varying types.\n\nIt is sometimes used to implement \"Callables\" - functions or modules that implement\nan interface but whose implementations differ and which can be swapped out.\n\nExample:\n.. testcode::\n\n    import torch\n    from typing import List\n\n    @torch.jit.interface\n    class InterfaceType:\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            pass\n\n    # implements InterfaceType\n    @torch.jit.script\n    class Impl1:\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            return x.relu()\n\n    class Impl2(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.val = torch.rand(())\n\n        @torch.jit.export\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            return x + self.val\n\n    def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:\n        return impls[idx].run(val)\n\n    user_fn_jit = torch.jit.script(user_fn)\n\n    impls = [Impl1(), torch.jit.script(Impl2())]\n    val = torch.rand(4, 4)\n    user_fn_jit(impls, 0, val)\n    user_fn_jit(impls, 1, val)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    from typing import List\n\n    @torch.jit.interface\n    class InterfaceType:\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            pass\n\n    # implements InterfaceType\n    @torch.jit.script\n    class Impl1:\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            return x.relu()\n\n    class Impl2(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.val = torch.rand(())\n\n        @torch.jit.export\n        def run(self, x: torch.Tensor) -> torch.Tensor:\n            return x + self.val\n\n    def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:\n        return impls[idx].run(val)\n\n    user_fn_jit = torch.jit.script(user_fn)\n\n    impls = [Impl1(), torch.jit.script(Impl2())]\n    val = torch.rand(4, 4)\n    user_fn_jit(impls, 0, val)\n    user_fn_jit(impls, 1, val)"
      }
    },
    {
      "name": "is_scripting",
      "signature": "is_scripting() -> bool",
      "documentation": {
        "description": "Function that returns True when in compilation and False otherwise. This\nis useful especially with the @unused decorator to leave code in your\nmodel that is not yet TorchScript compatible.\n.. testcode::\n\n    import torch\n\n    @torch.jit.unused\n    def unsupported_linear_op(x):\n        return x\n\n    def linear(x):\n        if torch.jit.is_scripting():\n            return torch.linear(x)\n        else:\n            return unsupported_linear_op(x)",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_tracing",
      "signature": "is_tracing()",
      "documentation": {
        "description": "Return a boolean value.\n\nReturns ``True`` in tracing (if a function is called during the\ntracing of code with ``torch.jit.trace``) and ``False`` otherwise.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "isinstance",
      "signature": "isinstance(obj, target_type)",
      "documentation": {
        "description": "Provide container type refinement in TorchScript.\n\nIt can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. ``List[str]``,\n``Dict[str, List[torch.Tensor]]``, ``Optional[Tuple[int,str,int]]``. It can also\nrefine basic types such as bools and ints that are available in TorchScript.",
        "parameters": {
          "obj": {
            "type": "",
            "description": "object to refine the type of"
          },
          "target_type": {
            "type": "",
            "description": "type to try to refine obj to"
          }
        },
        "returns": "``bool``: True if obj was successfully refined to the type of target_type,\n        False otherwise with no new type refinement\n\n\nExample (using ``torch.jit.isinstance`` for type refinement):\n.. testcode::\n\n    import torch\n    from typing import Any, Dict, List\n\n    class MyModule(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n\n        def forward(self, input: Any): # note the Any type\n            if torch.jit.isinstance(input, List[torch.Tensor]):\n                for t in input:\n                    y = t.clamp(0, 0.5)\n            elif torch.jit.isinstance(input, Dict[str, str]):\n                for val in input.values():\n                    print(val)\n\n    m = torch.jit.script(MyModule())\n    x = [torch.rand(3,3), torch.rand(4,3)]\n    m(x)\n    y = {\"key1\":\"val1\",\"key2\":\"val2\"}\n    m(y)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jit_module_from_flatbuffer",
      "signature": "jit_module_from_flatbuffer(f)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "last_executed_optimized_graph",
      "signature": "_last_executed_optimized_graph()",
      "documentation": {
        "description": "_last_executed_optimized_graph() -> torch._C.Graph\n\nRetrieve the optimized graph that was run the last time the graph executor ran on this thread",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "load",
      "signature": "load(f, map_location=None, _extra_files=None, _restore_shapes=False)",
      "documentation": {
        "description": "Load a :class:`ScriptModule` or :class:`ScriptFunction` previously saved with :func:`torch.jit.save <torch.jit.save>`.\n\nAll previously saved modules, no matter their device, are first loaded onto CPU,\nand then are moved to the devices they were saved from. If this fails (e.g.\nbecause the run time system doesn't have certain devices), an exception is\nraised.",
        "parameters": {
          "f": {
            "type": "",
            "description": "a file-like object (has to implement read, readline, tell, and seek),"
          },
          "or": {
            "type": "",
            "description": "a string containing a file name"
          },
          "map_location": {
            "type": "string or torch.device",
            "description": "A simplified version of\n``map_location`` in `torch.jit.save` used to dynamically remap"
          },
          "storages": {
            "type": "",
            "description": "to an alternative set of devices."
          },
          "_extra_files": {
            "type": "dictionary of filename to content",
            "description": "The extra"
          },
          "filenames": {
            "type": "",
            "description": "given in the map would be loaded and their content"
          },
          "would": {
            "type": "",
            "description": "be stored in the provided map."
          },
          "_restore_shapes": {
            "type": "bool",
            "description": "Whether or not to retrace the module on load using stored inputs"
          }
        },
        "returns": "A :class:`ScriptModule` object.\n\nExample:\n.. testcode::\n\n    import torch\n    import io\n\n    torch.jit.load('scriptmodule.pt')\n\n    # Load ScriptModule from io.BytesIO object\n    with open('scriptmodule.pt', 'rb') as f:\n        buffer = io.BytesIO(f.read())\n\n    # Load all tensors to the original device\n    torch.jit.load(buffer)\n\n    # Load all tensors onto CPU, using a device\n    buffer.seek(0)\n    torch.jit.load(buffer, map_location=torch.device('cpu'))\n\n    # Load all tensors onto CPU, using a string\n    buffer.seek(0)\n    torch.jit.load(buffer, map_location='cpu')\n\n    # Load with extra files.\n    extra_files = {'foo.txt': ''}  # values will be replaced with data\n    torch.jit.load('scriptmodule.pt', _extra_files=extra_files)\n    print(extra_files['foo.txt'])\n\n.. testoutput::\n    :hide:\n\n    ...\n\n.. testcleanup::\n\n    import os\n    os.remove(\"scriptmodule.pt\")",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    import io\n\n    torch.jit.load('scriptmodule.pt')\n\n    # Load ScriptModule from io.BytesIO object\n    with open('scriptmodule.pt', 'rb') as f:\n        buffer = io.BytesIO(f.read())\n\n    # Load all tensors to the original device\n    torch.jit.load(buffer)\n\n    # Load all tensors onto CPU, using a device\n    buffer.seek(0)\n    torch.jit.load(buffer, map_location=torch.device('cpu'))\n\n    # Load all tensors onto CPU, using a string\n    buffer.seek(0)\n    torch.jit.load(buffer, map_location='cpu')\n\n    # Load with extra files.\n    extra_files = {'foo.txt': ''}  # values will be replaced with data\n    torch.jit.load('scriptmodule.pt', _extra_files=extra_files)\n    print(extra_files['foo.txt'])\n\n.. testoutput::\n    :hide:\n\n    ...\n\n.. testcleanup::\n\n    import os\n    os.remove(\"scriptmodule.pt\")"
      }
    },
    {
      "name": "onednn_fusion_enabled",
      "signature": "onednn_fusion_enabled()",
      "documentation": {
        "description": "Return whether onednn JIT fusion is enabled.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "optimize_for_inference",
      "signature": "optimize_for_inference(mod: torch.jit._script.ScriptModule, other_methods: Optional[List[str]] = None) -> torch.jit._script.ScriptModule",
      "documentation": {
        "description": "Perform a set of optimization passes to optimize a model for the purposes of inference.\n\nIf the model is not already frozen, optimize_for_inference\nwill invoke `torch.jit.freeze` automatically.\n\nIn addition to generic optimizations that should speed up your model regardless\nof environment, prepare for inference will also bake in build specific settings\nsuch as the presence of CUDNN or MKLDNN, and may in the future make transformations\nwhich speed things up on one machine but slow things down on another. Accordingly,\nserialization is not implemented following invoking `optimize_for_inference` and\nis not guaranteed.\n\nThis is still in prototype, and may have the potential to slow down your model.\nPrimary use cases that have been targeted so far have been vision models on cpu\nand gpu to a lesser extent.\n\nExample (optimizing a module with Conv->Batchnorm)::\n\n    import torch\n    in_channels, out_channels = 3, 32\n    conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=True)\n    bn = torch.nn.BatchNorm2d(out_channels, eps=.001)\n    mod = torch.nn.Sequential(conv, bn)\n    frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))\n    assert \"batch_norm\" not in str(frozen_mod.graph)\n    # if built with MKLDNN, convolution will be run with MKLDNN weights\n    assert \"MKLDNN\" in frozen_mod.graph",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "optimized_execution",
      "signature": "optimized_execution(should_optimize)",
      "documentation": {
        "description": "Context manager that controls whether the JIT's executor will run optimizations before executing a function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "run_frozen_optimizations",
      "signature": "run_frozen_optimizations(mod, optimize_numerics: bool = True, preserved_methods: Optional[List[str]] = None)",
      "documentation": {
        "description": "Run a series of optimizations looking for patterns that occur in frozen graphs.\n\nThe current set of optimizations includes:\n    - Dropout Removal\n    - Pretranspose Linear Layers\n    - Concat Linear Layers with same input Tensor\n    - Conv -> Batchnorm folding\n    - Conv -> Add/Sub folding\n    - Conv -> Mul/Div folding",
        "parameters": {
          "mod": {
            "type": ":class:`ScriptModule`",
            "description": "a frozen module to be optimized"
          },
          "optimize_numerics": {
            "type": "bool",
            "description": "If ``True``, a set of optimization passes will be run that does not strictly"
          },
          "preserve": {
            "type": "",
            "description": "numerics. These optimizations preserve default rtol and atol of `torch.testing.assert_close`"
          },
          "when": {
            "type": "",
            "description": "applied on a single transformation, however in a module where many transformations are applied"
          },
          "the": {
            "type": "",
            "description": "rtol or atol may no longer fall within the default `assert_close` tolerance. Conv -> Batchnorm folding,"
          },
          "Conv": {
            "type": "",
            "description": "-Add/Sub, and Conv -> Mul/Div folding all may alter numerics."
          }
        },
        "returns": "None",
        "raises": "",
        "see_also": "",
        "notes": "In rare occassions, this can result in slower execution.\n\nExample (Freezing a module with Conv->Batchnorm)\n.. code-block:: python\n    import torch\n    in_channels, out_channels = 3, 32\n    conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=True)\n    bn = torch.nn.BatchNorm2d(out_channels, eps=.001)\n    mod = torch.nn.Sequential(conv, bn)\n    # set optimize to False here, by default freezing runs run_frozen_optimizations\n    frozen_mod = torch.jit.freeze(torch.jit.script(mod.eval()), optimize=False)\n    # inspect frozen mod\n    assert \"batch_norm\" in str(frozen_mod.graph)\n    torch.jit.run_frozen_optimizations(frozen_mod)\n    assert \"batch_norm\" not in str(frozen_mod.graph)",
        "examples": ""
      }
    },
    {
      "name": "save",
      "signature": "save(m, f, _extra_files=None)",
      "documentation": {
        "description": "Save an offline version of this module for use in a separate process.\n\nThe saved module serializes all of the methods, submodules, parameters, and\nattributes of this module. It can be loaded into the C++ API using\n``torch::jit::load(filename)`` or into the Python API with\n:func:`torch.jit.load <torch.jit.load>`.\n\nTo be able to save a module, it must not make any calls to native Python\nfunctions.  This means that all submodules must be subclasses of\n:class:`ScriptModule` as well.\n\n.. DANGER::\n    All modules, no matter their device, are always loaded onto the CPU\n    during loading.  This is different from :func:`torch.load`'s semantics\n    and may change in the future.",
        "parameters": {
          "m": {
            "type": "",
            "description": ".save(\"scriptmodule.pt\")\n# Save to io.BytesIO buffer"
          },
          "f": {
            "type": "",
            "description": "A file-like object (has to implement write and flush) or a string"
          },
          "containing": {
            "type": "",
            "description": "that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6"
          },
          "_extra_files": {
            "type": "",
            "description": "Map from filename to contents which will be stored as part of `f`.\n.. note::"
          },
          "torch": {
            "type": "",
            "description": ".jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)"
          },
          "across": {
            "type": "",
            "description": "versions. For example, dividing two integer tensors in"
          },
          "PyTorch": {
            "type": "",
            "description": "1.6 will fail to load in PyTorch 1.5, however, since the"
          },
          "its": {
            "type": "",
            "description": "division behavior will be preserved. The same module saved in"
          },
          "behavior": {
            "type": "",
            "description": "of division changed in 1.6, and 1.5 does not know how to"
          },
          "replicate": {
            "type": "",
            "description": "the 1.6 behavior."
          },
          "Example": {
            "type": "",
            "description": ".. testcode::"
          },
          "import": {
            "type": "",
            "description": "io"
          },
          "class": {
            "type": "",
            "description": "MyModule(torch.nn.Module):"
          },
          "def": {
            "type": "",
            "description": "forward(self, x):"
          },
          "return": {
            "type": "",
            "description": "x + 10"
          },
          "buffer": {
            "type": "",
            "description": "= io.BytesIO()"
          },
          "extra_files": {
            "type": "",
            "description": "= {'foo.txt': b'bar'}"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    import io\n\n    class MyModule(torch.nn.Module):\n        def forward(self, x):\n            return x + 10\n\n    m = torch.jit.script(MyModule())\n\n    # Save to file\n    torch.jit.save(m, 'scriptmodule.pt')\n    # This line is equivalent to the previous\n    m.save(\"scriptmodule.pt\")\n\n    # Save to io.BytesIO buffer\n    buffer = io.BytesIO()\n    torch.jit.save(m, buffer)\n\n    # Save with extra files\n    extra_files = {'foo.txt': b'bar'}\n    torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)"
      }
    },
    {
      "name": "save_jit_module_to_flatbuffer",
      "signature": "save_jit_module_to_flatbuffer(m, f, _extra_files=None)",
      "documentation": {
        "description": "Save an offline version of this module for use in a separate process.\n\nThe saved module serializes all of the methods, submodules, parameters, and\nattributes of this module. It can be loaded into the C++ API using\n``torch::jit::load_jit_module_from_file(filename)`` or into the Python API with\n:func:`torch.jit.jit_module_from_flatbuffer<torch.jit.jit_module_from_flatbuffer>`.\n\nTo be able to save a module, it must not make any calls to native Python\nfunctions.  This means that all submodules must be subclasses of\n:class:`ScriptModule` as well.\n\n.. DANGER::\n    All modules, no matter their device, are always loaded onto the CPU\n    during loading.  This is different from :func:`torch.load`'s semantics\n    and may change in the future.",
        "parameters": {
          "m": {
            "type": "",
            "description": "= torch.jit.script(MyModule())\n# Save to file"
          },
          "f": {
            "type": "",
            "description": "A string for file path"
          },
          "Example": {
            "type": "",
            "description": ".. testcode::"
          },
          "import": {
            "type": "",
            "description": "io"
          },
          "class": {
            "type": "",
            "description": "MyModule(torch.nn.Module):"
          },
          "def": {
            "type": "",
            "description": "forward(self, x):"
          },
          "return": {
            "type": "",
            "description": "x + 10"
          },
          "torch": {
            "type": "",
            "description": ".jit.save_jit_module_to_flatbuffer(m, 'scriptmodule.ff')"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    import io\n\n    class MyModule(torch.nn.Module):\n        def forward(self, x):\n            return x + 10\n\n    m = torch.jit.script(MyModule())\n\n    # Save to file\n    torch.jit.save_jit_module_to_flatbuffer(m, 'scriptmodule.ff')"
      }
    },
    {
      "name": "script",
      "signature": "script(obj, optimize=None, _frames_up=0, _rcb=None, example_inputs: Union[List[Tuple], Dict[Callable, List[Tuple]], NoneType] = None)",
      "documentation": {
        "description": "Script the function.\n\nScripting a function or ``nn.Module`` will inspect the source code, compile\nit as TorchScript code using the TorchScript compiler, and return a :class:`ScriptModule` or\n:class:`ScriptFunction`. TorchScript itself is a subset of the Python language, so not all\nfeatures in Python work, but we provide enough functionality to compute on\ntensors and do control-dependent operations. For a complete guide, see the\n:ref:`language-reference`.\n\nScripting a dictionary or list copies the data inside it into a TorchScript instance than can be\nsubsequently passed by reference between Python and TorchScript with zero copy overhead.\n\n``torch.jit.script`` can be used as a function for modules, functions, dictionaries and lists\n and as a decorator ``@torch.jit.script`` for :ref:`torchscript-classes` and functions.",
        "parameters": {
          "obj": {
            "type": "Callable, class, or nn.Module",
            "description": "The ``nn.Module``, function, class type,"
          },
          "dictionary": {
            "type": "",
            "description": ", or list to compile."
          },
          "example_inputs": {
            "type": "Union[List[Tuple], Dict[Callable, List[Tuple]], None]",
            "description": "Provide example inputs"
          },
          "to": {
            "type": "",
            "description": "annotate the arguments for a function or ``nn.Module``."
          }
        },
        "returns": "If ``obj`` is ``nn.Module``, ``script`` returns\n    a :class:`ScriptModule` object. The returned :class:`ScriptModule` will\n    have the same set of sub-modules and parameters as the\n    original ``nn.Module``. If ``obj`` is a standalone function,\n    a :class:`ScriptFunction` will be returned. If ``obj`` is a ``dict``, then\n    ``script`` returns an instance of `torch._C.ScriptDict`. If ``obj`` is a ``list``,\n    then ``script`` returns an instance of `torch._C.ScriptList`.\n\n**Scripting a function**\n    The ``@torch.jit.script`` decorator will construct a :class:`ScriptFunction`\n    by compiling the body of the function.\n\n    Example (scripting a function):\n\n    .. testcode::\n\n        import torch\n\n        @torch.jit.script\n        def foo(x, y):\n            if x.max() > y.max():\n                r = x\n            else:\n                r = y\n            return r\n\n        print(type(foo))  # torch.jit.ScriptFunction\n\n        # See the compiled graph as Python code\n        print(foo.code)\n\n        # Call the function using the TorchScript interpreter\n        foo(torch.ones(2, 2), torch.ones(2, 2))\n\n    .. testoutput::\n        :hide:\n\n        ...\n\n****Scripting a function using example_inputs**\n    Example inputs can be used to annotate a function arguments.\n\n    Example (annotating a function before scripting):\n\n    .. testcode::\n\n        import torch\n\n        def test_sum(a, b):\n            return a + b\n\n        # Annotate the arguments to be int\n        scripted_fn = torch.jit.script(test_sum, example_inputs=[(3, 4)])\n\n        print(type(scripted_fn))  # torch.jit.ScriptFunction\n\n        # See the compiled graph as Python code\n        print(scripted_fn.code)\n\n        # Call the function using the TorchScript interpreter\n        scripted_fn(20, 100)\n\n    .. testoutput::\n        :hide:\n\n        ...\n\n**Scripting an nn.Module**\n    Scripting an ``nn.Module`` by default will compile the ``forward`` method and recursively\n    compile any methods, submodules, and functions called by ``forward``. If a ``nn.Module`` only uses\n    features supported in TorchScript, no changes to the original module code should be necessary. ``script``\n    will construct :class:`ScriptModule` that has copies of the attributes, parameters, and methods of\n    the original module.\n\n    Example (scripting a simple module with a Parameter):\n\n    .. testcode::\n\n        import torch\n\n        class MyModule(torch.nn.Module):\n            def __init__(self, N, M):\n                super().__init__()\n                # This parameter will be copied to the new ScriptModule\n                self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n                # When this submodule is used, it will be compiled\n                self.linear = torch.nn.Linear(N, M)\n\n            def forward(self, input):\n                output = self.weight.mv(input)\n\n                # This calls the `forward` method of the `nn.Linear` module, which will\n                # cause the `self.linear` submodule to be compiled to a `ScriptModule` here\n                output = self.linear(output)\n                return output\n\n        scripted_module = torch.jit.script(MyModule(2, 3))\n\n    Example (scripting a module with traced submodules):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                # torch.jit.trace produces a ScriptModule's conv1 and conv2\n                self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))\n                self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))\n\n            def forward(self, input):\n                input = F.relu(self.conv1(input))\n                input = F.relu(self.conv2(input))\n                return input\n\n        scripted_module = torch.jit.script(MyModule())\n\n    To compile a method other than ``forward`` (and recursively compile anything it calls), add\n    the :func:`@torch.jit.export <torch.jit.export>` decorator to the method. To opt out of compilation\n    use :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`@torch.jit.unused <torch.jit.unused>`.\n\n    Example (an exported and ignored method in a module)::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n\n            @torch.jit.export\n            def some_entry_point(self, input):\n                return input + 10\n\n            @torch.jit.ignore\n            def python_only_fn(self, input):\n                # This function won't be compiled, so any\n                # Python APIs can be used\n                import pdb\n                pdb.set_trace()\n\n            def forward(self, input):\n                if self.training:\n                    self.python_only_fn(input)\n                return input * 99\n\n        scripted_module = torch.jit.script(MyModule())\n        print(scripted_module.some_entry_point(torch.randn(2, 2)))\n        print(scripted_module(torch.randn(2, 2)))\n\n    Example ( Annotating forward of nn.Module using example_inputs)::\n\n        import torch\n        import torch.nn as nn\n        from typing import NamedTuple\n\n        class MyModule(NamedTuple):\n        result: List[int]\n\n        class TestNNModule(torch.nn.Module):\n            def forward(self, a) -> MyModule:\n                result = MyModule(result=a)\n                return result\n\n        pdt_model = TestNNModule()\n\n        # Runs the pdt_model in eager model with the inputs provided and annotates the arguments of forward\n        scripted_model = torch.jit.script(pdt_model, example_inputs={pdt_model: [([10, 20, ], ), ], })\n\n        # Run the scripted_model with actual inputs\n        print(scripted_model([20]))",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "script_if_tracing",
      "signature": "script_if_tracing(fn)",
      "documentation": {
        "description": "Compiles ``fn`` when it is first called during tracing.\n\n``torch.jit.script`` has a non-negligible start up time when it is first called due to\nlazy-initializations of many compiler builtins. Therefore you should not use\nit in library code. However, you may want to have parts of your library work\nin tracing even if they use control flow. In these cases, you should use\n``@torch.jit.script_if_tracing`` to substitute for\n``torch.jit.script``.",
        "parameters": {
          "fn": {
            "type": "",
            "description": "A function to compile."
          }
        },
        "returns": "If called during tracing, a :class:`ScriptFunction` created by `torch.jit.script` is returned.\n    Otherwise, the original function `fn` is returned.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "script_method",
      "signature": "script_method(fn)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_fusion_strategy",
      "signature": "set_fusion_strategy(strategy: List[Tuple[str, int]])",
      "documentation": {
        "description": "Set the type and number of specializations that can occur during fusion.\n\nUsage: provide a list of pairs (type, depth) where type is one of \"STATIC\" or \"DYNAMIC\"\nand depth is an integer.\n\nBehavior - static vs dynamic:\n    In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined\n    based on some initial profiling runs.\n    In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple\n    shapes are possible.\n\nIn both cases, we also recompile on new striding behavior, device, or dtype.\n\nBehavior - fallback functions & depth:\n    When an input doesn't match the format required by the specialized compiled op, it will run\n    a fallback function. Fallback functions are recursively be compiled and specialized based\n    on the observed tensor shapes. Since compilation can be slow, the \"depth\" parameter is provided to\n    limit the number of specializations that can be compiled, before giving up on recompiling and\n    falling back to a completely un-fused, un-specialized implementation.\n\nThe list of (type, depth) pairs controls the type of specializations and the number of\nspecializations. For example: [(\"STATIC\", 2), (\"DYNAMIC\", 2)] indicates that the first\ntwo specializations will use static fusions, the following two specializations will use\ndynamic fusion, and any inputs that satisfy none of the 4 options will run an\nunfused implementation.\n\nNB: in the future, if more as more fusion backends are added there may be more granular\napis for specific fusers.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_module",
      "signature": "set_module(obj, mod)",
      "documentation": {
        "description": "Set the module attribute on a python object for a given object for nicer printing",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "trace",
      "signature": "trace(func, example_inputs=None, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x76e28096f770>, example_kwarg_inputs=None, _store_inputs=True)",
      "documentation": {
        "description": "Trace a function and return an executable  or :class:`ScriptFunction` that will be optimized using just-in-time compilation.\n\nTracing is ideal for code that operates only on\n``Tensor``\\\\s and lists, dictionaries, and\ntuples of ``Tensor``\\\\s.\n\nUsing `torch.jit.trace` and `torch.jit.trace_module`, you can turn an\nexisting module or Python function into a TorchScript\n:class:`ScriptFunction` or :class:`ScriptModule`. You must provide example\ninputs, and we run the function, recording the operations performed on all\nthe tensors.\n\n* The resulting recording of a standalone function produces `ScriptFunction`.\n* The resulting recording of `nn.Module.forward` or `nn.Module` produces\n  `ScriptModule`.\n\nThis module also contains any parameters that the original\nmodule had as well.",
        "parameters": {
          "func": {
            "type": "callable or torch.nn.Module",
            "description": "A Python function or `torch.nn.Module`"
          },
          "that": {
            "type": "",
            "description": "will be run with `example_inputs`. `func` arguments and return"
          },
          "values": {
            "type": "",
            "description": "must be tensors or (possibly nested) tuples that contain"
          },
          "tensors": {
            "type": "",
            "description": ". When a module is passed `torch.jit.trace`, only the\n``forward`` method is run and traced (see :func:`torch.jit.trace\n<torch.jit.trace_module>` for details)."
          },
          "Keyword": {
            "type": "",
            "description": "arguments:"
          },
          "example_inputs": {
            "type": "tuple or torch.Tensor or None, optional",
            "description": "A tuple of example"
          },
          "inputs": {
            "type": "",
            "description": "that will be passed to the function while tracing."
          },
          "Default": {
            "type": "",
            "description": "``None``. Either this argument or ``example_kwarg_inputs``"
          },
          "should": {
            "type": "",
            "description": "be specified. The dict will be unpacking by the arguments name"
          },
          "different": {
            "type": "",
            "description": "types and shapes assuming the traced operations support those"
          },
          "types": {
            "type": "",
            "description": "of inputs you expect the network to see.  If not specified,"
          },
          "case": {
            "type": "",
            "description": "it is automatically wrapped in a tuple. When the value is None,\n``example_kwarg_inputs`` should be specified."
          },
          "check_trace": {
            "type": "``bool``, optional",
            "description": "Check if the same inputs run through"
          },
          "traced": {
            "type": "",
            "description": "code produce the same outputs. Default: ``True``. You might want"
          },
          "to": {
            "type": "",
            "description": "use in the checker procedure.  This can be used to relax the"
          },
          "deterministic": {
            "type": "",
            "description": "ops or if you are sure that the network is correct despite"
          },
          "a": {
            "type": "",
            "description": "set of checking inputs representative of the space of shapes and"
          },
          "check_inputs": {
            "type": "list of tuples, optional",
            "description": "A list of tuples of input"
          },
          "arguments": {
            "type": "",
            "description": "of example inputs that will be passed to the function while"
          },
          "expected": {
            "type": "",
            "description": ". Each tuple is equivalent to a set of input arguments that"
          },
          "would": {
            "type": "",
            "description": "be specified in ``example_inputs``. For best results, pass in"
          },
          "the": {
            "type": "",
            "description": "traced function's arguments name, a runtime exception will be raised."
          },
          "check_tolerance": {
            "type": "float, optional",
            "description": "Floating-point comparison tolerance"
          },
          "checker": {
            "type": "",
            "description": "strictness in the event that results diverge numerically"
          },
          "for": {
            "type": "",
            "description": "a known reason, such as operator fusion."
          },
          "strict": {
            "type": "``bool``, optional",
            "description": "run the tracer in a strict mode or not\n(default: ``True``). Only turn this off when you want the tracer to"
          },
          "record": {
            "type": "",
            "description": "your mutable container types (currently ``list``/``dict``)"
          },
          "and": {
            "type": "",
            "description": "you are sure that the container you are using in your"
          },
          "problem": {
            "type": "",
            "description": "is a ``constant`` structure and does not get used as"
          },
          "control": {
            "type": "",
            "description": "flow (if, for) conditions."
          },
          "example_kwarg_inputs": {
            "type": "dict, optional",
            "description": "This parameter is a pack of keyword"
          },
          "tracing": {
            "type": "",
            "description": ". Default: ``None``. Either this argument or ``example_inputs``"
          },
          "of": {
            "type": "",
            "description": "the traced function. If the keys of the dict don't not match with"
          }
        },
        "returns": "If `func` is `nn.Module` or ``forward`` of `nn.Module`, `trace` returns\n    a :class:`ScriptModule` object with a single ``forward`` method\n    containing the traced code.  The returned `ScriptModule` will\n    have the same set of sub-modules and parameters as the original\n    ``nn.Module``.  If ``func`` is a standalone function, ``trace``\n    returns `ScriptFunction`.\n\nExample (tracing a function):\n\n.. testcode::\n\n    import torch\n\n    def foo(x, y):\n        return 2 * x + y\n\n    # Run `foo` with the provided inputs and record the tensor operations\n    traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n    # `traced_foo` can now be run with the TorchScript interpreter or saved\n    # and loaded in a Python-free environment\n\nExample (tracing an existing module)::\n\n    import torch\n    import torch.nn as nn\n\n    class Net(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv = nn.Conv2d(1, 1, 3)\n\n        def forward(self, x):\n            return self.conv(x)\n\n    n = Net()\n    example_weight = torch.rand(1, 1, 3, 3)\n    example_forward_input = torch.rand(1, 1, 3, 3)\n\n    # Trace a specific method and construct `ScriptModule` with\n    # a single `forward` method\n    module = torch.jit.trace(n.forward, example_forward_input)\n\n    # Trace a module (implicitly traces `forward`) and construct a\n    # `ScriptModule` with a single `forward` method\n    module = torch.jit.trace(n, example_forward_input)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "trace_module",
      "signature": "trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x76e28096f770>, example_inputs_is_kwarg=False, _store_inputs=True)",
      "documentation": {
        "description": "Trace a module and return an executable :class:`ScriptModule` that will be optimized using just-in-time compilation.\n\nWhen a module is passed to :func:`torch.jit.trace <torch.jit.trace>`, only\nthe ``forward`` method is run and traced. With ``trace_module``, you can specify a dictionary of\nmethod names to example inputs to trace (see the ``inputs``) argument below.\n\nSee :func:`torch.jit.trace <torch.jit.trace>` for more information on tracing.",
        "parameters": {
          "mod": {
            "type": "torch.nn.Module",
            "description": "A ``torch.nn.Module`` containing methods whose names are"
          },
          "specified": {
            "type": "",
            "description": "in ``inputs``. The given methods will be compiled"
          },
          "as": {
            "type": "",
            "description": "a part of a single `ScriptModule`."
          },
          "inputs": {
            "type": "dict",
            "description": "A dict containing sample inputs indexed by method names in ``mod``."
          },
          "The": {
            "type": "",
            "description": "inputs will be passed to methods whose names correspond to inputs'"
          },
          "keys": {
            "type": "",
            "description": "while tracing.\n``{ 'forward' : example_forward_input, 'method2': example_method2_input}``"
          },
          "Keyword": {
            "type": "",
            "description": "arguments:"
          },
          "check_trace": {
            "type": "``bool``, optional",
            "description": "Check if the same inputs run through"
          },
          "traced": {
            "type": "",
            "description": "code produce the same outputs. Default: ``True``. You might want"
          },
          "to": {
            "type": "",
            "description": "check the trace against what is expected. Each tuple"
          },
          "deterministic": {
            "type": "",
            "description": "ops or if you are sure that the network is correct despite"
          },
          "a": {
            "type": "",
            "description": "checker failure."
          },
          "check_inputs": {
            "type": "list of dicts, optional",
            "description": "A list of dicts of input arguments that should be used"
          },
          "is": {
            "type": "",
            "description": "equivalent to a set of input arguments that would"
          },
          "be": {
            "type": "",
            "description": "specified in ``inputs``. For best results, pass in a"
          },
          "set": {
            "type": "",
            "description": "of checking inputs representative of the space of"
          },
          "shapes": {
            "type": "",
            "description": "and types of inputs you expect the network to see."
          },
          "If": {
            "type": "",
            "description": "not specified, the original ``inputs`` are used for checking"
          },
          "check_tolerance": {
            "type": "float, optional",
            "description": "Floating-point comparison tolerance to use in the checker procedure."
          },
          "This": {
            "type": "",
            "description": "can be used to relax the checker strictness in the event that"
          },
          "results": {
            "type": "",
            "description": "diverge numerically for a known reason, such as operator fusion."
          },
          "example_inputs_is_kwarg": {
            "type": "``bool``, optional",
            "description": "This parameter indicate whether the example inputs is a pack"
          },
          "pack": {
            "type": "",
            "description": "of keyword arguments. Default: ``False``."
          }
        },
        "returns": "A :class:`ScriptModule` object with a single ``forward`` method containing the traced code.\n    When ``func`` is a ``torch.nn.Module``, the returned :class:`ScriptModule` will have the same set of\n    sub-modules and parameters as ``func``.\n\nExample (tracing a module with multiple methods)::\n\n    import torch\n    import torch.nn as nn\n\n    class Net(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv = nn.Conv2d(1, 1, 3)\n\n        def forward(self, x):\n            return self.conv(x)\n\n        def weighted_kernel_sum(self, weight):\n            return weight * self.conv.weight\n\n\n    n = Net()\n    example_weight = torch.rand(1, 1, 3, 3)\n    example_forward_input = torch.rand(1, 1, 3, 3)\n\n    # Trace a specific method and construct `ScriptModule` with\n    # a single `forward` method\n    module = torch.jit.trace(n.forward, example_forward_input)\n\n    # Trace a module (implicitly traces `forward`) and construct a\n    # `ScriptModule` with a single `forward` method\n    module = torch.jit.trace(n, example_forward_input)\n\n    # Trace specific methods on a module (specified in `inputs`), constructs\n    # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\n    inputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight}\n    module = torch.jit.trace_module(n, inputs)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "unused",
      "signature": "unused(fn)",
      "documentation": {
        "description": "This decorator indicates to the compiler that a function or method should\nbe ignored and replaced with the raising of an exception. This allows you\nto leave code in your model that is not yet TorchScript compatible and still\nexport your model.\n\n    Example (using ``@torch.jit.unused`` on a method)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class MyModule(nn.Module):\n            def __init__(self, use_memory_efficient):\n                super().__init__()\n                self.use_memory_efficient = use_memory_efficient\n\n            @torch.jit.unused\n            def memory_efficient(self, x):\n                import pdb\n\n                pdb.set_trace()\n                return x + 10\n\n            def forward(self, x):\n                # Use not-yet-scriptable memory efficient mode\n                if self.use_memory_efficient:\n                    return self.memory_efficient(x)\n                else:\n                    return x + 10\n\n\n        m = torch.jit.script(MyModule(use_memory_efficient=False))\n        m.save(\"m.pt\")\n\n        m = torch.jit.script(MyModule(use_memory_efficient=True))\n        # exception raised\n        m(torch.rand(100))",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "wait",
      "signature": "wait(future)",
      "documentation": {
        "description": "Force completion of a `torch.jit.Future[T]` asynchronous task, returning the result of the task.\n\nSee :func:`~fork` for docs and examples.",
        "parameters": {
          "future": {
            "type": "torch.jit.Future[T]",
            "description": "an asynchronous task reference, created through `torch.jit.fork`"
          }
        },
        "returns": "`T`: the return value of the completed task",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "Attribute",
      "documentation": {
        "description": "This method is a pass-through function that returns `value`, mostly\nused to indicate to the TorchScript compiler that the left-hand side\nexpression is a class instance attribute with type of `type`. Note that\n`torch.jit.Attribute` should only be used in `__init__` method of `jit.ScriptModule`\nsubclasses.\n\nThough TorchScript can infer correct type for most Python expressions, there are some cases where\ntype inference can be wrong, including:\n\n- Empty containers like `[]` and `{}`, which TorchScript assumes to be container of `Tensor`\n- Optional types like `Optional[T]` but assigned a valid value of type `T`, TorchScript would assume\n  it is type `T` rather than `Optional[T]`\n\nIn eager mode, it is simply a pass-through function that returns `value`\nwithout other implications.\n\nExample:\n\n.. testcode::\n\n    import torch\n    from typing import Dict\n\n    class AttributeModule(torch.jit.ScriptModule):\n        def __init__(self) -> None:\n            super().__init__()\n            self.foo = torch.jit.Attribute(0.1, float)\n\n            # we should be able to use self.foo as a float here\n            assert 0.0 < self.foo\n\n            self.names_ages = torch.jit.Attribute({}, Dict[str, int])\n            self.names_ages[\"someone\"] = 20\n            assert isinstance(self.names_ages[\"someone\"], int)\n\n    m = AttributeModule()\n    # m will contain two attributes\n    # 1. foo of type float\n    # 2. names_ages of type Dict[str, int]\n\n.. testcleanup::\n\n    del AttributeModule\n    del m",
        "parameters": {
          "value": {
            "type": "",
            "description": "An initial value to be assigned to attribute."
          },
          "type": {
            "type": "",
            "description": "A Python type"
          }
        },
        "returns": "Returns `value`",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ".. testcode::\n\n    import torch\n    from typing import Dict\n\n    class AttributeModule(torch.jit.ScriptModule):\n        def __init__(self) -> None:\n            super().__init__()\n            self.foo = torch.jit.Attribute(0.1, float)\n\n            # we should be able to use self.foo as a float here\n            assert 0.0 < self.foo\n\n            self.names_ages = torch.jit.Attribute({}, Dict[str, int])\n            self.names_ages[\"someone\"] = 20\n            assert isinstance(self.names_ages[\"someone\"], int)\n\n    m = AttributeModule()\n    # m will contain two attributes\n    # 1. foo of type float\n    # 2. names_ages of type Dict[str, int]\n\n.. testcleanup::\n\n    del AttributeModule\n    del m"
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.\n\nRaises ValueError if the value is not present.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "CompilationUnit",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "create_function",
          "signature": "create_function(self: torch._C.CompilationUnit, qualified_name: str, graph: torch._C.Graph, should_mangle: bool = False)",
          "documentation": {
            "description": "create_function(self: torch._C.CompilationUnit, qualified_name: str, graph: torch._C.Graph, should_mangle: bool = False) -> torch::jit::StrongFunctionPtr",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "define",
          "signature": "define(self: torch._C.CompilationUnit, src: str, rcb: Callable[[str], object] = None, _frames_up: int = 0)",
          "documentation": {
            "description": "define(self: torch._C.CompilationUnit, src: str, rcb: Callable[[str], object] = None, _frames_up: int = 0) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "drop_all_functions",
          "signature": "drop_all_functions(self: torch._C.CompilationUnit)",
          "documentation": {
            "description": "drop_all_functions(self: torch._C.CompilationUnit) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "find_function",
          "signature": "find_function(self: torch._C.CompilationUnit, arg0: str)",
          "documentation": {
            "description": "find_function(self: torch._C.CompilationUnit, arg0: str) -> Optional[torch::jit::StrongFunctionPtr]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_class",
          "signature": "get_class(self: torch._C.CompilationUnit, arg0: str)",
          "documentation": {
            "description": "get_class(self: torch._C.CompilationUnit, arg0: str) -> torch._C.ClassType",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_functions",
          "signature": "get_functions(self: torch._C.CompilationUnit)",
          "documentation": {
            "description": "get_functions(self: torch._C.CompilationUnit) -> list[torch::jit::StrongFunctionPtr]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_interface",
          "signature": "get_interface(self: torch._C.CompilationUnit, arg0: str)",
          "documentation": {
            "description": "get_interface(self: torch._C.CompilationUnit, arg0: str) -> torch._C.InterfaceType",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_optimized",
          "signature": "set_optimized(self: torch._C.CompilationUnit, arg0: bool)",
          "documentation": {
            "description": "set_optimized(self: torch._C.CompilationUnit, arg0: bool) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Error",
      "documentation": {
        "description": "Common base class for all non-exit exceptions.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Future",
      "documentation": {
        "description": "Wrapper around a ``torch._C.Future`` which encapsulates an asynchronous\nexecution of a callable, e.g. :meth:`~torch.distributed.rpc.rpc_async`. It\nalso exposes a set of APIs to add callback functions and set results.\n\n.. warning:: GPU support is a beta feature, subject to changes.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_done_callback",
          "signature": "add_done_callback(self, callback: 'Callable[[Future[T]], None]') -> 'None'",
          "documentation": {
            "description": "Append the given callback function to this ``Future``, which will be run\nwhen the ``Future`` is completed.  Multiple callbacks can be added to\nthe same ``Future``, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this ``Future``. The callback function can use the\n:meth:`value` method to get the value. Note that if this ``Future`` is\nalready completed, the given callback will be run inline.\n\nWe recommend that you use the :meth:`then` method as it provides a way\nto synchronize after your callback has completed. ``add_done_callback``\ncan be cheaper if your callback does not return anything. But both\n:meth:`then` and ``add_done_callback`` use the same callback\nregistration API under the hood.\n\nWith respect to GPU tensors, this method behaves in the same way as\n:meth:`then`.",
            "parameters": {
              "callback": {
                "type": "``Future``",
                "description": "a ``Callable`` that takes in one argument,"
              },
              "which": {
                "type": "",
                "description": "is the reference to this ``Future``.\n.. note:: Note that if the callback function throws, either"
              },
              "through": {
                "type": "",
                "description": "the original future being completed with an exception and"
              },
              "calling": {
                "type": "",
                "description": "``fut.wait()``, or through other code in the callback,"
              },
              "error": {
                "type": "",
                "description": "handling must be carefully taken care of. For example, if"
              },
              "this": {
                "type": "",
                "description": "callback later completes additional futures, those futures are"
              },
              "not": {
                "type": "",
                "description": "marked as completed with an error and the user is responsible"
              },
              "for": {
                "type": "",
                "description": "handling completion/waiting on those futures independently."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n>>> def callback(fut):\n...     print(\"This will run after the future has finished.\")\n...     print(fut.wait())\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)"
              },
              "This": {
                "type": "",
                "description": "will run after the future has finished."
              },
              "5": {
                "type": "",
                "description": ""
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "done",
          "signature": "done(self) -> 'bool'",
          "documentation": {
            "description": "Return ``True`` if this ``Future`` is done. A ``Future`` is done if it\nhas a result or an exception.\n\nIf the value contains tensors that reside on GPUs, ``Future.done()``\nwill return ``True`` even if the asynchronous kernels that are\npopulating those tensors haven't yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see :meth:`wait`).",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_exception",
          "signature": "set_exception(self, result: 'T') -> 'None'",
          "documentation": {
            "description": "Set an exception for this ``Future``, which will mark this ``Future`` as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this ``Future``, the exception set here\nwill be raised inline.",
            "parameters": {
              "result": {
                "type": "BaseException",
                "description": "the exception for this ``Future``."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()"
              },
              "Traceback": {
                "type": "most recent call last",
                "description": "..."
              },
              "ValueError": {
                "type": "",
                "description": "foo"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_result",
          "signature": "set_result(self, result: 'T') -> 'None'",
          "documentation": {
            "description": "Set the result for this ``Future``, which will mark this ``Future`` as\ncompleted and trigger all attached callbacks. Note that a ``Future``\ncannot be marked completed twice.\n\nIf the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven't yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it's safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn't change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\n``Future``.",
            "parameters": {
              "result": {
                "type": "object",
                "description": "the result object of this ``Future``."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n>>> import threading\n>>> import time\n>>> def slow_set_future(fut, value):\n...     time.sleep(0.5)\n...     fut.set_result(value)\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n...     target=slow_set_future,\n...     args=(fut, torch.ones(2) * 3)\n... )\n>>> t.start()\n>>> print(fut.wait())"
              },
              "tensor": {
                "type": "[3., 3.]",
                "description": ">>> t.join()"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "then",
          "signature": "then(self, callback: 'Callable[[Future[T]], S]') -> 'Future[S]'",
          "documentation": {
            "description": "Append the given callback function to this ``Future``, which will be run\nwhen the ``Future`` is completed.  Multiple callbacks can be added to\nthe same ``Future``, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\n``fut.then(cb1).then(cb2)``). The callback must take one argument, which\nis the reference to this ``Future``. The callback function can use the\n:meth:`value` method to get the value. Note that if this ``Future`` is\nalready completed, the given callback will be run immediately inline.\n\nIf the ``Future``'s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven't yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn't switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of :meth:`wait`.\n\nSimilarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn't change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked.",
            "parameters": {
              "callback": {
                "type": "``Callable``",
                "description": "a ``Callable`` that takes this ``Future`` as"
              },
              "the": {
                "type": "",
                "description": "only argument."
              }
            },
            "returns": "A new ``Future`` object that holds the return value of the\n    ``callback`` and will be marked as completed when the given\n    ``callback`` finishes.\n\n.. note:: Note that if the callback function throws, either\n    through the original future being completed with an exception and\n    calling ``fut.wait()``, or through other code in the callback, the\n    future returned by ``then`` will be marked appropriately with the\n    encountered error. However, if this callback later completes\n    additional futures, those futures are not marked as completed with\n    an error and the user is responsible for handling completion/waiting\n    on those futures independently.\n\nExample::\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n    >>> def callback(fut):\n    ...     print(f\"RPC return value is {fut.wait()}.\")\n    >>> fut = torch.futures.Future()\n    >>> # The inserted callback will print the return value when\n    >>> # receiving the response from \"worker1\"\n    >>> cb_fut = fut.then(callback)\n    >>> chain_cb_fut = cb_fut.then(\n    ...     lambda x : print(f\"Chained cb done. {x.wait()}\")\n    ... )\n    >>> fut.set_result(5)\n    RPC return value is 5.\n    Chained cb done. None",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "value",
          "signature": "value(self) -> 'T'",
          "documentation": {
            "description": "Obtain the value of an already-completed future.\n\nThis method should only be called after a call to :meth:`wait` has\ncompleted, or inside a callback function passed to :meth:`then`. In\nother cases this ``Future`` may not yet hold a value and calling\n``value()`` could fail.\n\nIf the value contains tensors that reside on GPUs, then this method will\n*not* perform any additional synchronization. This should be done\nbeforehand, separately, through a call to :meth:`wait` (except within\ncallbacks, for which it's already being taken care of by :meth:`then`).",
            "parameters": {},
            "returns": "The value held by this ``Future``. If the function (callback or RPC)\n    creating the value has thrown an error, this ``value()`` method will\n    also throw an error.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "wait",
          "signature": "wait(self) -> 'T'",
          "documentation": {
            "description": "Block until the value of this ``Future`` is ready.\n\nIf the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that ``wait()`` will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, ``wait()`` will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn't change streams.",
            "parameters": {},
            "returns": "The value held by this ``Future``. If the function (callback or RPC)\n    creating the value has thrown an error, this ``wait`` method will\n    also throw an error.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ONNXTracedModule",
      "documentation": {
        "description": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, *args: torch.Tensor)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RecursiveScriptClass",
      "documentation": {
        "description": "Wrapper for a TorchScript class instance for use in Python.\n\nAn analogue of RecursiveScriptModule for regular objects that are not modules.\nThis class is a wrapper around a torch._C.ScriptObject that represents an instance\nof a TorchScript class and allows it to be used in Python.\n\nAttributes:\n    _c [torch._C.ScriptObject]: The C++ object to which attribute lookups and method\n        calls are forwarded.\n    _props [Dict[str, property]]: A dictionary of properties fetched from self._c and\n        exposed on this wrppaer.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "forward_magic_method",
          "signature": "forward_magic_method(self, method_name, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RecursiveScriptModule",
      "documentation": {
        "description": "Retain the existing isinstance(ScriptModule) behavior.\n\nThe core data structure in TorchScript is the ``ScriptModule``. It is an\nanalogue of torch's ``nn.Module`` and represents an entire model as a tree of\nsubmodules. Like normal modules, each individual module in a ``ScriptModule`` can\nhave submodules, parameters, and methods. In ``nn.Module``\\s methods are implemented\nas Python functions, but in ``ScriptModule``\\s methods are implemented as\nTorchScript functions, a statically-typed subset of Python that contains all\nof PyTorch's built-in Tensor operations. This difference allows your\n``ScriptModule``\\s code to run without the need for a Python interpreter.\n\n``ScriptModule``\\s should not be created manually, instead use\neither :func:`tracing <torch.jit.trace>` or :func:`scripting <torch.jit.script>`.\nTracing and scripting can be applied incrementally and :ref:`composed as necessary <Types>`.\n\n* Tracing records the tensor operations as executed with a set of example inputs and uses these\n  operations to construct a computation graph. You can use the full dynamic behavior of Python with tracing,\n  but values other than Tensors and control flow aren't captured in the graph.\n\n* Scripting inspects the Python code of the model\n  and compiles it to TorchScript. Scripting allows the use of many `types`_ of values and supports dynamic control flow.\n  Many, but not all features of Python are supported by the compiler, so changes to the source code may be necessary.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "define",
          "signature": "define(self, src)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward_magic_method",
          "signature": "forward_magic_method(self, method_name, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_debug_state",
          "signature": "get_debug_state(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_for",
          "signature": "graph_for(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self, f, **kwargs)",
          "documentation": {
            "description": "Save with a file-like object.\n\nsave(f, _extra_files={})\n\nSee :func:`torch.jit.save <torch.jit.save>` which accepts a file-like object.\nThis function, torch.save(), converts the object to a string, treating it as a path.\nDO NOT confuse these two functions when it comes to the 'f' parameter functionality.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_to_buffer",
          "signature": "save_to_buffer(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "fail(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ScriptFunction",
      "documentation": {
        "description": "Functionally equivalent to a :class:`ScriptModule`, but represents a single\nfunction and does not have any attributes or Parameters.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "get_debug_state",
          "signature": "get_debug_state(self: torch._C.ScriptFunction)",
          "documentation": {
            "description": "get_debug_state(self: torch._C.ScriptFunction) -> torch._C.GraphExecutorState",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_for",
          "signature": "_graph_for(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self: torch._C.ScriptFunction, filename: str, _extra_files: dict[str, str] = {})",
          "documentation": {
            "description": "save(self: torch._C.ScriptFunction, filename: str, _extra_files: dict[str, str] = {}) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_to_buffer",
          "signature": "save_to_buffer(self: torch._C.ScriptFunction, _extra_files: dict[str, str] = {})",
          "documentation": {
            "description": "save_to_buffer(self: torch._C.ScriptFunction, _extra_files: dict[str, str] = {}) -> bytes",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ScriptModule",
      "documentation": {
        "description": "Wrapper for C++ torch::jit::Module with methods, attributes, and parameters.\n\nA wrapper around C++ ``torch::jit::Module``. ``ScriptModule``\\s\ncontain methods, attributes, parameters, and\nconstants. These can be accessed the same way as on a normal ``nn.Module``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "define",
          "signature": "define(self, src)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self) -> str",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward_magic_method",
          "signature": "forward_magic_method(self, method_name, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_debug_state",
          "signature": "get_debug_state(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_for",
          "signature": "graph_for(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self, f, **kwargs)",
          "documentation": {
            "description": "Save with a file-like object.\n\nsave(f, _extra_files={})\n\nSee :func:`torch.jit.save <torch.jit.save>` which accepts a file-like object.\nThis function, torch.save(), converts the object to a string, treating it as a path.\nDO NOT confuse these two functions when it comes to the 'f' parameter functionality.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_to_buffer",
          "signature": "save_to_buffer(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ScriptWarning",
      "documentation": {
        "description": "Base class for warning categories.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TopLevelTracedModule",
      "documentation": {
        "description": "Wrapper for C++ torch::jit::Module with methods, attributes, and parameters.\n\nA wrapper around C++ ``torch::jit::Module``. ``ScriptModule``\\s\ncontain methods, attributes, parameters, and\nconstants. These can be accessed the same way as on a normal ``nn.Module``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "define",
          "signature": "define(self, src)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward_magic_method",
          "signature": "forward_magic_method(self, method_name, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_debug_state",
          "signature": "get_debug_state(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_for",
          "signature": "graph_for(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self, f, **kwargs)",
          "documentation": {
            "description": "Save with a file-like object.\n\nsave(f, _extra_files={})\n\nSee :func:`torch.jit.save <torch.jit.save>` which accepts a file-like object.\nThis function, torch.save(), converts the object to a string, treating it as a path.\nDO NOT confuse these two functions when it comes to the 'f' parameter functionality.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_to_buffer",
          "signature": "save_to_buffer(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TracedModule",
      "documentation": {
        "description": "Wrapper for C++ torch::jit::Module with methods, attributes, and parameters.\n\nA wrapper around C++ ``torch::jit::Module``. ``ScriptModule``\\s\ncontain methods, attributes, parameters, and\nconstants. These can be accessed the same way as on a normal ``nn.Module``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_module",
          "signature": "add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Add a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the child module. The child module can be"
              },
              "accessed": {
                "type": "",
                "description": "from this module using the given name"
              },
              "module": {
                "type": "Module",
                "description": "child module to be added to the module."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "apply",
          "signature": "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
          "documentation": {
            "description": "Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).",
            "parameters": {
              "fn": {
                "type": ":class:`Module` -> None",
                "description": "function to be applied to each submodule"
              }
            },
            "returns": "Module: self\n\nExample::\n\n    >>> @torch.no_grad()\n    >>> def init_weights(m):\n    >>>     print(m)\n    >>>     if type(m) == nn.Linear:\n    >>>         m.weight.fill_(1.0)\n    >>>         print(m.weight)\n    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n    >>> net.apply(init_weights)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    Parameter containing:\n    tensor([[1., 1.],\n            [1., 1.]], requires_grad=True)\n    Sequential(\n      (0): Linear(in_features=2, out_features=2, bias=True)\n      (1): Linear(in_features=2, out_features=2, bias=True)\n    )",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bfloat16",
          "signature": "bfloat16(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "buffers",
          "signature": "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]",
          "documentation": {
            "description": "Return an iterator over module buffers.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "torch": {
                "type": "",
                "description": ".Tensor: module buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "children",
          "signature": "children(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compile",
          "signature": "compile(self, *args, **kwargs)",
          "documentation": {
            "description": "Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compiled and all arguments are passed as-is\nto :func:`torch.compile`.\n\nSee :func:`torch.compile` for details on the arguments for this function.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu",
          "signature": "cpu(self: ~T) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda",
          "signature": "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "define",
          "signature": "define(self, src)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "double",
          "signature": "double(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "eval",
          "signature": "eval(self: ~T) -> ~T",
          "documentation": {
            "description": "Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "extra_repr",
          "signature": "extra_repr(self)",
          "documentation": {
            "description": "Return the extra representation of the module.\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "float",
          "signature": "float(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, *args, **kwargs)",
          "documentation": {
            "description": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward_magic_method",
          "signature": "forward_magic_method(self, method_name, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_buffer",
          "signature": "get_buffer(self, target: str) -> 'Tensor'",
          "documentation": {
            "description": "Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the buffer"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.Tensor: The buffer referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not a\n        buffer",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_debug_state",
          "signature": "get_debug_state(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_extra_state",
          "signature": "get_extra_state(self) -> Any",
          "documentation": {
            "description": "Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.",
            "parameters": {},
            "returns": "object: Any extra state to store in the module's state_dict",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_parameter",
          "signature": "get_parameter(self, target: str) -> 'Parameter'",
          "documentation": {
            "description": "Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the Parameter"
              },
              "to": {
                "type": "",
                "description": "look for. (See ``get_submodule`` for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Parameter: The Parameter referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Parameter``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submodule",
          "signature": "get_submodule(self, target: str) -> 'Module'",
          "documentation": {
            "description": "Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              }
            },
            "returns": "torch.nn.Module: The submodule referenced by ``target``",
            "raises": "AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "graph_for",
          "signature": "graph_for(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "half",
          "signature": "half(self: ~T) -> ~T",
          "documentation": {
            "description": "Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {},
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ipu",
          "signature": "ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "load_state_dict",
          "signature": "load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
          "documentation": {
            "description": "Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\n.. warning::\n    If :attr:`assign` is ``True`` the optimizer must be created after\n    the call to :attr:`load_state_dict` unless\n    :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.",
            "parameters": {
              "state_dict": {
                "type": "dict",
                "description": "a dict containing parameters and"
              },
              "persistent": {
                "type": "",
                "description": "buffers."
              },
              "strict": {
                "type": "bool, optional",
                "description": "whether to strictly enforce that the keys"
              },
              "in": {
                "type": "",
                "description": "the current module are preserved whereas setting it to ``True`` preserves"
              },
              "assign": {
                "type": "bool, optional",
                "description": "When set to ``False``, the properties of the tensors"
              },
              "properties": {
                "type": "",
                "description": "of the Tensors in the state dict. The only"
              },
              "exception": {
                "type": "",
                "description": "is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s"
              },
              "for": {
                "type": "",
                "description": "which the value from the module is preserved."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              }
            },
            "returns": "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n        * **missing_keys** is a list of str containing any keys that are expected\n            by this module but missing from the provided ``state_dict``.\n        * **unexpected_keys** is a list of str containing the keys that are not\n            expected by this module but present in the provided ``state_dict``.",
            "raises": "",
            "see_also": "",
            "notes": "If a parameter or buffer is registered as ``None`` and its corresponding key\n    exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n    ``RuntimeError``.",
            "examples": ""
          }
        },
        {
          "name": "modules",
          "signature": "modules(self) -> Iterator[ForwardRef('Module')]",
          "documentation": {
            "description": "Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "mtia",
          "signature": "mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing the optimizer if the module will\nlive on MTIA while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_buffers",
          "signature": "named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
          "documentation": {
            "description": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all buffer names."
              },
              "recurse": {
                "type": "bool, optional",
                "description": "if True, then yields buffers of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only buffers that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module. Defaults to True."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated buffers in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, torch.Tensor): Tuple containing the name and buffer"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_children",
          "signature": "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]",
          "documentation": {
            "description": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\nYields:\n    (str, Module): Tuple containing a name and child module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for name, module in model.named_children():\n    >>>     if name in ['conv4', 'conv5']:\n    >>>         print(module)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "named_modules",
          "signature": "named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
          "documentation": {
            "description": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.",
            "parameters": {
              "memo": {
                "type": "",
                "description": "a memo to store the set of modules already added to the result"
              },
              "prefix": {
                "type": "",
                "description": "a prefix that will be added to the name of the module"
              },
              "remove_duplicate": {
                "type": "",
                "description": "whether to remove the duplicated module instances in the result"
              },
              "or": {
                "type": "",
                "description": "not"
              },
              "Yields": {
                "type": "",
                "description": "(str, Module): Tuple of name and module"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "Duplicate modules are returned only once. In the following\n    example, ``l`` will be returned only once.",
            "examples": ""
          }
        },
        {
          "name": "named_parameters",
          "signature": "named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
          "documentation": {
            "description": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.",
            "parameters": {
              "prefix": {
                "type": "str",
                "description": "prefix to prepend to all parameter names."
              },
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "remove_duplicate": {
                "type": "bool, optional",
                "description": "whether to remove the duplicated"
              },
              "parameters": {
                "type": "",
                "description": "in the result. Defaults to True."
              },
              "Yields": {
                "type": "",
                "description": "(str, Parameter): Tuple containing the name and parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "parameters",
          "signature": "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
          "documentation": {
            "description": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.",
            "parameters": {
              "recurse": {
                "type": "bool",
                "description": "if True, then yields parameters of this module"
              },
              "and": {
                "type": "",
                "description": "all submodules. Otherwise, yields only parameters that"
              },
              "are": {
                "type": "",
                "description": "direct members of this module."
              },
              "Yields": {
                "type": "",
                "description": ""
              },
              "Parameter": {
                "type": "",
                "description": "module parameter"
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_backward_hook",
          "signature": "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_buffer",
          "signature": "register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
          "documentation": {
            "description": "Add a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the buffer. The buffer can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "tensor": {
                "type": "Tensor or None",
                "description": "buffer to be registered. If ``None``, then operations"
              },
              "that": {
                "type": "",
                "description": "run on buffers, such as :attr:`cuda`, are ignored. If ``None``,"
              },
              "the": {
                "type": "",
                "description": "buffer is **not** included in the module's :attr:`state_dict`."
              },
              "persistent": {
                "type": "bool",
                "description": "whether the buffer is part of this module's\n:attr:`state_dict`."
              },
              "Example": {
                "type": "",
                "description": ":\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_hook",
          "signature": "register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\n    hook(module, args, output) -> None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\n    hook(module, args, kwargs, output) -> None or modified output",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If ``True``, the provided ``hook`` will be fired"
              },
              "before": {
                "type": "",
                "description": "all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on"
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks"
              },
              "registered": {
                "type": "",
                "description": "by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If ``True``, the ``hook`` will be passed the"
              },
              "kwargs": {
                "type": "",
                "description": "given to the forward function."
              },
              "always_call": {
                "type": "bool",
                "description": "If ``True`` the ``hook`` will be run regardless of"
              },
              "whether": {
                "type": "",
                "description": "an exception is raised while calling the Module."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_forward_pre_hook",
          "signature": "register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\n    hook(module, args) -> None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\n    hook(module, args, kwargs) -> None or a tuple of modified input and kwargs",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks"
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all"
              },
              "hooks": {
                "type": "",
                "description": "registered by this method."
              },
              "Default": {
                "type": "",
                "description": "``False``"
              },
              "with_kwargs": {
                "type": "bool",
                "description": "If true, the ``hook`` will be passed the kwargs"
              },
              "given": {
                "type": "",
                "description": "to the forward function."
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_hook",
          "signature": "register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\n    hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs or outputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "this": {
                "type": "",
                "description": "class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_full_backward_pre_hook",
          "signature": "register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
          "documentation": {
            "description": "Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\n    hook(module, grad_output) -> tuple[Tensor] or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\n    Modifying inputs inplace is not allowed when using backward hooks and\n    will raise an error.",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "The user-defined hook to be registered."
              },
              "prepend": {
                "type": "bool",
                "description": "If true, the provided ``hook`` will be fired before"
              },
              "all": {
                "type": "",
                "description": "hooks registered by this method."
              },
              "on": {
                "type": "",
                "description": "this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before"
              }
            },
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_post_hook",
          "signature": "register_load_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.",
            "parameters": {},
            "returns": ":class:`torch.utils.hooks.RemovableHandle`:\n        a handle that can be used to remove the added hook by calling\n        ``handle.remove()``",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_load_state_dict_pre_hook",
          "signature": "register_load_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950",
            "parameters": {
              "hook": {
                "type": "Callable",
                "description": "Callable hook that will be invoked before"
              },
              "loading": {
                "type": "",
                "description": "the state dict."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_module",
          "signature": "register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None",
          "documentation": {
            "description": "Alias for :func:`add_module`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_parameter",
          "signature": "register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
          "documentation": {
            "description": "Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.",
            "parameters": {
              "name": {
                "type": "str",
                "description": "name of the parameter. The parameter can be accessed"
              },
              "from": {
                "type": "",
                "description": "this module using the given name"
              },
              "param": {
                "type": "Parameter or None",
                "description": "parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,"
              },
              "are": {
                "type": "",
                "description": "ignored. If ``None``, the parameter is **not** included in the"
              },
              "module": {
                "type": "",
                "description": "'s :attr:`state_dict`."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_post_hook",
          "signature": "register_state_dict_post_hook(self, hook)",
          "documentation": {
            "description": "Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, state_dict, prefix, local_metadata) -> None\n\nThe registered hooks can modify the ``state_dict`` inplace.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_state_dict_pre_hook",
          "signature": "register_state_dict_pre_hook(self, hook)",
          "documentation": {
            "description": "Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the following signature::\n    hook(module, prefix, keep_vars) -> None\n\nThe registered hooks can be used to perform pre-processing before the ``state_dict``\ncall is made.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "requires_grad_",
          "signature": "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T",
          "documentation": {
            "description": "Change if autograd should record operations on parameters in this module.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.",
            "parameters": {
              "requires_grad": {
                "type": "bool",
                "description": "whether autograd should record operations on"
              },
              "parameters": {
                "type": "",
                "description": "in this module. Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save",
          "signature": "save(self, f, **kwargs)",
          "documentation": {
            "description": "Save with a file-like object.\n\nsave(f, _extra_files={})\n\nSee :func:`torch.jit.save <torch.jit.save>` which accepts a file-like object.\nThis function, torch.save(), converts the object to a string, treating it as a path.\nDO NOT confuse these two functions when it comes to the 'f' parameter functionality.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_to_buffer",
          "signature": "save_to_buffer(self, *args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_extra_state",
          "signature": "set_extra_state(self, state: Any) -> None",
          "documentation": {
            "description": "Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.",
            "parameters": {
              "state": {
                "type": "dict",
                "description": "Extra state from the `state_dict`"
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_submodule",
          "signature": "set_submodule(self, target: str, module: 'Module') -> None",
          "documentation": {
            "description": "Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\n    A(\n        (net_b): Module(\n            (net_c): Module(\n                (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n            )\n            (linear): Linear(in_features=100, out_features=200, bias=True)\n        )\n    )\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo overide the ``Conv2d`` with a new submodule ``Linear``, you\nwould call\n``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.",
            "parameters": {
              "target": {
                "type": "",
                "description": "The fully-qualified string name of the submodule"
              },
              "to": {
                "type": "",
                "description": "look for. (See above example for how to specify a"
              },
              "fully": {
                "type": "",
                "description": "-qualified string.)"
              },
              "module": {
                "type": "",
                "description": "The module to set the submodule to."
              }
            },
            "returns": "",
            "raises": "ValueError: If the target string is empty\n    AttributeError: If the target string references an invalid\n        path or resolves to something that is not an\n        ``nn.Module``",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "share_memory",
          "signature": "share_memory(self: ~T) -> ~T",
          "documentation": {
            "description": "See :meth:`torch.Tensor.share_memory_`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "state_dict",
          "signature": "state_dict(self, *args, destination=None, prefix='', keep_vars=False)",
          "documentation": {
            "description": "Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\n    The returned object is a shallow copy. It contains references\n    to the module's parameters and buffers.\n\n.. warning::\n    Currently ``state_dict()`` also accepts positional arguments for\n    ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n    this is being deprecated and keyword arguments will be enforced in\n    future releases.\n\n.. warning::\n    Please avoid the use of argument ``destination`` as it is not\n    designed for end-users.",
            "parameters": {
              "destination": {
                "type": "dict, optional",
                "description": "If provided, the state of module will"
              },
              "be": {
                "type": "",
                "description": "updated into the dict and the same object is returned."
              },
              "Otherwise": {
                "type": "",
                "description": ", an ``OrderedDict`` will be created and returned."
              },
              "Default": {
                "type": "",
                "description": "``False``."
              },
              "prefix": {
                "type": "str, optional",
                "description": "a prefix added to parameter and buffer"
              },
              "names": {
                "type": "",
                "description": "to compose the keys in state_dict. Default: ``''``."
              },
              "keep_vars": {
                "type": "bool, optional",
                "description": "by default the :class:`~torch.Tensor` s"
              },
              "returned": {
                "type": "",
                "description": "in the state dict are detached from autograd. If it's"
              },
              "set": {
                "type": "",
                "description": "to ``True``, detaching will not be performed."
              }
            },
            "returns": "dict:\n        a dictionary containing a whole state of the module\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> module.state_dict().keys()\n    ['bias', 'weight']",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to",
          "signature": "to(self, *args, **kwargs)",
          "documentation": {
            "description": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "the desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module"
              },
              "dtype": {
                "type": "",
                "description": "and device for all parameters and buffers in this module"
              },
              "the": {
                "type": "",
                "description": "parameters and buffers in this module"
              },
              "tensor": {
                "type": "torch.Tensor",
                "description": "Tensor whose dtype and device are the desired"
              },
              "memory_format": {
                "type": ":class:`torch.memory_format`",
                "description": "the desired memory"
              },
              "format": {
                "type": "",
                "description": "for 4D parameters and buffers in this module (keyword"
              },
              "only": {
                "type": "",
                "description": "argument)"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_empty",
          "signature": "to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
          "documentation": {
            "description": "Move the parameters and buffers to the specified device without copying storage.",
            "parameters": {
              "device": {
                "type": ":class:`torch.device`",
                "description": "The desired device of the parameters"
              },
              "and": {
                "type": "",
                "description": "buffers in this module."
              },
              "recurse": {
                "type": "bool",
                "description": "Whether parameters and buffers of submodules should"
              },
              "be": {
                "type": "",
                "description": "recursively moved to the specified device."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "train",
          "signature": "train(self: ~T, mode: bool = True) -> ~T",
          "documentation": {
            "description": "Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation of\nparticular modules for details of their behaviors in training/evaluation\nmode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.",
            "parameters": {
              "mode": {
                "type": "``False``",
                "description": ". Default: ``True``."
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "type",
          "signature": "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T",
          "documentation": {
            "description": "Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "dst_type": {
                "type": "type or string",
                "description": "the desired type"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "xpu",
          "signature": "xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T",
          "documentation": {
            "description": "Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\n    This method modifies the module in-place.",
            "parameters": {
              "device": {
                "type": "int, optional",
                "description": "if specified, all parameters will be"
              },
              "copied": {
                "type": "",
                "description": "to that device"
              }
            },
            "returns": "Module: self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "zero_grad",
          "signature": "zero_grad(self, set_to_none: bool = True) -> None",
          "documentation": {
            "description": "Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` for more context.",
            "parameters": {
              "set_to_none": {
                "type": "bool",
                "description": "instead of setting to zero, set the grads to None."
              },
              "See": {
                "type": "",
                "description": "meth:`torch.optim.Optimizer.zero_grad` for details."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TracerWarning",
      "documentation": {
        "description": "Base class for warning categories.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ignore_lib_warnings",
          "signature": "ignore_lib_warnings()",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TracingCheckError",
      "documentation": {
        "description": "Common base class for all non-exit exceptions.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "strict_fusion",
      "documentation": {
        "description": "Give errors if not all nodes have been fused in inference, or symbolically differentiated in training.\n\nExample:\nForcing fusion of additions.\n\n.. code-block:: python\n\n    @torch.jit.script\n    def foo(x):\n        with torch.jit.strict_fusion():\n            return x + x + x",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Forcing fusion of additions.\n\n.. code-block:: python\n\n    @torch.jit.script\n    def foo(x):\n        with torch.jit.strict_fusion():\n            return x + x + x"
      },
      "methods": []
    }
  ]
}