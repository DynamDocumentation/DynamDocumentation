{
  "description": "``torch.autograd`` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\n\nIt requires minimal changes to the existing code - you only need to declare :class:`Tensor` s\nfor which gradients should be computed with the ``requires_grad=True`` keyword.\nAs of now, we only support autograd for floating point :class:`Tensor` types (\nhalf, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).",
  "functions": [
    {
      "name": "List",
      "signature": "List(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of list.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Optional",
      "signature": "Optional(*args, **kwds)",
      "documentation": {
        "description": "Optional[X] is equivalent to Union[X, None].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Sequence",
      "signature": "Sequence(*args, **kwargs)",
      "documentation": {
        "description": "A generic version of collections.abc.Sequence.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Tuple",
      "signature": "Tuple(*args, **kwargs)",
      "documentation": {
        "description": "Deprecated alias to builtins.tuple.\n\nTuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tuple[T1, T2] is a tuple of two elements corresponding\nto type variables T1 and T2.  Tuple[int, float, str] is a tuple\nof an int, a float and a string.\n\nTo specify a variable-length tuple of homogeneous type, use Tuple[T, ...].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Union",
      "signature": "Union(*args, **kwds)",
      "documentation": {
        "description": "Union type; Union[X, Y] means either X or Y.\n\nOn Python 3.10 and higher, the | operator\ncan also be used to denote unions;\nX | Y means the same thing to the type checker as Union[X, Y].\n\nTo define a union, use e.g. Union[int, str]. Details:\n- The arguments must be types and there must be at least one.\n- None as an argument is a special case and is replaced by\n  type(None).\n- Unions of unions are flattened, e.g.::\n\n    assert Union[Union[int, str], float] == Union[int, str, float]\n\n- Unions of a single argument vanish, e.g.::\n\n    assert Union[int] == int  # The constructor actually returns int\n\n- Redundant arguments are skipped, e.g.::\n\n    assert Union[int, str, int] == Union[int, str]\n\n- When comparing unions, the argument order is ignored, e.g.::\n\n    assert Union[int, str] == Union[str, int]\n\n- You cannot subclass or instantiate a union.\n- You can use Optional[X] as a shorthand for Union[X, None].",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "backward",
      "signature": "backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, inputs: Union[torch.Tensor, Sequence[torch.Tensor], ForwardRef('GradientEdge'), Sequence[ForwardRef('GradientEdge')], NoneType] = None) -> None",
      "documentation": {
        "description": "Compute the sum of gradients of given tensors with respect to graph leaves.\n\nThe graph is differentiated using the chain rule. If any of ``tensors``\nare non-scalar (i.e. their data has more than one element) and require\ngradient, then the Jacobian-vector product would be computed, in this\ncase the function additionally requires specifying ``grad_tensors``.\nIt should be a sequence of matching length, that contains the \"vector\"\nin the Jacobian-vector product, usually the gradient of the differentiated\nfunction w.r.t. corresponding tensors (``None`` is an acceptable value for\nall tensors that don't need gradient tensors).\n\nThis function accumulates gradients in the leaves - you might need to zero\n``.grad`` attributes or set them to ``None`` before calling it.\nSee :ref:`Default gradient layouts<default-grad-layouts>`\nfor details on the memory layout of accumulated gradients.\n\n.. note::\n    Using this method with ``create_graph=True`` will create a reference cycle\n    between the parameter and its gradient which can cause a memory leak.\n    We recommend using ``autograd.grad`` when creating the graph to avoid this.\n    If you have to use this function, make sure to reset the ``.grad`` fields of your\n    parameters to ``None`` after use to break the cycle and avoid the leak.\n\n.. note::\n\n    If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\n    in a user-specified CUDA stream context, see\n    :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n.. note::\n\n    When ``inputs`` are provided and a given input is not a leaf,\n    the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\n    It is an implementation detail on which the user should not rely.\n    See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.",
        "parameters": {
          "tensors": {
            "type": "Sequence[Tensor] or Tensor",
            "description": "Tensors of which the derivative will be"
          },
          "computed": {
            "type": "",
            "description": "."
          },
          "grad_tensors": {
            "type": "",
            "description": ", then this argument is optional."
          },
          "the": {
            "type": "",
            "description": "Jacobian-vector product, usually gradients w.r.t. each element of"
          },
          "corresponding": {
            "type": "",
            "description": "tensors. None values can be specified for scalar Tensors or"
          },
          "ones": {
            "type": "",
            "description": "that don't require grad. If a None value would be acceptable for all"
          },
          "retain_graph": {
            "type": "bool, optional",
            "description": "If ``False``, the graph used to compute the grad"
          },
          "will": {
            "type": "",
            "description": "be freed. Note that in nearly all cases setting this option to ``True``"
          },
          "is": {
            "type": "",
            "description": "not needed and often can be worked around in a much more efficient"
          },
          "way": {
            "type": "",
            "description": ". Defaults to the value of ``create_graph``."
          },
          "create_graph": {
            "type": "bool, optional",
            "description": "If ``True``, graph of the derivative will"
          },
          "be": {
            "type": "",
            "description": "will accumulated into ``.grad``. All other Tensors will be ignored. If"
          },
          "Defaults": {
            "type": "",
            "description": "to ``False``."
          },
          "inputs": {
            "type": "Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional",
            "description": "Inputs w.r.t. which the gradient"
          },
          "not": {
            "type": "",
            "description": "provided, the gradient is accumulated into all the leaf Tensors that"
          },
          "were": {
            "type": "",
            "description": "used to compute the :attr:`tensors`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cast",
      "signature": "cast(typ, val)",
      "documentation": {
        "description": "Cast a value to a type.\n\nThis returns the value unchanged.  To the type checker this\nsignals that the return value has the designated type, but at\nruntime we intentionally don't check anything (we want this\nto be as fast as possible).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "grad",
      "signature": "grad(outputs: Union[torch.Tensor, Sequence[torch.Tensor], ForwardRef('GradientEdge'), Sequence[ForwardRef('GradientEdge')]], inputs: Union[torch.Tensor, Sequence[torch.Tensor], ForwardRef('GradientEdge'), Sequence[ForwardRef('GradientEdge')]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: Optional[bool] = None, is_grads_batched: bool = False, materialize_grads: bool = False) -> Tuple[torch.Tensor, ...]",
      "documentation": {
        "description": "Compute and return the sum of gradients of outputs with respect to the inputs.\n\n``grad_outputs`` should be a sequence of length matching ``output``\ncontaining the \"vector\" in vector-Jacobian product, usually the pre-computed\ngradients w.r.t. each of the outputs. If an output doesn't require_grad,\nthen the gradient can be ``None``).\n\n.. note::\n\n    If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\n    in a user-specified CUDA stream context, see\n    :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n.. note::\n\n    ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).\n    To accumulate gradient for other parts of the graph, please use\n    ``torch.autograd.backward``.",
        "parameters": {
          "outputs": {
            "type": "sequence of Tensor or GradientEdge",
            "description": "outputs of the differentiated function."
          },
          "inputs": {
            "type": "sequence of Tensor or GradientEdge",
            "description": "Inputs w.r.t. which the gradient will be"
          },
          "returned": {
            "type": "and not accumulated into ``.grad``",
            "description": "."
          },
          "grad_outputs": {
            "type": "sequence of Tensor",
            "description": "The \"vector\" in the vector-Jacobian product."
          },
          "Usually": {
            "type": "",
            "description": "gradients w.r.t. each output. None values can be specified for scalar"
          },
          "Tensors": {
            "type": "",
            "description": "or ones that don't require grad. If a None value would be acceptable"
          },
          "for": {
            "type": "",
            "description": "your use case. Defaults to ``False``."
          },
          "retain_graph": {
            "type": "bool, optional",
            "description": "If ``False``, the graph used to compute the grad"
          },
          "will": {
            "type": "",
            "description": "be raised. Defaults to ``False``."
          },
          "is": {
            "type": "",
            "description": "not needed and often can be worked around in a much more efficient"
          },
          "way": {
            "type": "",
            "description": ". Defaults to the value of ``create_graph``."
          },
          "create_graph": {
            "type": "bool, optional",
            "description": "If ``True``, graph of the derivative will"
          },
          "be": {
            "type": "",
            "description": "constructed, allowing to compute higher order derivative products."
          },
          "Default": {
            "type": "",
            "description": "``False``."
          },
          "allow_unused": {
            "type": "Optional[bool], optional",
            "description": "If ``False``, specifying inputs"
          },
          "that": {
            "type": "",
            "description": "were not used when computing outputs (and therefore their grad is"
          },
          "always": {
            "type": "",
            "description": "zero) is an error. Defaults to the value of ``materialize_grads``."
          },
          "is_grads_batched": {
            "type": "bool, optional",
            "description": "If ``True``, the first dimension of each"
          },
          "tensor": {
            "type": "",
            "description": "in ``grad_outputs`` will be interpreted as the batch dimension."
          },
          "Instead": {
            "type": "",
            "description": "of computing a single vector-Jacobian product, we compute a"
          },
          "batch": {
            "type": "",
            "description": "of vector-Jacobian products for each \"vector\" in the batch."
          },
          "We": {
            "type": "",
            "description": "use the vmap prototype feature as the backend to vectorize calls"
          },
          "to": {
            "type": "",
            "description": "zero instead of None. This is useful when computing higher-order derivatives."
          },
          "single": {
            "type": "",
            "description": "call. This should lead to performance improvements when compared"
          },
          "due": {
            "type": "",
            "description": "to this feature being experimental, there may be performance"
          },
          "cliffs": {
            "type": "",
            "description": ". Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``"
          },
          "materialize_grads": {
            "type": "bool, optional",
            "description": "If ``True``, set the gradient for unused inputs"
          },
          "If": {
            "type": "",
            "description": "``materialize_grads`` is ``True`` and ``allow_unused`` is ``False``, an error"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gradcheck",
      "signature": "gradcheck(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_batched_forward_grad: bool = False, check_forward_ad: bool = False, check_backward_ad: bool = True, fast_mode: bool = False, masked: Optional[bool] = None) -> bool",
      "documentation": {
        "description": "Check gradients computed via small finite differences against analytical\ngradients wrt tensors in :attr:`inputs` that are of floating point or complex type\nand with ``requires_grad=True``.\n\nThe check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\nFor most of the complex functions we consider for optimization purposes, no notion of\nJacobian exists. Instead, gradcheck verifies if the numerical and analytical values of\nthe Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient\ncomputation is done under the assumption that the overall function has a real-valued\noutput, we treat functions with complex output in a special way. For these functions,\ngradcheck is applied to two real-valued functions corresponding to taking the real\ncomponents of the complex outputs for the first, and taking the imaginary components\nof the complex outputs for the second. For more details, check out\n:ref:`complex_autograd-doc`.\n\n.. note::\n    The default values are designed for :attr:`input` of double precision.\n    This check will likely fail if :attr:`input` is of less precision, e.g.,\n    ``FloatTensor``.\n\n.. note::\n    Gradcheck may fail when evaluated on non-differentiable points\n    because the numerically computed gradients via finite differencing may differ\n    those computed analytically (not necessarily because either is incorrect).\n    For more context, see :ref:`non-differentiable-func-grad`.\n\n.. warning::\n   If any checked tensor in :attr:`input` has overlapping memory, i.e.,\n   different indices pointing to the same memory address (e.g., from\n   :func:`torch.Tensor.expand`), this check will likely fail because the numerical\n   gradients computed by point perturbation at such indices will change\n   values at all other indices that share the same memory address.",
        "parameters": {
          "func": {
            "type": "function",
            "description": "a Python function that takes Tensor inputs and returns"
          },
          "a": {
            "type": "",
            "description": "faster implementation of gradcheck that no longer computes the entire jacobian"
          },
          "inputs": {
            "type": "tuple of Tensor or Tensor",
            "description": "inputs to the function"
          },
          "eps": {
            "type": "float, optional",
            "description": "perturbation for finite differences"
          },
          "atol": {
            "type": "float, optional",
            "description": "absolute tolerance"
          },
          "rtol": {
            "type": "float, optional",
            "description": "relative tolerance"
          },
          "raise_exception": {
            "type": "bool, optional",
            "description": "indicating whether to raise an exception if"
          },
          "the": {
            "type": "",
            "description": "check fails. The exception gives more information about the"
          },
          "exact": {
            "type": "",
            "description": "nature of the failure. This is helpful when debugging gradchecks."
          },
          "nondet_tol": {
            "type": "float, optional",
            "description": "tolerance for non-determinism. When running"
          },
          "identical": {
            "type": "",
            "description": "inputs through the differentiation, the results must either match"
          },
          "exactly": {
            "type": "default, 0.0",
            "description": "or be within this tolerance."
          },
          "check_undefined_grad": {
            "type": "bool, optional",
            "description": "if ``True``, check if undefined output grads"
          },
          "are": {
            "type": "",
            "description": "supported and treated as zeros, for ``Tensor`` outputs."
          },
          "check_batched_grad": {
            "type": "bool, optional",
            "description": "if ``True``, check if we can compute"
          },
          "batched": {
            "type": "",
            "description": "forward gradients using forward ad and prototype vmap support. Defaults to ``False``."
          },
          "check_batched_forward_grad": {
            "type": "bool, optional",
            "description": "if ``True``, checks if we can compute"
          },
          "check_forward_ad": {
            "type": "bool, optional",
            "description": "if ``True``, check that the gradients computed with forward"
          },
          "mode": {
            "type": "",
            "description": "AD match the numerical ones. Defaults to ``False``."
          },
          "check_backward_ad": {
            "type": "bool, optional",
            "description": "if ``False``, do not perform any checks that rely on"
          },
          "backward": {
            "type": "",
            "description": "mode AD to be implemented. Defaults to ``True``."
          },
          "fast_mode": {
            "type": "bool, optional",
            "description": "Fast mode for gradcheck and gradgradcheck is currently only"
          },
          "implemented": {
            "type": "",
            "description": "for R to R functions. If none of the inputs and outputs are complex"
          },
          "is": {
            "type": "",
            "description": "run; otherwise, we fall back to the slow implementation."
          },
          "masked": {
            "type": "bool, optional",
            "description": "if ``True``, the gradients of unspecified elements of"
          },
          "sparse": {
            "type": "",
            "description": "tensors are ignored. Defaults to ``False``."
          }
        },
        "returns": "``True`` if all differences satisfy allclose condition",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gradgradcheck",
      "signature": "gradgradcheck(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, gen_non_contig_grad_outputs: bool = False, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_fwd_over_rev: bool = False, check_rev_over_rev: bool = True, fast_mode: bool = False, masked: bool = False) -> bool",
      "documentation": {
        "description": "Check gradients of gradients computed via small finite differences\nagainst analytical gradients wrt tensors in :attr:`inputs` and\n:attr:`grad_outputs` that are of floating point or complex type and with\n``requires_grad=True``.\n\nThis function checks that backpropagating through the gradients computed\nto the given :attr:`grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n.. note::\n    The default values are designed for :attr:`input` and\n    :attr:`grad_outputs` of double precision. This check will likely fail if\n    they are of less precision, e.g., ``FloatTensor``.\n\n.. warning::\n   If any checked tensor in :attr:`input` and :attr:`grad_outputs` has\n   overlapping memory, i.e., different indices pointing to the same memory\n   address (e.g., from :func:`torch.Tensor.expand`), this check will likely fail\n   because the numerical gradients computed by point perturbation at such\n   indices will change values at all other indices that share the same\n   memory address.",
        "parameters": {
          "func": {
            "type": "function",
            "description": "a Python function that takes Tensor inputs and returns"
          },
          "a": {
            "type": "",
            "description": "Tensor or a tuple of Tensors"
          },
          "inputs": {
            "type": "tuple of Tensor or Tensor",
            "description": "inputs to the function"
          },
          "grad_outputs": {
            "type": "tuple of Tensor or Tensor, optional",
            "description": "The gradients with"
          },
          "respect": {
            "type": "",
            "description": "to the function's outputs."
          },
          "eps": {
            "type": "float, optional",
            "description": "perturbation for finite differences"
          },
          "atol": {
            "type": "float, optional",
            "description": "absolute tolerance"
          },
          "rtol": {
            "type": "float, optional",
            "description": "relative tolerance"
          },
          "gen_non_contig_grad_outputs": {
            "type": "bool, optional",
            "description": "if :attr:`grad_outputs` is\n``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the"
          },
          "randomly": {
            "type": "",
            "description": "generated gradient outputs are made to be noncontiguous"
          },
          "raise_exception": {
            "type": "bool, optional",
            "description": "indicating whether to raise an exception if"
          },
          "the": {
            "type": "",
            "description": "second derivative."
          },
          "exact": {
            "type": "",
            "description": "nature of the failure. This is helpful when debugging gradchecks."
          },
          "nondet_tol": {
            "type": "float, optional",
            "description": "tolerance for non-determinism. When running"
          },
          "identical": {
            "type": "",
            "description": "inputs through the differentiation, the results must either match"
          },
          "exactly": {
            "type": "default, 0.0",
            "description": "or be within this tolerance. Note that a small amount"
          },
          "of": {
            "type": "",
            "description": "nondeterminism in the gradient will lead to larger inaccuracies in"
          },
          "check_undefined_grad": {
            "type": "bool, optional",
            "description": "if True, check if undefined output grads"
          },
          "are": {
            "type": "",
            "description": "supported and treated as zeros"
          },
          "check_batched_grad": {
            "type": "bool, optional",
            "description": "if True, check if we can compute"
          },
          "batched": {
            "type": "",
            "description": "gradients using prototype vmap support. Defaults to False."
          },
          "fast_mode": {
            "type": "bool, optional",
            "description": "if True, run a faster implementation of gradgradcheck that"
          },
          "no": {
            "type": "",
            "description": "longer computes the entire jacobian."
          },
          "masked": {
            "type": "bool, optional",
            "description": "if True, the gradients of unspecified elements of"
          },
          "sparse": {
            "type": "",
            "description": "tensors are ignored (default, False)."
          }
        },
        "returns": "True if all differences satisfy allclose condition",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "handle_torch_function",
      "signature": "handle_torch_function(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any",
      "documentation": {
        "description": "Implement a function with checks for ``__torch_function__`` overrides.\n\nSee torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.\n\nArguments\n---------\npublic_api : function\n    Function exposed by the public torch API originally called like\n    ``public_api(*args, **kwargs)`` on which arguments are now being\n    checked.\nrelevant_args : iterable\n    Iterable of arguments to check for __torch_function__ methods.\nargs : tuple\n    Arbitrary positional arguments originally passed into ``public_api``.\nkwargs : tuple\n    Arbitrary keyword arguments originally passed into ``public_api``.\n\nReturns\n-------\nobject\n    Result from calling ``implementation`` or an ``__torch_function__``\n    method, as appropriate.\n\nRaises\n------\nTypeError : if no implementation is found.\n\nExample\n-------\n>>> def func(a):\n...     if has_torch_function_unary(a):\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "has_torch_function",
      "signature": "_has_torch_function(...)",
      "documentation": {
        "description": "Check for __torch_function__ implementations in the elements of an iterable\nor if a __torch_function__ mode is enabled.  Considers exact ``Tensor`` s\nand ``Parameter`` s non-dispatchable.  Use this to guard a call to\n:func:`handle_torch_function`; don't use it to test if something\nis Tensor-like, use :func:`is_tensor_like` instead.\nArguments\n---------\nrelevant_args : iterable\n    Iterable or arguments to check for __torch_function__ methods.\nReturns\n-------\nbool\n    True if any of the elements of relevant_args have __torch_function__\n    implementations, False otherwise.\nSee Also\n________\ntorch.is_tensor_like\n    Checks if something is a Tensor-like, including an exact ``Tensor``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_multithreading_enabled",
      "signature": "_is_multithreading_enabled(...)",
      "documentation": {
        "description": "Returns True if multithreading is currently enabled.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_tensor_like",
      "signature": "is_tensor_like(inp)",
      "documentation": {
        "description": "Returns ``True`` if the passed-in input is a Tensor-like.\n\nCurrently, this occurs whenever there's a ``__torch_function__``\nattribute on the type of the input.\n\nExamples\n--------\nA subclass of tensor is generally a Tensor-like.\n\n>>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n\nBuilt-in or user types aren't usually Tensor-like.\n\n>>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n\nBut, they can be made Tensor-like by implementing __torch_function__.\n\n>>> class TensorLike:\n...     @classmethod\n...     def __torch_function__(cls, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_view_replay_enabled",
      "signature": "_is_view_replay_enabled(...)",
      "documentation": {
        "description": "Returns True if view-replay is currently enabled.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "kineto_available",
      "signature": "kineto_available()",
      "documentation": {
        "description": "kineto_available() -> bool",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "variable",
      "signature": "variable(*args, **kwargs)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "DeviceType",
      "documentation": {
        "description": "Members:\n\nCPU\n\nCUDA\n\nMKLDNN\n\nOPENGL\n\nOPENCL\n\nIDEEP\n\nHIP\n\nFPGA\n\nMAIA\n\nXLA\n\nVulkan\n\nMetal\n\nXPU\n\nMPS\n\nMTIA\n\nMeta\n\nHPU\n\nVE\n\nLazy\n\nIPU\n\nPrivateUse1",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Function",
      "documentation": {
        "description": "Base class to create custom `autograd.Function`.\n\nTo create a custom `autograd.Function`, subclass this class and implement\nthe :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\nop in the forward pass, call the class method ``apply``. Do not call\n:meth:`forward` directly.\n\nTo ensure correctness and best performance, make sure you are calling the\ncorrect methods on ``ctx`` and validating your backward function using\n:func:`torch.autograd.gradcheck`.\n\nSee :ref:`extending-autograd` for more details on how to use this class.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "apply",
          "signature": "apply(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "backward",
          "signature": "backward(ctx: Any, *grad_outputs: Any) -> Any",
          "documentation": {
            "description": "Define a formula for differentiating the operation with backward mode automatic differentiation.\n\nThis function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the ``vjp`` function.)\n\nIt must accept a context :attr:`ctx` as the first argument, followed by\nas many outputs as the :func:`forward` returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n:func:`forward`. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.\n\nThe context can be used to retrieve tensors saved during the forward\npass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n:func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\nfirst input to :func:`forward` needs gradient computed w.r.t. the\noutput.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(*args: Any, **kwargs: Any) -> Any",
          "documentation": {
            "description": "Define the forward of the custom autograd Function.\n\nThis function is to be overridden by all subclasses.\nThere are two ways to define forward:\n\nUsage 1 (Combined forward and ctx)::\n\n    @staticmethod\n    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n- It must accept a context ctx as the first argument, followed by any\n  number of arguments (tensors or other types).\n- See :ref:`combining-forward-context` for more details\n\nUsage 2 (Separate forward and ctx)::\n\n    @staticmethod\n    def forward(*args: Any, **kwargs: Any) -> Any:\n        pass\n\n    @staticmethod\n    def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> None:\n        pass\n\n- The forward no longer accepts a ctx argument.\n- Instead, you must also override the :meth:`torch.autograd.Function.setup_context`\n  staticmethod to handle setting up the ``ctx`` object.\n  ``output`` is the output of the forward, ``inputs`` are a Tuple of inputs\n  to the forward.\n- See :ref:`extending-autograd` for more details\n\nThe context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on `ctx` (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n:func:`ctx.save_for_backward` if they are intended to be used in\n``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`\nif they are intended to be used for in ``jvp``.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "jvp",
          "signature": "jvp(ctx: Any, *grad_inputs: Any) -> Any",
          "documentation": {
            "description": "Define a formula for differentiating the operation with forward mode automatic differentiation.\n\nThis function is to be overridden by all subclasses.\nIt must accept a context :attr:`ctx` as the first argument, followed by\nas many inputs as the :func:`forward` got (None will be passed in\nfor non tensor inputs of the forward function),\nand it should return as many tensors as there were outputs to\n:func:`forward`. Each argument is the gradient w.r.t the given input,\nand each returned value should be the gradient w.r.t. the\ncorresponding output. If an output is not a Tensor or the function is not\ndifferentiable with respect to that output, you can just pass None as a\ngradient for that input.\n\nYou can use the :attr:`ctx` object to pass any value from the forward to this\nfunctions.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_dirty",
          "signature": "mark_dirty(self, *args: torch.Tensor)",
          "documentation": {
            "description": "Mark given tensors as modified in an in-place operation.\n\nThis should be called at most once, in either the :func:`setup_context`\nor :func:`forward` methods, and all arguments should be inputs.\n\nEvery tensor that's been modified in-place in a call to :func:`forward`\nshould be given to this function, to ensure correctness of our checks.\nIt doesn't matter whether the function is called before or after\nmodification.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_non_differentiable",
          "signature": "mark_non_differentiable(self, *args: torch.Tensor)",
          "documentation": {
            "description": "Mark outputs as non-differentiable.\n\nThis should be called at most once, in either the :func:`setup_context`\nor :func:`forward` methods, and all arguments should be tensor outputs.\n\nThis will mark outputs as not requiring gradients, increasing the\nefficiency of backward computation. You still need to accept a gradient\nfor each output in :meth:`~Function.backward`, but it's always going to\nbe a zero tensor with the same shape as the shape of a corresponding\noutput.\n\nThis is used e.g. for indices returned from a sort. See example::\n    >>> class Func(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x):\n    >>>         sorted, idx = x.sort()\n    >>>         ctx.mark_non_differentiable(idx)\n    >>>         ctx.save_for_backward(x, idx)\n    >>>         return sorted, idx\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, g1, g2):  # still need to accept g2\n    >>>         x, idx = ctx.saved_tensors\n    >>>         grad_input = torch.zeros_like(x)\n    >>>         grad_input.index_add_(0, idx, g1)\n    >>>         return grad_input",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_shared_storage",
          "signature": "mark_shared_storage(self, *pairs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "maybe_clear_saved_tensors",
          "signature": "maybe_clear_saved_tensors(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_hook",
          "signature": "register_hook(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_prehook",
          "signature": "register_prehook(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_for_backward",
          "signature": "save_for_backward(self, *tensors: torch.Tensor)",
          "documentation": {
            "description": "Save given tensors for a future call to :func:`~Function.backward`.\n\n``save_for_backward`` should be called at most once, in either the\n:func:`setup_context` or :func:`forward` methods, and only with tensors.\n\nAll tensors intended to be used in the backward pass should be saved\nwith ``save_for_backward`` (as opposed to directly on ``ctx``) to prevent\nincorrect gradients and memory leaks, and enable the application of saved\ntensor hooks. See :class:`torch.autograd.graph.saved_tensors_hooks`.\n\nNote that if intermediary tensors, tensors that are neither inputs\nnor outputs of :func:`forward`, are saved for backward, your custom Function\nmay not support double backward.\nCustom Functions that do not support double backward should decorate their\n:func:`backward` method with ``@once_differentiable`` so that performing\ndouble backward raises an error. If you'd like to support double backward,\nyou can either recompute intermediaries based on the inputs during backward\nor return the intermediaries as the outputs of the custom Function. See the\n`double backward tutorial <https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html>`_\nfor more details.\n\nIn :func:`backward`, saved tensors can be accessed through the :attr:`saved_tensors`\nattribute. Before returning them to the user, a check is made to ensure\nthey weren't used in any in-place operation that modified their content.\n\nArguments can also be ``None``. This is a no-op.\n\nSee :ref:`extending-autograd` for more details on how to use this method.\n\nExample::\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n    >>> class Func(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n    >>>         w = x * z\n    >>>         out = x * y + y * z + w * y\n    >>>         ctx.save_for_backward(x, y, w, out)\n    >>>         ctx.z = z  # z is not a tensor\n    >>>         return out\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, grad_out):\n    >>>         x, y, w, out = ctx.saved_tensors\n    >>>         z = ctx.z\n    >>>         gx = grad_out * (y + y * z)\n    >>>         gy = grad_out * (x + z + w)\n    >>>         gz = None\n    >>>         return gx, gy, gz\n    >>>\n    >>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n    >>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n    >>> c = 4\n    >>> d = Func.apply(a, b, c)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_for_forward",
          "signature": "save_for_forward(self, *tensors: torch.Tensor)",
          "documentation": {
            "description": "Save given tensors for a future call to :func:`~Function.jvp`.\n\n``save_for_forward`` should be called at most once, in either the\n:func:`setup_context` or :func:`forward` methods, and all arguments\nshould be tensors.\n\nIn :func:`jvp`, saved objects can be accessed through the :attr:`saved_tensors`\nattribute.\n\nArguments can also be ``None``. This is a no-op.\n\nSee :ref:`extending-autograd` for more details on how to use this method.\n\nExample::\n    >>> # xdoctest: +SKIP\n    >>> class Func(torch.autograd.Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n    >>>         ctx.save_for_backward(x, y)\n    >>>         ctx.save_for_forward(x, y)\n    >>>         ctx.z = z\n    >>>         return x * y * z\n    >>>\n    >>>     @staticmethod\n    >>>     def jvp(ctx, x_t, y_t, _):\n    >>>         x, y = ctx.saved_tensors\n    >>>         z = ctx.z\n    >>>         return z * (y * x_t + x * y_t)\n    >>>\n    >>>     @staticmethod\n    >>>     def vjp(ctx, grad_out):\n    >>>         x, y = ctx.saved_tensors\n    >>>         z = ctx.z\n    >>>         return z * grad_out * y, z * grad_out * x, None\n    >>>\n    >>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n    >>>     t = torch.tensor(1., dtype=torch.double)\n    >>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n    >>>     c = 4\n    >>>\n    >>>     with fwAD.dual_level():\n    >>>         a_dual = fwAD.make_dual(a, t)\n    >>>         d = Func.apply(a_dual, b, c)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_materialize_grads",
          "signature": "set_materialize_grads(self, value: bool)",
          "documentation": {
            "description": "Set whether to materialize grad tensors. Default is ``True``.\n\nThis should be called only from either the :func:`setup_context` or\n:func:`forward` methods.\n\nIf ``True``, undefined grad tensors will be expanded to tensors full of zeros\nprior to calling the :func:`backward` and :func:`jvp` methods.\n\nExample::\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n    >>> class SimpleFunc(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x):\n    >>>         return x.clone(), x.clone()\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, g1, g2):\n    >>>         return g1 + g2  # No check for None necessary\n    >>>\n    >>> # We modify SimpleFunc to handle non-materialized grad outputs\n    >>> class Func(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x):\n    >>>         ctx.set_materialize_grads(False)\n    >>>         ctx.save_for_backward(x)\n    >>>         return x.clone(), x.clone()\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, g1, g2):\n    >>>         x, = ctx.saved_tensors\n    >>>         grad_input = torch.zeros_like(x)\n    >>>         if g1 is not None:  # We must check for None now\n    >>>             grad_input += g1\n    >>>         if g2 is not None:\n    >>>             grad_input += g2\n    >>>         return grad_input\n    >>>\n    >>> a = torch.tensor(1., requires_grad=True)\n    >>> b, _ = Func.apply(a)  # induces g2 to be undefined",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setup_context",
          "signature": "setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> Any",
          "documentation": {
            "description": "There are two ways to define the forward pass of an autograd.Function.\n\nEither:\n\n1. Override forward with the signature ``forward(ctx, *args, **kwargs)``.\n   ``setup_context`` is not overridden. Setting up the ctx for backward\n   happens inside the ``forward``.\n2. Override forward with the signature ``forward(*args, **kwargs)`` and\n   override ``setup_context``. Setting up the ctx for backward happens\n   inside ``setup_context`` (as opposed to inside the ``forward``)\n\nSee :meth:`torch.autograd.Function.forward` and :ref:`extending-autograd` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "vjp",
          "signature": "backward(ctx: Any, *grad_outputs: Any) -> Any",
          "documentation": {
            "description": "Define a formula for differentiating the operation with backward mode automatic differentiation.\n\nThis function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the ``vjp`` function.)\n\nIt must accept a context :attr:`ctx` as the first argument, followed by\nas many outputs as the :func:`forward` returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n:func:`forward`. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.\n\nThe context can be used to retrieve tensors saved during the forward\npass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n:func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\nfirst input to :func:`forward` needs gradient computed w.r.t. the\noutput.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "vmap",
          "signature": "vmap(info, in_dims, *args)",
          "documentation": {
            "description": "Define the behavior for this autograd.Function underneath :func:`torch.vmap`.\n\nFor a :func:`torch.autograd.Function` to support\n:func:`torch.vmap`, you must either override this static method, or set\n``generate_vmap_rule`` to ``True`` (you may not do both).\n\nIf you choose to override this staticmethod: it must accept\n\n- an ``info`` object as the first argument. ``info.batch_size``\n  specifies the size of the dimension being vmapped over,\n  while ``info.randomness`` is the randomness option passed to\n  :func:`torch.vmap`.\n- an ``in_dims`` tuple as the second argument.\n  For each arg in ``args``, ``in_dims`` has a corresponding\n  ``Optional[int]``. It is ``None`` if the arg is not a Tensor or if\n  the arg is not being vmapped over, otherwise, it is an integer\n  specifying what dimension of the Tensor is being vmapped over.\n- ``*args``, which is the same as the args to :meth:`~Function.forward`.\n\nThe return of the vmap staticmethod is a tuple of ``(output, out_dims)``.\nSimilar to ``in_dims``, ``out_dims`` should be of the same structure as\n``output`` and contain one ``out_dim`` per output that specifies if the\noutput has the vmapped dimension and what index it is in.\n\nPlease see :ref:`func-autograd-function` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NestedIOFunction",
      "documentation": {
        "description": "This class is here only for backward compatibility reasons.\nUse :class:`Function` instead of this for any new use case.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "apply",
          "signature": "apply(*args, **kwargs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "backward",
          "signature": "backward(self, *gradients: Any) -> Any",
          "documentation": {
            "description": "Shared backward utility.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "backward_extended",
          "signature": "backward_extended(self, *grad_output: Any) -> None",
          "documentation": {
            "description": "User defined backward.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward",
          "signature": "forward(self, *args: Any) -> Any",
          "documentation": {
            "description": "Shared forward utility.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "forward_extended",
          "signature": "forward_extended(self, *input: Any) -> None",
          "documentation": {
            "description": "User defined forward.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "jvp",
          "signature": "jvp(ctx: Any, *grad_inputs: Any) -> Any",
          "documentation": {
            "description": "Define a formula for differentiating the operation with forward mode automatic differentiation.\n\nThis function is to be overridden by all subclasses.\nIt must accept a context :attr:`ctx` as the first argument, followed by\nas many inputs as the :func:`forward` got (None will be passed in\nfor non tensor inputs of the forward function),\nand it should return as many tensors as there were outputs to\n:func:`forward`. Each argument is the gradient w.r.t the given input,\nand each returned value should be the gradient w.r.t. the\ncorresponding output. If an output is not a Tensor or the function is not\ndifferentiable with respect to that output, you can just pass None as a\ngradient for that input.\n\nYou can use the :attr:`ctx` object to pass any value from the forward to this\nfunctions.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_dirty",
          "signature": "mark_dirty(self, *args: Any, **kwargs: Any) -> None",
          "documentation": {
            "description": "See :meth:`Function.mark_dirty`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_non_differentiable",
          "signature": "mark_non_differentiable(self, *args: Any, **kwargs: Any) -> None",
          "documentation": {
            "description": "See :meth:`Function.mark_non_differentiable`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mark_shared_storage",
          "signature": "mark_shared_storage(self, *pairs)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "maybe_clear_saved_tensors",
          "signature": "maybe_clear_saved_tensors(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_hook",
          "signature": "register_hook(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register_prehook",
          "signature": "register_prehook(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_for_backward",
          "signature": "save_for_backward(self, *args: Any) -> None",
          "documentation": {
            "description": "See :meth:`Function.save_for_backward`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "save_for_forward",
          "signature": "save_for_forward(self, *tensors: torch.Tensor)",
          "documentation": {
            "description": "Save given tensors for a future call to :func:`~Function.jvp`.\n\n``save_for_forward`` should be called at most once, in either the\n:func:`setup_context` or :func:`forward` methods, and all arguments\nshould be tensors.\n\nIn :func:`jvp`, saved objects can be accessed through the :attr:`saved_tensors`\nattribute.\n\nArguments can also be ``None``. This is a no-op.\n\nSee :ref:`extending-autograd` for more details on how to use this method.\n\nExample::\n    >>> # xdoctest: +SKIP\n    >>> class Func(torch.autograd.Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n    >>>         ctx.save_for_backward(x, y)\n    >>>         ctx.save_for_forward(x, y)\n    >>>         ctx.z = z\n    >>>         return x * y * z\n    >>>\n    >>>     @staticmethod\n    >>>     def jvp(ctx, x_t, y_t, _):\n    >>>         x, y = ctx.saved_tensors\n    >>>         z = ctx.z\n    >>>         return z * (y * x_t + x * y_t)\n    >>>\n    >>>     @staticmethod\n    >>>     def vjp(ctx, grad_out):\n    >>>         x, y = ctx.saved_tensors\n    >>>         z = ctx.z\n    >>>         return z * grad_out * y, z * grad_out * x, None\n    >>>\n    >>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n    >>>     t = torch.tensor(1., dtype=torch.double)\n    >>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n    >>>     c = 4\n    >>>\n    >>>     with fwAD.dual_level():\n    >>>         a_dual = fwAD.make_dual(a, t)\n    >>>         d = Func.apply(a_dual, b, c)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_materialize_grads",
          "signature": "set_materialize_grads(self, value: bool)",
          "documentation": {
            "description": "Set whether to materialize grad tensors. Default is ``True``.\n\nThis should be called only from either the :func:`setup_context` or\n:func:`forward` methods.\n\nIf ``True``, undefined grad tensors will be expanded to tensors full of zeros\nprior to calling the :func:`backward` and :func:`jvp` methods.\n\nExample::\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n    >>> class SimpleFunc(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x):\n    >>>         return x.clone(), x.clone()\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, g1, g2):\n    >>>         return g1 + g2  # No check for None necessary\n    >>>\n    >>> # We modify SimpleFunc to handle non-materialized grad outputs\n    >>> class Func(Function):\n    >>>     @staticmethod\n    >>>     def forward(ctx, x):\n    >>>         ctx.set_materialize_grads(False)\n    >>>         ctx.save_for_backward(x)\n    >>>         return x.clone(), x.clone()\n    >>>\n    >>>     @staticmethod\n    >>>     @once_differentiable\n    >>>     def backward(ctx, g1, g2):\n    >>>         x, = ctx.saved_tensors\n    >>>         grad_input = torch.zeros_like(x)\n    >>>         if g1 is not None:  # We must check for None now\n    >>>             grad_input += g1\n    >>>         if g2 is not None:\n    >>>             grad_input += g2\n    >>>         return grad_input\n    >>>\n    >>> a = torch.tensor(1., requires_grad=True)\n    >>> b, _ = Func.apply(a)  # induces g2 to be undefined",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setup_context",
          "signature": "setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> Any",
          "documentation": {
            "description": "There are two ways to define the forward pass of an autograd.Function.\n\nEither:\n\n1. Override forward with the signature ``forward(ctx, *args, **kwargs)``.\n   ``setup_context`` is not overridden. Setting up the ctx for backward\n   happens inside the ``forward``.\n2. Override forward with the signature ``forward(*args, **kwargs)`` and\n   override ``setup_context``. Setting up the ctx for backward happens\n   inside ``setup_context`` (as opposed to inside the ``forward``)\n\nSee :meth:`torch.autograd.Function.forward` and :ref:`extending-autograd` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "vjp",
          "signature": "backward(ctx: Any, *grad_outputs: Any) -> Any",
          "documentation": {
            "description": "Define a formula for differentiating the operation with backward mode automatic differentiation.\n\nThis function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the ``vjp`` function.)\n\nIt must accept a context :attr:`ctx` as the first argument, followed by\nas many outputs as the :func:`forward` returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n:func:`forward`. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.\n\nThe context can be used to retrieve tensors saved during the forward\npass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n:func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\nfirst input to :func:`forward` needs gradient computed w.r.t. the\noutput.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "vmap",
          "signature": "vmap(info, in_dims, *args)",
          "documentation": {
            "description": "Define the behavior for this autograd.Function underneath :func:`torch.vmap`.\n\nFor a :func:`torch.autograd.Function` to support\n:func:`torch.vmap`, you must either override this static method, or set\n``generate_vmap_rule`` to ``True`` (you may not do both).\n\nIf you choose to override this staticmethod: it must accept\n\n- an ``info`` object as the first argument. ``info.batch_size``\n  specifies the size of the dimension being vmapped over,\n  while ``info.randomness`` is the randomness option passed to\n  :func:`torch.vmap`.\n- an ``in_dims`` tuple as the second argument.\n  For each arg in ``args``, ``in_dims`` has a corresponding\n  ``Optional[int]``. It is ``None`` if the arg is not a Tensor or if\n  the arg is not being vmapped over, otherwise, it is an integer\n  specifying what dimension of the Tensor is being vmapped over.\n- ``*args``, which is the same as the args to :meth:`~Function.forward`.\n\nThe return of the vmap staticmethod is a tuple of ``(output, out_dims)``.\nSimilar to ``in_dims``, ``out_dims`` should be of the same structure as\n``output`` and contain one ``out_dim`` per output that specifies if the\noutput has the vmapped dimension and what index it is in.\n\nPlease see :ref:`func-autograd-function` for more details.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProfilerActivity",
      "documentation": {
        "description": "Members:\n\nCPU\n\nXPU\n\nMTIA\n\nCUDA\n\nPrivateUse1",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "ProfilerConfig",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "ProfilerEvent",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "correlation_id",
          "signature": "correlation_id(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "correlation_id(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu_elapsed_us",
          "signature": "cpu_elapsed_us(self: torch._C._autograd.ProfilerEvent, arg0: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "cpu_elapsed_us(self: torch._C._autograd.ProfilerEvent, arg0: torch._C._autograd.ProfilerEvent) -> float",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cpu_memory_usage",
          "signature": "cpu_memory_usage(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "cpu_memory_usage(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda_elapsed_us",
          "signature": "cuda_elapsed_us(self: torch._C._autograd.ProfilerEvent, arg0: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "cuda_elapsed_us(self: torch._C._autograd.ProfilerEvent, arg0: torch._C._autograd.ProfilerEvent) -> float",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cuda_memory_usage",
          "signature": "cuda_memory_usage(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "cuda_memory_usage(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "device",
          "signature": "device(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "device(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "flops",
          "signature": "flops(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "flops(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fwd_thread_id",
          "signature": "fwd_thread_id(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "fwd_thread_id(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "handle",
          "signature": "handle(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "handle(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "has_cuda",
          "signature": "has_cuda(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "has_cuda(self: torch._C._autograd.ProfilerEvent) -> bool",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_async",
          "signature": "is_async(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "is_async(self: torch._C._autograd.ProfilerEvent) -> bool",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_remote",
          "signature": "is_remote(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "is_remote(self: torch._C._autograd.ProfilerEvent) -> bool",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kind",
          "signature": "kind(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "kind(self: torch._C._autograd.ProfilerEvent) -> str",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "name",
          "signature": "name(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "name(self: torch._C._autograd.ProfilerEvent) -> str",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "node_id",
          "signature": "node_id(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "node_id(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "scope",
          "signature": "scope(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "scope(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "sequence_nr",
          "signature": "sequence_nr(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "sequence_nr(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shapes",
          "signature": "shapes(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "shapes(self: torch._C._autograd.ProfilerEvent) -> list[list[int]]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "stack",
          "signature": "stack(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "stack(self: torch._C._autograd.ProfilerEvent) -> list[str]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "start_us",
          "signature": "start_us(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "start_us(self: torch._C._autograd.ProfilerEvent) -> float",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "thread_id",
          "signature": "thread_id(self: torch._C._autograd.ProfilerEvent)",
          "documentation": {
            "description": "thread_id(self: torch._C._autograd.ProfilerEvent) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ProfilerState",
      "documentation": {
        "description": "Members:\n\nDisabled\n\nCPU\n\nCUDA\n\nNVTX\n\nITT\n\nPRIVATEUSE1\n\nKINETO\n\nKINETO_GPU_FALLBACK\n\nKINETO_PRIVATEUSE1_FALLBACK",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "SavedTensor",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "register_hooks",
          "signature": "register_hooks(self: torch._C._autograd.SavedTensor, arg0: Callable, arg1: Callable)",
          "documentation": {
            "description": "register_hooks(self: torch._C._autograd.SavedTensor, arg0: Callable, arg1: Callable) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Variable",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "detect_anomaly",
      "documentation": {
        "description": "Context-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\n- Running the forward pass with detection enabled will allow the backward\n  pass to print the traceback of the forward operation that created the failing\n  backward function.\n- If ``check_nan`` is ``True``, any backward computation that generate \"nan\"\n  value will raise an error. Default ``True``.\n\n.. warning::\n    This mode should be enabled only for debugging as the different tests\n    will slow down your program execution.\n\nExample:\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ANOMALY)\n    >>> import torch\n    >>> from torch import autograd\n    >>> class MyFunc(autograd.Function):\n    ...     @staticmethod\n    ...     def forward(ctx, inp):\n    ...         return inp.clone()\n    ...     @staticmethod\n    ...     def backward(ctx, gO):\n    ...         # Error during the backward pass\n    ...         raise RuntimeError(\"Some error in backward\")\n    ...         return gO.clone()\n    >>> def run_fn(a):\n    ...     out = MyFunc.apply(a)\n    ...     return out.sum()\n    >>> inp = torch.rand(10, 10, requires_grad=True)\n    >>> out = run_fn(inp)\n    >>> out.backward()\n        Traceback (most recent call last):\n          File \"<stdin>\", line 1, in <module>\n          File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n            torch.autograd.backward(self, gradient, retain_graph, create_graph)\n          File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n            allow_unreachable=True)  # allow_unreachable flag\n          File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n            return self._forward_cls.backward(self, *args)\n          File \"<stdin>\", line 8, in backward\n        RuntimeError: Some error in backward\n    >>> with autograd.detect_anomaly():\n    ...     inp = torch.rand(10, 10, requires_grad=True)\n    ...     out = run_fn(inp)\n    ...     out.backward()\n        Traceback of forward call that caused the error:\n          File \"tmp.py\", line 53, in <module>\n            out = run_fn(inp)\n          File \"tmp.py\", line 44, in run_fn\n            out = MyFunc.apply(a)\n        Traceback (most recent call last):\n          File \"<stdin>\", line 4, in <module>\n          File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n            torch.autograd.backward(self, gradient, retain_graph, create_graph)\n          File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n            allow_unreachable=True)  # allow_unreachable flag\n          File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n            return self._forward_cls.backward(self, *args)\n          File \"<stdin>\", line 8, in backward\n        RuntimeError: Some error in backward",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ANOMALY)\n    >>> import torch\n    >>> from torch import autograd\n    >>> class MyFunc(autograd.Function):\n    ...     @staticmethod\n    ...     def forward(ctx, inp):\n    ...         return inp.clone()\n    ...     @staticmethod\n    ...     def backward(ctx, gO):\n    ...         # Error during the backward pass\n    ...         raise RuntimeError(\"Some error in backward\")\n    ...         return gO.clone()\n    >>> def run_fn(a):\n    ...     out = MyFunc.apply(a)\n    ...     return out.sum()\n    >>> inp = torch.rand(10, 10, requires_grad=True)\n    >>> out = run_fn(inp)\n    >>> out.backward()\n        Traceback (most recent call last):\n          File \"<stdin>\", line 1, in <module>\n          File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n            torch.autograd.backward(self, gradient, retain_graph, create_graph)\n          File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n            allow_unreachable=True)  # allow_unreachable flag\n          File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n            return self._forward_cls.backward(self, *args)\n          File \"<stdin>\", line 8, in backward\n        RuntimeError: Some error in backward\n    >>> with autograd.detect_anomaly():\n    ...     inp = torch.rand(10, 10, requires_grad=True)\n    ...     out = run_fn(inp)\n    ...     out.backward()\n        Traceback of forward call that caused the error:\n          File \"tmp.py\", line 53, in <module>\n            out = run_fn(inp)\n          File \"tmp.py\", line 44, in run_fn\n            out = MyFunc.apply(a)\n        Traceback (most recent call last):\n          File \"<stdin>\", line 4, in <module>\n          File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n            torch.autograd.backward(self, gradient, retain_graph, create_graph)\n          File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n            allow_unreachable=True)  # allow_unreachable flag\n          File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n            return self._forward_cls.backward(self, *args)\n          File \"<stdin>\", line 8, in backward\n        RuntimeError: Some error in backward"
      },
      "methods": []
    },
    {
      "name": "enable_grad",
      "documentation": {
        "description": "Context-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via :class:`~no_grad`\nor :class:`~set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation\nin other threads.\n\nAlso functions as a decorator.\n\n.. note::\n    enable_grad is one of several mechanisms that can enable or\n    disable gradients locally see :ref:`locally-disable-grad-doc` for\n    more information on how they compare.\n\n.. note::\n    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n\nExample::\n    >>> # xdoctest: +SKIP\n    >>> x = torch.tensor([1.], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     with torch.enable_grad():\n    ...         y = x * 2\n    >>> y.requires_grad\n    True\n    >>> y.backward()\n    >>> x.grad\n    tensor([2.])\n    >>> @torch.enable_grad()\n    ... def doubler(x):\n    ...     return x * 2\n    >>> with torch.no_grad():\n    ...     z = doubler(x)\n    >>> z.requires_grad\n    True\n    >>> @torch.enable_grad()\n    ... def tripler(x):\n    ...     return x * 3\n    >>> with torch.no_grad():\n    ...     z = tripler(x)\n    >>> z.requires_grad\n    True",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clone",
          "signature": "clone(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "inference_mode",
      "documentation": {
        "description": "Context-manager that enables or disables inference mode.\n\nInferenceMode is a context manager analogous to :class:`~no_grad`\nto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. Note that\nunlike some other mechanisms that locally enable or disable grad,\nentering inference_mode also disables to :ref:`forward-mode AD <forward-mode-ad>`.\n\nThis context manager is thread local; it will not affect computation\nin other threads.\n\nAlso functions as a decorator.\n\n.. note::\n    Inference mode is one of several mechanisms that can enable or\n    disable gradients locally see :ref:`locally-disable-grad-doc` for\n    more information on how they compare.",
        "parameters": {
          "mode": {
            "type": "bool or function",
            "description": "Either a boolean flag whether to enable or"
          },
          "disable": {
            "type": "",
            "description": "inference mode or a Python function to decorate with"
          },
          "inference": {
            "type": "",
            "description": "mode enabled"
          },
          "Example": {
            "type": "",
            "description": ":\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n>>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...     y = x * x\n>>> y.requires_grad"
          },
          "False": {
            "type": "",
            "description": ""
          },
          "Traceback": {
            "type": "most recent call last",
            "description": ""
          },
          "File": {
            "type": "",
            "description": "\"<stdin>\", line 1, in <module>"
          },
          "RuntimeError": {
            "type": "",
            "description": "Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...     return x * x\n>>> out = func(x)\n>>> out.requires_grad"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clone",
          "signature": "clone(self) -> 'inference_mode'",
          "documentation": {
            "description": "Create a copy of this class",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "no_grad",
      "documentation": {
        "description": "Context-manager that disables gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure\nthat you will not call :meth:`Tensor.backward()`. It will reduce memory\nconsumption for computations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have\n`requires_grad=False`, even when the inputs have `requires_grad=True`.\nThere is an exception! All factory functions, or functions that create\na new Tensor and take a requires_grad kwarg, will NOT be affected by\nthis mode.\n\nThis context manager is thread local; it will not affect computation\nin other threads.\n\nAlso functions as a decorator.\n\n.. note::\n    No-grad is one of several mechanisms that can enable or\n    disable gradients locally see :ref:`locally-disable-grad-doc` for\n    more information on how they compare.\n\n.. note::\n    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n    If you want to disable forward AD for a computation, you can unpack\n    your dual tensors.\n\nExample::\n    >>> # xdoctest: +SKIP\n    >>> x = torch.tensor([1.], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     y = x * 2\n    >>> y.requires_grad\n    False\n    >>> @torch.no_grad()\n    ... def doubler(x):\n    ...     return x * 2\n    >>> z = doubler(x)\n    >>> z.requires_grad\n    False\n    >>> @torch.no_grad()\n    ... def tripler(x):\n    ...     return x * 3\n    >>> z = tripler(x)\n    >>> z.requires_grad\n    False\n    >>> # factory function exception\n    >>> with torch.no_grad():\n    ...     a = torch.nn.Parameter(torch.rand(10))\n    >>> a.requires_grad\n    True",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clone",
          "signature": "clone(self)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "set_detect_anomaly",
      "documentation": {
        "description": "Context-manager that sets the anomaly detection for the autograd engine on or off.\n\n``set_detect_anomaly`` will enable or disable the autograd anomaly detection\nbased on its argument :attr:`mode`.\nIt can be used as a context-manager or as a function.\n\nSee ``detect_anomaly`` above for details of the anomaly detection behaviour.",
        "parameters": {
          "mode": {
            "type": "bool",
            "description": "Flag whether to enable anomaly detection (``True``),"
          },
          "or": {
            "type": "",
            "description": "disable (``False``)."
          },
          "check_nan": {
            "type": "bool",
            "description": "Flag whether to raise an error when the backward"
          },
          "generate": {
            "type": "",
            "description": "\"nan\""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "set_grad_enabled",
      "documentation": {
        "description": "Context-manager that sets gradient calculation on or off.\n\n``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation\nin other threads.",
        "parameters": {
          "mode": {
            "type": "bool",
            "description": "Flag whether to enable grad (``True``), or disable\n(``False``). This can be used to conditionally enable"
          },
          "gradients": {
            "type": "",
            "description": ".\n.. note::"
          },
          "set_grad_enabled": {
            "type": "",
            "description": "is one of several mechanisms that can enable or"
          },
          "disable": {
            "type": "",
            "description": "gradients locally see :ref:`locally-disable-grad-doc` for"
          },
          "more": {
            "type": "",
            "description": "information on how they compare.\n.. note::"
          },
          "This": {
            "type": "",
            "description": "API does not apply to :ref:`forward-mode AD <forward-mode-ad>`."
          },
          "Example": {
            "type": "",
            "description": ":\n>>> # xdoctest: +SKIP\n>>> x = torch.tensor([1.], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad"
          },
          "False": {
            "type": "",
            "description": ""
          },
          "True": {
            "type": "",
            "description": ">>> _ = torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clone",
          "signature": "clone(self) -> 'set_grad_enabled'",
          "documentation": {
            "description": "Create a copy of this class",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "set_multithreading_enabled",
      "documentation": {
        "description": "Context-manager that sets multithreaded backwards on or off.\n\n``set_multithreading_enabled`` will enable or disable multithreaded backwards based on its argument :attr:`mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation\nin other threads.",
        "parameters": {
          "mode": {
            "type": "bool",
            "description": "Flag whether to enable multithreaded backwards (``True``), or disable\n(``False``).\n.. note::"
          },
          "This": {
            "type": "",
            "description": "API does not apply to :ref:`forward-mode AD <forward-mode-ad>`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clone",
          "signature": "clone(self) -> 'set_multithreading_enabled'",
          "documentation": {
            "description": "Create a copy of this class",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}