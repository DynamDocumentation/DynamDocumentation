{
  "description": "``numpy.linalg``\n================\n\nThe NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient\nlow level implementations of standard linear algebra algorithms. Those\nlibraries may be provided by NumPy itself using C versions of a subset of their\nreference implementations but, when possible, highly optimized libraries that\ntake advantage of specialized processor functionality are preferred. Examples\nof such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries\nare multithreaded and processor dependent, environmental variables and external\npackages such as threadpoolctl may be needed to control the number of threads\nor specify the processor architecture.\n\n- OpenBLAS: https://www.openblas.net/\n- threadpoolctl: https://github.com/joblib/threadpoolctl\n\nPlease note that the most-used linear algebra functions in NumPy are present in\nthe main ``numpy`` namespace rather than in ``numpy.linalg``.  There are:\n``dot``, ``vdot``, ``inner``, ``outer``, ``matmul``, ``tensordot``, ``einsum``,\n``einsum_path`` and ``kron``.\n\nFunctions present in numpy.linalg are listed below.\n\n\nMatrix and vector products\n--------------------------\n\n   multi_dot\n   matrix_power\n\nDecompositions\n--------------\n\n   cholesky\n   qr\n   svd\n\nMatrix eigenvalues\n------------------\n\n   eig\n   eigh\n   eigvals\n   eigvalsh\n\nNorms and other numbers\n-----------------------\n\n   norm\n   cond\n   det\n   matrix_rank\n   slogdet\n\nSolving equations and inverting matrices\n----------------------------------------\n\n   solve\n   tensorsolve\n   lstsq\n   inv\n   pinv\n   tensorinv\n\nExceptions\n----------\n\n   LinAlgError",
  "functions": [
    {
      "name": "cholesky",
      "signature": "cholesky(a)",
      "documentation": {
        "description": "Cholesky decomposition.\nReturn the Cholesky decomposition, `L * L.H`, of the square matrix `a`,\nwhere `L` is lower-triangular and .H is the conjugate transpose operator\n(which is the ordinary transpose if `a` is real-valued).  `a` must be\nHermitian (symmetric if real-valued) and positive-definite. No\nchecking is performed to verify whether `a` is Hermitian or not.\nIn addition, only the lower-triangular and diagonal elements of `a`\nare used. Only `L` is actually returned.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Hermitian (symmetric if all elements are real), positive-definite\ninput matrix."
          }
        },
        "returns": "-------\nL : (..., M, M) array_like\nLower-triangular Cholesky factor of `a`.  Returns a matrix object if\n`a` is a matrix object.",
        "raises": "------\nLinAlgError\nIf the decomposition fails, for example, if `a` is not\npositive-definite.",
        "see_also": "--------\nscipy.linalg.cholesky : Similar function in SciPy.\nscipy.linalg.cholesky_banded : Cholesky decompose a banded Hermitian\npositive-definite matrix.\nscipy.linalg.cho_factor : Cholesky decomposition of a matrix, to use in\n`scipy.linalg.cho_solve`.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe Cholesky decomposition is often used as a fast way of solving\n.. math:: A \\mathbf{x} = \\mathbf{b}\n(when `A` is both Hermitian/symmetric and positive-definite).\nFirst, we solve for :math:`\\mathbf{y}` in\n.. math:: L \\mathbf{y} = \\mathbf{b},\nand then for :math:`\\mathbf{x}` in\n.. math:: L.H \\mathbf{x} = \\mathbf{y}.",
        "examples": "--------\n>>> A = np.array([[1,-2j],[2j,5]])\n>>> A\narray([[ 1.+0.j, -0.-2.j],\n[ 0.+2.j,  5.+0.j]])\n>>> L = np.linalg.cholesky(A)\n>>> L\narray([[1.+0.j, 0.+0.j],\n[0.+2.j, 1.+0.j]])\n>>> np.dot(L, L.T.conj()) # verify that L * L.H = A\narray([[1.+0.j, 0.-2.j],\n[0.+2.j, 5.+0.j]])\n>>> A = [[1,-2j],[2j,5]] # what happens if A is only array_like?\n>>> np.linalg.cholesky(A) # an ndarray object is returned\narray([[1.+0.j, 0.+0.j],\n[0.+2.j, 1.+0.j]])\n>>> # But a matrix object is returned if A is a matrix object\n>>> np.linalg.cholesky(np.matrix(A))\nmatrix([[ 1.+0.j,  0.+0.j],\n[ 0.+2.j,  1.+0.j]])"
      }
    },
    {
      "name": "cond",
      "signature": "cond(x, p=None)",
      "documentation": {
        "description": "Compute the condition number of a matrix.\nThis function is capable of returning the condition number using\none of seven different norms, depending on the value of `p` (see",
        "parameters": {
          "x": {
            "type": "(..., M, N) array_like",
            "description": "The matrix whose condition number is sought."
          },
          "p": {
            "type": "{None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional",
            "description": "Order of the norm used in the condition number computation:\n=====  ============================\np      norm for matrices\n=====  ============================\nNone   2-norm, computed directly using the ``SVD``\n'fro'  Frobenius norm\ninf    max(sum(abs(x), axis=1))\n-inf   min(sum(abs(x), axis=1))\n1      max(sum(abs(x), axis=0))\n-1     min(sum(abs(x), axis=0))\n2      2-norm (largest sing. value)\n-2     smallest singular value\n=====  ============================\ninf means the `numpy.inf` object, and the Frobenius norm is\nthe root-of-sum-of-squares norm."
          }
        },
        "returns": "-------\nc : {float, inf}\nThe condition number of the matrix. May be infinite.",
        "raises": "",
        "see_also": "--------\nnumpy.linalg.norm",
        "notes": "-----\nThe condition number of `x` is defined as the norm of `x` times the\nnorm of the inverse of `x` [1]_; the norm can be the usual L2-norm\n(root-of-sum-of-squares) or one of a number of other matrix norms.\nReferences\n----------\n.. [1] G. Strang, *Linear Algebra and Its Applications*, Orlando, FL,\nAcademic Press, Inc., 1980, pg. 285.",
        "examples": "--------\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])\n>>> a\narray([[ 1,  0, -1],\n[ 0,  1,  0],\n[ 1,  0,  1]])\n>>> LA.cond(a)\n1.4142135623730951\n>>> LA.cond(a, 'fro')\n3.1622776601683795\n>>> LA.cond(a, np.inf)\n2.0\n>>> LA.cond(a, -np.inf)\n1.0\n>>> LA.cond(a, 1)\n2.0\n>>> LA.cond(a, -1)\n1.0\n>>> LA.cond(a, 2)\n1.4142135623730951\n>>> LA.cond(a, -2)\n0.70710678118654746 # may vary\n>>> min(LA.svd(a, compute_uv=False))*min(LA.svd(LA.inv(a), compute_uv=False))\n0.70710678118654746 # may vary"
      }
    },
    {
      "name": "det",
      "signature": "det(a)",
      "documentation": {
        "description": "Compute the determinant of an array.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Input array to compute determinants for."
          }
        },
        "returns": "-------\ndet : (...) array_like\nDeterminant of `a`.",
        "raises": "",
        "see_also": "--------\nslogdet : Another way to represent the determinant, more suitable\nfor large matrices where underflow/overflow may occur.\nscipy.linalg.det : Similar function in SciPy.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe determinant is computed via LU factorization using the LAPACK\nroutine ``z/dgetrf``.",
        "examples": "--------\nThe determinant of a 2-D array [[a, b], [c, d]] is ad - bc:\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.linalg.det(a)\n-2.0 # may vary\nComputing determinants for a stack of matrices:\n>>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n>>> a.shape\n(3, 2, 2)\n>>> np.linalg.det(a)\narray([-2., -3., -8.])"
      }
    },
    {
      "name": "eig",
      "signature": "eig(a)",
      "documentation": {
        "description": "Compute the eigenvalues and right eigenvectors of a square array.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array",
            "description": "Matrices for which the eigenvalues and right eigenvectors will\nbe computed"
          }
        },
        "returns": "-------\nA namedtuple with the following attributes:\neigenvalues : (..., M) array\nThe eigenvalues, each repeated according to its multiplicity.\nThe eigenvalues are not necessarily ordered. The resulting\narray will be of complex type, unless the imaginary part is\nzero in which case it will be cast to a real type. When `a`\nis real the resulting eigenvalues will be real (0 imaginary\npart) or occur in conjugate pairs\neigenvectors : (..., M, M) array\nThe normalized (unit \"length\") eigenvectors, such that the\ncolumn ``eigenvectors[:,i]`` is the eigenvector corresponding to the\neigenvalue ``eigenvalues[i]``.",
        "raises": "------\nLinAlgError\nIf the eigenvalue computation does not converge.",
        "see_also": "--------\neigvals : eigenvalues of a non-symmetric array.\neigh : eigenvalues and eigenvectors of a real symmetric or complex\nHermitian (conjugate symmetric) array.\neigvalsh : eigenvalues of a real symmetric or complex Hermitian\n(conjugate symmetric) array.\nscipy.linalg.eig : Similar function in SciPy that also solves the\ngeneralized eigenvalue problem.\nscipy.linalg.schur : Best choice for unitary and other non-Hermitian\nnormal matrices.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThis is implemented using the ``_geev`` LAPACK routines which compute\nthe eigenvalues and eigenvectors of general square arrays.\nThe number `w` is an eigenvalue of `a` if there exists a vector `v` such\nthat ``a @ v = w * v``. Thus, the arrays `a`, `eigenvalues`, and\n`eigenvectors` satisfy the equations ``a @ eigenvectors[:,i] =\neigenvalues[i] * eigenvalues[:,i]`` for :math:`i \\in \\{0,...,M-1\\}`.\nThe array `eigenvectors` may not be of maximum rank, that is, some of the\ncolumns may be linearly dependent, although round-off error may obscure\nthat fact. If the eigenvalues are all different, then theoretically the\neigenvectors are linearly independent and `a` can be diagonalized by a\nsimilarity transformation using `eigenvectors`, i.e, ``inv(eigenvectors) @\na @ eigenvectors`` is diagonal.\nFor non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`\nis preferred because the matrix `eigenvectors` is guaranteed to be\nunitary, which is not the case when using `eig`. The Schur factorization\nproduces an upper triangular matrix rather than a diagonal matrix, but for\nnormal matrices only the diagonal of the upper triangular matrix is\nneeded, the rest is roundoff error.\nFinally, it is emphasized that `eigenvectors` consists of the *right* (as\nin right-hand side) eigenvectors of `a`. A vector `y` satisfying ``y.T @ a\n= z * y.T`` for some number `z` is called a *left* eigenvector of `a`,\nand, in general, the left and right eigenvectors of a matrix are not\nnecessarily the (perhaps conjugate) transposes of each other.\nReferences\n----------\nG. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,\nAcademic Press, Inc., 1980, Various pp.",
        "examples": "--------\n>>> from numpy import linalg as LA\n(Almost) trivial example with real eigenvalues and eigenvectors.\n>>> eigenvalues, eigenvectors = LA.eig(np.diag((1, 2, 3)))\n>>> eigenvalues\narray([1., 2., 3.])\n>>> eigenvectors\narray([[1., 0., 0.],\n[0., 1., 0.],\n[0., 0., 1.]])\nReal matrix possessing complex eigenvalues and eigenvectors; note that the\neigenvalues are complex conjugates of each other.\n>>> eigenvalues, eigenvectors = LA.eig(np.array([[1, -1], [1, 1]]))\n>>> eigenvalues\narray([1.+1.j, 1.-1.j])\n>>> eigenvectors\narray([[0.70710678+0.j        , 0.70710678-0.j        ],\n[0.        -0.70710678j, 0.        +0.70710678j]])\nComplex-valued matrix with real eigenvalues (but complex-valued eigenvectors);\nnote that ``a.conj().T == a``, i.e., `a` is Hermitian.\n>>> a = np.array([[1, 1j], [-1j, 1]])\n>>> eigenvalues, eigenvectors = LA.eig(a)\n>>> eigenvalues\narray([2.+0.j, 0.+0.j])\n>>> eigenvectors\narray([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary\n[ 0.70710678+0.j        , -0.        +0.70710678j]])\nBe careful about round-off error!\n>>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])\n>>> # Theor. eigenvalues are 1 +/- 1e-9\n>>> eigenvalues, eigenvectors = LA.eig(a)\n>>> eigenvalues\narray([1., 1.])\n>>> eigenvectors\narray([[1., 0.],\n[0., 1.]])"
      }
    },
    {
      "name": "eigh",
      "signature": "eigh(a, UPLO='L')",
      "documentation": {
        "description": "Return the eigenvalues and eigenvectors of a complex Hermitian\n(conjugate symmetric) or a real symmetric matrix.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array",
            "description": "Hermitian or real symmetric matrices whose eigenvalues and\neigenvectors are to be computed."
          },
          "UPLO": {
            "type": "{'L', 'U'}, optional",
            "description": "Specifies whether the calculation is done with the lower triangular\npart of `a` ('L', default) or the upper triangular part ('U').\nIrrespective of this value only the real parts of the diagonal will\nbe considered in the computation to preserve the notion of a Hermitian\nmatrix. It therefore follows that the imaginary part of the diagonal\nwill always be treated as zero."
          }
        },
        "returns": "a 2-D square array or matrix (depending on the input type) of the\ncorresponding eigenvectors (in columns).\n-------\nA namedtuple with the following attributes:\neigenvalues : (..., M) ndarray\nThe eigenvalues in ascending order, each repeated according to\nits multiplicity.\neigenvectors : {(..., M, M) ndarray, (..., M, M) matrix}\nThe column ``eigenvectors[:, i]`` is the normalized eigenvector\ncorresponding to the eigenvalue ``eigenvalues[i]``.  Will return a\nmatrix object if `a` is a matrix object.",
        "raises": "------\nLinAlgError\nIf the eigenvalue computation does not converge.",
        "see_also": "--------\neigvalsh : eigenvalues of real symmetric or complex Hermitian\n(conjugate symmetric) arrays.\neig : eigenvalues and right eigenvectors for non-symmetric arrays.\neigvals : eigenvalues of non-symmetric arrays.\nscipy.linalg.eigh : Similar function in SciPy (but also solves the\ngeneralized eigenvalue problem).",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,\n``_heevd``.\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal. [1]_ The array `eigenvalues` of (column) eigenvectors is unitary and\n`a`, `eigenvalues`, and `eigenvectors` satisfy the equations ``dot(a,\neigenvectors[:, i]) = eigenvalues[i] * eigenvectors[:, i]``.\nReferences\n----------\n.. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,\nFL, Academic Press, Inc., 1980, pg. 222.",
        "examples": "--------\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, -2j], [2j, 5]])\n>>> a\narray([[ 1.+0.j, -0.-2.j],\n[ 0.+2.j,  5.+0.j]])\n>>> eigenvalues, eigenvectors = LA.eigh(a)\n>>> eigenvalues\narray([0.17157288, 5.82842712])\n>>> eigenvectors\narray([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n[ 0.        +0.38268343j,  0.        -0.92387953j]])\n>>> np.dot(a, eigenvectors[:, 0]) - eigenvalues[0] * eigenvectors[:, 0] # verify 1st eigenval/vec pair\narray([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])\n>>> np.dot(a, eigenvectors[:, 1]) - eigenvalues[1] * eigenvectors[:, 1] # verify 2nd eigenval/vec pair\narray([0.+0.j, 0.+0.j])\n>>> A = np.matrix(a) # what happens if input is a matrix object\n>>> A\nmatrix([[ 1.+0.j, -0.-2.j],\n[ 0.+2.j,  5.+0.j]])\n>>> eigenvalues, eigenvectors = LA.eigh(A)\n>>> eigenvalues\narray([0.17157288, 5.82842712])\n>>> eigenvectors\nmatrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n[ 0.        +0.38268343j,  0.        -0.92387953j]])\n>>> # demonstrate the treatment of the imaginary part of the diagonal\n>>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n>>> a\narray([[5.+2.j, 9.-2.j],\n[0.+2.j, 2.-1.j]])\n>>> # with UPLO='L' this is numerically equivalent to using LA.eig() with:\n>>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n>>> b\narray([[5.+0.j, 0.-2.j],\n[0.+2.j, 2.+0.j]])\n>>> wa, va = LA.eigh(a)\n>>> wb, vb = LA.eig(b)\n>>> wa; wb\narray([1., 6.])\narray([6.+0.j, 1.+0.j])\n>>> va; vb\narray([[-0.4472136 +0.j        , -0.89442719+0.j        ], # may vary\n[ 0.        +0.89442719j,  0.        -0.4472136j ]])\narray([[ 0.89442719+0.j       , -0.        +0.4472136j],\n[-0.        +0.4472136j,  0.89442719+0.j       ]])"
      }
    },
    {
      "name": "eigvals",
      "signature": "eigvals(a)",
      "documentation": {
        "description": "Compute the eigenvalues of a general matrix.\nMain difference between `eigvals` and `eig`: the eigenvectors aren't\nreturned.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "A complex- or real-valued matrix whose eigenvalues will be computed."
          }
        },
        "returns": "-------\nw : (..., M,) ndarray\nThe eigenvalues, each repeated according to its multiplicity.\nThey are not necessarily ordered, nor are they necessarily\nreal for real matrices.",
        "raises": "------\nLinAlgError\nIf the eigenvalue computation does not converge.",
        "see_also": "--------\neig : eigenvalues and right eigenvectors of general arrays\neigvalsh : eigenvalues of real symmetric or complex Hermitian\n(conjugate symmetric) arrays.\neigh : eigenvalues and eigenvectors of real symmetric or complex\nHermitian (conjugate symmetric) arrays.\nscipy.linalg.eigvals : Similar function in SciPy.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThis is implemented using the ``_geev`` LAPACK routines which compute\nthe eigenvalues and eigenvectors of general square arrays.",
        "examples": "--------\nIllustration, using the fact that the eigenvalues of a diagonal matrix\nare its diagonal elements, that multiplying a matrix on the left\nby an orthogonal matrix, `Q`, and on the right by `Q.T` (the transpose\nof `Q`), preserves the eigenvalues of the \"middle\" matrix.  In other words,\nif `Q` is orthogonal, then ``Q * A * Q.T`` has the same eigenvalues as\n``A``:\n>>> from numpy import linalg as LA\n>>> x = np.random.random()\n>>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])\n>>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])\n(1.0, 1.0, 0.0)\nNow multiply a diagonal matrix by ``Q`` on one side and by ``Q.T`` on the other:\n>>> D = np.diag((-1,1))\n>>> LA.eigvals(D)\narray([-1.,  1.])\n>>> A = np.dot(Q, D)\n>>> A = np.dot(A, Q.T)\n>>> LA.eigvals(A)\narray([ 1., -1.]) # random"
      }
    },
    {
      "name": "eigvalsh",
      "signature": "eigvalsh(a, UPLO='L')",
      "documentation": {
        "description": "Compute the eigenvalues of a complex Hermitian or real symmetric matrix.\nMain difference from eigh: the eigenvectors are not computed.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "A complex- or real-valued matrix whose eigenvalues are to be\ncomputed."
          },
          "UPLO": {
            "type": "{'L', 'U'}, optional",
            "description": "Specifies whether the calculation is done with the lower triangular\npart of `a` ('L', default) or the upper triangular part ('U').\nIrrespective of this value only the real parts of the diagonal will\nbe considered in the computation to preserve the notion of a Hermitian\nmatrix. It therefore follows that the imaginary part of the diagonal\nwill always be treated as zero."
          }
        },
        "returns": "-------\nw : (..., M,) ndarray\nThe eigenvalues in ascending order, each repeated according to\nits multiplicity.",
        "raises": "------\nLinAlgError\nIf the eigenvalue computation does not converge.",
        "see_also": "--------\neigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian\n(conjugate symmetric) arrays.\neigvals : eigenvalues of general real or complex arrays.\neig : eigenvalues and right eigenvectors of general real or complex\narrays.\nscipy.linalg.eigvalsh : Similar function in SciPy.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe eigenvalues are computed using LAPACK routines ``_syevd``, ``_heevd``.",
        "examples": "--------\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, -2j], [2j, 5]])\n>>> LA.eigvalsh(a)\narray([ 0.17157288,  5.82842712]) # may vary\n>>> # demonstrate the treatment of the imaginary part of the diagonal\n>>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n>>> a\narray([[5.+2.j, 9.-2.j],\n[0.+2.j, 2.-1.j]])\n>>> # with UPLO='L' this is numerically equivalent to using LA.eigvals()\n>>> # with:\n>>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n>>> b\narray([[5.+0.j, 0.-2.j],\n[0.+2.j, 2.+0.j]])\n>>> wa = LA.eigvalsh(a)\n>>> wb = LA.eigvals(b)\n>>> wa; wb\narray([1., 6.])\narray([6.+0.j, 1.+0.j])"
      }
    },
    {
      "name": "inv",
      "signature": "inv(a)",
      "documentation": {
        "description": "Compute the (multiplicative) inverse of a matrix.\nGiven a square matrix `a`, return the matrix `ainv` satisfying\n``dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])``.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Matrix to be inverted."
          }
        },
        "returns": "-------\nainv : (..., M, M) ndarray or matrix\n(Multiplicative) inverse of the matrix `a`.",
        "raises": "------\nLinAlgError\nIf `a` is not square or inversion fails.",
        "see_also": "--------\nscipy.linalg.inv : Similar function in SciPy.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.",
        "examples": "--------\n>>> from numpy.linalg import inv\n>>> a = np.array([[1., 2.], [3., 4.]])\n>>> ainv = inv(a)\n>>> np.allclose(np.dot(a, ainv), np.eye(2))\nTrue\n>>> np.allclose(np.dot(ainv, a), np.eye(2))\nTrue\nIf a is a matrix object, then the return value is a matrix as well:\n>>> ainv = inv(np.matrix(a))\n>>> ainv\nmatrix([[-2. ,  1. ],\n[ 1.5, -0.5]])\nInverses of several matrices can be computed at once:\n>>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])\n>>> inv(a)\narray([[[-2.  ,  1.  ],\n[ 1.5 , -0.5 ]],\n[[-1.25,  0.75],\n[ 0.75, -0.25]]])"
      }
    },
    {
      "name": "lstsq",
      "signature": "lstsq(a, b, rcond='warn')",
      "documentation": {
        "description": "Return the least-squares solution to a linear matrix equation.\nComputes the vector `x` that approximately solves the equation\n``a @ x = b``. The equation may be under-, well-, or over-determined\n(i.e., the number of linearly independent rows of `a` can be less than,\nequal to, or greater than its number of linearly independent columns).\nIf `a` is square and of full rank, then `x` (but for round-off error)\nis the \"exact\" solution of the equation. Else, `x` minimizes the\nEuclidean 2-norm :math:`||b - ax||`. If there are multiple minimizing\nsolutions, the one with the smallest 2-norm :math:`||x||` is returned.",
        "parameters": {
          "a": {
            "type": "(M, N) array_like",
            "description": "\"Coefficient\" matrix."
          },
          "b": {
            "type": "{(M,), (M, K)} array_like",
            "description": "Ordinate or \"dependent variable\" values. If `b` is two-dimensional,\nthe least-squares solution is calculated for each of the `K` columns\nof `b`."
          },
          "rcond": {
            "type": "float, optional",
            "description": "Cut-off ratio for small singular values of `a`.\nFor the purposes of rank determination, singular values are treated\nas zero if they are smaller than `rcond` times the largest singular\nvalue of `a`.\n.. versionchanged:: 1.14.0\nIf not set, a FutureWarning is given. The previous default\nof ``-1`` will use the machine precision as `rcond` parameter,\nthe new default will use the machine precision times `max(M, N)`.\nTo silence the warning and use the new default, use ``rcond=None``,\nto keep using the old behavior, use ``rcond=-1``."
          }
        },
        "returns": "-------\nx : {(N,), (N, K)} ndarray\nLeast-squares solution. If `b` is two-dimensional,\nthe solutions are in the `K` columns of `x`.\nresiduals : {(1,), (K,), (0,)} ndarray\nSums of squared residuals: Squared Euclidean 2-norm for each column in\n``b - a @ x``.\nIf the rank of `a` is < N or M <= N, this is an empty array.\nIf `b` is 1-dimensional, this is a (1,) shape array.\nOtherwise the shape is (K,).\nrank : int\nRank of matrix `a`.\ns : (min(M, N),) ndarray\nSingular values of `a`.",
        "raises": "------\nLinAlgError\nIf computation does not converge.",
        "see_also": "--------\nscipy.linalg.lstsq : Similar function in SciPy.",
        "notes": "-----\nIf `b` is a matrix, then all array results are returned as matrices.",
        "examples": "--------\nFit a line, ``y = mx + c``, through some noisy data-points:\n>>> x = np.array([0, 1, 2, 3])\n>>> y = np.array([-1, 0.2, 0.9, 2.1])\nBy examining the coefficients, we see that the line should have a\ngradient of roughly 1 and cut the y-axis at, more or less, -1.\nWe can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``\nand ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:\n>>> A = np.vstack([x, np.ones(len(x))]).T\n>>> A\narray([[ 0.,  1.],\n[ 1.,  1.],\n[ 2.,  1.],\n[ 3.,  1.]])\n>>> m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n>>> m, c\n(1.0 -0.95) # may vary\nPlot the data along with the fitted line:\n>>> import matplotlib.pyplot as plt\n>>> _ = plt.plot(x, y, 'o', label='Original data', markersize=10)\n>>> _ = plt.plot(x, m*x + c, 'r', label='Fitted line')\n>>> _ = plt.legend()\n>>> plt.show()"
      }
    },
    {
      "name": "matrix_power",
      "signature": "matrix_power(a, n)",
      "documentation": {
        "description": "Raise a square matrix to the (integer) power `n`.\nFor positive integers `n`, the power is computed by repeated matrix\nsquarings and matrix multiplications. If ``n == 0``, the identity matrix\nof the same shape as M is returned. If ``n < 0``, the inverse\nis computed and then raised to the ``abs(n)``.\n.. note:: Stacks of object matrices are not currently supported.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Matrix to be \"powered\"."
          },
          "n": {
            "type": "int",
            "description": "The exponent can be any integer or long integer, positive,\nnegative, or zero."
          }
        },
        "returns": "-------\na**n : (..., M, M) ndarray or matrix object\nThe return value is the same shape and type as `M`;\nif the exponent is positive or zero then the type of the\nelements is the same as those of `M`. If the exponent is\nnegative the elements are floating-point.",
        "raises": "------\nLinAlgError\nFor matrices that are not square or that (for negative powers) cannot\nbe inverted numerically.",
        "see_also": "",
        "notes": "",
        "examples": "--------\n>>> from numpy.linalg import matrix_power\n>>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit\n>>> matrix_power(i, 3) # should = -i\narray([[ 0, -1],\n[ 1,  0]])\n>>> matrix_power(i, 0)\narray([[1, 0],\n[0, 1]])\n>>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements\narray([[ 0.,  1.],\n[-1.,  0.]])\nSomewhat more sophisticated example\n>>> q = np.zeros((4, 4))\n>>> q[0:2, 0:2] = -i\n>>> q[2:4, 2:4] = i\n>>> q # one of the three quaternion units not equal to 1\narray([[ 0., -1.,  0.,  0.],\n[ 1.,  0.,  0.,  0.],\n[ 0.,  0.,  0.,  1.],\n[ 0.,  0., -1.,  0.]])\n>>> matrix_power(q, 2) # = -np.eye(4)\narray([[-1.,  0.,  0.,  0.],\n[ 0., -1.,  0.,  0.],\n[ 0.,  0., -1.,  0.],\n[ 0.,  0.,  0., -1.]])"
      }
    },
    {
      "name": "matrix_rank",
      "signature": "matrix_rank(A, tol=None, hermitian=False)",
      "documentation": {
        "description": "Return matrix rank of array using SVD method\nRank of the array is the number of singular values of the array that are\ngreater than `tol`.\n.. versionchanged:: 1.14\nCan now operate on stacks of matrices",
        "parameters": {
          "A": {
            "type": "{(M,), (..., M, N)} array_like",
            "description": "Input vector or stack of matrices."
          },
          "tol": {
            "type": "(...) array_like, float, optional",
            "description": "Threshold below which SVD values are considered zero. If `tol` is\nNone, and ``S`` is an array with singular values for `M`, and\n``eps`` is the epsilon value for datatype of ``S``, then `tol` is\nset to ``S.max() * max(M, N) * eps``.\n.. versionchanged:: 1.14\nBroadcasted against the stack of matrices"
          },
          "hermitian": {
            "type": "bool, optional",
            "description": "If True, `A` is assumed to be Hermitian (symmetric if real-valued),\nenabling a more efficient method for finding singular values.\nDefaults to False.\n.. versionadded:: 1.14"
          }
        },
        "returns": "-------\nrank : (...) array_like\nRank of A.",
        "raises": "",
        "see_also": "",
        "notes": "-----\nThe default threshold to detect rank deficiency is a test on the magnitude\nof the singular values of `A`.  By default, we identify singular values less\nthan ``S.max() * max(M, N) * eps`` as indicating rank deficiency (with\nthe symbols defined above). This is the algorithm MATLAB uses [1].  It also\nappears in *Numerical recipes* in the discussion of SVD solutions for linear\nleast squares [2].\nThis default threshold is designed to detect rank deficiency accounting for\nthe numerical errors of the SVD computation.  Imagine that there is a column\nin `A` that is an exact (in floating point) linear combination of other\ncolumns in `A`. Computing the SVD on `A` will not produce a singular value\nexactly equal to 0 in general: any difference of the smallest SVD value from\n0 will be caused by numerical imprecision in the calculation of the SVD.\nOur threshold for small SVD values takes this numerical imprecision into\naccount, and the default threshold will detect such numerical rank\ndeficiency.  The threshold may declare a matrix `A` rank deficient even if\nthe linear combination of some columns of `A` is not exactly equal to\nanother column of `A` but only numerically very close to another column of\n`A`.\nWe chose our default threshold because it is in wide use.  Other thresholds\nare possible.  For example, elsewhere in the 2007 edition of *Numerical\nrecipes* there is an alternative threshold of ``S.max() *\nnp.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe\nthis threshold as being based on \"expected roundoff error\" (p 71).\nThe thresholds above deal with floating point roundoff error in the\ncalculation of the SVD.  However, you may have more information about the\nsources of error in `A` that would make you consider other tolerance values\nto detect *effective* rank deficiency.  The most useful measure of the\ntolerance depends on the operations you intend to use on your matrix.  For\nexample, if your data come from uncertain measurements with uncertainties\ngreater than floating point epsilon, choosing a tolerance near that\nuncertainty may be preferable.  The tolerance may be absolute if the\nuncertainties are absolute rather than relative.\nReferences\n----------\n.. [1] MATLAB reference documentation, \"Rank\"\nhttps://www.mathworks.com/help/techdoc/ref/rank.html\n.. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,\n\"Numerical Recipes (3rd edition)\", Cambridge University Press, 2007,\npage 795.",
        "examples": "--------\n>>> from numpy.linalg import matrix_rank\n>>> matrix_rank(np.eye(4)) # Full rank matrix\n4\n>>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix\n>>> matrix_rank(I)\n3\n>>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0\n1\n>>> matrix_rank(np.zeros((4,)))\n0"
      }
    },
    {
      "name": "multi_dot",
      "signature": "multi_dot(arrays, *, out=None)",
      "documentation": {
        "description": "Compute the dot product of two or more arrays in a single function call,\nwhile automatically selecting the fastest evaluation order.\n`multi_dot` chains `numpy.dot` and uses optimal parenthesization\nof the matrices [1]_ [2]_. Depending on the shapes of the matrices,\nthis can speed up the multiplication a lot.\nIf the first argument is 1-D it is treated as a row vector.\nIf the last argument is 1-D it is treated as a column vector.\nThe other arguments must be 2-D.\nThink of `multi_dot` as::\ndef multi_dot(arrays): return functools.reduce(np.dot, arrays)",
        "parameters": {
          "arrays": {
            "type": "sequence of array_like",
            "description": "If the first argument is 1-D it is treated as row vector.\nIf the last argument is 1-D it is treated as column vector.\nThe other arguments must be 2-D."
          },
          "out": {
            "type": "ndarray, optional",
            "description": "Output argument. This must have the exact kind that would be returned\nif it was not used. In particular, it must have the right type, must be\nC-contiguous, and its dtype must be the dtype that would be returned\nfor `dot(a, b)`. This is a performance feature. Therefore, if these\nconditions are not met, an exception is raised, instead of attempting\nto be flexible.\n.. versionadded:: 1.19.0"
          }
        },
        "returns": "-------\noutput : ndarray",
        "raises": "",
        "see_also": "--------\nnumpy.dot : dot multiplication with two arguments.\nReferences\n----------\n.. [1] Cormen, \"Introduction to Algorithms\", Chapter 15.2, p. 370-378\n.. [2] https://en.wikipedia.org/wiki/Matrix_chain_multiplication",
        "notes": "-----\nThe cost for a matrix multiplication can be calculated with the\nfollowing function::\ndef cost(A, B):\nreturn A.shape[0] * A.shape[1] * B.shape[1]\nAssume we have three matrices\n:math:`A_{10x100}, B_{100x5}, C_{5x50}`.\nThe costs for the two different parenthesizations are as follows::\ncost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500\ncost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000",
        "examples": "--------\n`multi_dot` allows you to write::\n>>> from numpy.linalg import multi_dot\n>>> # Prepare some data\n>>> A = np.random.random((10000, 100))\n>>> B = np.random.random((100, 1000))\n>>> C = np.random.random((1000, 5))\n>>> D = np.random.random((5, 333))\n>>> # the actual dot multiplication\n>>> _ = multi_dot([A, B, C, D])\ninstead of::\n>>> _ = np.dot(np.dot(np.dot(A, B), C), D)\n>>> # or\n>>> _ = A.dot(B).dot(C).dot(D)"
      }
    },
    {
      "name": "norm",
      "signature": "norm(x, ord=None, axis=None, keepdims=False)",
      "documentation": {
        "description": "Matrix or vector norm.\nThis function is able to return one of eight different matrix norms,\nor one of an infinite number of vector norms (described below), depending\non the value of the ``ord`` parameter.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": "Input array.  If `axis` is None, `x` must be 1-D or 2-D, unless `ord`\nis None. If both `axis` and `ord` are None, the 2-norm of\n``x.ravel`` will be returned."
          },
          "ord": {
            "type": "{non-zero int, inf, -inf, 'fro', 'nuc'}, optional",
            "description": "Order of the norm (see table under ``Notes``). inf means numpy's\n`inf` object. The default is None."
          },
          "axis": {
            "type": "{None, int, 2-tuple of ints}, optional.",
            "description": "If `axis` is an integer, it specifies the axis of `x` along which to\ncompute the vector norms.  If `axis` is a 2-tuple, it specifies the\naxes that hold 2-D matrices, and the matrix norms of these matrices\nare computed.  If `axis` is None then either a vector norm (when `x`\nis 1-D) or a matrix norm (when `x` is 2-D) is returned. The default\nis None.\n.. versionadded:: 1.8.0"
          },
          "keepdims": {
            "type": "bool, optional",
            "description": "If this is set to True, the axes which are normed over are left in the\nresult as dimensions with size one.  With this option the result will\nbroadcast correctly against the original `x`.\n.. versionadded:: 1.10.0"
          }
        },
        "returns": "-------\nn : float or ndarray\nNorm of the matrix or vector(s).",
        "raises": "",
        "see_also": "--------\nscipy.linalg.norm : Similar function in SciPy.",
        "notes": "-----\nFor values of ``ord < 1``, the result is, strictly speaking, not a\nmathematical 'norm', but it may still be useful for various numerical\npurposes.\nThe following norms can be calculated:\n=====  ============================  ==========================\nord    norm for matrices             norm for vectors\n=====  ============================  ==========================\nNone   Frobenius norm                2-norm\n'fro'  Frobenius norm                --\n'nuc'  nuclear norm                  --\ninf    max(sum(abs(x), axis=1))      max(abs(x))\n-inf   min(sum(abs(x), axis=1))      min(abs(x))\n0      --                            sum(x != 0)\n1      max(sum(abs(x), axis=0))      as below\n-1     min(sum(abs(x), axis=0))      as below\n2      2-norm (largest sing. value)  as below\n-2     smallest singular value       as below\nother  --                            sum(abs(x)**ord)**(1./ord)\n=====  ============================  ==========================\nThe Frobenius norm is given by [1]_:\n:math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`\nThe nuclear norm is the sum of the singular values.\nBoth the Frobenius and nuclear norm orders are only defined for\nmatrices and raise a ValueError when ``x.ndim != 2``.\nReferences\n----------\n.. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,\nBaltimore, MD, Johns Hopkins University Press, 1985, pg. 15",
        "examples": "--------\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) - 4\n>>> a\narray([-4, -3, -2, ...,  2,  3,  4])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[-4, -3, -2],\n[-1,  0,  1],\n[ 2,  3,  4]])\n>>> LA.norm(a)\n7.745966692414834\n>>> LA.norm(b)\n7.745966692414834\n>>> LA.norm(b, 'fro')\n7.745966692414834\n>>> LA.norm(a, np.inf)\n4.0\n>>> LA.norm(b, np.inf)\n9.0\n>>> LA.norm(a, -np.inf)\n0.0\n>>> LA.norm(b, -np.inf)\n2.0\n>>> LA.norm(a, 1)\n20.0\n>>> LA.norm(b, 1)\n7.0\n>>> LA.norm(a, -1)\n-4.6566128774142013e-010\n>>> LA.norm(b, -1)\n6.0\n>>> LA.norm(a, 2)\n7.745966692414834\n>>> LA.norm(b, 2)\n7.3484692283495345\n>>> LA.norm(a, -2)\n0.0\n>>> LA.norm(b, -2)\n1.8570331885190563e-016 # may vary\n>>> LA.norm(a, 3)\n5.8480354764257312 # may vary\n>>> LA.norm(a, -3)\n0.0\nUsing the `axis` argument to compute vector norms:\n>>> c = np.array([[ 1, 2, 3],\n...               [-1, 1, 4]])\n>>> LA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\n>>> LA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\n>>> LA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the `axis` argument to compute matrix norms:\n>>> m = np.arange(8).reshape(2,2,2)\n>>> LA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\n>>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)"
      }
    },
    {
      "name": "pinv",
      "signature": "pinv(a, rcond=1e-15, hermitian=False)",
      "documentation": {
        "description": "Compute the (Moore-Penrose) pseudo-inverse of a matrix.\nCalculate the generalized inverse of a matrix using its\nsingular-value decomposition (SVD) and including all\n*large* singular values.\n.. versionchanged:: 1.14\nCan now operate on stacks of matrices",
        "parameters": {
          "a": {
            "type": "(..., M, N) array_like",
            "description": "Matrix or stack of matrices to be pseudo-inverted."
          },
          "rcond": {
            "type": "(...) array_like of float",
            "description": "Cutoff for small singular values.\nSingular values less than or equal to\n``rcond * largest_singular_value`` are set to zero.\nBroadcasts against the stack of matrices."
          },
          "hermitian": {
            "type": "bool, optional",
            "description": "If True, `a` is assumed to be Hermitian (symmetric if real-valued),\nenabling a more efficient method for finding singular values.\nDefaults to False.\n.. versionadded:: 1.17.0"
          }
        },
        "returns": "-------\nB : (..., N, M) ndarray\nThe pseudo-inverse of `a`. If `a` is a `matrix` instance, then so\nis `B`.",
        "raises": "------\nLinAlgError\nIf the SVD computation does not converge.",
        "see_also": "--------\nscipy.linalg.pinv : Similar function in SciPy.\nscipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a\nHermitian matrix.",
        "notes": "-----\nThe pseudo-inverse of a matrix A, denoted :math:`A^+`, is\ndefined as: \"the matrix that 'solves' [the least-squares problem]\n:math:`Ax = b`,\" i.e., if :math:`\\bar{x}` is said solution, then\n:math:`A^+` is that matrix such that :math:`\\bar{x} = A^+b`.\nIt can be shown that if :math:`Q_1 \\Sigma Q_2^T = A` is the singular\nvalue decomposition of A, then\n:math:`A^+ = Q_2 \\Sigma^+ Q_1^T`, where :math:`Q_{1,2}` are\northogonal matrices, :math:`\\Sigma` is a diagonal matrix consisting\nof A's so-called singular values, (followed, typically, by\nzeros), and then :math:`\\Sigma^+` is simply the diagonal matrix\nconsisting of the reciprocals of A's singular values\n(again, followed by zeros). [1]_\nReferences\n----------\n.. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,\nFL, Academic Press, Inc., 1980, pp. 139-142.",
        "examples": "--------\nThe following example checks that ``a * a+ * a == a`` and\n``a+ * a * a+ == a+``:\n>>> a = np.random.randn(9, 6)\n>>> B = np.linalg.pinv(a)\n>>> np.allclose(a, np.dot(a, np.dot(B, a)))\nTrue\n>>> np.allclose(B, np.dot(B, np.dot(a, B)))\nTrue"
      }
    },
    {
      "name": "qr",
      "signature": "qr(a, mode='reduced')",
      "documentation": {
        "description": "Compute the qr factorization of a matrix.\nFactor the matrix `a` as *qr*, where `q` is orthonormal and `r` is\nupper-triangular.",
        "parameters": {
          "a": {
            "type": "array_like, shape (..., M, N)",
            "description": "An array-like object with the dimensionality of at least 2."
          },
          "mode": {
            "type": "{'reduced', 'complete', 'r', 'raw'}, optional",
            "description": "If K = min(M, N), then\n* 'reduced'  : returns Q, R with dimensions (..., M, K), (..., K, N) (default)\n* 'complete' : returns Q, R with dimensions (..., M, M), (..., M, N)\n* 'r'        : returns R only with dimensions (..., K, N)\n* 'raw'      : returns h, tau with dimensions (..., N, M), (..., K,)\nThe options 'reduced', 'complete, and 'raw' are new in numpy 1.8,\nsee the notes for more information. The default is 'reduced', and to\nmaintain backward compatibility with earlier versions of numpy both\nit and the old default 'full' can be omitted. Note that array h\nreturned in 'raw' mode is transposed for calling Fortran. The\n'economic' mode is deprecated.  The modes 'full' and 'economic' may\nbe passed using only the first letter for backwards compatibility,\nbut all others must be spelled out. See the Notes for more\nexplanation."
          }
        },
        "returns": "-------\nWhen mode is 'reduced' or 'complete', the result will be a namedtuple with\nthe attributes `Q` and `R`.\nQ : ndarray of float or complex, optional\nA matrix with orthonormal columns. When mode = 'complete' the\nresult is an orthogonal/unitary matrix depending on whether or not\na is real/complex. The determinant may be either +/- 1 in that\ncase. In case the number of dimensions in the input array is\ngreater than 2 then a stack of the matrices with above properties\nis returned.\nR : ndarray of float or complex, optional\nThe upper-triangular matrix or a stack of upper-triangular\nmatrices if the number of dimensions in the input array is greater\nthan 2.\n(h, tau) : ndarrays of np.double or np.cdouble, optional\nThe array h contains the Householder reflectors that generate q\nalong with r. The tau array contains scaling factors for the\nreflectors. In the deprecated  'economic' mode only h is returned.",
        "raises": "------\nLinAlgError\nIf factoring fails.",
        "see_also": "--------\nscipy.linalg.qr : Similar function in SciPy.\nscipy.linalg.rq : Compute RQ decomposition of a matrix.",
        "notes": "-----\nThis is an interface to the LAPACK routines ``dgeqrf``, ``zgeqrf``,\n``dorgqr``, and ``zungqr``.\nFor more information on the qr factorization, see for example:\nhttps://en.wikipedia.org/wiki/QR_factorization\nSubclasses of `ndarray` are preserved except for the 'raw' mode. So if\n`a` is of type `matrix`, all the return values will be matrices too.\nNew 'reduced', 'complete', and 'raw' options for mode were added in\nNumPy 1.8.0 and the old option 'full' was made an alias of 'reduced'.  In\naddition the options 'full' and 'economic' were deprecated.  Because\n'full' was the previous default and 'reduced' is the new default,\nbackward compatibility can be maintained by letting `mode` default.\nThe 'raw' option was added so that LAPACK routines that can multiply\narrays by q using the Householder reflectors can be used. Note that in\nthis case the returned arrays are of type np.double or np.cdouble and\nthe h array is transposed to be FORTRAN compatible.  No routines using\nthe 'raw' return are currently exposed by numpy, but some are available\nin lapack_lite and just await the necessary work.",
        "examples": "--------\n>>> a = np.random.randn(9, 6)\n>>> Q, R = np.linalg.qr(a)\n>>> np.allclose(a, np.dot(Q, R))  # a does equal QR\nTrue\n>>> R2 = np.linalg.qr(a, mode='r')\n>>> np.allclose(R, R2)  # mode='r' returns the same R as mode='full'\nTrue\n>>> a = np.random.normal(size=(3, 2, 2)) # Stack of 2 x 2 matrices as input\n>>> Q, R = np.linalg.qr(a)\n>>> Q.shape\n(3, 2, 2)\n>>> R.shape\n(3, 2, 2)\n>>> np.allclose(a, np.matmul(Q, R))\nTrue\nExample illustrating a common use of `qr`: solving of least squares\nproblems\nWhat are the least-squares-best `m` and `y0` in ``y = y0 + mx`` for\nthe following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points\nand you'll see that it should be y0 = 0, m = 1.)  The answer is provided\nby solving the over-determined matrix equation ``Ax = b``, where::\nA = array([[0, 1], [1, 1], [1, 1], [2, 1]])\nx = array([[y0], [m]])\nb = array([[1], [0], [2], [1]])\nIf A = QR such that Q is orthonormal (which is always possible via\nGram-Schmidt), then ``x = inv(R) * (Q.T) * b``.  (In numpy practice,\nhowever, we simply use `lstsq`.)\n>>> A = np.array([[0, 1], [1, 1], [1, 1], [2, 1]])\n>>> A\narray([[0, 1],\n[1, 1],\n[1, 1],\n[2, 1]])\n>>> b = np.array([1, 2, 2, 3])\n>>> Q, R = np.linalg.qr(A)\n>>> p = np.dot(Q.T, b)\n>>> np.dot(np.linalg.inv(R), p)\narray([  1.,   1.])"
      }
    },
    {
      "name": "slogdet",
      "signature": "slogdet(a)",
      "documentation": {
        "description": "Compute the sign and (natural) logarithm of the determinant of an array.\nIf an array has a very small or very large determinant, then a call to\n`det` may overflow or underflow. This routine is more robust against such\nissues, because it computes the logarithm of the determinant rather than\nthe determinant itself.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Input array, has to be a square 2-D array."
          }
        },
        "returns": "-------\nA namedtuple with the following attributes:\nsign : (...) array_like\nA number representing the sign of the determinant. For a real matrix,\nthis is 1, 0, or -1. For a complex matrix, this is a complex number\nwith absolute value 1 (i.e., it is on the unit circle), or else 0.\nlogabsdet : (...) array_like\nThe natural log of the absolute value of the determinant.\nIf the determinant is zero, then `sign` will be 0 and `logabsdet` will be\n-Inf. In all cases, the determinant is equal to ``sign * np.exp(logabsdet)``.",
        "raises": "",
        "see_also": "--------\ndet",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\n.. versionadded:: 1.6.0\nThe determinant is computed via LU factorization using the LAPACK\nroutine ``z/dgetrf``.",
        "examples": "--------\nThe determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:\n>>> a = np.array([[1, 2], [3, 4]])\n>>> (sign, logabsdet) = np.linalg.slogdet(a)\n>>> (sign, logabsdet)\n(-1, 0.69314718055994529) # may vary\n>>> sign * np.exp(logabsdet)\n-2.0\nComputing log-determinants for a stack of matrices:\n>>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n>>> a.shape\n(3, 2, 2)\n>>> sign, logabsdet = np.linalg.slogdet(a)\n>>> (sign, logabsdet)\n(array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))\n>>> sign * np.exp(logabsdet)\narray([-2., -3., -8.])\nThis routine succeeds where ordinary `det` does not:\n>>> np.linalg.det(np.eye(500) * 0.1)\n0.0\n>>> np.linalg.slogdet(np.eye(500) * 0.1)\n(1, -1151.2925464970228)"
      }
    },
    {
      "name": "solve",
      "signature": "solve(a, b)",
      "documentation": {
        "description": "Solve a linear matrix equation, or system of linear scalar equations.\nComputes the \"exact\" solution, `x`, of the well-determined, i.e., full\nrank, linear matrix equation `ax = b`.",
        "parameters": {
          "a": {
            "type": "(..., M, M) array_like",
            "description": "Coefficient matrix."
          },
          "b": {
            "type": "{(..., M,), (..., M, K)}, array_like",
            "description": "Ordinate or \"dependent variable\" values."
          }
        },
        "returns": "-------\nx : {(..., M,), (..., M, K)} ndarray\nSolution to the system a x = b.  Returned shape is identical to `b`.",
        "raises": "------\nLinAlgError\nIf `a` is singular or not square.",
        "see_also": "--------\nscipy.linalg.solve : Similar function in SciPy.",
        "notes": "-----\n.. versionadded:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe solutions are computed using LAPACK routine ``_gesv``.\n`a` must be square and of full-rank, i.e., all rows (or, equivalently,\ncolumns) must be linearly independent; if either is not true, use\n`lstsq` for the least-squares best \"solution\" of the\nsystem/equation.\nReferences\n----------\n.. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,\nFL, Academic Press, Inc., 1980, pg. 22.",
        "examples": "--------\nSolve the system of equations ``x0 + 2 * x1 = 1`` and ``3 * x0 + 5 * x1 = 2``:\n>>> a = np.array([[1, 2], [3, 5]])\n>>> b = np.array([1, 2])\n>>> x = np.linalg.solve(a, b)\n>>> x\narray([-1.,  1.])\nCheck that the solution is correct:\n>>> np.allclose(np.dot(a, x), b)\nTrue"
      }
    },
    {
      "name": "svd",
      "signature": "svd(a, full_matrices=True, compute_uv=True, hermitian=False)",
      "documentation": {
        "description": "Singular Value Decomposition.\nWhen `a` is a 2D array, and ``full_matrices=False``, then it is\nfactorized as ``u @ np.diag(s) @ vh = (u * s) @ vh``, where\n`u` and the Hermitian transpose of `vh` are 2D arrays with\northonormal columns and `s` is a 1D array of `a`'s singular\nvalues. When `a` is higher-dimensional, SVD is applied in\nstacked mode as explained below.",
        "parameters": {
          "a": {
            "type": "(..., M, N) array_like",
            "description": "A real or complex array with ``a.ndim >= 2``."
          },
          "full_matrices": {
            "type": "bool, optional",
            "description": "If True (default), `u` and `vh` have the shapes ``(..., M, M)`` and\n``(..., N, N)``, respectively.  Otherwise, the shapes are\n``(..., M, K)`` and ``(..., K, N)``, respectively, where\n``K = min(M, N)``."
          },
          "compute_uv": {
            "type": "bool, optional",
            "description": "Whether or not to compute `u` and `vh` in addition to `s`.  True\nby default."
          },
          "hermitian": {
            "type": "bool, optional",
            "description": "If True, `a` is assumed to be Hermitian (symmetric if real-valued),\nenabling a more efficient method for finding singular values.\nDefaults to False.\n.. versionadded:: 1.17.0"
          }
        },
        "returns": "-------\nWhen `compute_uv` is True, the result is a namedtuple with the following\nattribute names:\nU : { (..., M, M), (..., M, K) } array\nUnitary array(s). The first ``a.ndim - 2`` dimensions have the same\nsize as those of the input `a`. The size of the last two dimensions\ndepends on the value of `full_matrices`. Only returned when\n`compute_uv` is True.\nS : (..., K) array\nVector(s) with the singular values, within each vector sorted in\ndescending order. The first ``a.ndim - 2`` dimensions have the same\nsize as those of the input `a`.\nVh : { (..., N, N), (..., K, N) } array\nUnitary array(s). The first ``a.ndim - 2`` dimensions have the same\nsize as those of the input `a`. The size of the last two dimensions\ndepends on the value of `full_matrices`. Only returned when\n`compute_uv` is True.",
        "raises": "------\nLinAlgError\nIf SVD computation does not converge.",
        "see_also": "--------\nscipy.linalg.svd : Similar function in SciPy.\nscipy.linalg.svdvals : Compute singular values of a matrix.",
        "notes": "-----\n.. versionchanged:: 1.8.0\nBroadcasting rules apply, see the `numpy.linalg` documentation for\ndetails.\nThe decomposition is performed using LAPACK routine ``_gesdd``.\nSVD is usually described for the factorization of a 2D matrix :math:`A`.\nThe higher-dimensional case will be discussed below. In the 2D case, SVD is\nwritten as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,\n:math:`S= \\mathtt{np.diag}(s)` and :math:`V^H = vh`. The 1D array `s`\ncontains the singular values of `a` and `u` and `vh` are unitary. The rows\nof `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are\nthe eigenvectors of :math:`A A^H`. In both cases the corresponding\n(possibly non-zero) eigenvalues are given by ``s**2``.\nIf `a` has more than two dimensions, then broadcasting rules apply, as\nexplained in :ref:`routines.linalg-broadcasting`. This means that SVD is\nworking in \"stacked\" mode: it iterates over all indices of the first\n``a.ndim - 2`` dimensions and for each combination SVD is applied to the\nlast two indices. The matrix `a` can be reconstructed from the\ndecomposition with either ``(u * s[..., None, :]) @ vh`` or\n``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the\nfunction ``np.matmul`` for python versions below 3.5.)\nIf `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are\nall the return values.",
        "examples": "--------\n>>> a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)\n>>> b = np.random.randn(2, 7, 8, 3) + 1j*np.random.randn(2, 7, 8, 3)\nReconstruction based on full SVD, 2D case:\n>>> U, S, Vh = np.linalg.svd(a, full_matrices=True)\n>>> U.shape, S.shape, Vh.shape\n((9, 9), (6,), (6, 6))\n>>> np.allclose(a, np.dot(U[:, :6] * S, Vh))\nTrue\n>>> smat = np.zeros((9, 6), dtype=complex)\n>>> smat[:6, :6] = np.diag(S)\n>>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\nTrue\nReconstruction based on reduced SVD, 2D case:\n>>> U, S, Vh = np.linalg.svd(a, full_matrices=False)\n>>> U.shape, S.shape, Vh.shape\n((9, 6), (6,), (6, 6))\n>>> np.allclose(a, np.dot(U * S, Vh))\nTrue\n>>> smat = np.diag(S)\n>>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\nTrue\nReconstruction based on full SVD, 4D case:\n>>> U, S, Vh = np.linalg.svd(b, full_matrices=True)\n>>> U.shape, S.shape, Vh.shape\n((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))\n>>> np.allclose(b, np.matmul(U[..., :3] * S[..., None, :], Vh))\nTrue\n>>> np.allclose(b, np.matmul(U[..., :3], S[..., None] * Vh))\nTrue\nReconstruction based on reduced SVD, 4D case:\n>>> U, S, Vh = np.linalg.svd(b, full_matrices=False)\n>>> U.shape, S.shape, Vh.shape\n((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))\n>>> np.allclose(b, np.matmul(U * S[..., None, :], Vh))\nTrue\n>>> np.allclose(b, np.matmul(U, S[..., None] * Vh))\nTrue"
      }
    },
    {
      "name": "tensorinv",
      "signature": "tensorinv(a, ind=2)",
      "documentation": {
        "description": "Compute the 'inverse' of an N-dimensional array.\nThe result is an inverse for `a` relative to the tensordot operation\n``tensordot(a, b, ind)``, i. e., up to floating-point accuracy,\n``tensordot(tensorinv(a), a, ind)`` is the \"identity\" tensor for the\ntensordot operation.",
        "parameters": {
          "a": {
            "type": "array_like",
            "description": "Tensor to 'invert'. Its shape must be 'square', i. e.,\n``prod(a.shape[:ind]) == prod(a.shape[ind:])``."
          },
          "ind": {
            "type": "int, optional",
            "description": "Number of first indices that are involved in the inverse sum.\nMust be a positive integer, default is 2."
          }
        },
        "returns": "-------\nb : ndarray\n`a`'s tensordot inverse, shape ``a.shape[ind:] + a.shape[:ind]``.",
        "raises": "------\nLinAlgError\nIf `a` is singular or not 'square' (in the above sense).",
        "see_also": "--------\nnumpy.tensordot, tensorsolve",
        "notes": "",
        "examples": "--------\n>>> a = np.eye(4*6)\n>>> a.shape = (4, 6, 8, 3)\n>>> ainv = np.linalg.tensorinv(a, ind=2)\n>>> ainv.shape\n(8, 3, 4, 6)\n>>> b = np.random.randn(4, 6)\n>>> np.allclose(np.tensordot(ainv, b), np.linalg.tensorsolve(a, b))\nTrue\n>>> a = np.eye(4*6)\n>>> a.shape = (24, 8, 3)\n>>> ainv = np.linalg.tensorinv(a, ind=1)\n>>> ainv.shape\n(8, 3, 24)\n>>> b = np.random.randn(24)\n>>> np.allclose(np.tensordot(ainv, b, 1), np.linalg.tensorsolve(a, b))\nTrue"
      }
    },
    {
      "name": "tensorsolve",
      "signature": "tensorsolve(a, b, axes=None)",
      "documentation": {
        "description": "Solve the tensor equation ``a x = b`` for x.\nIt is assumed that all indices of `x` are summed over in the product,\ntogether with the rightmost indices of `a`, as is done in, for example,\n``tensordot(a, x, axes=x.ndim)``.",
        "parameters": {
          "a": {
            "type": "array_like",
            "description": "Coefficient tensor, of shape ``b.shape + Q``. `Q`, a tuple, equals\nthe shape of that sub-tensor of `a` consisting of the appropriate\nnumber of its rightmost indices, and must be such that\n``prod(Q) == prod(b.shape)`` (in which sense `a` is said to be\n'square')."
          },
          "b": {
            "type": "array_like",
            "description": "Right-hand tensor, which can be of any shape."
          },
          "axes": {
            "type": "tuple of ints, optional",
            "description": "Axes in `a` to reorder to the right, before inversion.\nIf None (default), no reordering is done."
          }
        },
        "returns": "-------\nx : ndarray, shape Q",
        "raises": "------\nLinAlgError\nIf `a` is singular or not 'square' (in the above sense).",
        "see_also": "--------\nnumpy.tensordot, tensorinv, numpy.einsum",
        "notes": "",
        "examples": "--------\n>>> a = np.eye(2*3*4)\n>>> a.shape = (2*3, 4, 2, 3, 4)\n>>> b = np.random.randn(2*3, 4)\n>>> x = np.linalg.tensorsolve(a, b)\n>>> x.shape\n(2, 3, 4)\n>>> np.allclose(np.tensordot(a, x, axes=3), b)\nTrue"
      }
    }
  ],
  "classes": [
    {
      "name": "LinAlgError",
      "documentation": {
        "description": "Generic Python-exception-derived object raised by linalg functions.\nGeneral purpose exception class, derived from Python's ValueError\nclass, programmatically raised in linalg functions when a Linear\nAlgebra-related condition would prevent further correct execution of the\nfunction.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n>>> from numpy import linalg as LA\n>>> LA.inv(np.zeros((2,2)))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"...linalg.py\", line 350,\nin inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))\nFile \"...linalg.py\", line 249,\nin solve\nraise LinAlgError('Singular matrix')\nnumpy.linalg.LinAlgError: Singular matrix"
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "documentation": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "documentation": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}