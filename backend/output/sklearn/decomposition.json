{
  "description": "Matrix decomposition algorithms.\n\nThese include PCA, NMF, ICA, and more. Most of the algorithms of this module can be\nregarded as dimensionality reduction techniques.",
  "functions": [
    {
      "name": "dict_learning",
      "signature": "dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-08, method='lars', n_jobs=None, dict_init=None, code_init=None, callback=None, verbose=False, random_state=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000)",
      "documentation": {
        "description": "Solve a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int\n        Number of dictionary atoms to extract.\n\n    alpha : int or float\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        The method used:\n\n        * `'lars'`: uses the least angle regression method to solve the lasso\n           problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the sparse code for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "matrix.",
            "description": ""
          },
          "n_components": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of iterations run. Returned only if `return_n_iter` is",
            "description": ""
          },
          "alpha": {
            "type": "int or float",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform.",
            "description": ".. versionadded:: 0.22\nReturns\n-------"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "8"
          },
          "Tolerance": {
            "type": "for the stopping condition.",
            "description": ""
          },
          "method": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": ""
          },
          "The": {
            "type": "dictionary factor in the matrix factorization.",
            "description": ""
          },
          "problem": {
            "type": "online.",
            "description": ""
          },
          "Lasso": {
            "type": "solution (`linear_model.Lasso`). Lars will be faster if",
            "description": ""
          },
          "the": {
            "type": "original signal:",
            "description": ">>> X_hat = U @ V\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\nnp.float64(0.01...)"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "dict_init": {
            "type": "ndarray of shape (n_components, n_features), default=None",
            "description": ""
          },
          "Initial": {
            "type": "value for the sparse code for warm restart scenarios. Only used",
            "description": ""
          },
          "if": {
            "type": "`code_init` and `dict_init` are not None.",
            "description": ""
          },
          "code_init": {
            "type": "ndarray of shape (n_samples, n_components), default=None",
            "description": ""
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "Callable": {
            "type": "that gets invoked every five iterations.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "To": {
            "type": "control the verbosity of the procedure.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for randomly initializing the dictionary. Pass an int for",
            "description": ""
          },
          "reproducible": {
            "type": "results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the code.",
            "description": ".. versionadded:: 0.20"
          },
          "positive_dict": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive_code": {
            "type": "bool, default=False",
            "description": ""
          },
          "method_max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "code": {
            "type": "ndarray of shape (n_samples, n_components)",
            "description": ""
          },
          "dictionary": {
            "type": "ndarray of shape (n_components, n_features),",
            "description": ""
          },
          "errors": {
            "type": "array",
            "description": ""
          },
          "Vector": {
            "type": "of errors at each iteration.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "set": {
            "type": "to True.",
            "description": ""
          },
          "dict_learning_online": {
            "type": "Solve a dictionary learning matrix factorization",
            "description": ""
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "MiniBatchDictionaryLearning": {
            "type": "A faster, less accurate version",
            "description": ""
          },
          "of": {
            "type": "the dictionary learning algorithm.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import dict_learning\n>>> X, _, _ = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)"
          },
          "We": {
            "type": "can compare the average squared euclidean norm of the reconstruction",
            "description": ""
          },
          "error": {
            "type": "of the sparse coded signal relative to the squared euclidean norm of",
            "description": ""
          }
        },
        "returns": "-------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.6...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.01...)",
        "raises": "",
        "see_also": "--------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.6...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.01...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.6...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.01...)"
      }
    },
    {
      "name": "dict_learning_online",
      "signature": "dict_learning_online(X, n_components=2, *, alpha=1, max_iter=100, return_code=True, dict_init=None, callback=None, batch_size=256, verbose=False, shuffle=True, n_jobs=None, method='lars', random_state=None, positive_dict=False, positive_code=False, method_max_iter=1000, tol=0.001, max_no_improvement=10)",
      "documentation": {
        "description": "Solve a dictionary learning matrix factorization problem online.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n    This is accomplished by repeatedly iterating over mini-batches by slicing\n    the input data.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int or None, default=2\n        Number of dictionary atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n    return_code : bool, default=True\n        Whether to also return the code U or just the dictionary `V`.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the dictionary for warm restart scenarios.\n        If `None`, the initial values for the dictionary are created\n        with an SVD decomposition of the data via\n        :func:`~sklearn.utils.extmath.randomized_svd`.\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n    batch_size : int, default=256\n        The number of samples to take in each batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform when solving the lasso problem.\n\n        .. versionadded:: 0.22\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components),\n        The sparse code (only returned if `return_code=True`).\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The solutions to the dictionary learning problem.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to `True`.\n\n    See Also\n    --------\n    dict_learning : Solve a dictionary learning matrix factorization problem.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\n        learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "matrix.",
            "description": ""
          },
          "n_components": {
            "type": "int or None, default=2",
            "description": ""
          },
          "Number": {
            "type": "of iterations run. Returned only if `return_n_iter` is",
            "description": ""
          },
          "is": {
            "type": "set to ``n_features``.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform when solving the lasso problem.",
            "description": ".. versionadded:: 0.22"
          },
          "stopping": {
            "type": "independently of any early stopping criterion heuristics.",
            "description": ".. versionadded:: 1.1"
          },
          "return_code": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the code.",
            "description": ".. versionadded:: 0.20"
          },
          "dict_init": {
            "type": "ndarray of shape (n_components, n_features), default=None",
            "description": ""
          },
          "Initial": {
            "type": "values for the dictionary for warm restart scenarios.",
            "description": ""
          },
          "If": {
            "type": "`None`, the initial values for the dictionary are created",
            "description": ""
          },
          "with": {
            "type": "an SVD decomposition of the data via",
            "description": ":func:`~sklearn.utils.extmath.randomized_svd`."
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "A": {
            "type": "callable that gets invoked at the end of each iteration.",
            "description": ""
          },
          "batch_size": {
            "type": "int, default=256",
            "description": ""
          },
          "The": {
            "type": "solutions to the dictionary learning problem.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "To": {
            "type": "disable convergence detection based on cost function, set",
            "description": "`max_no_improvement` to None.\n.. versionadded:: 1.1\nReturns\n-------"
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "method": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": "* `'lars'`: uses the least angle regression method to solve the lasso"
          },
          "problem": {
            "type": "`linear_model.lars_path`",
            "description": ";\n* `'cd'`: uses the coordinate descent method to compute the"
          },
          "Lasso": {
            "type": "solution (`linear_model.Lasso`). Lars will be faster if",
            "description": ""
          },
          "the": {
            "type": "original signal:",
            "description": ">>> X_hat = U @ V\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\nnp.float64(0.05...)"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for initializing the dictionary when ``dict_init`` is not",
            "description": "specified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "positive_dict": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive_code": {
            "type": "bool, default=False",
            "description": ""
          },
          "method_max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Control": {
            "type": "early stopping based on the consecutive number of mini batches",
            "description": ""
          },
          "dictionary": {
            "type": "ndarray of shape (n_components, n_features),",
            "description": ""
          },
          "max_no_improvement": {
            "type": "int, default=10",
            "description": ""
          },
          "that": {
            "type": "does not yield an improvement on the smoothed cost function.",
            "description": ""
          },
          "code": {
            "type": "ndarray of shape (n_samples, n_components),",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "set": {
            "type": "to `True`.",
            "description": ""
          },
          "dict_learning": {
            "type": "Solve a dictionary learning matrix factorization problem.",
            "description": ""
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "MiniBatchDictionaryLearning": {
            "type": "A faster, less accurate, version of the dictionary",
            "description": ""
          },
          "learning": {
            "type": "algorithm.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import dict_learning_online\n>>> X, _, _ = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> U, V = dict_learning_online(\n...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\n... )"
          },
          "We": {
            "type": "can compare the average squared euclidean norm of the reconstruction",
            "description": ""
          },
          "error": {
            "type": "of the sparse coded signal relative to the squared euclidean norm of",
            "description": ""
          }
        },
        "returns": "-------\n    code : ndarray of shape (n_samples, n_components),\n        The sparse code (only returned if `return_code=True`).\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The solutions to the dictionary learning problem.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to `True`.\n\n    See Also\n    --------\n    dict_learning : Solve a dictionary learning matrix factorization problem.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\n        learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning_online\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V = dict_learning_online(\n    ...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\n    ... )\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.53...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)",
        "raises": "",
        "see_also": "--------\n    dict_learning : Solve a dictionary learning matrix factorization problem.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\n        learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning_online\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V = dict_learning_online(\n    ...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\n    ... )\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.53...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning_online\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V = dict_learning_online(\n    ...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\n    ... )\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.53...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)"
      }
    },
    {
      "name": "fastica",
      "signature": "fastica(X, n_components=None, *, algorithm='parallel', whiten='unit-variance', fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, whiten_solver='svd', random_state=None, return_X_mean=False, compute_sources=True, return_n_iter=False)",
      "documentation": {
        "description": "Perform Fast Independent Component Analysis.\n\n    The implementation is based on [1]_.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    n_components : int, default=None\n        Number of components to use. If None is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Specify which algorithm to use for FastICA.\n\n    whiten : str or bool, default='unit-variance'\n        Specify the whitening strategy to use.\n\n        - If 'arbitrary-variance', a whitening with variance\n          arbitrary is used.\n        - If 'unit-variance', the whitening matrix is rescaled to ensure that\n          each recovered source has unit variance.\n        - If False, the data is already considered to be whitened, and no\n          whitening is performed.\n\n        .. versionchanged:: 1.3\n            The default value of `whiten` changed to 'unit-variance' in 1.3.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Training": {
            "type": "vector, where `n_samples` is the number of samples and",
            "description": "`n_features` is the number of features."
          },
          "n_components": {
            "type": "< n_features. It this case K is not a square matrix",
            "description": ""
          },
          "Number": {
            "type": "of components to use. If None is passed, all are used.",
            "description": ""
          },
          "algorithm": {
            "type": "{'parallel', 'deflation'}, default='parallel'",
            "description": ""
          },
          "Specify": {
            "type": "the whitening strategy to use.",
            "description": "- If 'arbitrary-variance', a whitening with variance"
          },
          "whiten": {
            "type": "str or bool, default='unit",
            "description": "variance'"
          },
          "arbitrary": {
            "type": "is used.",
            "description": "- If 'unit-variance', the whitening matrix is rescaled to ensure that"
          },
          "each": {
            "type": "recovered source has unit variance.",
            "description": "- If False, the data is already considered to be whitened, and no"
          },
          "whitening": {
            "type": "is performed.",
            "description": ".. versionchanged:: 1.3"
          },
          "The": {
            "type": "data matrix X is considered to be a linear combination of",
            "description": "non-Gaussian (independent) components i.e. X = AS where columns of S"
          },
          "fun": {
            "type": "{'logcosh', 'exp', 'cube'} or callable, default='logcosh'",
            "description": ""
          },
          "approximation": {
            "type": "to neg-entropy. Could be either 'logcosh', 'exp',",
            "description": ""
          },
          "or": {
            "type": "'cube'.",
            "description": ""
          },
          "You": {
            "type": "can also provide your own function. It should return a tuple",
            "description": ""
          },
          "containing": {
            "type": "the value of the function, and of its derivative, in the",
            "description": "point. The derivative should be averaged along its last dimension."
          },
          "Example": {
            "type": ":",
            "description": ""
          },
          "def": {
            "type": "my_g(x):",
            "description": ""
          },
          "return": {
            "type": "x ** 3, (3 * x ** 2).mean(axis=-1)",
            "description": ""
          },
          "fun_args": {
            "type": "dict, default=None",
            "description": ""
          },
          "Arguments": {
            "type": "to send to the functional form.",
            "description": ""
          },
          "If": {
            "type": "the algorithm is \"deflation\", n_iter is the",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "A": {
            "type": "positive scalar giving the tolerance at which the",
            "description": "un-mixing matrix is considered to have converged."
          },
          "w_init": {
            "type": "ndarray of shape (n_components, n_components), default=None",
            "description": ""
          },
          "Initial": {
            "type": "un-mixing array. If `w_init=None`, then an array of values",
            "description": ""
          },
          "drawn": {
            "type": "from a normal distribution is used.",
            "description": ""
          },
          "whiten_solver": {
            "type": "{\"eigh\", \"svd\"}, default=\"svd\"",
            "description": ""
          },
          "often": {
            "type": "faster when `n_samples <= n_features`.",
            "description": "- \"eigh\" is generally more memory efficient when\n`n_samples >= n_features`, and can be faster when\n`n_samples >= 50 * n_features`.\n.. versionadded:: 1.2"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "to initialize ``w_init`` when not specified, with a",
            "description": ""
          },
          "normal": {
            "type": "distribution. Pass an int, for reproducible results",
            "description": ""
          },
          "across": {
            "type": "multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "return_X_mean": {
            "type": "bool, default=False",
            "description": ""
          },
          "compute_sources": {
            "type": "bool, default=True",
            "description": ""
          },
          "This": {
            "type": "implementation was originally made for data of shape",
            "description": "[n_features, n_samples]. Now the input is transposed"
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": "Returns\n-------"
          },
          "K": {
            "type": "is 'None'.",
            "description": ""
          },
          "onto": {
            "type": "the first n_components principal components. If whiten is 'False',",
            "description": ""
          },
          "W": {
            "type": "ndarray of shape (n_components, n_components)",
            "description": ""
          },
          "if": {
            "type": "K is not None, else it is the inverse of W.",
            "description": ""
          },
          "S": {
            "type": "ndarray of shape (n_samples, n_components) or None",
            "description": ""
          },
          "Estimated": {
            "type": "source matrix.",
            "description": ""
          },
          "X_mean": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "maximum": {
            "type": "number of iterations run across all components. Else",
            "description": ""
          },
          "they": {
            "type": "are just the number of iterations taken to converge. This is",
            "description": ""
          },
          "returned": {
            "type": "only when return_n_iter is set to `True`.",
            "description": "Notes\n-----"
          },
          "contain": {
            "type": "the independent components and A is a linear mixing",
            "description": "matrix. In short ICA attempts to `un-mix' the data by estimating an\nun-mixing matrix W where ``S = W K X.``"
          },
          "While": {
            "type": "FastICA was proposed to estimate as many sources",
            "description": ""
          },
          "as": {
            "type": "features, it is possible to estimate less by setting",
            "description": ""
          },
          "and": {
            "type": "the estimated A is the pseudo-inverse of ``W K``.",
            "description": ""
          },
          "before": {
            "type": "the algorithm is applied. This makes it slightly",
            "description": ""
          },
          "faster": {
            "type": "for Fortran-ordered input.",
            "description": "References\n----------\n.. [1] A. Hyvarinen and E. Oja, \"Fast Independent Component Analysis\","
          },
          "Algorithms": {
            "type": "and Applications, Neural Networks, 13(4-5), 2000,",
            "description": "pp. 411-430.\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import fastica\n>>> X, _ = load_digits(return_X_y=True)\n>>> K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')\n>>> K.shape\n(7, 64)\n>>> W.shape\n(7, 7)\n>>> S.shape\n(1797, 7)"
          },
          "to": {
            "type": "send to the functional form.",
            "description": ""
          }
        },
        "returns": "x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : ndarray of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_X_mean : bool, default=False\n        If True, X_mean is returned too.\n\n    compute_sources : bool, default=True\n        If False, sources are not computed, but only the rotation matrix.\n        This can save memory when working with big data. Defaults to True.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    K : ndarray of shape (n_components, n_features) or None\n        If whiten is 'True', K is the pre-whitening matrix that projects data\n        onto the first n_components principal components. If whiten is 'False',\n        K is 'None'.\n\n    W : ndarray of shape (n_components, n_components)\n        The square matrix that unmixes the data after whitening.\n        The mixing matrix is the pseudo-inverse of matrix ``W K``\n        if K is not None, else it is the inverse of W.\n\n    S : ndarray of shape (n_samples, n_components) or None\n        Estimated source matrix.\n\n    X_mean : ndarray of shape (n_features,)\n        The mean over features. Returned only if return_X_mean is True.\n\n    n_iter : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge. This is\n        returned only when return_n_iter is set to `True`.\n\n    Notes\n    -----\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n    While FastICA was proposed to estimate as many sources\n    as features, it is possible to estimate less by setting\n    n_components < n_features. It this case K is not a square matrix\n    and the estimated A is the pseudo-inverse of ``W K``.\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, \"Fast Independent Component Analysis\",\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import fastica\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')\n    >>> K.shape\n    (7, 64)\n    >>> W.shape\n    (7, 7)\n    >>> S.shape\n    (1797, 7)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n    While FastICA was proposed to estimate as many sources\n    as features, it is possible to estimate less by setting\n    n_components < n_features. It this case K is not a square matrix\n    and the estimated A is the pseudo-inverse of ``W K``.\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, \"Fast Independent Component Analysis\",\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import fastica\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')\n    >>> K.shape\n    (7, 64)\n    >>> W.shape\n    (7, 7)\n    >>> S.shape\n    (1797, 7)",
        "examples": ":\n\n            def my_g(x):\n                return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : ndarray of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_X_mean : bool, default=False\n        If True, X_mean is returned too.\n\n    compute_sources : bool, default=True\n        If False, sources are not computed, but only the rotation matrix.\n        This can save memory when working with big data. Defaults to True.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    K : ndarray of shape (n_components, n_features) or None\n        If whiten is 'True', K is the pre-whitening matrix that projects data\n        onto the first n_components principal components. If whiten is 'False',\n        K is 'None'.\n\n    W : ndarray of shape (n_components, n_components)\n        The square matrix that unmixes the data after whitening.\n        The mixing matrix is the pseudo-inverse of matrix ``W K``\n        if K is not None, else it is the inverse of W.\n\n    S : ndarray of shape (n_samples, n_components) or None\n        Estimated source matrix.\n\n    X_mean : ndarray of shape (n_features,)\n        The mean over features. Returned only if return_X_mean is True.\n\n    n_iter : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge. This is\n        returned only when return_n_iter is set to `True`.\n\n    Notes\n    -----\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n    While FastICA was proposed to estimate as many sources\n    as features, it is possible to estimate less by setting\n    n_components < n_features. It this case K is not a square matrix\n    and the estimated A is the pseudo-inverse of ``W K``.\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, \"Fast Independent Component Analysis\",\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import fastica\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')\n    >>> K.shape\n    (7, 64)\n    >>> W.shape\n    (7, 7)\n    >>> S.shape\n    (1797, 7)"
      }
    },
    {
      "name": "non_negative_factorization",
      "signature": "non_negative_factorization(X, W=None, H=None, n_components='auto', *, init=None, update_H=True, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, random_state=None, verbose=0, shuffle=False)",
      "documentation": {
        "description": "Compute Non-negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like of shape (n_samples, n_components), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is initialised as an array of zeros, unless\n        `solver='mu'`, then it is filled with values calculated by\n        `np.sqrt(X.mean() / self._n_components)`.\n        If `None`, uses the initialisation method specified in `init`.\n\n    H : array-like of shape (n_components, n_features), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is used as a constant, to solve for W only.\n        If `None`, uses the initialisation method specified in `init`.\n\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from `W` or `H` shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n\n        Valid options:\n\n        - None: 'nndsvda' if n_components < n_features, otherwise 'random'.\n        - 'random': non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n        - 'custom': If `update_H=True`, use custom matrices W and H which must both\n          be provided. If `update_H=False`, then only custom matrix H is used.\n\n        .. versionchanged:: 0.23\n            The default value of `init` changed from 'random' to None in 0.23.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n          Alternating Least Squares (Fast HALS).\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "Constant": {
            "type": "that multiplies the regularization terms of `H`. Set it to zero to",
            "description": ""
          },
          "W": {
            "type": "ndarray of shape (n_samples, n_components)",
            "description": ""
          },
          "If": {
            "type": "true, randomize the order of coordinates in the CD solver.",
            "description": "Returns\n-------"
          },
          "H": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "n_components": {
            "type": "int or {'auto'} or None, default='auto'",
            "description": ""
          },
          "Number": {
            "type": "of components. If `None`, all features are kept.",
            "description": ""
          },
          "from": {
            "type": "`W` or `H` shapes.",
            "description": ".. versionchanged:: 1.4"
          },
          "Added": {
            "type": "`'auto'` value.",
            "description": ".. versionchanged:: 1.6"
          },
          "Default": {
            "type": "value changed from `None` to `'auto'`.",
            "description": ""
          },
          "init": {
            "type": "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None",
            "description": ""
          },
          "Method": {
            "type": "used to initialize the procedure.",
            "description": ""
          },
          "Valid": {
            "type": "options:",
            "description": "- None: 'nndsvda' if n_components < n_features, otherwise 'random'.\n- 'random': non-negative random matrices, scaled with:\n`sqrt(X.mean() / n_components)`\n- 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)"
          },
          "initialization": {
            "type": "better for sparseness",
            "description": "- 'nndsvda': NNDSVD with zeros filled with the average of X\n(better when sparsity is not desired)\n- 'nndsvdar': NNDSVD with zeros filled with small random values\n(generally faster, less accurate alternative to NNDSVDa"
          },
          "for": {
            "type": "when sparsity is not desired)",
            "description": "- 'custom': If `update_H=True`, use custom matrices W and H which must both"
          },
          "be": {
            "type": "provided. If `update_H=False`, then only custom matrix H is used.",
            "description": ".. versionchanged:: 0.23"
          },
          "The": {
            "type": "verbosity level.",
            "description": ""
          },
          "When": {
            "type": "`init=None` and n_components is less than n_samples and n_features",
            "description": ""
          },
          "defaults": {
            "type": "to `nndsvda` instead of `nndsvd`.",
            "description": ""
          },
          "update_H": {
            "type": "bool, default=True",
            "description": ""
          },
          "Set": {
            "type": "to False, only W will be estimated.",
            "description": ""
          },
          "solver": {
            "type": "{'cd', 'mu'}, default='cd'",
            "description": ""
          },
          "Numerical": {
            "type": "solver to use:",
            "description": "- 'cd' is a Coordinate Descent solver that uses Fast Hierarchical"
          },
          "Alternating": {
            "type": "Least Squares (Fast HALS).",
            "description": "- 'mu' is a Multiplicative Update solver.\n.. versionadded:: 0.17"
          },
          "Coordinate": {
            "type": "Descent solver.",
            "description": ".. versionadded:: 0.19"
          },
          "Multiplicative": {
            "type": "Update solver.",
            "description": ""
          },
          "beta_loss": {
            "type": "float or {'frobenius', 'kullback",
            "description": "leibler',             'itakura-saito'}, default='frobenius'"
          },
          "Beta": {
            "type": "divergence to be minimized, measuring the distance between X",
            "description": ""
          },
          "and": {
            "type": "the dot product WH. Note that values different from 'frobenius'",
            "description": "(or 2) and 'kullback-leibler' (or 1) lead to significantly slower\nfits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input"
          },
          "matrix": {
            "type": "X cannot contain zeros. Used only in 'mu' solver.",
            "description": ".. versionadded:: 0.19"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Tolerance": {
            "type": "of the stopping condition.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations before timing out.",
            "description": ""
          },
          "alpha_W": {
            "type": "float, default=0.0",
            "description": ""
          },
          "alpha_H": {
            "type": "float or \"same\", default=\"same\"",
            "description": ""
          },
          "have": {
            "type": "no regularization on `H`. If \"same\" (default), it takes the same value as",
            "description": "`alpha_W`.\n.. versionadded:: 1.0"
          },
          "l1_ratio": {
            "type": "float, default=0.0",
            "description": ""
          },
          "For": {
            "type": "0 < l1_ratio < 1, the penalty is a combination of L1 and L2.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for NMF initialisation (when ``init`` == 'nndsvdar' or",
            "description": "'random'), and in Coordinate Descent. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=False",
            "description": ""
          },
          "Solution": {
            "type": "to the non-negative least squares problem.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations.",
            "description": "References\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\nfactorizations\" <10.1587/transfun.E92.A.708>`\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals"
          },
          "of": {
            "type": "electronics, communications and computer sciences 92.3: 708-721, 2009.",
            "description": ".. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\nbeta-divergence\" <10.1162/NECO_a_00168>`\nFevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import non_negative_factorization\n>>> W, H, n_iter = non_negative_factorization(\n...     X, n_components=2, init='random', random_state=0)"
          }
        },
        "returns": "-------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(\n    ...     X, n_components=2, init='random', random_state=0)",
        "raises": "",
        "see_also": "",
        "notes": "that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like of shape (n_samples, n_components), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is initialised as an array of zeros, unless\n        `solver='mu'`, then it is filled with values calculated by\n        `np.sqrt(X.mean() / self._n_components)`.\n        If `None`, uses the initialisation method specified in `init`.\n\n    H : array-like of shape (n_components, n_features), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is used as a constant, to solve for W only.\n        If `None`, uses the initialisation method specified in `init`.\n\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from `W` or `H` shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n\n        Valid options:\n\n        - None: 'nndsvda' if n_components < n_features, otherwise 'random'.\n        - 'random': non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n        - 'custom': If `update_H=True`, use custom matrices W and H which must both\n          be provided. If `update_H=False`, then only custom matrix H is used.\n\n        .. versionchanged:: 0.23\n            The default value of `init` changed from 'random' to None in 0.23.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n          Alternating Least Squares (Fast HALS).\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(\n    ...     X, n_components=2, init='random', random_state=0)",
        "examples": "--------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(\n    ...     X, n_components=2, init='random', random_state=0)"
      }
    },
    {
      "name": "randomized_svd",
      "signature": "randomized_svd(M, n_components, *, n_oversamples=10, n_iter='auto', power_iteration_normalizer='auto', transpose='auto', flip_sign=True, random_state=None, svd_lapack_driver='gesdd')",
      "documentation": {
        "description": "Compute a truncated randomized SVD.\n\n    This method solves the fixed-rank approximation problem described in [1]_\n    (problem (1.5), p5).\n\n    Refer to\n    :ref:`sphx_glr_auto_examples_applications_wikipedia_principal_eigenvector.py`\n    for a typical example where the power iteration algorithm is used to rank web pages.\n    This algorithm is also known to be used as a building block in Google's PageRank\n    algorithm.\n\n    Parameters\n    ----------\n    M : {ndarray, sparse matrix}\n        Matrix to decompose.\n\n    n_components : int\n        Number of singular values and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of `M` so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of `M` is `n_components + n_oversamples`. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See [1]_\n        (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see [1]_ page 9).\n\n        .. versionchanged:: 0.18\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    transpose : bool or 'auto', default='auto'\n        Whether the algorithm should be applied to M.T instead of M. The\n        result should approximately be the same. The 'auto' mode will\n        trigger the transposition if M.shape[1] > M.shape[0] since this\n        implementation of randomized SVD tend to be a little faster in that\n        case.\n\n        .. versionchanged:: 0.18\n\n    flip_sign : bool, default=True\n        The output of a singular value decomposition is only unique up to a\n        permutation of the signs of the singular vectors. If `flip_sign` is\n        set to `True`, the sign ambiguity is resolved by making the largest\n        loadings for each component in the left singular vectors positive.\n\n    random_state : int, RandomState instance or None, default='warn'\n        The seed of the pseudo random number generator to use when\n        shuffling the data, i.e. getting the random vectors to initialize\n        the algorithm. Pass an int for reproducible results across multiple\n        function calls. See :term:`Glossary <random_state>`.\n\n        .. versionchanged:: 1.2\n            The default value changed from 0 to None.\n\n    svd_lapack_driver : {\"gesdd\", \"gesvd\"}, default=\"gesdd\"\n        Whether to use the more efficient divide-and-conquer approach\n        (`\"gesdd\"`) or more general rectangular approach (`\"gesvd\"`) to compute\n        the SVD of the matrix B, which is the projection of M into a low\n        dimensional subspace, as described in [1]_.\n\n        .. versionadded:: 1.2\n\n    Returns\n    -------\n    u : ndarray of shape (n_samples, n_components)\n        Unitary matrix having left singular vectors with signs flipped as columns.\n    s : ndarray of shape (n_components,)\n        The singular values, sorted in non-increasing order.\n    vh : ndarray of shape (n_components, n_features)\n        Unitary matrix having right singular vectors with signs flipped as rows.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Finding structure with randomness:\n      Stochastic algorithms for constructing approximate matrix decompositions\"\n      <0909.4061>`\n      Halko, et al. (2009)\n\n    .. [2] A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    .. [3] An implementation of a randomized algorithm for principal component\n      analysis A. Szlam et al. 2014",
        "parameters": {
          "M": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "Matrix": {
            "type": "to decompose.",
            "description": ""
          },
          "n_components": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of power iterations. It can be used to deal with very noisy",
            "description": "problems. When 'auto', it is set to 4, unless `n_components` is small\n(< .1 * min(X.shape)) in which case `n_iter` is set to 7."
          },
          "n_oversamples": {
            "type": "int, default=10",
            "description": ""
          },
          "Additional": {
            "type": "number of random vectors to sample the range of `M` so as",
            "description": ""
          },
          "to": {
            "type": "increase this parameter up to `2*k - n_components` where k is the",
            "description": ""
          },
          "used": {
            "type": "to find the range of `M` is `n_components + n_oversamples`. Smaller",
            "description": ""
          },
          "number": {
            "type": "can improve speed but can negatively impact the quality of",
            "description": ""
          },
          "approximation": {
            "type": "of singular vectors and singular values. Users might wish",
            "description": ""
          },
          "effective": {
            "type": "rank. Usually, `n_components` is chosen to be greater than k",
            "description": ""
          },
          "slowly": {
            "type": "decaying spectrums, or to increase precision accuracy. See [1]_",
            "description": "(pages 5, 23 and 26)."
          },
          "n_iter": {
            "type": "int or 'auto', default='auto'",
            "description": ""
          },
          "This": {
            "type": "algorithm finds a (usually very good) approximate truncated",
            "description": ""
          },
          "users": {
            "type": "should rather increase `n_oversamples` before increasing `n_iter`",
            "description": ""
          },
          "as": {
            "type": "the principle of the randomized method is to avoid usage of these",
            "description": ""
          },
          "more": {
            "type": "costly power iterations steps. When `n_components` is equal",
            "description": ""
          },
          "or": {
            "type": "greater to the effective matrix rank and the spectrum does not",
            "description": ""
          },
          "present": {
            "type": "a slow decay, `n_iter=0` or `1` should even work fine in theory",
            "description": "(see [1]_ page 9).\n.. versionchanged:: 0.18"
          },
          "power_iteration_normalizer": {
            "type": "{'auto', 'QR', 'LU', 'none'}, default='auto'",
            "description": ""
          },
          "Whether": {
            "type": "to use the more efficient divide-and-conquer approach",
            "description": "(`\"gesdd\"`) or more general rectangular approach (`\"gesvd\"`) to compute"
          },
          "QR": {
            "type": "factorization (the slowest but most accurate), 'none'",
            "description": "(the fastest but numerically unstable when `n_iter` is large, e.g."
          },
          "typically": {
            "type": "5 or larger), or 'LU' factorization (numerically stable",
            "description": ""
          },
          "but": {
            "type": "can lose slightly in accuracy). The 'auto' mode applies no",
            "description": ""
          },
          "normalization": {
            "type": "if `n_iter` <= 2 and switches to LU otherwise.",
            "description": ".. versionadded:: 0.18"
          },
          "transpose": {
            "type": "bool or 'auto', default='auto'",
            "description": ""
          },
          "result": {
            "type": "should approximately be the same. The 'auto' mode will",
            "description": ""
          },
          "trigger": {
            "type": "the transposition if M.shape[1] > M.shape[0] since this",
            "description": ""
          },
          "implementation": {
            "type": "of randomized SVD tend to be a little faster in that",
            "description": "case.\n.. versionchanged:: 0.18"
          },
          "flip_sign": {
            "type": "bool, default=True",
            "description": ""
          },
          "The": {
            "type": "singular values, sorted in non-increasing order.",
            "description": ""
          },
          "permutation": {
            "type": "of the signs of the singular vectors. If `flip_sign` is",
            "description": ""
          },
          "set": {
            "type": "to `True`, the sign ambiguity is resolved by making the largest",
            "description": ""
          },
          "loadings": {
            "type": "for each component in the left singular vectors positive.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default='warn'",
            "description": ""
          },
          "shuffling": {
            "type": "the data, i.e. getting the random vectors to initialize",
            "description": ""
          },
          "the": {
            "type": "SVD of the matrix B, which is the projection of M into a low",
            "description": ""
          },
          "function": {
            "type": "calls. See :term:`Glossary <random_state>`.",
            "description": ".. versionchanged:: 1.2"
          },
          "svd_lapack_driver": {
            "type": "{\"gesdd\", \"gesvd\"}, default=\"gesdd\"",
            "description": ""
          },
          "dimensional": {
            "type": "subspace, as described in [1]_.",
            "description": ".. versionadded:: 1.2\nReturns\n-------"
          },
          "u": {
            "type": "ndarray of shape (n_samples, n_components)",
            "description": ""
          },
          "Unitary": {
            "type": "matrix having right singular vectors with signs flipped as rows.",
            "description": "Notes\n-----"
          },
          "s": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "vh": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "singular": {
            "type": "value decomposition using randomization to speed up the",
            "description": "computations. It is particularly fast on large matrices on which"
          },
          "you": {
            "type": "wish to extract only a small number of components. In order to",
            "description": ""
          },
          "obtain": {
            "type": "further speed up, `n_iter` can be set <=2 (at the cost of",
            "description": ""
          },
          "loss": {
            "type": "of precision). To increase the precision it is recommended to",
            "description": ""
          },
          "increase": {
            "type": "`n_oversamples`, up to `2*k-n_components` where k is the",
            "description": ""
          },
          "so": {
            "type": "increasing `n_oversamples` up to `n_components` should be enough.",
            "description": "References\n----------\n.. [1] :arxiv:`\"Finding structure with randomness:"
          },
          "Stochastic": {
            "type": "algorithms for constructing approximate matrix decompositions\"",
            "description": "<0909.4061>`\nHalko, et al. (2009)\n.. [2] A randomized algorithm for the decomposition of matrices\nPer-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n.. [3] An implementation of a randomized algorithm for principal component"
          },
          "analysis": {
            "type": "A. Szlam et al. 2014",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.utils.extmath import randomized_svd\n>>> a = np.array([[1, 2, 3, 5],\n...               [3, 4, 5, 6],\n...               [7, 8, 9, 10]])\n>>> U, s, Vh = randomized_svd(a, n_components=2, random_state=0)\n>>> U.shape, s.shape, Vh.shape\n((3, 2), (2,), (2, 4))"
          }
        },
        "returns": "-------\n    u : ndarray of shape (n_samples, n_components)\n        Unitary matrix having left singular vectors with signs flipped as columns.\n    s : ndarray of shape (n_components,)\n        The singular values, sorted in non-increasing order.\n    vh : ndarray of shape (n_components, n_features)\n        Unitary matrix having right singular vectors with signs flipped as rows.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Finding structure with randomness:\n      Stochastic algorithms for constructing approximate matrix decompositions\"\n      <0909.4061>`\n      Halko, et al. (2009)\n\n    .. [2] A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    .. [3] An implementation of a randomized algorithm for principal component\n      analysis A. Szlam et al. 2014\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import randomized_svd\n    >>> a = np.array([[1, 2, 3, 5],\n    ...               [3, 4, 5, 6],\n    ...               [7, 8, 9, 10]])\n    >>> U, s, Vh = randomized_svd(a, n_components=2, random_state=0)\n    >>> U.shape, s.shape, Vh.shape\n    ((3, 2), (2,), (2, 4))",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Finding structure with randomness:\n      Stochastic algorithms for constructing approximate matrix decompositions\"\n      <0909.4061>`\n      Halko, et al. (2009)\n\n    .. [2] A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    .. [3] An implementation of a randomized algorithm for principal component\n      analysis A. Szlam et al. 2014\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import randomized_svd\n    >>> a = np.array([[1, 2, 3, 5],\n    ...               [3, 4, 5, 6],\n    ...               [7, 8, 9, 10]])\n    >>> U, s, Vh = randomized_svd(a, n_components=2, random_state=0)\n    >>> U.shape, s.shape, Vh.shape\n    ((3, 2), (2,), (2, 4))",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import randomized_svd\n    >>> a = np.array([[1, 2, 3, 5],\n    ...               [3, 4, 5, 6],\n    ...               [7, 8, 9, 10]])\n    >>> U, s, Vh = randomized_svd(a, n_components=2, random_state=0)\n    >>> U.shape, s.shape, Vh.shape\n    ((3, 2), (2,), (2, 4))"
      }
    },
    {
      "name": "sparse_encode",
      "signature": "sparse_encode(X, dictionary, *, gram=None, cov=None, algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None, max_iter=1000, n_jobs=None, check_input=True, verbose=0, positive=False)",
      "documentation": {
        "description": "Sparse coding.\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    dictionary : array-like of shape (n_components, n_features)\n        The dictionary matrix against which to solve the sparse coding of\n        the data. Some of the algorithms assume normalized rows for meaningful\n        output.\n\n    gram : array-like of shape (n_components, n_components), default=None\n        Precomputed Gram matrix, `dictionary * dictionary'`.\n\n    cov : array-like of shape (n_components, n_samples), default=None\n        Precomputed covariance, `dictionary' * X`.\n\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'},             default='lasso_lars'\n        The algorithm used:\n\n        * `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n          the estimated components are sparse;\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        * `'threshold'`: squashes to zero all coefficients less than\n          regularization from the projection `dictionary * data'`.\n\n    n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `n_nonzero_coefs=int(n_features / 10)`.\n\n    alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    copy_cov : bool, default=True\n        Whether to copy the precomputed covariance matrix; if `False`, it may\n        be overwritten.\n\n    init : ndarray of shape (n_samples, n_components), default=None\n        Initialization value of the sparse codes. Only used if\n        `algorithm='lasso_cd'`.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    check_input : bool, default=True\n        If `False`, the input arrays X and dictionary will not be checked.\n\n    verbose : int, default=0\n        Controls the verbosity; the higher, the more messages.\n\n    positive : bool, default=False\n        Whether to enforce positivity when finding the encoding.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse codes.\n\n    See Also\n    --------\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\n        dictionary.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "matrix.",
            "description": ""
          },
          "dictionary": {
            "type": "array",
            "description": "like of shape (n_components, n_features)"
          },
          "The": {
            "type": "sparse codes.",
            "description": ""
          },
          "the": {
            "type": "reconstruction error targeted. In this case, it overrides",
            "description": "`n_nonzero_coefs`."
          },
          "gram": {
            "type": "array",
            "description": "like of shape (n_components, n_components), default=None"
          },
          "Precomputed": {
            "type": "covariance, `dictionary' * X`.",
            "description": ""
          },
          "cov": {
            "type": "array",
            "description": "like of shape (n_components, n_samples), default=None"
          },
          "algorithm": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'},             default='lasso_lars'",
            "description": ""
          },
          "Lasso": {
            "type": "solution (`linear_model.Lasso`). lasso_lars will be faster if",
            "description": ""
          },
          "regularization": {
            "type": "from the projection `dictionary * data'`.",
            "description": ""
          },
          "n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of parallel jobs to run.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "and": {
            "type": "is overridden by `alpha` in the `omp` case. If `None`, then",
            "description": "`n_nonzero_coefs=int(n_features / 10)`."
          },
          "alpha": {
            "type": "float, default=None",
            "description": ""
          },
          "If": {
            "type": "`False`, the input arrays X and dictionary will not be checked.",
            "description": ""
          },
          "penalty": {
            "type": "applied to the L1 norm.",
            "description": ""
          },
          "threshold": {
            "type": "below which coefficients will be squashed to zero.",
            "description": ""
          },
          "copy_cov": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the encoding.",
            "description": ".. versionadded:: 0.20\nReturns\n-------"
          },
          "be": {
            "type": "overwritten.",
            "description": ""
          },
          "init": {
            "type": "ndarray of shape (n_samples, n_components), default=None",
            "description": ""
          },
          "Initialization": {
            "type": "value of the sparse codes. Only used if",
            "description": "`algorithm='lasso_cd'`."
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform if `algorithm='lasso_cd'` or",
            "description": "`'lasso_lars'`."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "check_input": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Controls": {
            "type": "the verbosity; the higher, the more messages.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "code": {
            "type": "ndarray of shape (n_samples, n_components)",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso"
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": "sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\nsklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer."
          },
          "SparseCoder": {
            "type": "Find a sparse representation of data from a fixed precomputed",
            "description": "dictionary.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import sparse_encode\n>>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n>>> dictionary = np.array(\n...     [[0, 1, 0],\n...      [-1, -1, 2],\n...      [1, 1, 1],\n...      [0, 1, 1],\n...      [0, 2, 1]],\n...    dtype=np.float64\n... )\n>>> sparse_encode(X, dictionary, alpha=1e-10)\narray([[ 0.,  0., -1.,  0.,  0.],\n[ 0.,  1.,  1.,  0.,  0.]])"
          }
        },
        "returns": "-------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse codes.\n\n    See Also\n    --------\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\n        dictionary.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import sparse_encode\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> sparse_encode(X, dictionary, alpha=1e-10)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])",
        "raises": "",
        "see_also": "--------\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\n        dictionary.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import sparse_encode\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> sparse_encode(X, dictionary, alpha=1e-10)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import sparse_encode\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> sparse_encode(X, dictionary, alpha=1e-10)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])"
      }
    }
  ],
  "classes": [
    {
      "name": "DictionaryLearning",
      "documentation": {
        "description": "Dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1.0\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for numerical error.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (:func:`~sklearn.linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n          faster if the estimated components are sparse.\n\n        .. versionadded:: 0.17\n           *cd* coordinate descent method to improve speed.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (:func:`~sklearn.linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n          will be faster if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n        .. versionadded:: 0.17\n           *lasso_cd* coordinate descent method to improve speed.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    n_jobs : int or None, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the code, for warm restart. Only used if `code_init`\n        and `dict_init` are not None.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the dictionary, for warm restart. Only used if\n        `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n        .. versionadded:: 1.3\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        dictionary atoms extracted from the data\n\n    error_ : array\n        vector of errors at each iteration\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of iterations run.",
            "description": ""
          },
          "is": {
            "type": "set to ``n_features``.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform if `algorithm='lasso_cd'` or",
            "description": "`'lasso_lars'`.\n.. versionadded:: 0.22\nAttributes\n----------"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "8"
          },
          "Tolerance": {
            "type": "for numerical error.",
            "description": ""
          },
          "fit_algorithm": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": "* `'lars'`: uses the least angle regression method to solve the lasso"
          },
          "problem": {
            "type": ":func:`~sklearn.linear_model.lars_path`",
            "description": ";\n* `'cd'`: uses the coordinate descent method to compute the"
          },
          "Lasso": {
            "type": "solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`",
            "description": ""
          },
          "faster": {
            "type": "if the estimated components are sparse.",
            "description": ".. versionadded:: 0.17\n*cd* coordinate descent method to improve speed."
          },
          "transform_algorithm": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to transform the data:",
            "description": "- `'lars'`: uses the least angle regression method\n(:func:`~sklearn.linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution.\n- `'lasso_cd'`: uses the coordinate descent method to compute the"
          },
          "will": {
            "type": "be faster if the estimated components are sparse.",
            "description": "- `'omp'`: uses orthogonal matching pursuit to estimate the sparse\nsolution.\n- `'threshold'`: squashes to zero all coefficients less than alpha from"
          },
          "the": {
            "type": "original signal:",
            "description": ">>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\nnp.float64(0.05...)"
          },
          "transform_n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "transform_alpha": {
            "type": "float, default=None",
            "description": ""
          },
          "If": {
            "type": "`None`, defaults to `alpha`.",
            "description": ".. versionchanged:: 1.2"
          },
          "penalty": {
            "type": "applied to the L1 norm.",
            "description": ""
          },
          "threshold": {
            "type": "below which coefficients will be squashed to zero.",
            "description": ""
          },
          "When": {
            "type": "None, default value changed from 1.0 to `alpha`.",
            "description": ""
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": ""
          },
          "for": {
            "type": "sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import DictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> dict_learner = DictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n...     random_state=42,\n... )\n>>> X_transformed = dict_learner.fit(X).transform(X)"
          },
          "code_init": {
            "type": "ndarray of shape (n_samples, n_components), default=None",
            "description": ""
          },
          "Initial": {
            "type": "values for the dictionary, for warm restart. Only used if",
            "description": "`code_init` and `dict_init` are not None."
          },
          "and": {
            "type": "`dict_init` are not None.",
            "description": ""
          },
          "dict_init": {
            "type": "ndarray of shape (n_components, n_features), default=None",
            "description": ""
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "Callable": {
            "type": "that gets invoked every five iterations.",
            "description": ".. versionadded:: 1.3"
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "To": {
            "type": "control the verbosity of the procedure.",
            "description": ""
          },
          "split_sign": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the dictionary.",
            "description": ".. versionadded:: 0.20"
          },
          "its": {
            "type": "negative part and its positive part. This can improve the",
            "description": ""
          },
          "performance": {
            "type": "of downstream classifiers.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for initializing the dictionary when ``dict_init`` is not",
            "description": "specified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "positive_code": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive_dict": {
            "type": "bool, default=False",
            "description": ""
          },
          "transform_max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "dictionary": {
            "type": "learning algorithm.",
            "description": ""
          },
          "error_": {
            "type": "array",
            "description": ""
          },
          "vector": {
            "type": "of errors at each iteration",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "MiniBatchDictionaryLearning": {
            "type": "A faster, less accurate, version of the",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis."
          },
          "SparseCoder": {
            "type": "Find a sparse representation of data from a fixed,",
            "description": ""
          },
          "precomputed": {
            "type": "dictionary.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": "References\n----------\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning"
          },
          "We": {
            "type": "can compare the average squared euclidean norm of the reconstruction",
            "description": ""
          },
          "error": {
            "type": "of the sparse coded signal relative to the squared euclidean norm of",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import DictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> dict_learner = DictionaryLearning(\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n    ...     random_state=42,\n    ... )\n    >>> X_transformed = dict_learner.fit(X).transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0)\n    np.float64(0.52...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import DictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> dict_learner = DictionaryLearning(\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n    ...     random_state=42,\n    ... )\n    >>> X_transformed = dict_learner.fit(X).transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0)\n    np.float64(0.52...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X and return the transformed data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "V": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        V : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Encode the data as a sparse combination of the dictionary atoms.\n\n        Coding method is determined by the object parameter\n        `transform_algorithm`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Test": {
                "type": "data to be transformed, must have the same number of",
                "description": ""
              },
              "features": {
                "type": "as the data used to train the model.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FactorAnalysis",
      "documentation": {
        "description": "Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, default=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : array-like of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Dimensionality": {
            "type": "of latent space, the number of components",
            "description": ""
          },
          "of": {
            "type": "``X`` that are obtained after ``transform``.",
            "description": ""
          },
          "If": {
            "type": "not None, apply the indicated rotation. Currently, varimax and",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "2"
          },
          "Stopping": {
            "type": "tolerance for log-likelihood increase.",
            "description": ""
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to make a copy of X. If ``False``, the input X gets overwritten",
            "description": ""
          },
          "during": {
            "type": "fitting.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations.",
            "description": ""
          },
          "noise_variance_init": {
            "type": "array",
            "description": "like of shape (n_features,), default=None"
          },
          "The": {
            "type": "estimated noise variance for each feature.",
            "description": ""
          },
          "svd_method": {
            "type": "{'lapack', 'randomized'}, default='randomized'",
            "description": ""
          },
          "Which": {
            "type": "SVD method to use. If 'lapack' use standard SVD from",
            "description": "scipy.linalg, if 'randomized' use fast ``randomized_svd`` function."
          },
          "Defaults": {
            "type": "to 'randomized'. For most applications 'randomized' will",
            "description": ""
          },
          "be": {
            "type": "sufficiently precise while providing significant speed gains.",
            "description": ""
          },
          "Accuracy": {
            "type": "can also be improved by setting higher values for",
            "description": "`iterated_power`. If this is not sufficient, for maximum precision"
          },
          "you": {
            "type": "should choose 'lapack'.",
            "description": ""
          },
          "iterated_power": {
            "type": "int, default=3",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "if": {
            "type": "``svd_method`` equals 'randomized'.",
            "description": ""
          },
          "rotation": {
            "type": "{'varimax', 'quartimax'}, default=None",
            "description": ""
          },
          "quartimax": {
            "type": "are implemented. See",
            "description": "`\"The varimax criterion for analytic rotation in factor analysis\"\n<https://link.springer.com/article/10.1007%2FBF02289233>`_\nH. F. Kaiser, 1958.\n.. versionadded:: 0.24"
          },
          "random_state": {
            "type": "int or RandomState instance, default=0",
            "description": ""
          },
          "Only": {
            "type": "used when ``svd_method`` equals 'randomized'. Pass an int for",
            "description": ""
          },
          "reproducible": {
            "type": "results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Components": {
            "type": "with maximum variance.",
            "description": ""
          },
          "loglike_": {
            "type": "list of shape (n_iterations,)",
            "description": ""
          },
          "noise_variance_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "mean_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical mean, estimated from the training set."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "PCA": {
            "type": "Principal component analysis is also a latent linear variable model",
            "description": ""
          },
          "which": {
            "type": "however assumes equal noise variance for each feature.",
            "description": ""
          },
          "This": {
            "type": "extra assumption makes probabilistic PCA faster as it can be",
            "description": ""
          },
          "computed": {
            "type": "in closed form.",
            "description": ""
          },
          "FastICA": {
            "type": "Independent component analysis, a latent variable model with",
            "description": "non-Gaussian latent variables.\nReferences\n----------\n- David Barber, Bayesian Reasoning and Machine Learning,"
          },
          "Algorithm": {
            "type": "21.1.",
            "description": "- Christopher M. Bishop: Pattern Recognition and Machine Learning,"
          },
          "Chapter": {
            "type": "12.2.4.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FactorAnalysis\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FactorAnalysis(n_components=7, random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the FactorAnalysis model to X using SVD based approach.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Ignored parameter.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Ignored": {
                "type": "parameter.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "FactorAnalysis": {
                "type": "class instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            FactorAnalysis class instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_covariance",
          "signature": "get_covariance(self)",
          "documentation": {
            "description": "Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``",
            "parameters": {},
            "returns": "-------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Compute data precision matrix with the FactorAnalysis model.",
            "parameters": {},
            "returns": "-------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None)",
          "documentation": {
            "description": "Compute the average log-likelihood of the samples.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Ignored parameter.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "The": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Ignored": {
                "type": "parameter.",
                "description": "Returns\n-------"
              },
              "ll": {
                "type": "float",
                "description": ""
              },
              "Average": {
                "type": "log-likelihood of the samples under the current model.",
                "description": ""
              }
            },
            "returns": "-------\n        ll : float\n            Average log-likelihood of the samples under the current model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Compute the log-likelihood of each sample.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "The": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "ll": {
                "type": "ndarray of shape (n_samples,)",
                "description": "Log-likelihood of each sample under the current model."
              }
            },
            "returns": "-------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "The": {
                "type": "latent variables of X.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FastICA",
      "documentation": {
        "description": "FastICA: a fast algorithm for Independent Component Analysis.\n\n    The implementation is based on [1]_.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to use. If None is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Specify which algorithm to use for FastICA.\n\n    whiten : str or bool, default='unit-variance'\n        Specify the whitening strategy to use.\n\n        - If 'arbitrary-variance', a whitening with variance\n          arbitrary is used.\n        - If 'unit-variance', the whitening matrix is rescaled to ensure that\n          each recovered source has unit variance.\n        - If False, the data is already considered to be whitened, and no\n          whitening is performed.\n\n        .. versionchanged:: 1.3\n            The default value of `whiten` changed to 'unit-variance' in 1.3.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "algorithm": {
            "type": "{'parallel', 'deflation'}, default='parallel'",
            "description": ""
          },
          "Specify": {
            "type": "the whitening strategy to use.",
            "description": "- If 'arbitrary-variance', a whitening with variance"
          },
          "whiten": {
            "type": "str or bool, default='unit",
            "description": "variance'"
          },
          "arbitrary": {
            "type": "is used.",
            "description": "- If 'unit-variance', the whitening matrix is rescaled to ensure that"
          },
          "each": {
            "type": "recovered source has unit variance.",
            "description": "- If False, the data is already considered to be whitened, and no"
          },
          "whitening": {
            "type": "is performed.",
            "description": ".. versionchanged:: 1.3"
          },
          "The": {
            "type": "mean over features. Only set if `self.whiten` is True.",
            "description": ""
          },
          "fun": {
            "type": "{'logcosh', 'exp', 'cube'} or callable, default='logcosh'",
            "description": ""
          },
          "approximation": {
            "type": "to neg-entropy. Could be either 'logcosh', 'exp',",
            "description": ""
          },
          "or": {
            "type": "'cube'.",
            "description": ""
          },
          "You": {
            "type": "can also provide your own function. It should return a tuple",
            "description": ""
          },
          "containing": {
            "type": "the value of the function, and of its derivative, in the",
            "description": "point. The derivative should be averaged along its last dimension."
          },
          "Example": {
            "type": ":",
            "description": ""
          },
          "def": {
            "type": "my_g(x):",
            "description": ""
          },
          "return": {
            "type": "x ** 3, (3 * x ** 2).mean(axis=-1)",
            "description": ""
          },
          "fun_args": {
            "type": "dict, default=None",
            "description": ""
          },
          "Arguments": {
            "type": "to send to the functional form.",
            "description": ""
          },
          "If": {
            "type": "the algorithm is \"deflation\", n_iter is the",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations during fit.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "A": {
            "type": "positive scalar giving the tolerance at which the",
            "description": "un-mixing matrix is considered to have converged."
          },
          "w_init": {
            "type": "array",
            "description": "like of shape (n_components, n_components), default=None"
          },
          "Initial": {
            "type": "un-mixing array. If `w_init=None`, then an array of values",
            "description": ""
          },
          "drawn": {
            "type": "from a normal distribution is used.",
            "description": ""
          },
          "whiten_solver": {
            "type": "{\"eigh\", \"svd\"}, default=\"svd\"",
            "description": ""
          },
          "often": {
            "type": "faster when `n_samples <= n_features`.",
            "description": "- \"eigh\" is generally more memory efficient when\n`n_samples >= n_features`, and can be faster when\n`n_samples >= 50 * n_features`.\n.. versionadded:: 1.2"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "to initialize ``w_init`` when not specified, with a",
            "description": ""
          },
          "normal": {
            "type": "distribution. Pass an int, for reproducible results",
            "description": ""
          },
          "across": {
            "type": "multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "mixing_": {
            "type": "ndarray of shape (n_features, n_components)",
            "description": ""
          },
          "that": {
            "type": "projects data onto the first `n_components` principal components.",
            "description": ""
          },
          "mean_": {
            "type": "ndarray of shape(n_features,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "maximum": {
            "type": "number of iterations run across all components. Else",
            "description": ""
          },
          "they": {
            "type": "are just the number of iterations taken to converge.",
            "description": ""
          },
          "whitening_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Only": {
            "type": "set if whiten is 'True'. This is the pre-whitening matrix",
            "description": ""
          },
          "PCA": {
            "type": "Principal component analysis (PCA).",
            "description": ""
          },
          "IncrementalPCA": {
            "type": "Incremental principal components analysis (IPCA).",
            "description": ""
          },
          "KernelPCA": {
            "type": "Kernel Principal component analysis (KPCA).",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis."
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis (SparsePCA).",
            "description": "References\n----------\n.. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:"
          },
          "Algorithms": {
            "type": "and Applications, Neural Networks, 13(4-5), 2000,",
            "description": "pp. 411-430.\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FastICA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FastICA(n_components=7,\n...         random_state=0,\n...         whiten='unit-variance')\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)"
          },
          "to": {
            "type": "send to the functional form.",
            "description": ""
          }
        },
        "returns": "x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations during fit.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : array-like of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The linear operator to apply to the data to get the independent\n        sources. This is equal to the unmixing matrix when ``whiten`` is\n        False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n        ``whiten`` is True.\n\n    mixing_ : ndarray of shape (n_features, n_components)\n        The pseudo-inverse of ``components_``. It is the linear operator\n        that maps independent sources to the data.\n\n    mean_ : ndarray of shape(n_features,)\n        The mean over features. Only set if `self.whiten` is True.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge.\n\n    whitening_ : ndarray of shape (n_components, n_features)\n        Only set if whiten is 'True'. This is the pre-whitening matrix\n        that projects data onto the first `n_components` principal components.\n\n    See Also\n    --------\n    PCA : Principal component analysis (PCA).\n    IncrementalPCA : Incremental principal components analysis (IPCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0,\n    ...         whiten='unit-variance')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)",
        "raises": "",
        "see_also": "--------\n    PCA : Principal component analysis (PCA).\n    IncrementalPCA : Incremental principal components analysis (IPCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0,\n    ...         whiten='unit-variance')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)",
        "notes": "",
        "examples": ":\n\n            def my_g(x):\n                return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations during fit.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : array-like of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The linear operator to apply to the data to get the independent\n        sources. This is equal to the unmixing matrix when ``whiten`` is\n        False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n        ``whiten`` is True.\n\n    mixing_ : ndarray of shape (n_features, n_components)\n        The pseudo-inverse of ``components_``. It is the linear operator\n        that maps independent sources to the data.\n\n    mean_ : ndarray of shape(n_features,)\n        The mean over features. Only set if `self.whiten` is True.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge.\n\n    whitening_ : ndarray of shape (n_components, n_features)\n        Only set if whiten is 'True'. This is the pre-whitening matrix\n        that projects data onto the first `n_components` principal components.\n\n    See Also\n    --------\n    PCA : Principal component analysis (PCA).\n    IncrementalPCA : Incremental principal components analysis (IPCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0,\n    ...         whiten='unit-variance')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit the model and recover the sources from X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Estimated": {
                "type": "sources obtained by transforming the data with the",
                "description": ""
              },
              "estimated": {
                "type": "unmixing matrix.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Estimated sources obtained by transforming the data with the\n            estimated unmixing matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X, copy=True)",
          "documentation": {
            "description": "Transform the sources back to the mixed data (apply mixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            Sources, where `n_samples` is the number of samples\n            and `n_components` is the number of components.\n        copy : bool, default=True\n            If False, data passed to fit are overwritten. Defaults to True.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_components)\nSources, where `n_samples` is the number of samples"
              },
              "and": {
                "type": "`n_components` is the number of components.",
                "description": ""
              },
              "copy": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, data passed to fit are overwritten. Defaults to True.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Reconstructed": {
                "type": "data obtained with the mixing matrix.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_features)\n            Reconstructed data obtained with the mixing matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_inverse_transform_request",
          "signature": "set_inverse_transform_request(self: sklearn.decomposition._fastica.FastICA, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.decomposition._fastica.FastICA",
          "documentation": {
            "description": "Request metadata passed to the ``inverse_transform`` method.",
            "parameters": {
              "copy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``copy`` parameter in ``inverse_transform``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``copy`` parameter in ``inverse_transform``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_transform_request",
          "signature": "set_transform_request(self: sklearn.decomposition._fastica.FastICA, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.decomposition._fastica.FastICA",
          "documentation": {
            "description": "Request metadata passed to the ``transform`` method.",
            "parameters": {
              "copy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``copy`` parameter in ``transform``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``copy`` parameter in ``transform``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, copy=True)",
          "documentation": {
            "description": "Recover the sources from X (apply the unmixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to transform, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        copy : bool, default=True\n            If False, data passed to fit can be overwritten. Defaults to True.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "to transform, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "copy": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, data passed to fit can be overwritten. Defaults to True.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Estimated": {
                "type": "sources obtained by transforming the data with the",
                "description": ""
              },
              "estimated": {
                "type": "unmixing matrix.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Estimated sources obtained by transforming the data with the\n            estimated unmixing matrix.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "IncrementalPCA",
      "documentation": {
        "description": "Incremental principal components analysis (IPCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of\n    the data, keeping only the most significant singular vectors to\n    project the data to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    Depending on the size of the input data, this algorithm can be much more\n    memory efficient than a PCA, and allows sparse input.\n\n    This algorithm has constant memory complexity, on the order\n    of ``batch_size * n_features``, enabling use of np.memmap files without\n    loading the entire file into memory. For sparse matrices, the input\n    is converted to dense in batches (in order to be able to subtract the\n    mean) which avoids storing the entire dense matrix at any one time.\n\n    The computational overhead of each SVD is\n    ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n    remain in memory at a time. There will be ``n_samples / batch_size`` SVD\n    computations to get the principal components, versus 1 large SVD of\n    complexity ``O(n_samples * n_features ** 2)`` for PCA.\n\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`.\n\n    Read more in the :ref:`User Guide <IncrementalPCA>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to keep. If ``n_components`` is ``None``,\n        then ``n_components`` is set to ``min(n_samples, n_features)``.\n\n    whiten : bool, default=False\n        When True (False by default) the ``components_`` vectors are divided\n        by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n        with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometimes\n        improve the predictive accuracy of the downstream estimators by\n        making data respect some hard-wired assumptions.\n\n    copy : bool, default=True\n        If False, X will be overwritten. ``copy=False`` can be used to\n        save memory but is unsafe for general use.\n\n    batch_size : int, default=None\n        The number of samples to use for each batch. Only used when calling\n        ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n        is inferred from the data and set to ``5 * n_features``, to provide a\n        balance between approximation accuracy and memory consumption.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        Variance explained by each of the selected components.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If all components are stored, the sum of explained variances is equal\n        to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\n    var_ : ndarray of shape (n_features,)\n        Per-feature empirical variance, aggregate over calls to\n        ``partial_fit``.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf.\n\n    n_components_ : int\n        The estimated number of components. Relevant when\n        ``n_components=None``.\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator. Will be reset on\n        new calls to fit, but increments across ``partial_fit`` calls.\n\n    batch_size_ : int\n        Inferred batch size from ``batch_size``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA : Principal component analysis (PCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Notes\n    -----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    :doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "8, pp. 1371-1374, August 2000. <10.1109/83.855432>`",
            "description": ""
          },
          "then": {
            "type": "``n_components`` is set to ``min(n_samples, n_features)``.",
            "description": ""
          },
          "whiten": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "True (False by default) the ``components_`` vectors are divided",
            "description": ""
          },
          "by": {
            "type": "``n_samples`` times ``components_`` to ensure uncorrelated outputs",
            "description": ""
          },
          "with": {
            "type": "unit component-wise variances.",
            "description": ""
          },
          "Whitening": {
            "type": "will remove some information from the transformed signal",
            "description": "(the relative variance scales of the components) but can sometimes"
          },
          "improve": {
            "type": "the predictive accuracy of the downstream estimators by",
            "description": ""
          },
          "making": {
            "type": "data respect some hard-wired assumptions.",
            "description": ""
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "all components are stored, the sum of explained variances is equal",
            "description": ""
          },
          "save": {
            "type": "memory but is unsafe for general use.",
            "description": ""
          },
          "batch_size": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of samples processed by the estimator. Will be reset on",
            "description": ""
          },
          "is": {
            "type": "inferred from the data and set to ``5 * n_features``, to provide a",
            "description": ""
          },
          "balance": {
            "type": "between approximation accuracy and memory consumption.",
            "description": "Attributes\n----------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Principal": {
            "type": "axes in feature space, representing the directions of",
            "description": ""
          },
          "maximum": {
            "type": "variance in the data. Equivalently, the right singular",
            "description": ""
          },
          "vectors": {
            "type": "of the centered input data, parallel to its eigenvectors.",
            "description": ""
          },
          "explained_variance_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Variance": {
            "type": "explained by each of the selected components.",
            "description": ""
          },
          "explained_variance_ratio_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Percentage": {
            "type": "of variance explained by each of the selected components.",
            "description": ""
          },
          "to": {
            "type": "1.0.",
            "description": ""
          },
          "singular_values_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "variables": {
            "type": "in the lower-dimensional space.",
            "description": ""
          },
          "mean_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical mean, aggregate over calls to ``partial_fit``."
          },
          "var_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical variance, aggregate over calls to\n``partial_fit``."
          },
          "noise_variance_": {
            "type": "float",
            "description": ""
          },
          "from": {
            "type": "Tipping and Bishop 1999. See \"Pattern Recognition and",
            "description": ""
          },
          "Machine": {
            "type": "Learning\" by C. Bishop, 12.2.1 p. 574 or",
            "description": ""
          },
          "http": {
            "type": "//www.miketipping.com/papers/met",
            "description": "mppca.pdf."
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "n_samples_seen_": {
            "type": "int",
            "description": ""
          },
          "new": {
            "type": "calls to fit, but increments across ``partial_fit`` calls.",
            "description": ""
          },
          "batch_size_": {
            "type": "int",
            "description": ""
          },
          "Inferred": {
            "type": "batch size from ``batch_size``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf",
            "description": ""
          },
          "PCA": {
            "type": "Principal component analysis (PCA).",
            "description": ""
          },
          "KernelPCA": {
            "type": "Kernel Principal component analysis (KPCA).",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis (SparsePCA).",
            "description": ""
          },
          "TruncatedSVD": {
            "type": "Dimensionality reduction using truncated SVD.",
            "description": "Notes\n-----"
          },
          "Implements": {
            "type": "the incremental PCA model from:",
            "description": "*D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\npp. 125-141, May 2008.*"
          },
          "This": {
            "type": "model is an extension of the Sequential Karhunen-Loeve Transform from:",
            "description": ":doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and"
          },
          "its": {
            "type": "Application to Images, IEEE Transactions on Image Processing, Volume 9,",
            "description": ""
          },
          "We": {
            "type": "have specifically abstained from an optimization used by authors of both",
            "description": "papers, a QR decomposition used in specific situations to reduce the"
          },
          "algorithmic": {
            "type": "complexity of the SVD. The source for this technique is",
            "description": "*Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,"
          },
          "section": {
            "type": "5.4.4, pp 252-253.*. This technique has been omitted because it is",
            "description": ""
          },
          "advantageous": {
            "type": "only when decomposing a matrix with ``n_samples`` (rows)",
            "description": ">= 5/3 * ``n_features`` (columns), and hurts the readability of the"
          },
          "implemented": {
            "type": "algorithm. This would be a good opportunity for future",
            "description": "optimization, if it is deemed necessary.\nReferences\n----------\nD. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77,"
          },
          "Issue": {
            "type": "1-3, pp. 125-141, May 2008.",
            "description": "G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,"
          },
          "Section": {
            "type": "5.4.4, pp. 252-253.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import IncrementalPCA\n>>> from scipy import sparse\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n>>> # either partially fit on smaller batches of data\n>>> transformer.partial_fit(X[:100, :])"
          },
          "IncrementalPCA": {
            "type": "batch_size=200, n_components=7",
            "description": ">>> # or let the fit function itself divide the data into batches\n>>> X_sparse = sparse.csr_matrix(X)\n>>> X_transformed = transformer.fit_transform(X_sparse)\n>>> X_transformed.shape\n(1797, 7)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    PCA : Principal component analysis (PCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Notes\n    -----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    :doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)",
        "notes": "-----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    :doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)",
        "examples": "--------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X, using minibatches of size batch_size.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_covariance",
          "signature": "get_covariance(self)",
          "documentation": {
            "description": "Compute data covariance with the generative model.\n\n        ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n        where S**2 contains the explained variances, and sigma2 contains the\n        noise variances.",
            "parameters": {},
            "returns": "-------\n        cov : array of shape=(n_features, n_features)\n            Estimated covariance of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Compute data precision matrix with the generative model.\n\n        Equals the inverse of the covariance but computed with\n        the matrix inversion lemma for efficiency.",
            "parameters": {},
            "returns": "-------\n        precision : array, shape=(n_features, n_features)\n            Estimated precision of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform data back to its original space.\n\n        In other words, return an input `X_original` whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data, where `n_samples` is the number of samples\n            and `n_components` is the number of components.\n\n        Returns\n        -------\n        X_original array-like of shape (n_samples, n_features)\n            Original data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_components)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Notes\n-----"
              },
              "X_original": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": ""
              },
              "Original": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "If": {
                "type": "whitening is enabled, inverse_transform will compute the",
                "description": ""
              },
              "exact": {
                "type": "inverse operation, which includes reversing whitening.",
                "description": ""
              }
            },
            "returns": "-------\n        X_original array-like of shape (n_samples, n_features)\n            Original data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Notes\n        -----\n        If whitening is enabled, inverse_transform will compute the\n        exact inverse operation, which includes reversing whitening.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        If whitening is enabled, inverse_transform will compute the\n        exact inverse operation, which includes reversing whitening.",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, check_input=True)",
          "documentation": {
            "description": "Incremental fit with X. All of X is processed as a single batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        check_input : bool, default=True\n            Run check_array on X.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Run": {
                "type": "check_array on X.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set, using minibatches of size batch_size if X is\n        sparse.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Projection of X in the first principal components.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Projection": {
                "type": "of X in the first principal components.",
                "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import IncrementalPCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n...               [1, 1], [2, 1], [3, 2]])\n>>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n>>> ipca.fit(X)"
              },
              "IncrementalPCA": {
                "type": "batch_size=3, n_components=2",
                "description": ">>> ipca.transform(X) # doctest: +SKIP"
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Projection of X in the first principal components.\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> from sklearn.decomposition import IncrementalPCA\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n        ...               [1, 1], [2, 1], [3, 2]])\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n        >>> ipca.fit(X)\n        IncrementalPCA(batch_size=3, n_components=2)\n        >>> ipca.transform(X) # doctest: +SKIP",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n\n        >>> import numpy as np\n        >>> from sklearn.decomposition import IncrementalPCA\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n        ...               [1, 1], [2, 1], [3, 2]])\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n        >>> ipca.fit(X)\n        IncrementalPCA(batch_size=3, n_components=2)\n        >>> ipca.transform(X) # doctest: +SKIP"
          }
        }
      ]
    },
    {
      "name": "KernelPCA",
      "documentation": {
        "description": "Kernel Principal component analysis (KPCA).\n\n    Non-linear dimensionality reduction through the use of kernels [1]_, see also\n    :ref:`metrics`.\n\n    It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD\n    or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the\n    truncated SVD, depending on the shape of the input data and the number of\n    components to extract. It can also use a randomized truncated SVD by the\n    method proposed in [3]_, see `eigen_solver`.\n\n    For a usage example and comparison between\n    Principal Components Analysis (PCA) and its kernelized version (KPCA), see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.\n\n    For a usage example in denoising images using KPCA, see\n    :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`.\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'\n        Kernel used for PCA.\n\n    gamma : float, default=None\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\n    degree : float, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Parameters (keyword arguments) and\n        values for kernel passed as callable object.\n        Ignored by other kernels.\n\n    alpha : float, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels\n        (i.e. learn to find the pre-image of a point). This method is based\n        on [2]_.\n\n    eigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto'\n        Select eigensolver to use. If `n_components` is much\n        less than the number of training samples, randomized (or arpack to a\n        smaller extent) may be more efficient than the dense eigensolver.\n        Randomized SVD is performed according to the method of Halko et al\n        [3]_.\n\n        auto :\n            the solver is selected by a default policy based on n_samples\n            (the number of training samples) and `n_components`:\n            if the number of components to extract is less than 10 (strict) and\n            the number of samples is more than 200 (strict), the 'arpack'\n            method is enabled. Otherwise the exact full eigenvalue\n            decomposition is computed and optionally truncated afterwards\n            ('dense' method).\n        dense :\n            run exact full eigenvalue decomposition calling the standard\n            LAPACK solver via `scipy.linalg.eigh`, and select the components\n            by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver using\n            `scipy.sparse.linalg.eigsh`. It requires strictly\n            0 < n_components < n_samples\n        randomized :\n            run randomized SVD by the method of Halko et al. [3]_. The current\n            implementation selects eigenvalues based on their module; therefore\n            using this method can lead to unexpected results if the kernel is\n            not positive semi-definite. See also [4]_.\n\n        .. versionchanged:: 1.0\n           `'randomized'` was added.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    iterated_power : int >= 0, or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'. When 'auto', it is set to 7 when\n        `n_components < 0.1 * min(X.shape)`, other it is set to 4.\n\n        .. versionadded:: 1.0\n\n    remove_zero_eig : bool, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, default=None\n        Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18\n\n    copy_X : bool, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    eigenvalues_ : ndarray of shape (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    eigenvectors_ : ndarray of shape (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : ndarray of shape (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : ndarray of shape (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : ndarray of shape (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    gamma_ : float\n        Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`\n        is explicitly provided, this is just the same as `gamma`. When `gamma`\n        is `None`, this is the actual value of kernel coefficient.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    FastICA : A fast algorithm for Independent Component Analysis.\n    IncrementalPCA : Incremental Principal Component Analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] `Schlkopf, Bernhard, Alexander Smola, and Klaus-Robert Mller.\n       \"Kernel principal component analysis.\"\n       International conference on artificial neural networks.\n       Springer, Berlin, Heidelberg, 1997.\n       <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n    .. [2] `Bakr, Gkhan H., Jason Weston, and Bernhard Schlkopf.\n       \"Learning to find pre-images.\"\n       Advances in neural information processing systems 16 (2004): 449-456.\n       <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n    .. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n       \"Finding structure with randomness: Probabilistic algorithms for\n       constructing approximate matrix decompositions.\"\n       SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n    .. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n       \"A randomized algorithm for the decomposition of matrices.\"\n       Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n       <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of components. If None, all non-zero components are kept.",
            "description": ""
          },
          "kernel": {
            "type": "{'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'",
            "description": ""
          },
          "Kernel": {
            "type": "coefficient for rbf, poly and sigmoid kernels. Ignored by other",
            "description": "kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``."
          },
          "gamma": {
            "type": "float, default=None",
            "description": ""
          },
          "degree": {
            "type": "float, default=3",
            "description": ""
          },
          "Degree": {
            "type": "for poly kernels. Ignored by other kernels.",
            "description": ""
          },
          "coef0": {
            "type": "float, default=1",
            "description": ""
          },
          "Independent": {
            "type": "term in poly and sigmoid kernels.",
            "description": ""
          },
          "Ignored": {
            "type": "by other kernels.",
            "description": ""
          },
          "kernel_params": {
            "type": "dict, default=None",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    FastICA : A fast algorithm for Independent Component Analysis.\n    IncrementalPCA : Incremental Principal Component Analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] `Schlkopf, Bernhard, Alexander Smola, and Klaus-Robert Mller.\n       \"Kernel principal component analysis.\"\n       International conference on artificial neural networks.\n       Springer, Berlin, Heidelberg, 1997.\n       <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n    .. [2] `Bakr, Gkhan H., Jason Weston, and Bernhard Schlkopf.\n       \"Learning to find pre-images.\"\n       Advances in neural information processing systems 16 (2004): 449-456.\n       <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n    .. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n       \"Finding structure with randomness: Probabilistic algorithms for\n       constructing approximate matrix decompositions.\"\n       SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n    .. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n       \"A randomized algorithm for the decomposition of matrices.\"\n       Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n       <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **params)",
          "documentation": {
            "description": "Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : kwargs\n            Parameters (keyword arguments) and values passed to\n            the fit_transform instance.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**params : kwargs"
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform X back to original space.\n\n        ``inverse_transform`` approximates the inverse transformation using\n        a learned pre-image. The pre-image is learned by kernel ridge\n        regression of the original data on their low-dimensional representation\n        vectors.\n\n        .. note:\n            :meth:`~sklearn.decomposition.fit` internally uses a centered\n            kernel. As the centered kernel no longer contains the information\n            of the mean of kernel features, such information is not taken into\n            account in reconstruction.\n\n        .. note::\n            When users want to compute inverse transformation for 'linear'\n            kernel, it is recommended that they use\n            :class:`~sklearn.decomposition.PCA` instead. Unlike\n            :class:`~sklearn.decomposition.PCA`,\n            :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n            does not reconstruct the mean of data when 'linear' kernel is used\n            due to the use of centered kernel.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_components)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_components)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": "References\n----------\n`Bakr, Gkhan H., Jason Weston, and Bernhard Schlkopf.\n\"Learning to find pre-images.\""
              },
              "Advances": {
                "type": "in neural information processing systems 16 (2004): 449-456.",
                "description": "<https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_"
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_features)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LatentDirichletAllocation",
      "documentation": {
        "description": "Latent Dirichlet Allocation with online variational Bayes algorithm.\n\n    The implementation is based on [1]_ and [2]_.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\n    Parameters\n    ----------\n    n_components : int, default=10\n        Number of topics.\n\n        .. versionchanged:: 0.19\n            ``n_topics`` was renamed to ``n_components``\n\n    doc_topic_prior : float, default=None\n        Prior of document topic distribution `theta`. If the value is None,\n        defaults to `1 / n_components`.\n        In [1]_, this is called `alpha`.\n\n    topic_word_prior : float, default=None\n        Prior of topic word distribution `beta`. If the value is None, defaults\n        to `1 / n_components`.\n        In [1]_, this is called `eta`.\n\n    learning_method : {'batch', 'online'}, default='batch'\n        Method used to update `_component`. Only used in :meth:`fit` method.\n        In general, if the data size is large, the online update will be much\n        faster than the batch update.\n\n        Valid options:\n\n        - 'batch': Batch variational Bayes method. Use all training data in each EM\n          update. Old `components_` will be overwritten in each iteration.\n        - 'online': Online variational Bayes method. In each EM update, use mini-batch\n          of training data to update the ``components_`` variable incrementally. The\n          learning rate is controlled by the ``learning_decay`` and the\n          ``learning_offset`` parameters.\n\n        .. versionchanged:: 0.20\n            The default learning method is now ``\"batch\"``.\n\n    learning_decay : float, default=0.7\n        It is a parameter that control learning rate in the online learning\n        method. The value should be set between (0.5, 1.0] to guarantee\n        asymptotic convergence. When the value is 0.0 and batch_size is\n        ``n_samples``, the update method is same as batch learning. In the\n        literature, this is called kappa.\n\n    learning_offset : float, default=10.0\n        A (positive) parameter that downweights early iterations in online\n        learning.  It should be greater than 1.0. In the literature, this is\n        called tau_0.\n\n    max_iter : int, default=10\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the :meth:`fit` method, and not the\n        :meth:`partial_fit` method.\n\n    batch_size : int, default=128\n        Number of documents to use in each EM iteration. Only used in online\n        learning.\n\n    evaluate_every : int, default=-1\n        How often to evaluate perplexity. Only used in `fit` method.\n        set it to 0 or negative number to not evaluate perplexity in\n        training at all. Evaluating perplexity can help you check convergence\n        in training process, but it will also increase total training time.\n        Evaluating perplexity in every iteration might increase training time\n        up to two-fold.\n\n    total_samples : int, default=1e6\n        Total number of documents. Only used in the :meth:`partial_fit` method.\n\n    perp_tol : float, default=1e-1\n        Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0.\n\n    mean_change_tol : float, default=1e-3\n        Stopping tolerance for updating document topic distribution in E-step.\n\n    max_doc_update_iter : int, default=100\n        Max number of iterations for updating document topic distribution in\n        the E-step.\n\n    n_jobs : int, default=None\n        The number of jobs to use in the E-step.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, default=None\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Variational parameters for topic word distribution. Since the complete\n        conditional for topic word distribution is a Dirichlet,\n        ``components_[i, j]`` can be viewed as pseudocount that represents the\n        number of times word `j` was assigned to topic `i`.\n        It can also be viewed as distribution over the words for each topic\n        after normalization:\n        ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\n    exp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n        Exponential value of expectation of log topic word distribution.\n        In the literature, this is `exp(E[log(beta)])`.\n\n    n_batch_iter_ : int\n        Number of iterations of the EM step.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of passes over the dataset.\n\n    bound_ : float\n        Final perplexity score on training set.\n\n    doc_topic_prior_ : float\n        Prior of document topic distribution `theta`. If the value is None,\n        it is `1 / n_components`.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generator or by `np.random`.\n\n    topic_word_prior_ : float\n        Prior of topic word distribution `beta`. If the value is None, it is\n        `1 / n_components`.\n\n    See Also\n    --------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n        A classifier with a linear decision boundary, generated by fitting\n        class conditional densities to the data and using Bayes' rule.\n\n    References\n    ----------\n    .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n           Hoffman, David M. Blei, Francis Bach, 2010\n           https://github.com/blei-lab/onlineldavb\n\n    .. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,\n           David M. Blei, Chong Wang, John Paisley, 2013",
        "parameters": {
          "n_components": {
            "type": "int, default=10",
            "description": ""
          },
          "Number": {
            "type": "of passes over the dataset.",
            "description": ""
          },
          "doc_topic_prior": {
            "type": "float, default=None",
            "description": ""
          },
          "Prior": {
            "type": "of topic word distribution `beta`. If the value is None, it is",
            "description": "`1 / n_components`."
          },
          "defaults": {
            "type": "to `1 / n_components`.",
            "description": ""
          },
          "In": {
            "type": "the literature, this is `exp(E[log(beta)])`.",
            "description": ""
          },
          "topic_word_prior": {
            "type": "float, default=None",
            "description": ""
          },
          "to": {
            "type": "`1 / n_components`.",
            "description": ""
          },
          "learning_method": {
            "type": "{'batch', 'online'}, default='batch'",
            "description": ""
          },
          "Method": {
            "type": "used to update `_component`. Only used in :meth:`fit` method.",
            "description": ""
          },
          "faster": {
            "type": "than the batch update.",
            "description": ""
          },
          "Valid": {
            "type": "options:",
            "description": "- 'batch': Batch variational Bayes method. Use all training data in each EM\nupdate. Old `components_` will be overwritten in each iteration.\n- 'online': Online variational Bayes method. In each EM update, use mini-batch"
          },
          "of": {
            "type": "training data to update the ``components_`` variable incrementally. The",
            "description": ""
          },
          "learning": {
            "type": "rate is controlled by the ``learning_decay`` and the",
            "description": "``learning_offset`` parameters.\n.. versionchanged:: 0.20"
          },
          "The": {
            "type": "number of jobs to use in the E-step.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "learning_decay": {
            "type": "float, default=0.7",
            "description": ""
          },
          "It": {
            "type": "can also be viewed as distribution over the words for each topic",
            "description": ""
          },
          "asymptotic": {
            "type": "convergence. When the value is 0.0 and batch_size is",
            "description": "``n_samples``, the update method is same as batch learning. In the\nliterature, this is called kappa."
          },
          "learning_offset": {
            "type": "float, default=10.0",
            "description": ""
          },
          "A": {
            "type": "classifier with a linear decision boundary, generated by fitting",
            "description": ""
          },
          "called": {
            "type": "tau_0.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=10",
            "description": ""
          },
          "batch_size": {
            "type": "int, default=128",
            "description": ""
          },
          "evaluate_every": {
            "type": "int, default=",
            "description": "1"
          },
          "How": {
            "type": "often to evaluate perplexity. Only used in `fit` method.",
            "description": ""
          },
          "set": {
            "type": "it to 0 or negative number to not evaluate perplexity in",
            "description": ""
          },
          "training": {
            "type": "at all. Evaluating perplexity can help you check convergence",
            "description": ""
          },
          "in": {
            "type": "training process, but it will also increase total training time.",
            "description": ""
          },
          "Evaluating": {
            "type": "perplexity in every iteration might increase training time",
            "description": ""
          },
          "up": {
            "type": "to two-fold.",
            "description": ""
          },
          "total_samples": {
            "type": "int, default=1e6",
            "description": ""
          },
          "Total": {
            "type": "number of documents. Only used in the :meth:`partial_fit` method.",
            "description": ""
          },
          "perp_tol": {
            "type": "float, default=1e",
            "description": "1"
          },
          "Perplexity": {
            "type": "tolerance. Only used when ``evaluate_every`` is greater than 0.",
            "description": ""
          },
          "mean_change_tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Stopping": {
            "type": "tolerance for updating document topic distribution in E-step.",
            "description": ""
          },
          "max_doc_update_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Max": {
            "type": "number of iterations for updating document topic distribution in",
            "description": ""
          },
          "the": {
            "type": "E-step.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Verbosity": {
            "type": "level.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis:"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Variational": {
            "type": "parameters for topic word distribution. Since the complete",
            "description": ""
          },
          "conditional": {
            "type": "for topic word distribution is a Dirichlet,",
            "description": "``components_[i, j]`` can be viewed as pseudocount that represents the"
          },
          "number": {
            "type": "generator or by `np.random`.",
            "description": ""
          },
          "after": {
            "type": "normalization:",
            "description": "``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``."
          },
          "exp_dirichlet_component_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Exponential": {
            "type": "value of expectation of log topic word distribution.",
            "description": ""
          },
          "n_batch_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "bound_": {
            "type": "float",
            "description": ""
          },
          "Final": {
            "type": "perplexity score on training set.",
            "description": ""
          },
          "doc_topic_prior_": {
            "type": "float",
            "description": ""
          },
          "it": {
            "type": "is `1 / n_components`.",
            "description": ""
          },
          "random_state_": {
            "type": "RandomState instance",
            "description": ""
          },
          "RandomState": {
            "type": "instance that is generated either from a seed, the random",
            "description": ""
          },
          "topic_word_prior_": {
            "type": "float",
            "description": ""
          },
          "class": {
            "type": "conditional densities to the data and using Bayes' rule.",
            "description": "References\n----------\n.. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\nHoffman, David M. Blei, Francis Bach, 2010"
          },
          "https": {
            "type": "//github.com/blei",
            "description": "lab/onlineldavb\n.. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,"
          },
          "David": {
            "type": "M. Blei, Chong Wang, John Paisley, 2013",
            "description": "Examples\n--------\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> from sklearn.datasets import make_multilabel_classification\n>>> # This produces a feature matrix of token counts, similar to what\n>>> # CountVectorizer would produce on text.\n>>> X, _ = make_multilabel_classification(random_state=0)\n>>> lda = LatentDirichletAllocation(n_components=5,\n...     random_state=0)\n>>> lda.fit(X)"
          },
          "LatentDirichletAllocation": {
            "type": "...",
            "description": ">>> # get topics for some given samples:\n>>> lda.transform(X[-2:])\narray([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n[0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n        A classifier with a linear decision boundary, generated by fitting\n        class conditional densities to the data and using Bayes' rule.\n\n    References\n    ----------\n    .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n           Hoffman, David M. Blei, Francis Bach, 2010\n           https://github.com/blei-lab/onlineldavb\n\n    .. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,\n           David M. Blei, Chong Wang, John Paisley, 2013\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import LatentDirichletAllocation\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> # This produces a feature matrix of token counts, similar to what\n    >>> # CountVectorizer would produce on text.\n    >>> X, _ = make_multilabel_classification(random_state=0)\n    >>> lda = LatentDirichletAllocation(n_components=5,\n    ...     random_state=0)\n    >>> lda.fit(X)\n    LatentDirichletAllocation(...)\n    >>> # get topics for some given samples:\n    >>> lda.transform(X[-2:])\n    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.decomposition import LatentDirichletAllocation\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> # This produces a feature matrix of token counts, similar to what\n    >>> # CountVectorizer would produce on text.\n    >>> X, _ = make_multilabel_classification(random_state=0)\n    >>> lda = LatentDirichletAllocation(n_components=5,\n    ...     random_state=0)\n    >>> lda.fit(X)\n    LatentDirichletAllocation(...)\n    >>> # get topics for some given samples:\n    >>> lda.transform(X[-2:])\n    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Learn model for the data X with variational Bayes method.\n\n        When `learning_method` is 'online', use mini-batch update.\n        Otherwise, use batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Document": {
                "type": "word matrix.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------\nself"
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, *, normalize=True)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        normalize : bool, default=True\n            Whether to normalize the document topic distribution in `transform`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": ""
              },
              "normalize": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to normalize the document topic distribution in `transform`.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None)",
          "documentation": {
            "description": "Online VB with Mini-Batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Document": {
                "type": "word matrix.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------\nself"
              },
              "Partially": {
                "type": "fitted estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Partially fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "perplexity",
          "signature": "perplexity(self, X, sub_sampling=False)",
          "documentation": {
            "description": "Calculate approximate perplexity for data X.\n\n        Perplexity is defined as exp(-1. * log-likelihood per word)\n\n        .. versionchanged:: 0.19\n           *doc_topic_distr* argument has been deprecated and is ignored\n           because user no longer has access to unnormalized distribution\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        sub_sampling : bool\n            Do sub-sampling or not.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Document": {
                "type": "word matrix.",
                "description": ""
              },
              "sub_sampling": {
                "type": "bool",
                "description": ""
              },
              "Do": {
                "type": "sub-sampling or not.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Perplexity": {
                "type": "score.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Perplexity score.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None)",
          "documentation": {
            "description": "Calculate approximate log-likelihood as score.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Document": {
                "type": "word matrix.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Use": {
                "type": "approximate bound as score.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Use approximate bound as score.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_transform_request",
          "signature": "set_transform_request(self: sklearn.decomposition._lda.LatentDirichletAllocation, *, normalize: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.decomposition._lda.LatentDirichletAllocation",
          "documentation": {
            "description": "Request metadata passed to the ``transform`` method.",
            "parameters": {
              "normalize": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``normalize`` parameter in ``transform``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        normalize : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``normalize`` parameter in ``transform``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, *, normalize=True)",
          "documentation": {
            "description": "Transform data X according to the fitted model.\n\n        .. versionchanged:: 0.18\n            `doc_topic_distr` is now normalized.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        normalize : bool, default=True\n            Whether to normalize the document topic distribution.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Document": {
                "type": "topic distribution for X.",
                "description": ""
              },
              "normalize": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to normalize the document topic distribution.",
                "description": "Returns\n-------"
              },
              "doc_topic_distr": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              }
            },
            "returns": "-------\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Document topic distribution for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MiniBatchDictionaryLearning",
      "documentation": {
        "description": "Mini-batch dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        The algorithm used:\n\n        - `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`)\n        - `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int, default=256\n        Number of samples in each mini-batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples before forming batches.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value of the dictionary for warm restart scenarios.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n          if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    verbose : bool or int, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n        .. versionadded:: 1.1\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components extracted from the data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n        .. versionadded:: 1.1\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of mini-batches processed.",
            "description": ".. versionadded:: 1.1"
          },
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1_000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform if `algorithm='lasso_cd'` or",
            "description": "`'lasso_lars'`.\n.. versionadded:: 0.22"
          },
          "stopping": {
            "type": "independently of any early stopping criterion heuristics.",
            "description": ".. versionadded:: 1.1"
          },
          "fit_algorithm": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": ""
          },
          "The": {
            "type": "default value of `batch_size` changed from 3 to 256 in version 1.3.",
            "description": ""
          },
          "problem": {
            "type": "`linear_model.lars_path`",
            "description": "- `'cd'`: uses the coordinate descent method to compute the"
          },
          "Lasso": {
            "type": "solution (`linear_model.Lasso`). `'lasso_lars'` will be faster",
            "description": ""
          },
          "the": {
            "type": "original signal:",
            "description": ">>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\nnp.float64(0.052...)"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import MiniBatchDictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42)\n>>> dict_learner = MiniBatchDictionaryLearning(\n...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n...     transform_alpha=0.1, max_iter=20, random_state=42)\n>>> X_transformed = dict_learner.fit_transform(X)"
          },
          "batch_size": {
            "type": "int, default=256",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the dictionary.",
            "description": ".. versionadded:: 0.20"
          },
          "dict_init": {
            "type": "ndarray of shape (n_components, n_features), default=None",
            "description": ""
          },
          "Initial": {
            "type": "value of the dictionary for warm restart scenarios.",
            "description": ""
          },
          "transform_algorithm": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to transform the data:",
            "description": "- `'lars'`: uses the least angle regression method\n(`linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution.\n- `'lasso_cd'`: uses the coordinate descent method to compute the"
          },
          "if": {
            "type": "the estimated components are sparse.",
            "description": "- `'omp'`: uses orthogonal matching pursuit to estimate the sparse\nsolution.\n- `'threshold'`: squashes to zero all coefficients less than alpha from"
          },
          "transform_n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "transform_alpha": {
            "type": "float, default=None",
            "description": ""
          },
          "If": {
            "type": "`None`, defaults to `alpha`.",
            "description": ".. versionchanged:: 1.2"
          },
          "penalty": {
            "type": "applied to the L1 norm.",
            "description": ""
          },
          "threshold": {
            "type": "below which coefficients will be squashed to zero.",
            "description": ""
          },
          "When": {
            "type": "None, default value changed from 1.0 to `alpha`.",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "To": {
            "type": "disable convergence detection based on cost function, set",
            "description": "`max_no_improvement` to None.\n.. versionadded:: 1.1\nAttributes\n----------"
          },
          "split_sign": {
            "type": "bool, default=False",
            "description": ""
          },
          "its": {
            "type": "negative part and its positive part. This can improve the",
            "description": ""
          },
          "performance": {
            "type": "of downstream classifiers.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for initializing the dictionary when ``dict_init`` is not",
            "description": "specified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "positive_code": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive_dict": {
            "type": "bool, default=False",
            "description": ""
          },
          "transform_max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "A": {
            "type": "callable that gets invoked at the end of each iteration.",
            "description": ".. versionadded:: 1.1"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Control": {
            "type": "early stopping based on the consecutive number of mini batches",
            "description": ""
          },
          "dictionary": {
            "type": "between 2 steps.",
            "description": ""
          },
          "max_no_improvement": {
            "type": "int, default=10",
            "description": ""
          },
          "that": {
            "type": "does not yield an improvement on the smoothed cost function.",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Components": {
            "type": "extracted from the data.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_steps_": {
            "type": "int",
            "description": ""
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis."
          },
          "SparseCoder": {
            "type": "Find a sparse representation of data from a fixed,",
            "description": ""
          },
          "precomputed": {
            "type": "dictionary.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": "References\n----------\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning"
          },
          "We": {
            "type": "can compare the average squared euclidean norm of the reconstruction",
            "description": ""
          },
          "error": {
            "type": "of the sparse coded signal relative to the squared euclidean norm of",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n    ...     transform_alpha=0.1, max_iter=20, random_state=42)\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0) > 0.5\n    np.True_\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.052...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n    ...     transform_alpha=0.1, max_iter=20, random_state=42)\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0) > 0.5\n    np.True_\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.052...)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None)",
          "documentation": {
            "description": "Update the model using the data in X as a mini-batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Return": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Encode the data as a sparse combination of the dictionary atoms.\n\n        Coding method is determined by the object parameter\n        `transform_algorithm`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Test": {
                "type": "data to be transformed, must have the same number of",
                "description": ""
              },
              "features": {
                "type": "as the data used to train the model.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MiniBatchNMF",
      "documentation": {
        "description": "Mini-Batch Non-Negative Matrix Factorization (NMF).\n\n    .. versionadded:: 1.1\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements,\n    (`W`, `H`) whose product approximates the non-negative matrix `X`. This\n    factorization can be used for example for dimensionality reduction, source\n    separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm).\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of `W`\n    and `H`.\n\n    Note that the transformed data is named `W` and the components matrix is\n    named `H`. In the NMF literature, the naming convention is usually the opposite\n    since the data matrix `X` is transposed.\n\n    Read more in the :ref:`User Guide <MiniBatchNMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness).\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired).\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired).\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    batch_size : int, default=1024\n        Number of samples in each mini-batch. Large batch sizes\n        give better long-term convergence at the cost of a slower start.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between `X`\n        and the dot product `WH`. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n        matrix `X` cannot contain zeros.\n\n    tol : float, default=1e-4\n        Control early stopping based on the norm of the differences in `H`\n        between 2 steps. To disable early stopping based on changes in `H`, set\n        `tol` to 0.0.\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n    max_iter : int, default=200\n        Maximum number of iterations over the complete dataset before\n        timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    forget_factor : float, default=0.7\n        Amount of rescaling of past information. Its value could be 1 with\n        finite datasets. Choosing values < 1 is recommended with online\n        learning as more recent batches will weight more than past batches.\n\n    fresh_restarts : bool, default=False\n        Whether to completely solve for W at each step. Doing fresh restarts will likely\n        lead to a better solution for a same number of iterations but it is much slower.\n\n    fresh_restarts_max_iter : int, default=30\n        Maximum number of iterations when solving for W at each step. Only used when\n        doing fresh restarts. These iterations may be stopped early based on a small\n        change of W controlled by `tol`.\n\n    transform_max_iter : int, default=None\n        Maximum number of iterations when solving for W at transform time.\n        If None, it defaults to `max_iter`.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data `X` and the reconstructed data `WH` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of started iterations over the whole dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.",
        "parameters": {
          "n_components": {
            "type": "int or {'auto'} or None, default='auto'",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ""
          },
          "If": {
            "type": "None, it defaults to `max_iter`.",
            "description": ""
          },
          "from": {
            "type": "W or H shapes.",
            "description": ".. versionchanged:: 1.4"
          },
          "Added": {
            "type": "`'auto'` value.",
            "description": ".. versionchanged:: 1.6"
          },
          "Default": {
            "type": "value changed from `None` to `'auto'`.",
            "description": ""
          },
          "init": {
            "type": "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None",
            "description": ""
          },
          "Method": {
            "type": "used to initialize the procedure.",
            "description": ""
          },
          "Valid": {
            "type": "options:",
            "description": "- `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,"
          },
          "otherwise": {
            "type": "random.",
            "description": "- `'random'`: non-negative random matrices, scaled with:\n`sqrt(X.mean() / n_components)`\n- `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)"
          },
          "initialization": {
            "type": "better for sparseness",
            "description": ".\n- `'nndsvda'`: NNDSVD with zeros filled with the average of X\n(better when sparsity is not desired).\n- `'nndsvdar'` NNDSVD with zeros filled with small random values\n(generally faster, less accurate alternative to NNDSVDa"
          },
          "for": {
            "type": "when sparsity is not desired).",
            "description": "- `'custom'`: Use custom matrices `W` and `H` which must both be provided."
          },
          "batch_size": {
            "type": "int, default=1024",
            "description": ""
          },
          "give": {
            "type": "better long-term convergence at the cost of a slower start.",
            "description": ""
          },
          "beta_loss": {
            "type": "float or {'frobenius', 'kullback",
            "description": "leibler',             'itakura-saito'}, default='frobenius'"
          },
          "Beta": {
            "type": "divergence to be minimized, measuring the distance between `X`",
            "description": ""
          },
          "and": {
            "type": "the dot product `WH`. Note that values different from 'frobenius'",
            "description": "(or 2) and 'kullback-leibler' (or 1) lead to significantly slower\nfits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input"
          },
          "matrix": {
            "type": "`X` cannot contain zeros.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Control": {
            "type": "early stopping based on the consecutive number of mini batches",
            "description": ""
          },
          "between": {
            "type": "2 steps. To disable early stopping based on changes in `H`, set",
            "description": "`tol` to 0.0."
          },
          "max_no_improvement": {
            "type": "int, default=10",
            "description": ""
          },
          "that": {
            "type": "does not yield an improvement on the smoothed cost function.",
            "description": ""
          },
          "To": {
            "type": "disable convergence detection based on cost function, set",
            "description": "`max_no_improvement` to None."
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations when solving for W at transform time.",
            "description": ""
          },
          "timing": {
            "type": "out.",
            "description": ""
          },
          "alpha_W": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the regularization terms of `H`. Set it to zero to",
            "description": ""
          },
          "alpha_H": {
            "type": "float or \"same\", default=\"same\"",
            "description": ""
          },
          "have": {
            "type": "no regularization on `H`. If \"same\" (default), it takes the same value as",
            "description": "`alpha_W`."
          },
          "l1_ratio": {
            "type": "float, default=0.0",
            "description": ""
          },
          "The": {
            "type": "number of components. It is same as the `n_components` parameter",
            "description": ""
          },
          "For": {
            "type": "0 < l1_ratio < 1, the penalty is a combination of L1 and L2.",
            "description": ""
          },
          "forget_factor": {
            "type": "float, default=0.7",
            "description": ""
          },
          "Amount": {
            "type": "of rescaling of past information. Its value could be 1 with",
            "description": ""
          },
          "finite": {
            "type": "datasets. Choosing values < 1 is recommended with online",
            "description": ""
          },
          "learning": {
            "type": "as more recent batches will weight more than past batches.",
            "description": ""
          },
          "fresh_restarts": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to be verbose.",
            "description": "Attributes\n----------"
          },
          "lead": {
            "type": "to a better solution for a same number of iterations but it is much slower.",
            "description": ""
          },
          "fresh_restarts_max_iter": {
            "type": "int, default=30",
            "description": ""
          },
          "doing": {
            "type": "fresh restarts. These iterations may be stopped early based on a small",
            "description": ""
          },
          "change": {
            "type": "of W controlled by `tol`.",
            "description": ""
          },
          "transform_max_iter": {
            "type": "int, default=None",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for initialisation (when ``init`` == 'nndsvdar' or",
            "description": "'random'), and in Coordinate Descent. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Factorization": {
            "type": "matrix, sometimes called 'dictionary'.",
            "description": ""
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "if": {
            "type": "it was given. Otherwise, it will be same as the number of",
            "description": "features."
          },
          "reconstruction_err_": {
            "type": "float",
            "description": ""
          },
          "Frobenius": {
            "type": "norm of the matrix difference, or beta-divergence, between",
            "description": ""
          },
          "the": {
            "type": "fitted model.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of started iterations over the whole dataset.",
            "description": ""
          },
          "n_steps_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ""
          },
          "NMF": {
            "type": "Non",
            "description": "negative matrix factorization."
          },
          "MiniBatchDictionaryLearning": {
            "type": "Finds a dictionary that can best be used to represent",
            "description": ""
          },
          "data": {
            "type": "using a sparse code.",
            "description": "References\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\nfactorizations\" <10.1587/transfun.E92.A.708>`\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals"
          },
          "of": {
            "type": "electronics, communications and computer sciences 92.3: 708-721, 2009.",
            "description": ".. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\nbeta-divergence\" <10.1162/NECO_a_00168>`\nFevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n.. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\nItakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\nLefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import MiniBatchNMF\n>>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_",
        "notes": "that the transformed data is named `W` and the components matrix is\n    named `H`. In the NMF literature, the naming convention is usually the opposite\n    since the data matrix `X` is transposed.\n\n    Read more in the :ref:`User Guide <MiniBatchNMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness).\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired).\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired).\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    batch_size : int, default=1024\n        Number of samples in each mini-batch. Large batch sizes\n        give better long-term convergence at the cost of a slower start.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between `X`\n        and the dot product `WH`. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n        matrix `X` cannot contain zeros.\n\n    tol : float, default=1e-4\n        Control early stopping based on the norm of the differences in `H`\n        between 2 steps. To disable early stopping based on changes in `H`, set\n        `tol` to 0.0.\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n    max_iter : int, default=200\n        Maximum number of iterations over the complete dataset before\n        timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    forget_factor : float, default=0.7\n        Amount of rescaling of past information. Its value could be 1 with\n        finite datasets. Choosing values < 1 is recommended with online\n        learning as more recent batches will weight more than past batches.\n\n    fresh_restarts : bool, default=False\n        Whether to completely solve for W at each step. Doing fresh restarts will likely\n        lead to a better solution for a same number of iterations but it is much slower.\n\n    fresh_restarts_max_iter : int, default=30\n        Maximum number of iterations when solving for W at each step. Only used when\n        doing fresh restarts. These iterations may be stopped early based on a small\n        change of W controlled by `tol`.\n\n    transform_max_iter : int, default=None\n        Maximum number of iterations when solving for W at transform time.\n        If None, it defaults to `max_iter`.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data `X` and the reconstructed data `WH` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of started iterations over the whole dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_",
        "examples": "--------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "documentation": {
            "description": "Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : kwargs\n            Parameters (keyword arguments) and values passed to\n            the fit_transform instance.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**params : kwargs"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, W=None, H=None)",
          "documentation": {
            "description": "Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "matrix to be decomposed.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "W": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "If": {
                "type": "`None`, uses the initialisation method specified in `init`.",
                "description": "Returns\n-------"
              },
              "H": {
                "type": "array",
                "description": "like of shape (n_components, n_features), default=None"
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X=None, *, Xt=None)",
          "documentation": {
            "description": "Transform data back to its original space.\n\n        .. versionadded:: 0.18\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Transformed": {
                "type": "data matrix.",
                "description": ".. deprecated:: 1.5\n`Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\nReturns\n-------"
              },
              "Xt": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_components)",
                "description": ""
              },
              "Returns": {
                "type": "a data matrix of the original shape.",
                "description": ""
              }
            },
            "returns": "-------\n        X : ndarray of shape (n_samples, n_features)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, W=None, H=None)",
          "documentation": {
            "description": "Update the model using the data in `X` as a mini-batch.\n\n        This method is expected to be called several times consecutively\n        on different chunks of a dataset so as to implement out-of-core\n        or online learning.\n\n        This is especially useful when the whole dataset is too big to fit in\n        memory at once (see :ref:`scaling_strategies`).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            Only used for the first call to `partial_fit`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            Only used for the first call to `partial_fit`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "matrix to be decomposed.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "W": {
                "type": "array",
                "description": "like of shape (n_samples, n_components), default=None"
              },
              "If": {
                "type": "`init='custom'`, it is used as initial guess for the solution.",
                "description": ""
              },
              "Only": {
                "type": "used for the first call to `partial_fit`.",
                "description": "Returns\n-------\nself"
              },
              "H": {
                "type": "array",
                "description": "like of shape (n_components, n_features), default=None"
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform the data X according to the fitted MiniBatchNMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be transformed by the model.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "matrix to be transformed by the model.",
                "description": "Returns\n-------"
              },
              "W": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MiniBatchSparsePCA",
      "documentation": {
        "description": "Mini-batch Sparse Principal Components Analysis.\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    For an example comparing sparse PCA to PCA, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : int, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.2\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    batch_size : int, default=3\n        The number of features to take in each mini batch.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for random shuffling when ``shuffle`` is set to ``True``,\n        during online dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int or None, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to `None`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    IncrementalPCA : Incremental principal components analysis.\n    PCA : Principal component analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "is": {
            "type": "set to ``n_features``.",
            "description": ""
          },
          "alpha": {
            "type": "int, default=1",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter. Higher values lead to sparser",
            "description": "components."
          },
          "ridge_alpha": {
            "type": "float, default=0.01",
            "description": ""
          },
          "Amount": {
            "type": "of ridge shrinkage to apply in order to improve",
            "description": ""
          },
          "conditioning": {
            "type": "when calling the transform method.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1_000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations over the complete dataset before",
            "description": ""
          },
          "stopping": {
            "type": "independently of any early stopping criterion heuristics.",
            "description": ".. versionadded:: 1.2"
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "Callable": {
            "type": "that gets invoked every five iterations.",
            "description": ""
          },
          "batch_size": {
            "type": "int, default=3",
            "description": ""
          },
          "The": {
            "type": "number of features to take in each mini batch.",
            "description": ""
          },
          "verbose": {
            "type": "int or bool, default=False",
            "description": ""
          },
          "Controls": {
            "type": "the verbosity; the higher, the more messages. Defaults to 0.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to shuffle the data before splitting it in batches.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "method": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": ""
          },
          "Method": {
            "type": "to be used for optimization.",
            "description": ""
          },
          "lars": {
            "type": "uses the least angle regression method to solve the lasso problem",
            "description": "(linear_model.lars_path)"
          },
          "cd": {
            "type": "uses the coordinate descent method to compute the",
            "description": ""
          },
          "Lasso": {
            "type": "solution (linear_model.Lasso). Lars will be faster if",
            "description": ""
          },
          "the": {
            "type": "estimated components are sparse.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for random shuffling when ``shuffle`` is set to ``True``,",
            "description": ""
          },
          "during": {
            "type": "online dictionary learning. Pass an int for reproducible results",
            "description": ""
          },
          "across": {
            "type": "multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Control": {
            "type": "early stopping based on the consecutive number of mini batches",
            "description": ""
          },
          "dictionary": {
            "type": "between 2 steps.",
            "description": ""
          },
          "To": {
            "type": "disable convergence detection based on cost function, set",
            "description": "`max_no_improvement` to `None`.\n.. versionadded:: 1.1\nAttributes\n----------"
          },
          "max_no_improvement": {
            "type": "int or None, default=10",
            "description": ""
          },
          "that": {
            "type": "does not yield an improvement on the smoothed cost function.",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Sparse": {
            "type": "components extracted from the data.",
            "description": ""
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "Estimated": {
            "type": "number of components.",
            "description": ".. versionadded:: 0.23"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "mean_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical mean, estimated from the training set."
          },
          "Equal": {
            "type": "to ``X.mean(axis=0)``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "IncrementalPCA": {
            "type": "Incremental principal components analysis.",
            "description": ""
          },
          "PCA": {
            "type": "Principal component analysis.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": ""
          },
          "TruncatedSVD": {
            "type": "Dimensionality reduction using truncated SVD.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import MiniBatchSparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n...                                  max_iter=10, random_state=0)\n>>> transformer.fit(X)"
          },
          "MiniBatchSparsePCA": {
            "type": "...",
            "description": ">>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\nnp.float64(0.9...)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    IncrementalPCA : Incremental principal components analysis.\n    PCA : Principal component analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  max_iter=10, random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  max_iter=10, random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9...)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform data from the latent space to the original space.\n\n        This inversion is an approximation due to the loss of information\n        induced by the forward decomposition.\n\n        .. versionadded:: 1.2\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_components)\n            Data in the latent space.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Data": {
                "type": "in the latent space.",
                "description": "Returns\n-------"
              },
              "X_original": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Reconstructed": {
                "type": "data in the original space.",
                "description": ""
              }
            },
            "returns": "-------\n        X_original : ndarray of shape (n_samples, n_features)\n            Reconstructed data in the original space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Least Squares projection of the data onto the sparse components.\n\n        To avoid instability issues in case the system is under-determined,\n        regularization can be applied (Ridge regression) via the\n        `ridge_alpha` parameter.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Test": {
                "type": "data to be transformed, must have the same number of",
                "description": ""
              },
              "features": {
                "type": "as the data used to train the model.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "that Sparse PCA components orthogonality is not enforced as in PCA\n        hence one cannot use a simple linear projection.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NMF",
      "documentation": {
        "description": "Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n    whose product approximates the non-negative matrix X. This factorization can be used\n    for example for dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm).\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).",
        "parameters": {
          "n_components": {
            "type": "int or {'auto'} or None, default='auto'",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "If": {
            "type": "true, randomize the order of coordinates in the CD solver.",
            "description": ".. versionadded:: 0.17\n*shuffle* parameter used in the Coordinate Descent solver.\nAttributes\n----------"
          },
          "from": {
            "type": "W or H shapes.",
            "description": ".. versionchanged:: 1.4"
          },
          "Added": {
            "type": "`'auto'` value.",
            "description": ".. versionchanged:: 1.6"
          },
          "Default": {
            "type": "value changed from `None` to `'auto'`.",
            "description": ""
          },
          "init": {
            "type": "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None",
            "description": ""
          },
          "Method": {
            "type": "used to initialize the procedure.",
            "description": ""
          },
          "Valid": {
            "type": "options:",
            "description": "- `None`: 'nndsvda' if n_components <= min(n_samples, n_features),"
          },
          "otherwise": {
            "type": "random.",
            "description": "- `'random'`: non-negative random matrices, scaled with:\n`sqrt(X.mean() / n_components)`\n- `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)"
          },
          "initialization": {
            "type": "better for sparseness",
            "description": "- `'nndsvda'`: NNDSVD with zeros filled with the average of X\n(better when sparsity is not desired)\n- `'nndsvdar'` NNDSVD with zeros filled with small random values\n(generally faster, less accurate alternative to NNDSVDa"
          },
          "for": {
            "type": "when sparsity is not desired)",
            "description": "- `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n.. versionchanged:: 1.1"
          },
          "When": {
            "type": "`init=None` and n_components is less than n_samples and n_features",
            "description": ""
          },
          "defaults": {
            "type": "to `nndsvda` instead of `nndsvd`.",
            "description": ""
          },
          "solver": {
            "type": "{'cd', 'mu'}, default='cd'",
            "description": ""
          },
          "Numerical": {
            "type": "solver to use:",
            "description": "- 'cd' is a Coordinate Descent solver.\n- 'mu' is a Multiplicative Update solver.\n.. versionadded:: 0.17"
          },
          "Coordinate": {
            "type": "Descent solver.",
            "description": ".. versionadded:: 0.19"
          },
          "Multiplicative": {
            "type": "Update solver.",
            "description": ""
          },
          "beta_loss": {
            "type": "float or {'frobenius', 'kullback",
            "description": "leibler',             'itakura-saito'}, default='frobenius'"
          },
          "Beta": {
            "type": "divergence to be minimized, measuring the distance between X",
            "description": ""
          },
          "and": {
            "type": "the dot product WH. Note that values different from 'frobenius'",
            "description": "(or 2) and 'kullback-leibler' (or 1) lead to significantly slower\nfits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input"
          },
          "matrix": {
            "type": "X cannot contain zeros. Used only in 'mu' solver.",
            "description": ".. versionadded:: 0.19"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Tolerance": {
            "type": "of the stopping condition.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations before timing out.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "for initialisation (when ``init`` == 'nndsvdar' or",
            "description": "'random'), and in Coordinate Descent. Pass an int for reproducible"
          },
          "results": {
            "type": "across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "alpha_W": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the regularization terms of `H`. Set it to zero to",
            "description": ""
          },
          "alpha_H": {
            "type": "float or \"same\", default=\"same\"",
            "description": ""
          },
          "have": {
            "type": "no regularization on `H`. If \"same\" (default), it takes the same value as",
            "description": "`alpha_W`.\n.. versionadded:: 1.0"
          },
          "l1_ratio": {
            "type": "float, default=0.0",
            "description": ""
          },
          "The": {
            "type": "number of components. It is same as the `n_components` parameter",
            "description": ""
          },
          "For": {
            "type": "0 < l1_ratio < 1, the penalty is a combination of L1 and L2.",
            "description": ".. versionadded:: 0.17"
          },
          "Regularization": {
            "type": "parameter *l1_ratio* used in the Coordinate Descent",
            "description": "solver."
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Whether": {
            "type": "to be verbose.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=False",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Factorization": {
            "type": "matrix, sometimes called 'dictionary'.",
            "description": ""
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "if": {
            "type": "it was given. Otherwise, it will be same as the number of",
            "description": "features."
          },
          "reconstruction_err_": {
            "type": "float",
            "description": ""
          },
          "Frobenius": {
            "type": "norm of the matrix difference, or beta-divergence, between",
            "description": ""
          },
          "the": {
            "type": "fitted model.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis."
          },
          "PCA": {
            "type": "Principal component analysis.",
            "description": ""
          },
          "SparseCoder": {
            "type": "Find a sparse representation of data from a fixed,",
            "description": ""
          },
          "precomputed": {
            "type": "dictionary.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": ""
          },
          "TruncatedSVD": {
            "type": "Dimensionality reduction using truncated SVD.",
            "description": "References\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\nfactorizations\" <10.1587/transfun.E92.A.708>`\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals"
          },
          "of": {
            "type": "electronics, communications and computer sciences 92.3: 708-721, 2009.",
            "description": ".. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\nbeta-divergence\" <10.1162/NECO_a_00168>`\nFevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_",
        "notes": "that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_",
        "examples": "--------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "documentation": {
            "description": "Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : kwargs\n            Parameters (keyword arguments) and values passed to\n            the fit_transform instance.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**params : kwargs"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, W=None, H=None)",
          "documentation": {
            "description": "Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        W : array-like of shape (n_samples, n_components), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.\n\n        H : array-like of shape (n_components, n_features), default=None\n            If `init='custom'`, it is used as initial guess for the solution.\n            If `None`, uses the initialisation method specified in `init`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": ""
              },
              "W": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "If": {
                "type": "`None`, uses the initialisation method specified in `init`.",
                "description": "Returns\n-------"
              },
              "H": {
                "type": "array",
                "description": "like of shape (n_components, n_features), default=None"
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X=None, *, Xt=None)",
          "documentation": {
            "description": "Transform data back to its original space.\n\n        .. versionadded:: 0.18\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Transformed": {
                "type": "data matrix.",
                "description": ".. deprecated:: 1.5\n`Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\nReturns\n-------"
              },
              "Xt": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_components)",
                "description": ""
              },
              "Returns": {
                "type": "a data matrix of the original shape.",
                "description": ""
              }
            },
            "returns": "-------\n        X : ndarray of shape (n_samples, n_features)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform the data X according to the fitted NMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "W": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PCA",
      "documentation": {
        "description": "Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    With sparse inputs, the ARPACK implementation of the truncated SVD can be\n    used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one\n    may consider :class:`TruncatedSVD` where the data are not centered.\n\n    Notice that this class only supports sparse inputs for some solvers such as\n    \"arpack\" and \"covariance_eigh\". See :class:`TruncatedSVD` for an\n    alternative with sparse data.\n\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float or 'mle', default=None\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, default=False\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'\n        \"auto\" :\n            The solver is selected by a default 'auto' policy is based on `X.shape` and\n            `n_components`: if the input data has fewer than 1000 features and\n            more than 10 times as many samples, then the \"covariance_eigh\"\n            solver is used. Otherwise, if the input data is larger than 500x500\n            and the number of components to extract is lower than 80% of the\n            smallest dimension of the data, then the more efficient\n            \"randomized\" method is selected. Otherwise the exact \"full\" SVD is\n            computed and optionally truncated afterwards.\n        \"full\" :\n            Run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        \"covariance_eigh\" :\n            Precompute the covariance matrix (on centered data), run a\n            classical eigenvalue decomposition on the covariance matrix\n            typically using LAPACK and select the components by postprocessing.\n            This solver is very efficient for n_samples >> n_features and small\n            n_features. It is, however, not tractable otherwise for large\n            n_features (large memory footprint required to materialize the\n            covariance matrix). Also note that compared to the \"full\" solver,\n            this solver effectively doubles the condition number and is\n            therefore less numerical stable (e.g. on input data with a large\n            range of singular values).\n        \"arpack\" :\n            Run SVD truncated to `n_components` calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            `0 < n_components < min(X.shape)`\n        \"randomized\" :\n            Run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n        .. versionchanged:: 1.5\n            Added the 'covariance_eigh' solver.\n\n    tol : float, default=0.0\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n        Must be of range [0.0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n        Must be of range [0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    n_oversamples : int, default=10\n        This parameter is only relevant when `svd_solver=\"randomized\"`.\n        It corresponds to the additional number of random vectors to sample the\n        range of `X` so as to ensure proper conditioning. See\n        :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The amount of variance explained by each of the selected components.\n        The variance estimation uses `n_samples - 1` degrees of freedom.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <10.1137/090771806>`\n    and also\n    :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.\n    <10.1016/j.acha.2010.02.003>`",
        "parameters": {
          "n_components": {
            "type": "== min(n_samples, n_features) - 1",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "if": {
            "type": "n_components is None.",
            "description": ""
          },
          "If": {
            "type": "``n_components`` is not set then all components are stored and the",
            "description": ""
          },
          "MLE": {
            "type": "is used to guess the dimension. Use of ``n_components == 'mle'``",
            "description": ""
          },
          "will": {
            "type": "interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.",
            "description": ""
          },
          "number": {
            "type": "is estimated from input data. Otherwise it equals the parameter",
            "description": "n_components, or the lesser value of n_features and n_samples"
          },
          "explained": {
            "type": "is greater than the percentage specified by n_components.",
            "description": ""
          },
          "strictly": {
            "type": "less than the minimum of n_features and n_samples.",
            "description": "Hence, the None case results in::"
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "fit": {
            "type": "X",
            "description": ".transform(X) will not yield the expected results,"
          },
          "use": {
            "type": "fit_transform(X) instead.",
            "description": ""
          },
          "whiten": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "True (False by default) the `components_` vectors are multiplied",
            "description": ""
          },
          "by": {
            "type": "the square root of n_samples and then divided by the singular values",
            "description": ""
          },
          "to": {
            "type": "'mle' or a number between 0 and 1 (with svd_solver == 'full') this",
            "description": ""
          },
          "Whitening": {
            "type": "will remove some information from the transformed signal",
            "description": "(the relative variance scales of the components) but can sometime"
          },
          "improve": {
            "type": "the predictive accuracy of the downstream estimators by",
            "description": ""
          },
          "making": {
            "type": "their data respect some hard-wired assumptions.",
            "description": ""
          },
          "svd_solver": {
            "type": "== 'randomized'.",
            "description": ""
          },
          "The": {
            "type": "estimated noise covariance following the Probabilistic PCA model",
            "description": ""
          },
          "more": {
            "type": "than 10 times as many samples, then the \"covariance_eigh\"",
            "description": ""
          },
          "solver": {
            "type": "is used. Otherwise, if the input data is larger than 500x500",
            "description": ""
          },
          "and": {
            "type": "also",
            "description": ":doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\"."
          },
          "smallest": {
            "type": "eigenvalues of the covariance matrix of X.",
            "description": ""
          },
          "computed": {
            "type": "and optionally truncated afterwards.",
            "description": "\"full\" :"
          },
          "Run": {
            "type": "randomized SVD by the method of Halko et al.",
            "description": ".. versionadded:: 0.18.0\n.. versionchanged:: 1.5"
          },
          "Precompute": {
            "type": "the covariance matrix (on centered data), run a",
            "description": ""
          },
          "classical": {
            "type": "eigenvalue decomposition on the covariance matrix",
            "description": ""
          },
          "typically": {
            "type": "using LAPACK and select the components by postprocessing.",
            "description": ""
          },
          "This": {
            "type": "parameter is only relevant when `svd_solver=\"randomized\"`.",
            "description": ""
          },
          "n_features": {
            "type": "(large memory footprint required to materialize the",
            "description": ""
          },
          "covariance": {
            "type": "matrix). Also note that compared to the \"full\" solver,",
            "description": ""
          },
          "this": {
            "type": "solver effectively doubles the condition number and is",
            "description": ""
          },
          "therefore": {
            "type": "less numerical stable (e.g. on input data with a large",
            "description": ""
          },
          "range": {
            "type": "of `X` so as to ensure proper conditioning. See",
            "description": ":func:`~sklearn.utils.extmath.randomized_svd` for more details.\n.. versionadded:: 1.1"
          },
          "Added": {
            "type": "the 'covariance_eigh' solver.",
            "description": ""
          },
          "tol": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Tolerance": {
            "type": "for singular values computed by svd_solver == 'arpack'.",
            "description": ""
          },
          "Must": {
            "type": "be of range [0, infinity).",
            "description": ".. versionadded:: 0.18.0"
          },
          "iterated_power": {
            "type": "int or 'auto', default='auto'",
            "description": ""
          },
          "n_oversamples": {
            "type": "int, default=10",
            "description": ""
          },
          "It": {
            "type": "corresponds to the additional number of random vectors to sample the",
            "description": ""
          },
          "power_iteration_normalizer": {
            "type": "{'auto', 'QR', 'LU', 'none'}, default='auto'",
            "description": ""
          },
          "Power": {
            "type": "iteration normalizer for randomized SVD solver.",
            "description": ""
          },
          "Not": {
            "type": "used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`",
            "description": ""
          },
          "for": {
            "type": "reproducible results across multiple function calls.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "when the 'arpack' or 'randomized' solvers are used. Pass an int",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Principal": {
            "type": "axes in feature space, representing the directions of",
            "description": ""
          },
          "maximum": {
            "type": "variance in the data. Equivalently, the right singular",
            "description": ""
          },
          "vectors": {
            "type": "of the centered input data, parallel to its eigenvectors.",
            "description": ""
          },
          "explained_variance_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Equal": {
            "type": "to the average of (min(n_features, n_samples) - n_components)",
            "description": ""
          },
          "of": {
            "type": "the covariance matrix of X.",
            "description": ".. versionadded:: 0.18"
          },
          "explained_variance_ratio_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Percentage": {
            "type": "of variance explained by each of the selected components.",
            "description": ""
          },
          "sum": {
            "type": "of the ratios is equal to 1.0.",
            "description": ""
          },
          "singular_values_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "variables": {
            "type": "in the lower-dimensional space.",
            "description": ".. versionadded:: 0.19"
          },
          "mean_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical mean, estimated from the training set."
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "n_samples_": {
            "type": "int",
            "description": ""
          },
          "noise_variance_": {
            "type": "float",
            "description": ""
          },
          "from": {
            "type": "Tipping and Bishop 1999. See \"Pattern Recognition and",
            "description": ""
          },
          "Machine": {
            "type": "Learning\" by C. Bishop, 12.2.1 p. 574 or",
            "description": ""
          },
          "http": {
            "type": "//www.miketipping.com/papers/met",
            "description": "mppca.pdf. It is required to"
          },
          "compute": {
            "type": "the estimated data covariance and score samples.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "KernelPCA": {
            "type": "Kernel Principal Component Analysis.",
            "description": ""
          },
          "SparsePCA": {
            "type": "Sparse Principal Component Analysis.",
            "description": ""
          },
          "TruncatedSVD": {
            "type": "Dimensionality reduction using truncated SVD.",
            "description": ""
          },
          "IncrementalPCA": {
            "type": "Incremental Principal Component Analysis.",
            "description": "References\n----------"
          },
          "For": {
            "type": "svd_solver == 'randomized', see:",
            "description": ":doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for"
          },
          "In": {
            "type": "NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_",
            "description": ""
          },
          "Implements": {
            "type": "the probabilistic PCA model from:",
            "description": "`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal"
          },
          "component": {
            "type": "analysis\". Journal of the Royal Statistical Society:",
            "description": ""
          },
          "Series": {
            "type": "B (Statistical Methodology), 61(3), 611-622.",
            "description": "<http://www.miketipping.com/papers/met-mppca.pdf>`_"
          },
          "via": {
            "type": "the score and score_samples methods.",
            "description": ""
          },
          "constructing": {
            "type": "approximate matrix decompositions\".",
            "description": ""
          },
          "SIAM": {
            "type": "review, 53(2), 217-288.",
            "description": "<10.1137/090771806>`"
          },
          "Applied": {
            "type": "and Computational Harmonic Analysis, 30(1), 47-68.",
            "description": "<10.1016/j.acha.2010.02.003>`\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)"
          },
          "PCA": {
            "type": "n_components=1, svd_solver='arpack'",
            "description": ">>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <10.1137/090771806>`\n    and also\n    :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.\n    <10.1016/j.acha.2010.02.003>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Ignored.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": "Ignored.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Ignored.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": "Ignored.\nReturns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "values.",
                "description": "Notes\n-----"
              },
              "This": {
                "type": "method returns a Fortran-ordered array. To convert it to a",
                "description": "C-ordered array, use 'np.ascontiguousarray'."
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed values.\n\n        Notes\n        -----\n        This method returns a Fortran-ordered array. To convert it to a\n        C-ordered array, use 'np.ascontiguousarray'.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        This method returns a Fortran-ordered array. To convert it to a\n        C-ordered array, use 'np.ascontiguousarray'.",
            "examples": ""
          }
        },
        {
          "name": "get_covariance",
          "signature": "get_covariance(self)",
          "documentation": {
            "description": "Compute data covariance with the generative model.\n\n        ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n        where S**2 contains the explained variances, and sigma2 contains the\n        noise variances.",
            "parameters": {},
            "returns": "-------\n        cov : array of shape=(n_features, n_features)\n            Estimated covariance of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Compute data precision matrix with the generative model.\n\n        Equals the inverse of the covariance but computed with\n        the matrix inversion lemma for efficiency.",
            "parameters": {},
            "returns": "-------\n        precision : array, shape=(n_features, n_features)\n            Estimated precision of data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform data back to its original space.\n\n        In other words, return an input `X_original` whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data, where `n_samples` is the number of samples\n            and `n_components` is the number of components.\n\n        Returns\n        -------\n        X_original array-like of shape (n_samples, n_features)\n            Original data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_components)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Notes\n-----"
              },
              "X_original": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": ""
              },
              "Original": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "If": {
                "type": "whitening is enabled, inverse_transform will compute the",
                "description": ""
              },
              "exact": {
                "type": "inverse operation, which includes reversing whitening.",
                "description": ""
              }
            },
            "returns": "-------\n        X_original array-like of shape (n_samples, n_features)\n            Original data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        Notes\n        -----\n        If whitening is enabled, inverse_transform will compute the\n        exact inverse operation, which includes reversing whitening.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        If whitening is enabled, inverse_transform will compute the\n        exact inverse operation, which includes reversing whitening.",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None)",
          "documentation": {
            "description": "Return the average log-likelihood of all samples.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Ignored.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": "Ignored.\nReturns\n-------"
              },
              "ll": {
                "type": "float",
                "description": ""
              },
              "Average": {
                "type": "log-likelihood of the samples under the current model.",
                "description": ""
              }
            },
            "returns": "-------\n        ll : float\n            Average log-likelihood of the samples under the current model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Return the log-likelihood of each sample.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "ll": {
                "type": "ndarray of shape (n_samples,)",
                "description": "Log-likelihood of each sample under the current model."
              }
            },
            "returns": "-------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "array",
                "description": "like of shape (n_samples, n_components)"
              },
              "Projection": {
                "type": "of X in the first principal components, where `n_samples`",
                "description": ""
              },
              "is": {
                "type": "the number of samples and `n_components` is the number of the components.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : array-like of shape (n_samples, n_components)\n            Projection of X in the first principal components, where `n_samples`\n            is the number of samples and `n_components` is the number of the components.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SparseCoder",
      "documentation": {
        "description": "Sparse coding.\n\n    Finds a sparse representation of data against a fixed, precomputed\n    dictionary.\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    dictionary : ndarray of shape (n_components, n_features)\n        The dictionary atoms used for sparse coding. Lines are assumed to be\n        normalized to unit norm.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n          the estimated components are sparse;\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `lasso_lars`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_components_ : int\n        Number of atoms.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    sparse_encode : Sparse coding where each row of the result is the solution\n        to a sparse coding problem.",
        "parameters": {
          "dictionary": {
            "type": "learning algorithm.",
            "description": ""
          },
          "The": {
            "type": "dictionary atoms used for sparse coding. Lines are assumed to be",
            "description": ""
          },
          "normalized": {
            "type": "to unit norm.",
            "description": ""
          },
          "transform_algorithm": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to transform the data:",
            "description": "- `'lars'`: uses the least angle regression method\n(`linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution;\n- `'lasso_cd'`: uses the coordinate descent method to compute the"
          },
          "Lasso": {
            "type": "solution (linear_model.Lasso). `'lasso_lars'` will be faster if",
            "description": ""
          },
          "the": {
            "type": "reconstruction error targeted. In this case, it overrides",
            "description": "`n_nonzero_coefs`."
          },
          "transform_n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "and": {
            "type": "is overridden by `alpha` in the `omp` case. If `None`, then",
            "description": "`transform_n_nonzero_coefs=int(n_features / 10)`."
          },
          "transform_alpha": {
            "type": "float, default=None",
            "description": ""
          },
          "If": {
            "type": "`None`, default to 1.",
            "description": ""
          },
          "penalty": {
            "type": "applied to the L1 norm.",
            "description": ""
          },
          "threshold": {
            "type": "below which coefficients will be squashed to zero.",
            "description": ""
          },
          "split_sign": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to enforce positivity when finding the code.",
            "description": ".. versionadded:: 0.20"
          },
          "its": {
            "type": "negative part and its positive part. This can improve the",
            "description": ""
          },
          "performance": {
            "type": "of downstream classifiers.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "positive_code": {
            "type": "bool, default=False",
            "description": ""
          },
          "transform_max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform if `algorithm='lasso_cd'` or",
            "description": "`lasso_lars`.\n.. versionadded:: 0.22\nAttributes\n----------"
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "MiniBatchDictionaryLearning": {
            "type": "A faster, less accurate, version of the",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini",
            "description": "batch Sparse Principal Components Analysis."
          },
          "SparsePCA": {
            "type": "Sparse Principal Components Analysis.",
            "description": ""
          },
          "sparse_encode": {
            "type": "Sparse coding where each row of the result is the solution",
            "description": ""
          },
          "to": {
            "type": "a sparse coding problem.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import SparseCoder\n>>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n>>> dictionary = np.array(\n...     [[0, 1, 0],\n...      [-1, -1, 2],\n...      [1, 1, 1],\n...      [0, 1, 1],\n...      [0, 2, 1]],\n...    dtype=np.float64\n... )\n>>> coder = SparseCoder(\n...     dictionary=dictionary, transform_algorithm='lasso_lars',\n...     transform_alpha=1e-10,\n... )\n>>> coder.transform(X)\narray([[ 0.,  0., -1.,  0.,  0.],\n[ 0.,  1.,  1.,  0.,  0.]])"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    sparse_encode : Sparse coding where each row of the result is the solution\n        to a sparse coding problem.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import SparseCoder\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> coder = SparseCoder(\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\n    ...     transform_alpha=1e-10,\n    ... )\n    >>> coder.transform(X)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import SparseCoder\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> coder = SparseCoder(\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\n    ...     transform_alpha=1e-10,\n    ... )\n    >>> coder.transform(X)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Do nothing and return the estimator unchanged.\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n\n        Parameters\n        ----------\n        X : Ignored\n            Not used, present for API consistency by convention.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, y=None)",
          "documentation": {
            "description": "Encode the data as a sparse combination of the dictionary atoms.\n\n        Coding method is determined by the object parameter\n        `transform_algorithm`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SparsePCA",
      "documentation": {
        "description": "Sparse Principal Components Analysis (SparsePCA).\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    U_init : ndarray of shape (n_samples, n_components), default=None\n        Initial values for the loadings for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    V_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the components for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    random_state : int, RandomState instance or None, default=None\n        Used during dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    error_ : ndarray\n        Vector of errors at each iteration.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA : Principal Component Analysis implementation.\n    MiniBatchSparsePCA : Mini batch variant of `SparsePCA` that is faster but less\n        accurate.\n    DictionaryLearning : Generic dictionary learning problem using a sparse code.",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "is": {
            "type": "set to ``n_features``.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Sparsity": {
            "type": "controlling parameter. Higher values lead to sparser",
            "description": "components."
          },
          "ridge_alpha": {
            "type": "float, default=0.01",
            "description": ""
          },
          "Amount": {
            "type": "of ridge shrinkage to apply in order to improve",
            "description": ""
          },
          "conditioning": {
            "type": "when calling the transform method.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "8"
          },
          "Tolerance": {
            "type": "for the stopping condition.",
            "description": ""
          },
          "method": {
            "type": "{'lars', 'cd'}, default='lars'",
            "description": ""
          },
          "Method": {
            "type": "to be used for optimization.",
            "description": ""
          },
          "lars": {
            "type": "uses the least angle regression method to solve the lasso problem",
            "description": "(linear_model.lars_path)"
          },
          "cd": {
            "type": "uses the coordinate descent method to compute the",
            "description": ""
          },
          "Lasso": {
            "type": "solution (linear_model.Lasso). Lars will be faster if",
            "description": ""
          },
          "the": {
            "type": "estimated components are sparse.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "U_init": {
            "type": "ndarray of shape (n_samples, n_components), default=None",
            "description": ""
          },
          "Initial": {
            "type": "values for the components for warm restart scenarios. Only used",
            "description": ""
          },
          "if": {
            "type": "`U_init` and `V_init` are not None.",
            "description": ""
          },
          "V_init": {
            "type": "ndarray of shape (n_components, n_features), default=None",
            "description": ""
          },
          "verbose": {
            "type": "int or bool, default=False",
            "description": ""
          },
          "Controls": {
            "type": "the verbosity; the higher, the more messages. Defaults to 0.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "during dictionary learning. Pass an int for reproducible results",
            "description": ""
          },
          "across": {
            "type": "multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Sparse": {
            "type": "components extracted from the data.",
            "description": ""
          },
          "error_": {
            "type": "ndarray",
            "description": ""
          },
          "Vector": {
            "type": "of errors at each iteration.",
            "description": ""
          },
          "n_components_": {
            "type": "int",
            "description": ""
          },
          "Estimated": {
            "type": "number of components.",
            "description": ".. versionadded:: 0.23"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "mean_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Per-feature empirical mean, estimated from the training set."
          },
          "Equal": {
            "type": "to ``X.mean(axis=0)``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "PCA": {
            "type": "Principal Component Analysis implementation.",
            "description": ""
          },
          "MiniBatchSparsePCA": {
            "type": "Mini batch variant of `SparsePCA` that is faster but less",
            "description": "accurate."
          },
          "DictionaryLearning": {
            "type": "Generic dictionary learning problem using a sparse code.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import SparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = SparsePCA(n_components=5, random_state=0)\n>>> transformer.fit(X)"
          },
          "SparsePCA": {
            "type": "...",
            "description": ">>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\nnp.float64(0.9666...)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    PCA : Principal Component Analysis implementation.\n    MiniBatchSparsePCA : Mini batch variant of `SparsePCA` that is faster but less\n        accurate.\n    DictionaryLearning : Generic dictionary learning problem using a sparse code.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import SparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = SparsePCA(n_components=5, random_state=0)\n    >>> transformer.fit(X)\n    SparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9666...)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import SparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = SparsePCA(n_components=5, random_state=0)\n    >>> transformer.fit(X)\n    SparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9666...)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform data from the latent space to the original space.\n\n        This inversion is an approximation due to the loss of information\n        induced by the forward decomposition.\n\n        .. versionadded:: 1.2\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_components)\n            Data in the latent space.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Data": {
                "type": "in the latent space.",
                "description": "Returns\n-------"
              },
              "X_original": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Reconstructed": {
                "type": "data in the original space.",
                "description": ""
              }
            },
            "returns": "-------\n        X_original : ndarray of shape (n_samples, n_features)\n            Reconstructed data in the original space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Least Squares projection of the data onto the sparse components.\n\n        To avoid instability issues in case the system is under-determined,\n        regularization can be applied (Ridge regression) via the\n        `ridge_alpha` parameter.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Test": {
                "type": "data to be transformed, must have the same number of",
                "description": ""
              },
              "features": {
                "type": "as the data used to train the model.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "that Sparse PCA components orthogonality is not enforced as in PCA\n        hence one cannot use a simple linear projection.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TruncatedSVD",
      "documentation": {
        "description": "Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        If algorithm='arpack', must be strictly less than the number of features.\n        If algorithm='randomized', must be less than or equal to the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    n_oversamples : int, default=10\n        Number of oversamples for randomized SVD solver. Not used by ARPACK.\n        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n        description.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The right singular vectors of the input data.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`",
        "parameters": {
          "n_components": {
            "type": "int, default=2",
            "description": ""
          },
          "Desired": {
            "type": "dimensionality of output data.",
            "description": ""
          },
          "If": {
            "type": "algorithm='randomized', must be less than or equal to the number of features.",
            "description": ""
          },
          "The": {
            "type": "singular values are equal to the 2-norms of the ``n_components``",
            "description": ""
          },
          "100": {
            "type": "is recommended.",
            "description": ""
          },
          "algorithm": {
            "type": "and random state. To work around this, fit instances of this",
            "description": ""
          },
          "SVD": {
            "type": "suffers from a problem called \"sign indeterminacy\", which means the",
            "description": ""
          },
          "n_iter": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "default": {
            "type": "is larger than the default in",
            "description": ":func:`~sklearn.utils.extmath.randomized_svd` to handle sparse"
          },
          "matrices": {
            "type": "that may have large slowly decaying spectrum.",
            "description": ""
          },
          "n_oversamples": {
            "type": "int, default=10",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "power_iteration_normalizer": {
            "type": "{'auto', 'QR', 'LU', 'none'}, default='auto'",
            "description": ""
          },
          "Power": {
            "type": "iteration normalizer for randomized SVD solver.",
            "description": ""
          },
          "Not": {
            "type": "used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ".. versionadded:: 1.1"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Used": {
            "type": "during randomized svd. Pass an int for reproducible results across",
            "description": ""
          },
          "multiple": {
            "type": "function calls.",
            "description": ""
          },
          "tol": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Tolerance": {
            "type": "for ARPACK. 0 means machine precision. Ignored by randomized",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "explained_variance_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "each": {
            "type": "component.",
            "description": ""
          },
          "explained_variance_ratio_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Percentage": {
            "type": "of variance explained by each of the selected components.",
            "description": ""
          },
          "singular_values_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "variables": {
            "type": "in the lower-dimensional space.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "DictionaryLearning": {
            "type": "Find a dictionary that sparsely encodes data.",
            "description": ""
          },
          "FactorAnalysis": {
            "type": "A simple linear generative model with",
            "description": ""
          },
          "Gaussian": {
            "type": "latent variables.",
            "description": ""
          },
          "IncrementalPCA": {
            "type": "Incremental principal components analysis.",
            "description": ""
          },
          "KernelPCA": {
            "type": "Kernel Principal component analysis.",
            "description": ""
          },
          "NMF": {
            "type": "Non",
            "description": "Negative Matrix Factorization."
          },
          "PCA": {
            "type": "Principal component analysis.",
            "description": "Notes\n-----"
          },
          "sign": {
            "type": "of the ``components_`` and the output from transform depend on the",
            "description": ""
          },
          "class": {
            "type": "to data once, then keep the instance around to do transformations.",
            "description": "References\n----------\n:arxiv:`Halko, et al. (2009). \"Finding structure with randomness:"
          },
          "Stochastic": {
            "type": "algorithms for constructing approximate matrix decompositions\"",
            "description": "<0909.4061>`\nExamples\n--------\n>>> from sklearn.decomposition import TruncatedSVD\n>>> from scipy.sparse import csr_matrix\n>>> import numpy as np\n>>> np.random.seed(0)\n>>> X_dense = np.random.rand(100, 100)\n>>> X_dense[:, 2 * np.arange(50)] = 0\n>>> X = csr_matrix(X_dense)\n>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> svd.fit(X)"
          },
          "TruncatedSVD": {
            "type": "n_components=5, n_iter=7, random_state=42",
            "description": ">>> print(svd.explained_variance_ratio_)\n[0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n>>> print(svd.explained_variance_ratio_.sum())\n0.2102...\n>>> print(svd.singular_values_)\n[35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]",
        "notes": "-----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]",
        "examples": "--------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the transformer object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Reduced": {
                "type": "version of X. This will always be a dense array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "documentation": {
            "description": "Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_components)"
              },
              "New": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "X_original": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Note": {
                "type": "that this is always a dense array.",
                "description": ""
              }
            },
            "returns": "an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.",
            "raises": "",
            "see_also": "",
            "notes": "that this is always a dense array.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Reduced": {
                "type": "version of X. This will always be a dense array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}