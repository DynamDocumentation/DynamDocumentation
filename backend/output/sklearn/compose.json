{
  "description": "Meta-estimators for building composite models with transformers.\n\nIn addition to its current contents, this module will eventually be home to\nrefurbished versions of :class:`~sklearn.pipeline.Pipeline` and\n:class:`~sklearn.pipeline.FeatureUnion`.",
  "functions": [
    {
      "name": "make_column_transformer",
      "signature": "make_column_transformer(*transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, verbose=False, verbose_feature_names_out=True, force_int_remainder_cols=True)",
      "documentation": {
        "description": "Construct a ColumnTransformer from the given transformers.\n\n    This is a shorthand for the ColumnTransformer constructor; it does not\n    require, and does not permit, naming the transformers. Instead, they will\n    be given names automatically based on their types. It also does not allow\n    weighting with ``transformer_weights``.\n\n    Read more in the :ref:`User Guide <make_column_transformer>`.\n\n    Parameters\n    ----------\n    *transformers : tuples\n        Tuples of the form (transformer, columns) specifying the\n        transformer objects to be applied to subsets of the data.\n\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name. A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n\n    sparse_threshold : float, default=0.3\n        If the transformed output consists of a mix of sparse and dense data,\n        it will be stacked as a sparse matrix if the density is lower than this\n        value. Use ``sparse_threshold=0`` to always return dense.\n        When the transformed output consists of all sparse or all dense data,\n        the stacked result will be sparse or dense, respectively, and this\n        keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    verbose_feature_names_out : bool, default=True\n        If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\n        all feature names with the name of the transformer that generated that\n        feature.\n        If False, :meth:`ColumnTransformer.get_feature_names_out` will not\n        prefix any feature names and will error if feature names are not\n        unique.\n\n        .. versionadded:: 1.0\n\n    force_int_remainder_cols : bool, default=True\n        Force the columns of the last entry of `transformers_`, which\n        corresponds to the \"remainder\" transformer, to always be stored as\n        indices (int) rather than column names (str). See description of the\n        :attr:`ColumnTransformer.transformers_` attribute for details.\n\n        .. note::\n            If you do not access the list of columns for the remainder columns\n            in the :attr:`ColumnTransformer.transformers_` fitted attribute,\n            you do not need to set this parameter.\n\n        .. versionadded:: 1.5\n\n        .. versionchanged:: 1.7\n           The default value for `force_int_remainder_cols` will change from\n           `True` to `False` in version 1.7.\n\n    Returns\n    -------\n    ct : ColumnTransformer\n        Returns a :class:`ColumnTransformer` object.\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.",
        "parameters": {
          "Tuples": {
            "type": "of the form (transformer, columns) specifying the",
            "description": ""
          },
          "transformer": {
            "type": "{'drop', 'passthrough'} or estimator",
            "description": ""
          },
          "Estimator": {
            "type": "must support :term:`fit` and :term:`transform`.",
            "description": "Special-cased strings 'drop' and 'passthrough' are accepted as\nwell, to indicate to drop the columns or to pass them through\nuntransformed, respectively."
          },
          "columns": {
            "type": "are dropped. (default of ``'drop'``).",
            "description": ""
          },
          "Indexes": {
            "type": "the data on its second axis. Integers are interpreted as",
            "description": ""
          },
          "positional": {
            "type": "columns, while strings can reference DataFrame columns",
            "description": ""
          },
          "by": {
            "type": "name. A scalar string or int should be used where",
            "description": "``transformer`` expects X to be a 1d array-like (vector),"
          },
          "otherwise": {
            "type": "a 2d array will be passed to the transformer.",
            "description": ""
          },
          "A": {
            "type": "callable is passed the input data `X` and can return any of the",
            "description": "above. To select multiple columns by name or dtype, you can use\n:obj:`make_column_selector`."
          },
          "remainder": {
            "type": "{'drop', 'passthrough'} or estimator, default='drop'",
            "description": ""
          },
          "By": {
            "type": "setting ``remainder`` to be an estimator, the remaining",
            "description": "non-specified columns will use the ``remainder`` estimator. The"
          },
          "transformed": {
            "type": "and combined in the output, and the non-specified",
            "description": ""
          },
          "were": {
            "type": "not specified in `transformers` will be automatically passed",
            "description": "through. This subset of columns is concatenated with the output of"
          },
          "the": {
            "type": "stacked result will be sparse or dense, respectively, and this",
            "description": ""
          },
          "estimator": {
            "type": "must support :term:`fit` and :term:`transform`.",
            "description": ""
          },
          "sparse_threshold": {
            "type": "float, default=0.3",
            "description": ""
          },
          "If": {
            "type": "you do not access the list of columns for the remainder columns",
            "description": ""
          },
          "it": {
            "type": "will be stacked as a sparse matrix if the density is lower than this",
            "description": "value. Use ``sparse_threshold=0`` to always return dense."
          },
          "When": {
            "type": "the transformed output consists of all sparse or all dense data,",
            "description": ""
          },
          "keyword": {
            "type": "will be ignored.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of jobs to run in parallel.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "printed": {
            "type": "as it is completed.",
            "description": ""
          },
          "verbose_feature_names_out": {
            "type": "bool, default=True",
            "description": ""
          },
          "all": {
            "type": "feature names with the name of the transformer that generated that",
            "description": "feature."
          },
          "prefix": {
            "type": "any feature names and will error if feature names are not",
            "description": "unique.\n.. versionadded:: 1.0"
          },
          "force_int_remainder_cols": {
            "type": "bool, default=True",
            "description": ""
          },
          "Force": {
            "type": "the columns of the last entry of `transformers_`, which",
            "description": ""
          },
          "corresponds": {
            "type": "to the \"remainder\" transformer, to always be stored as",
            "description": ""
          },
          "indices": {
            "type": "int",
            "description": "rather than column names (str). See description of the\n:attr:`ColumnTransformer.transformers_` attribute for details.\n.. note::"
          },
          "in": {
            "type": "the :attr:`ColumnTransformer.transformers_` fitted attribute,",
            "description": ""
          },
          "you": {
            "type": "do not need to set this parameter.",
            "description": ".. versionadded:: 1.5\n.. versionchanged:: 1.7"
          },
          "The": {
            "type": "default value for `force_int_remainder_cols` will change from",
            "description": "`True` to `False` in version 1.7.\nReturns\n-------"
          },
          "ct": {
            "type": "ColumnTransformer",
            "description": ""
          },
          "Returns": {
            "type": "a :class:`ColumnTransformer` object.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "ColumnTransformer": {
            "type": "transformers=[('standardscaler', StandardScaler(...",
            "description": ",\n['numerical_column']),\n('onehotencoder', OneHotEncoder(...),\n['categorical_column'])])"
          },
          "outputs": {
            "type": "of multiple transformer objects used on column subsets",
            "description": ""
          },
          "of": {
            "type": "the data into a single feature space.",
            "description": "Examples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> make_column_transformer(\n...     (StandardScaler(), ['numerical_column']),\n...     (OneHotEncoder(), ['categorical_column']))"
          }
        },
        "returns": "-------\n    ct : ColumnTransformer",
        "raises": "",
        "see_also": "--------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> make_column_transformer(\n    ...     (StandardScaler(), ['numerical_column']),\n    ...     (OneHotEncoder(), ['categorical_column']))\n    ColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                     ['numerical_column']),\n                                    ('onehotencoder', OneHotEncoder(...),\n                                     ['categorical_column'])])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> make_column_transformer(\n    ...     (StandardScaler(), ['numerical_column']),\n    ...     (OneHotEncoder(), ['categorical_column']))\n    ColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                     ['numerical_column']),\n                                    ('onehotencoder', OneHotEncoder(...),\n                                     ['categorical_column'])])"
      }
    }
  ],
  "classes": [
    {
      "name": "ColumnTransformer",
      "documentation": {
        "description": "Applies transformers to columns of an array or pandas DataFrame.\n\n    This estimator allows different columns or column subsets of the input\n    to be transformed separately and the features generated by each transformer\n    will be concatenated to form a single feature space.\n    This is useful for heterogeneous or columnar data, to combine several\n    feature extraction mechanisms or transformations into a single transformer.\n\n    Read more in the :ref:`User Guide <column_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    transformers : list of tuples\n        List of (name, transformer, columns) tuples specifying the\n        transformer objects to be applied to subsets of the data.\n\n        name : str\n            Like in Pipeline and FeatureUnion, this allows the transformer and\n            its parameters to be set using ``set_params`` and searched in grid\n            search.\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name.  A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers`, but present in the data passed\n        to `fit` will be automatically passed through. This subset of columns\n        is concatenated with the output of the transformers. For dataframes,\n        extra columns not seen during `fit` will be excluded from the output\n        of `transform`.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n        Note that using this feature requires that the DataFrame columns\n        input at :term:`fit` and :term:`transform` have identical order.\n\n    sparse_threshold : float, default=0.3\n        If the output of the different transformers contains sparse matrices,\n        these will be stacked as a sparse matrix if the overall density is\n        lower than this value. Use ``sparse_threshold=0`` to always return\n        dense.  When the transformed output consists of all dense data, the\n        stacked result will be dense, and this keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer. The output of the\n        transformer is multiplied by these weights. Keys are transformer names,\n        values the weights.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    verbose_feature_names_out : bool, str or Callable[[str, str], str], default=True\n\n        - If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\n          all feature names with the name of the transformer that generated that\n          feature. It is equivalent to setting\n          `verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.\n        - If False, :meth:`ColumnTransformer.get_feature_names_out` will not\n          prefix any feature names and will error if feature names are not\n          unique.\n        - If ``Callable[[str, str], str]``,\n          :meth:`ColumnTransformer.get_feature_names_out` will rename all the features\n          using the name of the transformer. The first argument of the callable is the\n          transformer name and the second argument is the feature name. The returned\n          string will be the new feature name.\n        - If ``str``, it must be a string ready for formatting. The given string will\n          be formatted using two field names: ``transformer_name`` and ``feature_name``.\n          e.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method\n          from the standard library for more info.\n\n        .. versionadded:: 1.0\n\n        .. versionchanged:: 1.6\n            `verbose_feature_names_out` can be a callable or a string to be formatted.\n\n    force_int_remainder_cols : bool, default=True\n        Force the columns of the last entry of `transformers_`, which\n        corresponds to the \"remainder\" transformer, to always be stored as\n        indices (int) rather than column names (str). See description of the\n        `transformers_` attribute for details.\n\n        .. note::\n            If you do not access the list of columns for the remainder columns\n            in the `transformers_` fitted attribute, you do not need to set\n            this parameter.\n\n        .. versionadded:: 1.5\n\n        .. versionchanged:: 1.7\n           The default value for `force_int_remainder_cols` will change from\n           `True` to `False` in version 1.7.\n\n    Attributes\n    ----------\n    transformers_ : list\n        The collection of fitted transformers as tuples of (name,\n        fitted_transformer, column). `fitted_transformer` can be an estimator,\n        or `'drop'`; `'passthrough'` is replaced with an equivalent\n        :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were\n        no columns selected, this will be the unfitted transformer. If there\n        are remaining columns, the final element is a tuple of the form:\n        ('remainder', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n        .. versionchanged:: 1.5\n            If there are remaining columns and `force_int_remainder_cols` is\n            True, the remaining columns are always represented by their\n            positional indices in the input `X` (as in older versions). If\n            `force_int_remainder_cols` is False, the format attempts to match\n            that of the other transformers: if all columns were provided as\n            column names (`str`), the remaining columns are stored as column\n            names; if all columns were provided as mask arrays (`bool`), so are\n            the remaining columns; in all other cases the remaining columns are\n            stored as indices (`int`).\n\n    named_transformers_ : :class:`~sklearn.utils.Bunch`\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : bool\n        Boolean flag indicating whether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    output_indices_ : dict\n        A dictionary from each transformer name to a slice, where the slice\n        corresponds to indices in the transformed output. This is useful to\n        inspect which transformer is responsible for which transformed\n        feature(s).\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying transformers expose such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_column_transformer : Convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n    make_column_selector : Convenience function for selecting\n        columns based on datatype or the columns name with a regex pattern.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.",
        "parameters": {
          "transformers": {
            "type": "list of tuples",
            "description": ""
          },
          "List": {
            "type": "of (name, transformer, columns) tuples specifying the",
            "description": ""
          },
          "transformer": {
            "type": "name and the second argument is the feature name. The returned",
            "description": ""
          },
          "name": {
            "type": "str",
            "description": ""
          },
          "Like": {
            "type": "in Pipeline and FeatureUnion, this allows the transformer and",
            "description": ""
          },
          "its": {
            "type": "parameters to be set using ``set_params`` and searched in grid",
            "description": "search."
          },
          "Estimator": {
            "type": "must support :term:`fit` and :term:`transform`.",
            "description": "Special-cased strings 'drop' and 'passthrough' are accepted as\nwell, to indicate to drop the columns or to pass them through\nuntransformed, respectively."
          },
          "columns": {
            "type": "based on datatype or the columns name with a regex pattern.",
            "description": "Notes\n-----"
          },
          "Indexes": {
            "type": "the data on its second axis. Integers are interpreted as",
            "description": ""
          },
          "positional": {
            "type": "indices in the input `X` (as in older versions). If",
            "description": "`force_int_remainder_cols` is False, the format attempts to match"
          },
          "by": {
            "type": "name.  A scalar string or int should be used where",
            "description": "``transformer`` expects X to be a 1d array-like (vector),"
          },
          "otherwise": {
            "type": "a 2d array will be passed to the transformer.",
            "description": ""
          },
          "A": {
            "type": "dictionary from each transformer name to a slice, where the slice",
            "description": ""
          },
          "remainder": {
            "type": "{'drop', 'passthrough'} or estimator, default='drop'",
            "description": ""
          },
          "By": {
            "type": "setting ``remainder`` to be an estimator, the remaining",
            "description": "non-specified columns will use the ``remainder`` estimator. The"
          },
          "transformed": {
            "type": "and combined in the output, and the non-specified",
            "description": ""
          },
          "were": {
            "type": "not specified in `transformers`, but present in the data passed",
            "description": ""
          },
          "to": {
            "type": "`fit` will be automatically passed through. This subset of columns",
            "description": ""
          },
          "is": {
            "type": "concatenated with the output of the transformers. For dataframes,",
            "description": ""
          },
          "extra": {
            "type": "columns not seen during `fit` will be excluded from the output",
            "description": ""
          },
          "of": {
            "type": "the individual transformers and the `sparse_threshold` keyword.",
            "description": ""
          },
          "estimator": {
            "type": "must support :term:`fit` and :term:`transform`.",
            "description": ""
          },
          "Note": {
            "type": "that using this feature requires that the DataFrame columns",
            "description": ""
          },
          "input": {
            "type": "at :term:`fit` and :term:`transform` have identical order.",
            "description": ""
          },
          "sparse_threshold": {
            "type": "float, default=0.3",
            "description": ""
          },
          "If": {
            "type": "there are remaining columns and `force_int_remainder_cols` is",
            "description": "True, the remaining columns are always represented by their"
          },
          "these": {
            "type": "will be stacked as a sparse matrix if the overall density is",
            "description": ""
          },
          "lower": {
            "type": "than this value. Use ``sparse_threshold=0`` to always return",
            "description": "dense.  When the transformed output consists of all dense data, the"
          },
          "stacked": {
            "type": "result will be dense, and this keyword will be ignored.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`. Only defined if the",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "transformer_weights": {
            "type": "dict, default=None",
            "description": ""
          },
          "Multiplicative": {
            "type": "weights for features per transformer. The output of the",
            "description": ""
          },
          "values": {
            "type": "the weights.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "printed": {
            "type": "as it is completed.",
            "description": ""
          },
          "verbose_feature_names_out": {
            "type": "bool, str or Callable[[str, str], str], default=True",
            "description": "- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix"
          },
          "all": {
            "type": "feature names with the name of the transformer that generated that",
            "description": "feature. It is equivalent to setting\n`verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.\n- If False, :meth:`ColumnTransformer.get_feature_names_out` will not"
          },
          "prefix": {
            "type": "any feature names and will error if feature names are not",
            "description": "unique.\n- If ``Callable[[str, str], str]``,\n:meth:`ColumnTransformer.get_feature_names_out` will rename all the features"
          },
          "using": {
            "type": "the name of the transformer. The first argument of the callable is the",
            "description": ""
          },
          "string": {
            "type": "will be the new feature name.",
            "description": "- If ``str``, it must be a string ready for formatting. The given string will"
          },
          "be": {
            "type": "formatted using two field names: ``transformer_name`` and ``feature_name``.",
            "description": "e.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method"
          },
          "from": {
            "type": "the standard library for more info.",
            "description": ".. versionadded:: 1.0\n.. versionchanged:: 1.6\n`verbose_feature_names_out` can be a callable or a string to be formatted."
          },
          "force_int_remainder_cols": {
            "type": "bool, default=True",
            "description": ""
          },
          "Force": {
            "type": "the columns of the last entry of `transformers_`, which",
            "description": ""
          },
          "corresponds": {
            "type": "to indices in the transformed output. This is useful to",
            "description": ""
          },
          "indices": {
            "type": "int",
            "description": "rather than column names (str). See description of the\n`transformers_` attribute for details.\n.. note::"
          },
          "in": {
            "type": "the `passthrough` keyword. Those columns specified with `passthrough`",
            "description": ""
          },
          "this": {
            "type": "parameter.",
            "description": ".. versionadded:: 1.5\n.. versionchanged:: 1.7"
          },
          "The": {
            "type": "order of the columns in the transformed feature matrix follows the",
            "description": ""
          },
          "transformers_": {
            "type": "list",
            "description": ""
          },
          "or": {
            "type": "`'drop'`; `'passthrough'` is replaced with an equivalent",
            "description": ":class:`~sklearn.preprocessing.FunctionTransformer`. In case there were"
          },
          "no": {
            "type": "columns selected, this will be the unfitted transformer. If there",
            "description": ""
          },
          "are": {
            "type": "added at the right to the output of the transformers.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n[0.5, 0.5, 0. , 1. ]])\n:class:`ColumnTransformer` can be configured with a transformer that requires"
          },
          "that": {
            "type": "of the other transformers: if all columns were provided as",
            "description": ""
          },
          "column": {
            "type": "subsets of the original feature space.",
            "description": ""
          },
          "the": {
            "type": "remaining columns; in all other cases the remaining columns are",
            "description": ""
          },
          "stored": {
            "type": "as indices (`int`).",
            "description": ""
          },
          "named_transformers_": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Read-only attribute to access any transformer by given name."
          },
          "Keys": {
            "type": "are transformer names and values are the fitted transformer",
            "description": "objects."
          },
          "sparse_output_": {
            "type": "bool",
            "description": ""
          },
          "Boolean": {
            "type": "flag indicating whether the output of ``transform`` is a",
            "description": ""
          },
          "sparse": {
            "type": "matrix or a dense numpy array, which depends on the output",
            "description": ""
          },
          "output_indices_": {
            "type": "dict",
            "description": ""
          },
          "inspect": {
            "type": "which transformer is responsible for which transformed",
            "description": ""
          },
          "feature": {
            "type": "s",
            "description": ".\n.. versionadded:: 1.0"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "underlying": {
            "type": "transformers expose such an attribute when fit.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "make_column_transformer": {
            "type": "Convenience function for",
            "description": ""
          },
          "combining": {
            "type": "the outputs of multiple transformer objects applied to",
            "description": ""
          },
          "make_column_selector": {
            "type": "Convenience function for selecting",
            "description": ""
          },
          "order": {
            "type": "of how the columns are specified in the `transformers` list.",
            "description": ""
          },
          "Columns": {
            "type": "of the original feature matrix that are not specified are",
            "description": ""
          },
          "dropped": {
            "type": "from the resulting transformed feature matrix, unless specified",
            "description": ""
          },
          "a": {
            "type": "1d array by setting the column to a string:",
            "description": ">>> from sklearn.feature_extraction.text import CountVectorizer\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> import pandas as pd   # doctest: +SKIP\n>>> X = pd.DataFrame({\n...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n...     \"width\": [3, 4, 5],\n... })  # doctest: +SKIP\n>>> # \"documents\" is a string which configures ColumnTransformer to\n>>> # pass the documents column as a 1d array to the CountVectorizer\n>>> ct = ColumnTransformer(\n...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n>>> X_trans = ct.fit_transform(X)  # doctest: +SKIP"
          },
          "For": {
            "type": "a more detailed example of usage, see",
            "description": ":ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    make_column_transformer : Convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n    make_column_selector : Convenience function for selecting\n        columns based on datatype or the columns name with a regex pattern.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    :class:`ColumnTransformer` can be configured with a transformer that requires\n    a 1d array by setting the column to a string:\n\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.preprocessing import MinMaxScaler\n    >>> import pandas as pd   # doctest: +SKIP\n    >>> X = pd.DataFrame({\n    ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n    ...     \"width\": [3, 4, 5],\n    ... })  # doctest: +SKIP\n    >>> # \"documents\" is a string which configures ColumnTransformer to\n    >>> # pass the documents column as a 1d array to the CountVectorizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n    ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n    >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.",
        "notes": "that using this feature requires that the DataFrame columns\n        input at :term:`fit` and :term:`transform` have identical order.\n\n    sparse_threshold : float, default=0.3\n        If the output of the different transformers contains sparse matrices,\n        these will be stacked as a sparse matrix if the overall density is\n        lower than this value. Use ``sparse_threshold=0`` to always return\n        dense.  When the transformed output consists of all dense data, the\n        stacked result will be dense, and this keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer. The output of the\n        transformer is multiplied by these weights. Keys are transformer names,\n        values the weights.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    verbose_feature_names_out : bool, str or Callable[[str, str], str], default=True\n\n        - If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\n          all feature names with the name of the transformer that generated that\n          feature. It is equivalent to setting\n          `verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.\n        - If False, :meth:`ColumnTransformer.get_feature_names_out` will not\n          prefix any feature names and will error if feature names are not\n          unique.\n        - If ``Callable[[str, str], str]``,\n          :meth:`ColumnTransformer.get_feature_names_out` will rename all the features\n          using the name of the transformer. The first argument of the callable is the\n          transformer name and the second argument is the feature name. The returned\n          string will be the new feature name.\n        - If ``str``, it must be a string ready for formatting. The given string will\n          be formatted using two field names: ``transformer_name`` and ``feature_name``.\n          e.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method\n          from the standard library for more info.\n\n        .. versionadded:: 1.0\n\n        .. versionchanged:: 1.6\n            `verbose_feature_names_out` can be a callable or a string to be formatted.\n\n    force_int_remainder_cols : bool, default=True\n        Force the columns of the last entry of `transformers_`, which\n        corresponds to the \"remainder\" transformer, to always be stored as\n        indices (int) rather than column names (str). See description of the\n        `transformers_` attribute for details.\n\n        .. note::\n            If you do not access the list of columns for the remainder columns\n            in the `transformers_` fitted attribute, you do not need to set\n            this parameter.\n\n        .. versionadded:: 1.5\n\n        .. versionchanged:: 1.7\n           The default value for `force_int_remainder_cols` will change from\n           `True` to `False` in version 1.7.\n\n    Attributes\n    ----------\n    transformers_ : list\n        The collection of fitted transformers as tuples of (name,\n        fitted_transformer, column). `fitted_transformer` can be an estimator,\n        or `'drop'`; `'passthrough'` is replaced with an equivalent\n        :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were\n        no columns selected, this will be the unfitted transformer. If there\n        are remaining columns, the final element is a tuple of the form:\n        ('remainder', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n        .. versionchanged:: 1.5\n            If there are remaining columns and `force_int_remainder_cols` is\n            True, the remaining columns are always represented by their\n            positional indices in the input `X` (as in older versions). If\n            `force_int_remainder_cols` is False, the format attempts to match\n            that of the other transformers: if all columns were provided as\n            column names (`str`), the remaining columns are stored as column\n            names; if all columns were provided as mask arrays (`bool`), so are\n            the remaining columns; in all other cases the remaining columns are\n            stored as indices (`int`).\n\n    named_transformers_ : :class:`~sklearn.utils.Bunch`\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : bool\n        Boolean flag indicating whether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    output_indices_ : dict\n        A dictionary from each transformer name to a slice, where the slice\n        corresponds to indices in the transformed output. This is useful to\n        inspect which transformer is responsible for which transformed\n        feature(s).\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying transformers expose such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    make_column_transformer : Convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n    make_column_selector : Convenience function for selecting\n        columns based on datatype or the columns name with a regex pattern.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    :class:`ColumnTransformer` can be configured with a transformer that requires\n    a 1d array by setting the column to a string:\n\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.preprocessing import MinMaxScaler\n    >>> import pandas as pd   # doctest: +SKIP\n    >>> X = pd.DataFrame({\n    ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n    ...     \"width\": [3, 4, 5],\n    ... })  # doctest: +SKIP\n    >>> # \"documents\" is a string which configures ColumnTransformer to\n    >>> # pass the documents column as a 1d array to the CountVectorizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n    ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n    >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    :class:`ColumnTransformer` can be configured with a transformer that requires\n    a 1d array by setting the column to a string:\n\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.preprocessing import MinMaxScaler\n    >>> import pandas as pd   # doctest: +SKIP\n    >>> X = pd.DataFrame({\n    ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n    ...     \"width\": [3, 4, 5],\n    ... })  # doctest: +SKIP\n    >>> # \"documents\" is a string which configures ColumnTransformer to\n    >>> # pass the documents column as a 1d array to the CountVectorizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"text_preprocess\", CountVectorizer(), \"documents\"),\n    ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n    >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "documentation": {
            "description": "Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,...), default=None\n            Targets for supervised learning.\n\n        **params : dict, default=None\n            Parameters to be passed to the underlying transformers' ``fit`` and\n            ``transform`` methods.\n\n            You can only pass this if metadata routing is enabled, which you\n            can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, dataframe} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data, of which specified subsets are used to fit the",
                "description": "transformers."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,...), default=None"
              },
              "Targets": {
                "type": "for supervised learning.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : ColumnTransformer\n            This estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **params)",
          "documentation": {
            "description": "Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,), default=None\n            Targets for supervised learning.\n\n        **params : dict, default=None\n            Parameters to be passed to the underlying transformers' ``fit`` and\n            ``transform`` methods.\n\n            You can only pass this if metadata routing is enabled, which you\n            can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, dataframe} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data, of which specified subsets are used to fit the",
                "description": "transformers."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Targets": {
                "type": "for supervised learning.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n            Horizontally stacked results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Input features.\n\n            - If `input_features` is `None`, then `feature_names_in_` is\n              used as feature names in. If `feature_names_in_` is not defined,\n              then the following input feature names are generated:\n              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n            - If `input_features` is an array-like, then `input_features` must\n              match `feature_names_in_` if `feature_names_in_` is defined.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Input": {
                "type": "features.",
                "description": "- If `input_features` is `None`, then `feature_names_in_` is"
              },
              "used": {
                "type": "as feature names in. If `feature_names_in_` is not defined,",
                "description": ""
              },
              "then": {
                "type": "the following input feature names are generated:",
                "description": "`[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n- If `input_features` is an array-like, then `input_features` must"
              },
              "match": {
                "type": "`feature_names_in_` if `feature_names_in_` is defined.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "the parameters given in the constructor as well as the\n        estimators contained within the `transformers` of the\n        `ColumnTransformer`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        Calling `set_output` will set the output of all estimators in `transformers`\n        and `transformers_`.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **kwargs)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that you\n        can directly set the parameters of the estimators contained in\n        `transformers` of `ColumnTransformer`.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "parameters.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "ColumnTransformer",
                "description": ""
              },
              "This": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : ColumnTransformer\n            This estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, **params)",
          "documentation": {
            "description": "Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            The data to be transformed by subset.\n\n        **params : dict, default=None\n            Parameters to be passed to the underlying transformers' ``transform``\n            method.\n\n            You can only pass this if metadata routing is enabled, which you\n            can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, dataframe} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data to be transformed by subset.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n            Horizontally stacked results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TransformedTargetRegressor",
      "documentation": {
        "description": "Meta-estimator to regress on a transformed target.\n\n    Useful for applying a non-linear transformation to the target `y` in\n    regression problems. This transformation can be given as a Transformer\n    such as the :class:`~sklearn.preprocessing.QuantileTransformer` or as a\n    function and its inverse such as `np.log` and `np.exp`.\n\n    The computation during :meth:`fit` is::\n\n        regressor.fit(X, func(y))\n\n    or::\n\n        regressor.fit(X, transformer.transform(y))\n\n    The computation during :meth:`predict` is::\n\n        inverse_func(regressor.predict(X))\n\n    or::\n\n        transformer.inverse_transform(regressor.predict(X))\n\n    Read more in the :ref:`User Guide <transformed_target_regressor>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    regressor : object, default=None\n        Regressor object such as derived from\n        :class:`~sklearn.base.RegressorMixin`. This regressor will\n        automatically be cloned each time prior to fitting. If `regressor is\n        None`, :class:`~sklearn.linear_model.LinearRegression` is created and used.\n\n    transformer : object, default=None\n        Estimator object such as derived from\n        :class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time\n        as `func` and `inverse_func`. If `transformer is None` as well as\n        `func` and `inverse_func`, the transformer will be an identity\n        transformer. Note that the transformer will be cloned during fitting.\n        Also, the transformer is restricting `y` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to `y` before passing to :meth:`fit`. Cannot be set\n        at the same time as `transformer`. If `func is None`, the function used will be\n        the identity function. If `func` is set, `inverse_func` also needs to be\n        provided. The function needs to return a 2-dimensional array.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as `transformer`. The inverse function is used to return\n        predictions to the same space of the original training labels. If\n        `inverse_func` is set, `func` also needs to be provided. The inverse\n        function needs to return a 2-dimensional array.\n\n    check_inverse : bool, default=True\n        Whether to check that `transform` followed by `inverse_transform`\n        or `func` followed by `inverse_func` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in :meth:`fit` and :meth:`predict`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying regressor exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an\n        arbitrary callable.\n\n    Notes\n    -----\n    Internally, the target `y` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as `y`.",
        "parameters": {
          "regressor": {
            "type": "object, default=None",
            "description": ""
          },
          "Regressor": {
            "type": "object such as derived from",
            "description": ":class:`~sklearn.base.RegressorMixin`. This regressor will"
          },
          "automatically": {
            "type": "be cloned each time prior to fitting. If `regressor is",
            "description": "None`, :class:`~sklearn.linear_model.LinearRegression` is created and used."
          },
          "transformer": {
            "type": "object, default=None",
            "description": ""
          },
          "Estimator": {
            "type": "object such as derived from",
            "description": ":class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time"
          },
          "as": {
            "type": "`func` and `inverse_func`. If `transformer is None` as well as",
            "description": "`func` and `inverse_func`, the transformer will be an identity\ntransformer. Note that the transformer will be cloned during fitting.\nAlso, the transformer is restricting `y` to be a numpy array."
          },
          "func": {
            "type": "function, default=None",
            "description": ""
          },
          "Function": {
            "type": "to apply to the prediction of the regressor. Cannot be set at",
            "description": ""
          },
          "at": {
            "type": "the same time as `transformer`. If `func is None`, the function used will be",
            "description": ""
          },
          "the": {
            "type": "same time as `transformer`. The inverse function is used to return",
            "description": ""
          },
          "inverse_func": {
            "type": "function, default=None",
            "description": ""
          },
          "predictions": {
            "type": "to the same space of the original training labels. If",
            "description": "`inverse_func` is set, `func` also needs to be provided. The inverse"
          },
          "function": {
            "type": "needs to return a 2-dimensional array.",
            "description": ""
          },
          "check_inverse": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to check that `transform` followed by `inverse_transform`",
            "description": ""
          },
          "or": {
            "type": "`func` followed by `inverse_func` leads to the original targets.",
            "description": "Attributes\n----------"
          },
          "regressor_": {
            "type": "object",
            "description": ""
          },
          "Fitted": {
            "type": "regressor.",
            "description": ""
          },
          "transformer_": {
            "type": "object",
            "description": ""
          },
          "Transformer": {
            "type": "used in :meth:`fit` and :meth:`predict`.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`. Only defined if the",
            "description": ""
          },
          "underlying": {
            "type": "regressor exposes such an attribute when fit.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.preprocessing.FunctionTransformer : Construct a transformer from an"
          },
          "arbitrary": {
            "type": "callable.",
            "description": "Notes\n-----\nInternally, the target `y` is always converted into a 2-dimensional array"
          },
          "to": {
            "type": "be used by scikit-learn transformers. At the time of prediction, the",
            "description": ""
          },
          "output": {
            "type": "will be reshaped to a have the same number of dimensions as `y`.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.compose import TransformedTargetRegressor\n>>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n...                                 func=np.log, inverse_func=np.exp)\n>>> X = np.arange(4).reshape(-1, 1)\n>>> y = np.exp(2 * X).ravel()\n>>> tt.fit(X, y)"
          },
          "TransformedTargetRegressor": {
            "type": "...",
            "description": ">>> tt.score(X, y)\n1.0\n>>> tt.regressor_.coef_"
          },
          "array": {
            "type": "[2.]",
            "description": ""
          },
          "For": {
            "type": "a more detailed example use case refer to",
            "description": ":ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an\n        arbitrary callable.\n\n    Notes\n    -----\n    Internally, the target `y` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as `y`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    For a more detailed example use case refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`.",
        "notes": "-----\n    Internally, the target `y` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as `y`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    For a more detailed example use case refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    For a more detailed example use case refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **fit_params)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            - If `enable_metadata_routing=False` (default): Parameters directly passed\n              to the `fit` method of the underlying regressor.\n\n            - If `enable_metadata_routing=True`: Parameters safely routed to the `fit`\n              method of the underlying regressor.\n\n            .. versionchanged:: 1.6\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "**fit_params : dict\n- If `enable_metadata_routing=False` (default): Parameters directly passed"
              },
              "to": {
                "type": "the `fit` method of the underlying regressor.",
                "description": "- If `enable_metadata_routing=True`: Parameters safely routed to the `fit`"
              },
              "method": {
                "type": "of the underlying regressor.",
                "description": ".. versionchanged:: 1.6"
              },
              "See": {
                "type": "ref:`Metadata Routing User Guide <metadata_routing>` for",
                "description": ""
              },
              "more": {
                "type": "details.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.6",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, **predict_params)",
          "documentation": {
            "description": "Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the `inverse_func` or\n        `inverse_transform` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        **predict_params : dict of str -> object\n            - If `enable_metadata_routing=False` (default): Parameters directly passed\n              to the `predict` method of the underlying regressor.\n\n            - If `enable_metadata_routing=True`: Parameters safely routed to the\n              `predict` method of the underlying regressor.\n\n            .. versionchanged:: 1.6\n                See :ref:`Metadata Routing User Guide <metadata_routing>`\n                for more details.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples.\n**predict_params : dict of str -> object\n- If `enable_metadata_routing=False` (default): Parameters directly passed"
              },
              "to": {
                "type": "the `predict` method of the underlying regressor.",
                "description": "- If `enable_metadata_routing=True`: Parameters safely routed to the\n`predict` method of the underlying regressor.\n.. versionchanged:: 1.6"
              },
              "See": {
                "type": "ref:`Metadata Routing User Guide <metadata_routing>`",
                "description": ""
              },
              "for": {
                "type": "more details.",
                "description": "Returns\n-------"
              },
              "y_hat": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Predicted": {
                "type": "values.",
                "description": ""
              }
            },
            "returns": "-------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.compose._target.TransformedTargetRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.compose._target.TransformedTargetRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "make_column_selector",
      "documentation": {
        "description": "Create a callable to select columns to be used with\n    :class:`ColumnTransformer`.\n\n    :func:`make_column_selector` can select columns based on datatype or the\n    columns name with a regex. When using multiple selection criteria, **all**\n    criteria must match for a column to be selected.\n\n    For an example of how to use :func:`make_column_selector` within a\n    :class:`ColumnTransformer` to select columns based on data type (i.e.\n    `dtype`), refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.\n\n    Parameters\n    ----------\n    pattern : str, default=None\n        Name of columns containing this regex pattern will be included. If\n        None, column selection will not be selected based on pattern.\n\n    dtype_include : column dtype or list of column dtypes, default=None\n        A selection of dtypes to include. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    dtype_exclude : column dtype or list of column dtypes, default=None\n        A selection of dtypes to exclude. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    Returns\n    -------\n    selector : callable\n        Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.",
        "parameters": {
          "pattern": {
            "type": "str, default=None",
            "description": ""
          },
          "Name": {
            "type": "of columns containing this regex pattern will be included. If",
            "description": "None, column selection will not be selected based on pattern."
          },
          "dtype_include": {
            "type": "column dtype or list of column dtypes, default=None",
            "description": ""
          },
          "A": {
            "type": "selection of dtypes to exclude. For more details, see",
            "description": ":meth:`pandas.DataFrame.select_dtypes`.\nReturns\n-------"
          },
          "dtype_exclude": {
            "type": "column dtype or list of column dtypes, default=None",
            "description": ""
          },
          "selector": {
            "type": "callable",
            "description": ""
          },
          "Callable": {
            "type": "for column selection to be used by a",
            "description": ":class:`ColumnTransformer`."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "ColumnTransformer": {
            "type": "Class that allows combining the",
            "description": ""
          },
          "outputs": {
            "type": "of multiple transformer objects used on column subsets",
            "description": ""
          },
          "of": {
            "type": "the data into a single feature space.",
            "description": "Examples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> from sklearn.compose import make_column_selector\n>>> import numpy as np\n>>> import pandas as pd  # doctest: +SKIP\n>>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n>>> ct = make_column_transformer(\n...       (StandardScaler(),\n...        make_column_selector(dtype_include=np.number)),  # rating\n...       (OneHotEncoder(),\n...        make_column_selector(dtype_include=object)))  # city\n>>> ct.fit_transform(X)  # doctest: +SKIP\narray([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n[-1.50755672,  1.        ,  0.        ,  0.        ],\n[-0.30151134,  0.        ,  1.        ,  0.        ],\n[ 0.90453403,  0.        ,  0.        ,  1.        ]])"
          }
        },
        "returns": "-------\n    selector : callable\n        Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> from sklearn.compose import make_column_selector\n    >>> import numpy as np\n    >>> import pandas as pd  # doctest: +SKIP\n    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n    >>> ct = make_column_transformer(\n    ...       (StandardScaler(),\n    ...        make_column_selector(dtype_include=np.number)),  # rating\n    ...       (OneHotEncoder(),\n    ...        make_column_selector(dtype_include=object)))  # city\n    >>> ct.fit_transform(X)  # doctest: +SKIP\n    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n           [-1.50755672,  1.        ,  0.        ,  0.        ],\n           [-0.30151134,  0.        ,  1.        ,  0.        ],\n           [ 0.90453403,  0.        ,  0.        ,  1.        ]])",
        "raises": "",
        "see_also": "--------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> from sklearn.compose import make_column_selector\n    >>> import numpy as np\n    >>> import pandas as pd  # doctest: +SKIP\n    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n    >>> ct = make_column_transformer(\n    ...       (StandardScaler(),\n    ...        make_column_selector(dtype_include=np.number)),  # rating\n    ...       (OneHotEncoder(),\n    ...        make_column_selector(dtype_include=object)))  # city\n    >>> ct.fit_transform(X)  # doctest: +SKIP\n    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n           [-1.50755672,  1.        ,  0.        ,  0.        ],\n           [-0.30151134,  0.        ,  1.        ,  0.        ],\n           [ 0.90453403,  0.        ,  0.        ,  1.        ]])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> from sklearn.compose import make_column_selector\n    >>> import numpy as np\n    >>> import pandas as pd  # doctest: +SKIP\n    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n    >>> ct = make_column_transformer(\n    ...       (StandardScaler(),\n    ...        make_column_selector(dtype_include=np.number)),  # rating\n    ...       (OneHotEncoder(),\n    ...        make_column_selector(dtype_include=object)))  # city\n    >>> ct.fit_transform(X)  # doctest: +SKIP\n    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n           [-1.50755672,  1.        ,  0.        ,  0.        ],\n           [-0.30151134,  0.        ,  1.        ,  0.        ],\n           [ 0.90453403,  0.        ,  0.        ,  1.        ]])"
      },
      "methods": []
    }
  ]
}