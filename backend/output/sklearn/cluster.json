{
  "description": "Popular unsupervised clustering algorithms.",
  "functions": [
    {
      "name": "affinity_propagation",
      "signature": "affinity_propagation(S, *, preference=None, convergence_iter=15, max_iter=200, damping=0.5, copy=True, verbose=False, return_n_iter=False, random_state=None)",
      "documentation": {
        "description": "Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    S : array-like of shape (n_samples, n_samples)\n        Matrix of similarities between points.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, default=200\n        Maximum number of iterations.\n\n    damping : float, default=0.5\n        Damping factor between 0.5 and 1.\n\n    copy : bool, default=True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency.\n\n    verbose : bool, default=False\n        The verbosity level.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Returns\n    -------\n    cluster_centers_indices : ndarray of shape (n_clusters,)\n        Index of clusters centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n    You may also check out,\n    :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007",
        "parameters": {
          "S": {
            "type": "array",
            "description": "like of shape (n_samples, n_samples)"
          },
          "Matrix": {
            "type": "of similarities between points.",
            "description": ""
          },
          "preference": {
            "type": "array",
            "description": "like of shape (n_samples,) or float, default=None"
          },
          "Preferences": {
            "type": "for each point - points with larger values of",
            "description": ""
          },
          "preferences": {
            "type": "are more likely to be chosen as exemplars. The number of",
            "description": "exemplars, i.e. of clusters, is influenced by the input preferences\nvalue. If the preferences are not passed as arguments, they will be"
          },
          "set": {
            "type": "to True.",
            "description": "Notes\n-----"
          },
          "number": {
            "type": "of clusters). For a smaller amount of clusters, this can be set",
            "description": ""
          },
          "to": {
            "type": "the minimum value of the similarities.",
            "description": ""
          },
          "convergence_iter": {
            "type": "int, default=15",
            "description": ""
          },
          "Number": {
            "type": "of iterations run. Returned only if `return_n_iter` is",
            "description": ""
          },
          "of": {
            "type": "estimated clusters that stops the convergence.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations.",
            "description": ""
          },
          "damping": {
            "type": "float, default=0.5",
            "description": ""
          },
          "Damping": {
            "type": "factor between 0.5 and 1.",
            "description": ""
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "the preference is smaller than the similarities, a single cluster center",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "The": {
            "type": "verbosity level.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Pseudo-random number generator to control the starting state."
          },
          "Use": {
            "type": "an int for reproducible results across function calls.",
            "description": ""
          },
          "See": {
            "type": "the :term:`Glossary <random_state>`.",
            "description": ".. versionadded:: 0.23"
          },
          "this": {
            "type": "parameter was previously hardcoded as 0.",
            "description": "Returns\n-------"
          },
          "cluster_centers_indices": {
            "type": "ndarray of shape (n_clusters,)",
            "description": ""
          },
          "Index": {
            "type": "of clusters centers.",
            "description": ""
          },
          "labels": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "For": {
            "type": "an example usage,",
            "description": ""
          },
          "see": {
            "type": "ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.",
            "description": ""
          },
          "You": {
            "type": "may also check out,",
            "description": ":ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`"
          },
          "When": {
            "type": "all training samples have equal similarities and equal preferences,",
            "description": ""
          },
          "however": {
            "type": "they may be degenerate and should be used with caution.",
            "description": ""
          },
          "the": {
            "type": "assignment of cluster centers and labels depends on the preference.",
            "description": ""
          },
          "and": {
            "type": "label ``0`` for every sample will be returned. Otherwise, every",
            "description": ""
          },
          "training": {
            "type": "sample becomes its own cluster center and is assigned a unique",
            "description": "label.\nReferences\n----------"
          },
          "Brendan": {
            "type": "J. Frey and Delbert Dueck, \"Clustering by Passing Messages",
            "description": ""
          },
          "Between": {
            "type": "Data Points\", Science Feb. 2007",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import affinity_propagation\n>>> from sklearn.metrics.pairwise import euclidean_distances\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> S = -euclidean_distances(X, squared=True)\n>>> cluster_centers_indices, labels = affinity_propagation(S, random_state=0)\n>>> cluster_centers_indices"
          },
          "array": {
            "type": "[0, 0, 0, 1, 1, 1]",
            "description": ""
          }
        },
        "returns": "-------\n    cluster_centers_indices : ndarray of shape (n_clusters,)\n        Index of clusters centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n    You may also check out,\n    :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import affinity_propagation\n    >>> from sklearn.metrics.pairwise import euclidean_distances\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> S = -euclidean_distances(X, squared=True)\n    >>> cluster_centers_indices, labels = affinity_propagation(S, random_state=0)\n    >>> cluster_centers_indices\n    array([0, 3])\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n    You may also check out,\n    :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import affinity_propagation\n    >>> from sklearn.metrics.pairwise import euclidean_distances\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> S = -euclidean_distances(X, squared=True)\n    >>> cluster_centers_indices, labels = affinity_propagation(S, random_state=0)\n    >>> cluster_centers_indices\n    array([0, 3])\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import affinity_propagation\n    >>> from sklearn.metrics.pairwise import euclidean_distances\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> S = -euclidean_distances(X, squared=True)\n    >>> cluster_centers_indices, labels = affinity_propagation(S, random_state=0)\n    >>> cluster_centers_indices\n    array([0, 3])\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])"
      }
    },
    {
      "name": "cluster_optics_dbscan",
      "signature": "cluster_optics_dbscan(*, reachability, core_distances, ordering, eps)",
      "documentation": {
        "description": "Perform DBSCAN extraction for an arbitrary epsilon.\n\n    Extracting the clusters runs in linear time. Note that this results in\n    ``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\n    similar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\n    Parameters\n    ----------\n    reachability : ndarray of shape (n_samples,)\n        Reachability distances calculated by OPTICS (``reachability_``).\n\n    core_distances : ndarray of shape (n_samples,)\n        Distances at which points become core (``core_distances_``).\n\n    ordering : ndarray of shape (n_samples,)\n        OPTICS ordered point indices (``ordering_``).\n\n    eps : float\n        DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n        will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n        to one another.\n\n    Returns\n    -------\n    labels_ : array of shape (n_samples,)\n        The estimated labels.",
        "parameters": {
          "reachability": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Reachability": {
            "type": "distances calculated by OPTICS (``reachability_``).",
            "description": ""
          },
          "core_distances": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Distances": {
            "type": "at which points become core (``core_distances_``).",
            "description": ""
          },
          "ordering": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "OPTICS": {
            "type": "ordered point indices (``ordering_``).",
            "description": ""
          },
          "eps": {
            "type": "float",
            "description": ""
          },
          "DBSCAN": {
            "type": "``eps`` parameter. Must be set to < ``max_eps``. Results",
            "description": ""
          },
          "will": {
            "type": "be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close",
            "description": ""
          },
          "to": {
            "type": "one another.",
            "description": "Returns\n-------"
          },
          "labels_": {
            "type": "array of shape (n_samples,)",
            "description": ""
          },
          "The": {
            "type": "estimated labels.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import cluster_optics_dbscan, compute_optics_graph\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n...     X,\n...     min_samples=2,\n...     max_eps=np.inf,\n...     metric=\"minkowski\",\n...     p=2,\n...     metric_params=None,\n...     algorithm=\"auto\",\n...     leaf_size=30,\n...     n_jobs=None,\n... )\n>>> eps = 4.5\n>>> labels = cluster_optics_dbscan(\n...     reachability=reachability,\n...     core_distances=core_distances,\n...     ordering=ordering,\n...     eps=eps,\n... )\n>>> labels"
          },
          "array": {
            "type": "[0, 0, 0, 1, 1, 1]",
            "description": ""
          }
        },
        "returns": "-------\n    labels_ : array of shape (n_samples,)\n        The estimated labels.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import cluster_optics_dbscan, compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None,\n    ... )\n    >>> eps = 4.5\n    >>> labels = cluster_optics_dbscan(\n    ...     reachability=reachability,\n    ...     core_distances=core_distances,\n    ...     ordering=ordering,\n    ...     eps=eps,\n    ... )\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import cluster_optics_dbscan, compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None,\n    ... )\n    >>> eps = 4.5\n    >>> labels = cluster_optics_dbscan(\n    ...     reachability=reachability,\n    ...     core_distances=core_distances,\n    ...     ordering=ordering,\n    ...     eps=eps,\n    ... )\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])"
      }
    },
    {
      "name": "cluster_optics_xi",
      "signature": "cluster_optics_xi(*, reachability, predecessor, ordering, min_samples, min_cluster_size=None, xi=0.05, predecessor_correction=True)",
      "documentation": {
        "description": "Automatically extract clusters according to the Xi-steep method.\n\n    Parameters\n    ----------\n    reachability : ndarray of shape (n_samples,)\n        Reachability distances calculated by OPTICS (`reachability_`).\n\n    predecessor : ndarray of shape (n_samples,)\n        Predecessors calculated by OPTICS.\n\n    ordering : ndarray of shape (n_samples,)\n        OPTICS ordered point indices (`ordering_`).\n\n    min_samples : int > 1 or float between 0 and 1\n        The same as the min_samples given to OPTICS. Up and down steep regions\n        can't have more then ``min_samples`` consecutive non-steep points.\n        Expressed as an absolute number or a fraction of the number of samples\n        (rounded to be at least 2).\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n\n    predecessor_correction : bool, default=True\n        Correct clusters based on the calculated predecessors.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        The labels assigned to samples. Points which are not included\n        in any cluster are labeled as -1.\n\n    clusters : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to ``(end,\n        -start)`` (ascending) so that larger clusters encompassing smaller\n        clusters come after such nested smaller clusters. Since ``labels`` does\n        not reflect the hierarchy, usually ``len(clusters) >\n        np.unique(labels)``.",
        "parameters": {
          "reachability": {
            "type": "plot is defined by the ratio from one point to its",
            "description": ""
          },
          "Reachability": {
            "type": "distances calculated by OPTICS (`reachability_`).",
            "description": ""
          },
          "predecessor": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Predecessors": {
            "type": "calculated by OPTICS.",
            "description": ""
          },
          "ordering": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "OPTICS": {
            "type": "ordered point indices (`ordering_`).",
            "description": ""
          },
          "min_samples": {
            "type": "int > 1 or float between 0 and 1",
            "description": ""
          },
          "The": {
            "type": "list of clusters in the form of ``[start, end]`` in each row, with",
            "description": ""
          },
          "Expressed": {
            "type": "as an absolute number or a fraction of the number of samples",
            "description": "(rounded to be at least 2)."
          },
          "min_cluster_size": {
            "type": "int > 1 or float between 0 and 1, default=None",
            "description": ""
          },
          "Minimum": {
            "type": "number of samples in an OPTICS cluster, expressed as an",
            "description": ""
          },
          "absolute": {
            "type": "number or a fraction of the number of samples (rounded to be",
            "description": ""
          },
          "at": {
            "type": "least 2). If ``None``, the value of ``min_samples`` is used instead.",
            "description": ""
          },
          "xi": {
            "type": "float between 0 and 1, default=0.05",
            "description": ""
          },
          "Determines": {
            "type": "the minimum steepness on the reachability plot that",
            "description": ""
          },
          "constitutes": {
            "type": "a cluster boundary. For example, an upwards point in the",
            "description": ""
          },
          "successor": {
            "type": "being at most 1-xi.",
            "description": ""
          },
          "predecessor_correction": {
            "type": "bool, default=True",
            "description": ""
          },
          "Correct": {
            "type": "clusters based on the calculated predecessors.",
            "description": "Returns\n-------"
          },
          "labels": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "in": {
            "type": "any cluster are labeled as -1.",
            "description": ""
          },
          "clusters": {
            "type": "come after such nested smaller clusters. Since ``labels`` does",
            "description": ""
          },
          "all": {
            "type": "indices inclusive. The clusters are ordered according to ``(end,",
            "description": "-start)`` (ascending) so that larger clusters encompassing smaller"
          },
          "not": {
            "type": "reflect the hierarchy, usually ``len(clusters) >",
            "description": "np.unique(labels)``.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import cluster_optics_xi, compute_optics_graph\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n...     X,\n...     min_samples=2,\n...     max_eps=np.inf,\n...     metric=\"minkowski\",\n...     p=2,\n...     metric_params=None,\n...     algorithm=\"auto\",\n...     leaf_size=30,\n...     n_jobs=None\n... )\n>>> min_samples = 2\n>>> labels, clusters = cluster_optics_xi(\n...     reachability=reachability,\n...     predecessor=predecessor,\n...     ordering=ordering,\n...     min_samples=min_samples,\n... )\n>>> labels"
          },
          "array": {
            "type": "[0, 0, 0, 1, 1, 1]",
            "description": ">>> clusters\narray([[0, 2],\n[3, 5],\n[0, 5]])"
          }
        },
        "returns": "-------\n    labels : ndarray of shape (n_samples,)\n        The labels assigned to samples. Points which are not included\n        in any cluster are labeled as -1.\n\n    clusters : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to ``(end,\n        -start)`` (ascending) so that larger clusters encompassing smaller\n        clusters come after such nested smaller clusters. Since ``labels`` does\n        not reflect the hierarchy, usually ``len(clusters) >\n        np.unique(labels)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import cluster_optics_xi, compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None\n    ... )\n    >>> min_samples = 2\n    >>> labels, clusters = cluster_optics_xi(\n    ...     reachability=reachability,\n    ...     predecessor=predecessor,\n    ...     ordering=ordering,\n    ...     min_samples=min_samples,\n    ... )\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])\n    >>> clusters\n    array([[0, 2],\n           [3, 5],\n           [0, 5]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import cluster_optics_xi, compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None\n    ... )\n    >>> min_samples = 2\n    >>> labels, clusters = cluster_optics_xi(\n    ...     reachability=reachability,\n    ...     predecessor=predecessor,\n    ...     ordering=ordering,\n    ...     min_samples=min_samples,\n    ... )\n    >>> labels\n    array([0, 0, 0, 1, 1, 1])\n    >>> clusters\n    array([[0, 2],\n           [3, 5],\n           [0, 5]])"
      }
    },
    {
      "name": "compute_optics_graph",
      "signature": "compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)",
      "documentation": {
        "description": "Compute the OPTICS reachability graph.\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples) if metric='precomputed'\n        A feature array, or array of distances between samples if\n        metric='precomputed'.\n\n    min_samples : int > 1 or float between 0 and 1\n        The number of samples in a neighborhood for a point to be considered\n        as a core point. Expressed as an absolute number or a fraction of the\n        number of samples (rounded to be at least 2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n        .. note::\n           `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n        - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to `fit` method. (default)\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n        :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n        construction and query, as well as the memory required to store the\n        tree. The optimal value depends on the nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    ordering_ : array of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : array of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    reachability_ : array of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : array of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.",
        "parameters": {
          "X": {
            "type": "{ndarray, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples) if metric='precomputed'",
            "description": ""
          },
          "A": {
            "type": "feature array, or array of distances between samples if",
            "description": "metric='precomputed'."
          },
          "min_samples": {
            "type": "int > 1 or float between 0 and 1",
            "description": ""
          },
          "The": {
            "type": "cluster ordered list of sample indices.",
            "description": ""
          },
          "as": {
            "type": "a core point. Expressed as an absolute number or a fraction of the",
            "description": ""
          },
          "number": {
            "type": "of samples (rounded to be at least 2).",
            "description": ""
          },
          "max_eps": {
            "type": "float, default=np.inf",
            "description": ""
          },
          "in": {
            "type": "shorter run times.",
            "description": ""
          },
          "identify": {
            "type": "clusters across all scales; reducing ``max_eps`` will result",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Any metric from scikit-learn",
            "description": ""
          },
          "or": {
            "type": "scipy.spatial.distance can be used.",
            "description": ""
          },
          "If": {
            "type": "metric is a callable function, it is called on each",
            "description": ""
          },
          "pair": {
            "type": "of instances (rows) and the resulting value recorded. The callable",
            "description": ""
          },
          "should": {
            "type": "take two arrays as input and return one value indicating the",
            "description": ""
          },
          "distance": {
            "type": "between them. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string. If metric is",
            "description": "\"precomputed\", X is assumed to be a distance matrix and must be square."
          },
          "Valid": {
            "type": "values for metric are:",
            "description": "- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n'manhattan']\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n'yule']"
          },
          "See": {
            "type": "the documentation for scipy.spatial.distance for details on these",
            "description": "metrics.\n.. note::\n`'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11."
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": ":class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is"
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n- 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to `fit` method. (default)",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to :class:`~sklearn.neighbors.BallTree` or",
            "description": ":class:`~sklearn.neighbors.KDTree`. This can affect the speed of the"
          },
          "construction": {
            "type": "and query, as well as the memory required to store the",
            "description": "tree. The optimal value depends on the nature of the problem."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": "Returns\n-------"
          },
          "ordering_": {
            "type": "array of shape (n_samples,)",
            "description": ""
          },
          "core_distances_": {
            "type": "array of shape (n_samples,)",
            "description": ""
          },
          "Distance": {
            "type": "at which each sample becomes a core point, indexed by object",
            "description": "order. Points which will never be core have a distance of inf. Use\n``clust.core_distances_[clust.ordering_]`` to access in cluster order."
          },
          "reachability_": {
            "type": "array of shape (n_samples,)",
            "description": ""
          },
          "Reachability": {
            "type": "distances per sample, indexed by object order. Use",
            "description": "``clust.reachability_[clust.ordering_]`` to access in cluster order."
          },
          "predecessor_": {
            "type": "array of shape (n_samples,)",
            "description": ""
          },
          "Point": {
            "type": "that a sample was reached from, indexed by object order.",
            "description": ""
          },
          "Seed": {
            "type": "points have a predecessor of -1.",
            "description": "References\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,"
          },
          "and": {
            "type": "Jörg Sander. \"OPTICS: ordering points to identify the clustering",
            "description": "structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import compute_optics_graph\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n...     X,\n...     min_samples=2,\n...     max_eps=np.inf,\n...     metric=\"minkowski\",\n...     p=2,\n...     metric_params=None,\n...     algorithm=\"auto\",\n...     leaf_size=30,\n...     n_jobs=None,\n... )\n>>> ordering"
          },
          "array": {
            "type": "[-1,  0,  1,  5,  3,  2]",
            "description": ""
          }
        },
        "returns": "-------\n    ordering_ : array of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : array of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    reachability_ : array of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : array of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None,\n    ... )\n    >>> ordering\n    array([0, 1, 2, 5, 3, 4])\n    >>> core_distances\n    array([3.16..., 1.41..., 1.41..., 1.        , 1.        ,\n           4.12...])\n    >>> reachability\n    array([       inf, 3.16..., 1.41..., 4.12..., 1.        ,\n           5.        ])\n    >>> predecessor\n    array([-1,  0,  1,  5,  3,  2])",
        "raises": "",
        "see_also": "",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n        :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n        construction and query, as well as the memory required to store the\n        tree. The optimal value depends on the nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    ordering_ : array of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : array of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    reachability_ : array of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : array of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None,\n    ... )\n    >>> ordering\n    array([0, 1, 2, 5, 3, 4])\n    >>> core_distances\n    array([3.16..., 1.41..., 1.41..., 1.        , 1.        ,\n           4.12...])\n    >>> reachability\n    array([       inf, 3.16..., 1.41..., 4.12..., 1.        ,\n           5.        ])\n    >>> predecessor\n    array([-1,  0,  1,  5,  3,  2])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import compute_optics_graph\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> ordering, core_distances, reachability, predecessor = compute_optics_graph(\n    ...     X,\n    ...     min_samples=2,\n    ...     max_eps=np.inf,\n    ...     metric=\"minkowski\",\n    ...     p=2,\n    ...     metric_params=None,\n    ...     algorithm=\"auto\",\n    ...     leaf_size=30,\n    ...     n_jobs=None,\n    ... )\n    >>> ordering\n    array([0, 1, 2, 5, 3, 4])\n    >>> core_distances\n    array([3.16..., 1.41..., 1.41..., 1.        , 1.        ,\n           4.12...])\n    >>> reachability\n    array([       inf, 3.16..., 1.41..., 4.12..., 1.        ,\n           5.        ])\n    >>> predecessor\n    array([-1,  0,  1,  5,  3,  2])"
      }
    },
    {
      "name": "dbscan",
      "signature": "dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski', metric_params=None, algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=None)",
      "documentation": {
        "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : str or callable, default='minkowski'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit.\n        X may be a :term:`sparse graph <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=2\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. ``None`` means\n        1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n        using all processors. See :term:`Glossary <n_jobs>` for more details.\n        If precomputed distance are used, parallel execution is not available\n        and thus n_jobs will have no effect.\n\n    Returns\n    -------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower\n    memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.",
        "parameters": {
          "X": {
            "type": "may be a :term:`sparse graph <sparse graph>`,",
            "description": ""
          },
          "A": {
            "type": "feature array, or array of distances between samples if",
            "description": "``metric='precomputed'``."
          },
          "eps": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "number of parallel jobs to run for neighbors search. ``None`` means",
            "description": ""
          },
          "as": {
            "type": "in the neighborhood of the other. This is not a maximum bound",
            "description": ""
          },
          "on": {
            "type": "the ``algorithm``.",
            "description": ""
          },
          "important": {
            "type": "DBSCAN parameter to choose appropriately for your data set",
            "description": ""
          },
          "and": {
            "type": "Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996",
            "description": "Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n:doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n<10.1145/3068335>`"
          },
          "min_samples": {
            "type": "int, default=5",
            "description": ""
          },
          "to": {
            "type": "store the tree. The optimal value depends",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "feature": {
            "type": "array. If metric is a string or callable, it must be one of",
            "description": ""
          },
          "the": {
            "type": "memory complexity to O(n.d) where d is the average number of neighbors,",
            "description": ""
          },
          "its": {
            "type": "metric parameter.",
            "description": ""
          },
          "If": {
            "type": "precomputed distance are used, parallel execution is not available",
            "description": ""
          },
          "must": {
            "type": "be square during fit.",
            "description": ""
          },
          "in": {
            "type": "which case only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ".. versionadded:: 0.19"
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or cKDTree. This can affect the speed",
            "description": ""
          },
          "of": {
            "type": "the construction and query, as well as the memory required",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "between": {
            "type": "points.",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Weight": {
            "type": "of each sample, such that a sample with a weight of at least",
            "description": "``min_samples`` is by itself a core sample; a sample with negative"
          },
          "weight": {
            "type": "may inhibit its eps-neighbor from being core.",
            "description": ""
          },
          "Note": {
            "type": "that weights are absolute, and default to 1.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "1": {
            "type": "unless in a :obj:`joblib.parallel_backend` context. ``-1`` means",
            "description": ""
          },
          "using": {
            "type": "all processors. See :term:`Glossary <n_jobs>` for more details.",
            "description": ""
          },
          "core_samples": {
            "type": "ndarray of shape (n_core_samples,)",
            "description": ""
          },
          "Indices": {
            "type": "of core samples.",
            "description": ""
          },
          "labels": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point.  Noisy samples are given the label -1.",
            "description": ""
          },
          "DBSCAN": {
            "type": "An estimator interface for this clustering algorithm.",
            "description": ""
          },
          "OPTICS": {
            "type": "A similar estimator interface clustering at multiple values of",
            "description": "eps. Our implementation is optimized for memory usage.\nNotes\n-----"
          },
          "For": {
            "type": "an example, see :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.",
            "description": ""
          },
          "This": {
            "type": "implementation bulk-computes all neighborhood queries, which increases",
            "description": ""
          },
          "while": {
            "type": "original DBSCAN had memory complexity O(n). It may attract a higher",
            "description": ""
          },
          "memory": {
            "type": "usage.",
            "description": "References\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based"
          },
          "One": {
            "type": "way to avoid the query complexity is to pre-compute sparse",
            "description": ""
          },
          "neighborhoods": {
            "type": "in chunks using",
            "description": ":func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here."
          },
          "Another": {
            "type": "way to reduce memory and computation time is to remove",
            "description": "(near-)duplicate points and use ``sample_weight`` instead.\n:class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower"
          },
          "Algorithm": {
            "type": "for Discovering Clusters in Large Spatial Databases with Noise\"",
            "description": "<https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_."
          },
          "In": {
            "type": "Proceedings of the 2nd International Conference on Knowledge Discovery",
            "description": ""
          },
          "ACM": {
            "type": "Transactions on Database Systems (TODS), 42(3), 19.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import dbscan\n>>> X = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n>>> core_samples, labels = dbscan(X, eps=3, min_samples=2)\n>>> core_samples"
          },
          "array": {
            "type": "[ 0,  0,  0,  1,  1, -1]",
            "description": ""
          }
        },
        "returns": "-------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower\n    memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import dbscan\n    >>> X = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n    >>> core_samples, labels = dbscan(X, eps=3, min_samples=2)\n    >>> core_samples\n    array([0, 1, 2, 3, 4])\n    >>> labels\n    array([ 0,  0,  0,  1,  1, -1])",
        "raises": "",
        "see_also": "--------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower\n    memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import dbscan\n    >>> X = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n    >>> core_samples, labels = dbscan(X, eps=3, min_samples=2)\n    >>> core_samples\n    array([0, 1, 2, 3, 4])\n    >>> labels\n    array([ 0,  0,  0,  1,  1, -1])",
        "notes": "that weights are absolute, and default to 1.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. ``None`` means\n        1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n        using all processors. See :term:`Glossary <n_jobs>` for more details.\n        If precomputed distance are used, parallel execution is not available\n        and thus n_jobs will have no effect.\n\n    Returns\n    -------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower\n    memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import dbscan\n    >>> X = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n    >>> core_samples, labels = dbscan(X, eps=3, min_samples=2)\n    >>> core_samples\n    array([0, 1, 2, 3, 4])\n    >>> labels\n    array([ 0,  0,  0,  1,  1, -1])",
        "examples": "--------\n    >>> from sklearn.cluster import dbscan\n    >>> X = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n    >>> core_samples, labels = dbscan(X, eps=3, min_samples=2)\n    >>> core_samples\n    array([0, 1, 2, 3, 4])\n    >>> labels\n    array([ 0,  0,  0,  1,  1, -1])"
      }
    },
    {
      "name": "estimate_bandwidth",
      "signature": "estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)",
      "documentation": {
        "description": "Estimate the bandwidth to use with the mean-shift algorithm.\n\n    This function takes time at least quadratic in `n_samples`. For large\n    datasets, it is wise to subsample by setting `n_samples`. Alternatively,\n    the parameter `bandwidth` can be set to a small value without estimating\n    it.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input points.\n\n    quantile : float, default=0.3\n        Should be between [0, 1]\n        0.5 means that the median of all pairwise distances is used.\n\n    n_samples : int, default=None\n        The number of samples to use. If not given, all samples are used.\n\n    random_state : int, RandomState instance, default=None\n        The generator used to randomly select the samples from input points\n        for bandwidth estimation. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    bandwidth : float\n        The bandwidth parameter.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Input": {
            "type": "points.",
            "description": ""
          },
          "quantile": {
            "type": "float, default=0.3",
            "description": ""
          },
          "Should": {
            "type": "be between [0, 1]",
            "description": "0.5 means that the median of all pairwise distances is used."
          },
          "n_samples": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "bandwidth parameter.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import estimate_bandwidth\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> estimate_bandwidth(X, quantile=0.5)\nnp.float64(1.61...)"
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": "Returns\n-------"
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "bandwidth": {
            "type": "float",
            "description": ""
          }
        },
        "returns": "-------\n    bandwidth : float\n        The bandwidth parameter.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import estimate_bandwidth\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> estimate_bandwidth(X, quantile=0.5)\n    np.float64(1.61...)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import estimate_bandwidth\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> estimate_bandwidth(X, quantile=0.5)\n    np.float64(1.61...)"
      }
    },
    {
      "name": "get_bin_seeds",
      "signature": "get_bin_seeds(X, bin_size, min_bin_freq=1)",
      "documentation": {
        "description": "Find seeds for mean_shift.\n\n    Finds seeds by first binning data onto a grid whose lines are\n    spaced bin_size apart, and then choosing those bins with at least\n    min_bin_freq points.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input points, the same points that will be used in mean_shift.\n\n    bin_size : float\n        Controls the coarseness of the binning. Smaller values lead\n        to more seeding (which is computationally more expensive). If you're\n        not sure how to set this, set it to the value of the bandwidth used\n        in clustering.mean_shift.\n\n    min_bin_freq : int, default=1\n        Only bins with at least min_bin_freq will be selected as seeds.\n        Raising this value decreases the number of seeds found, which\n        makes mean_shift computationally cheaper.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Input": {
            "type": "points, the same points that will be used in mean_shift.",
            "description": ""
          },
          "bin_size": {
            "type": "float",
            "description": ""
          },
          "Controls": {
            "type": "the coarseness of the binning. Smaller values lead",
            "description": ""
          },
          "to": {
            "type": "more seeding (which is computationally more expensive). If you're",
            "description": ""
          },
          "not": {
            "type": "sure how to set this, set it to the value of the bandwidth used",
            "description": ""
          },
          "in": {
            "type": "clustering.mean_shift.",
            "description": ""
          },
          "min_bin_freq": {
            "type": "int, default=1",
            "description": ""
          },
          "Only": {
            "type": "bins with at least min_bin_freq will be selected as seeds.",
            "description": ""
          },
          "Raising": {
            "type": "this value decreases the number of seeds found, which",
            "description": ""
          },
          "makes": {
            "type": "mean_shift computationally cheaper.",
            "description": "Returns\n-------"
          },
          "bin_seeds": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Points": {
            "type": "used as initial kernel positions in clustering.mean_shift.",
            "description": ""
          }
        },
        "returns": "-------\n    bin_seeds : array-like of shape (n_samples, n_features)\n        Points used as initial kernel positions in clustering.mean_shift.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "k_means",
      "signature": "k_means(X, n_clusters, *, sample_weight=None, init='k-means++', n_init='auto', max_iter=300, verbose=False, tol=0.0001, random_state=None, copy_x=True, algorithm='lloyd', return_n_iter=False)",
      "documentation": {
        "description": "Perform K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is not used during\n        initialization if `init` is a callable or a user provided array.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        - `'k-means++'` : selects initial cluster centers for k-mean\n          clustering in a smart way to speed up convergence. See section\n          Notes in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "The": {
            "type": "final value of the inertia criterion (sum of squared distances to",
            "description": ""
          },
          "will": {
            "type": "be converted to C ordering, which will cause a memory copy",
            "description": ""
          },
          "if": {
            "type": "the given data is not C-contiguous.",
            "description": ""
          },
          "n_clusters": {
            "type": "int",
            "description": ""
          },
          "centroids": {
            "type": "to generate.",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "are": {
            "type": "assigned equal weight. `sample_weight` is not used during",
            "description": ""
          },
          "initialization": {
            "type": "if `init` is a callable or a user provided array.",
            "description": ""
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'"
          },
          "Method": {
            "type": "for initialization:",
            "description": "- `'k-means++'` : selects initial cluster centers for k-mean"
          },
          "clustering": {
            "type": "in a smart way to speed up convergence. See section",
            "description": ""
          },
          "Notes": {
            "type": "in k_init for more details.",
            "description": "- `'random'`: choose `n_clusters` observations (rows) at random from data"
          },
          "for": {
            "type": "the initial centroids.",
            "description": "- If an array is passed, it should be of shape `(n_clusters, n_features)`"
          },
          "and": {
            "type": "gives the initial centers.",
            "description": "- If a callable is passed, it should take arguments `X`, `n_clusters` and a"
          },
          "random": {
            "type": "state and return an initialization.",
            "description": ""
          },
          "n_init": {
            "type": "consecutive runs in terms of inertia.",
            "description": ""
          },
          "Number": {
            "type": "of iterations corresponding to the best results.",
            "description": ""
          },
          "centroid": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "When": {
            "type": "pre-computing distances it is more numerically accurate to center",
            "description": ""
          },
          "10": {
            "type": "if using `init='random'` or `init` is a callable;",
            "description": ""
          },
          "1": {
            "type": "if using `init='k-means++'` or `init` is an array-like.",
            "description": ".. versionadded:: 1.2"
          },
          "Added": {
            "type": "Elkan algorithm",
            "description": ".. versionchanged:: 1.1"
          },
          "Default": {
            "type": "value for `n_init` changed to `'auto'`.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations of the k-means algorithm to run.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Verbosity": {
            "type": "mode.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Relative": {
            "type": "tolerance with regards to Frobenius norm of the difference",
            "description": ""
          },
          "in": {
            "type": "the cluster centers of two consecutive iterations to declare",
            "description": "convergence."
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for centroid initialization. Use",
            "description": ""
          },
          "an": {
            "type": "int to make the randomness deterministic.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "copy_x": {
            "type": "bool, default=True",
            "description": ""
          },
          "the": {
            "type": "closest centroid for all observations in the training set).",
            "description": ""
          },
          "not": {
            "type": "modified. If False, the original data is modified, and put back",
            "description": ""
          },
          "before": {
            "type": "the function returns, but small numerical differences may be",
            "description": ""
          },
          "introduced": {
            "type": "by subtracting and then adding the data mean. Note that if",
            "description": ""
          },
          "a": {
            "type": "copy will be made even if `copy_x` is False.",
            "description": ""
          },
          "algorithm": {
            "type": "{\"lloyd\", \"elkan\"}, default=\"lloyd\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`."
          },
          "more": {
            "type": "memory intensive due to the allocation of an extra array of shape",
            "description": "`(n_samples, n_clusters)`.\n.. versionchanged:: 0.18"
          },
          "Renamed": {
            "type": "\"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".",
            "description": ""
          },
          "Changed": {
            "type": "\"auto\" to use \"lloyd\" instead of \"elkan\".",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": "Returns\n-------"
          },
          "Centroids": {
            "type": "found at the last iteration of k-means.",
            "description": ""
          },
          "label": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "inertia": {
            "type": "float",
            "description": ""
          },
          "best_n_iter": {
            "type": "int",
            "description": ""
          },
          "Returned": {
            "type": "only if `return_n_iter` is set to True.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import k_means\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> centroid, label, inertia = k_means(\n...     X, n_clusters=2, n_init=\"auto\", random_state=0\n... )\n>>> centroid\narray([[10.,  2.],\n[ 1.,  2.]])\n>>> label"
          },
          "array": {
            "type": "[1, 1, 1, 0, 0, 0], dtype=int32",
            "description": ">>> inertia\n16.0"
          }
        },
        "returns": "-------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import k_means\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centroid, label, inertia = k_means(\n    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n    ... )\n    >>> centroid\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    >>> label\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> inertia\n    16.0",
        "raises": "",
        "see_also": "",
        "notes": "in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import k_means\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centroid, label, inertia = k_means(\n    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n    ... )\n    >>> centroid\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    >>> label\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> inertia\n    16.0",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import k_means\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centroid, label, inertia = k_means(\n    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n    ... )\n    >>> centroid\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    >>> label\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> inertia\n    16.0"
      }
    },
    {
      "name": "kmeans_plusplus",
      "signature": "kmeans_plusplus(X, n_clusters, *, sample_weight=None, x_squared_norms=None, random_state=None, n_local_trials=None)",
      "documentation": {
        "description": "Init n_clusters seeds according to k-means++.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds from.\n\n    n_clusters : int\n        The number of centroids to initialize.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is ignored if `init`\n        is a callable or a user provided array.\n\n        .. versionadded:: 1.3\n\n    x_squared_norms : array-like of shape (n_samples,), default=None\n        Squared Euclidean norm of each data point.\n\n    random_state : int or RandomState instance, default=None\n        Determines random number generation for centroid initialization. Pass\n        an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)) which is the recommended setting.\n        Setting to 1 disables the greedy cluster selection and recovers the\n        vanilla k-means++ algorithm which was empirically shown to work less\n        well than its greedy variant.\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "The": {
            "type": "index location of the chosen centers in the data array X. For a",
            "description": ""
          },
          "n_clusters": {
            "type": "int",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "are": {
            "type": "assigned equal weight. `sample_weight` is ignored if `init`",
            "description": ""
          },
          "is": {
            "type": "a callable or a user provided array.",
            "description": ".. versionadded:: 1.3"
          },
          "x_squared_norms": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Squared": {
            "type": "Euclidean norm of each data point.",
            "description": ""
          },
          "random_state": {
            "type": "int or RandomState instance, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for centroid initialization. Pass",
            "description": ""
          },
          "an": {
            "type": "int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "n_local_trials": {
            "type": "int, default=None",
            "description": ""
          },
          "of": {
            "type": "which the one reducing inertia the most is greedily chosen.",
            "description": ""
          },
          "Set": {
            "type": "to None to make the number of trials depend logarithmically",
            "description": ""
          },
          "on": {
            "type": "Discrete algorithms. 2007",
            "description": "Examples\n--------\n>>> from sklearn.cluster import kmeans_plusplus\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n>>> centers\narray([[10,  2],\n[ 1,  0]])\n>>> indices"
          },
          "Setting": {
            "type": "to 1 disables the greedy cluster selection and recovers the",
            "description": ""
          },
          "vanilla": {
            "type": "k-means++ algorithm which was empirically shown to work less",
            "description": ""
          },
          "well": {
            "type": "than its greedy variant.",
            "description": "Returns\n-------"
          },
          "centers": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "indices": {
            "type": "ndarray of shape (n_clusters,)",
            "description": ""
          },
          "given": {
            "type": "index and center, X[index] = center.",
            "description": "Notes\n-----"
          },
          "Selects": {
            "type": "initial cluster centers for k-mean clustering in a smart way",
            "description": ""
          },
          "to": {
            "type": "speed up convergence. see: Arthur, D. and Vassilvitskii, S.",
            "description": "\"k-means++: the advantages of careful seeding\". ACM-SIAM symposium"
          },
          "array": {
            "type": "[3, 2]",
            "description": ""
          }
        },
        "returns": "-------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  2],\n           [ 1,  0]])\n    >>> indices\n    array([3, 2])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  2],\n           [ 1,  0]])\n    >>> indices\n    array([3, 2])",
        "examples": "--------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  2],\n           [ 1,  0]])\n    >>> indices\n    array([3, 2])"
      }
    },
    {
      "name": "linkage_tree",
      "signature": "linkage_tree(X, connectivity=None, n_clusters=None, linkage='complete', affinity='euclidean', return_distance=False)",
      "documentation": {
        "description": "Linkage agglomerative clustering based on a Feature matrix.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Feature matrix representing `n_samples` samples to be clustered.\n\n    connectivity : sparse matrix, default=None\n        Connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is `None`, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        Stop early the construction of the tree at `n_clusters`. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    linkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n        Which linkage criteria to use. The linkage criterion determines which\n        distance to use between sets of observation.\n            - \"average\" uses the average of the distances of each observation of\n              the two sets.\n            - \"complete\" or maximum linkage uses the maximum distances between\n              all observations of the two sets.\n            - \"single\" uses the minimum of the distances between all\n              observations of the two sets.\n\n    affinity : str or callable, default='euclidean'\n        Which metric to use. Can be 'euclidean', 'manhattan', or any\n        distance known to paired distance (see metric.pairwise).\n\n    return_distance : bool, default=False\n        Whether or not to return the distances between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`.\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Returned when `return_distance` is set to `True`.\n\n        distances[i] refers to the distance between children[i][0] and\n        children[i][1] when they are merged.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Feature": {
            "type": "matrix representing `n_samples` samples to be clustered.",
            "description": ""
          },
          "connectivity": {
            "type": "sparse matrix, default=None",
            "description": ""
          },
          "Connectivity": {
            "type": "matrix. Defines for each sample the neighboring samples",
            "description": ""
          },
          "following": {
            "type": "a given structure of the data. The matrix is assumed to",
            "description": ""
          },
          "be": {
            "type": "symmetric and only the upper triangular half is used.",
            "description": ""
          },
          "Default": {
            "type": "is `None`, i.e, the Ward algorithm is unstructured.",
            "description": ""
          },
          "n_clusters": {
            "type": "int, default=None",
            "description": ""
          },
          "Stop": {
            "type": "early the construction of the tree at `n_clusters`. This is",
            "description": ""
          },
          "useful": {
            "type": "to decrease computation time if the number of clusters is",
            "description": ""
          },
          "not": {
            "type": "small compared to the number of samples. In this case, the",
            "description": ""
          },
          "complete": {
            "type": "tree is not computed, thus the 'children' output is of",
            "description": ""
          },
          "limited": {
            "type": "use, and the 'parents' output should rather be used.",
            "description": ""
          },
          "This": {
            "type": "option is valid only when specifying a connectivity matrix.",
            "description": ""
          },
          "linkage": {
            "type": "{\"average\", \"complete\", \"single\"}, default=\"complete\"",
            "description": ""
          },
          "Which": {
            "type": "metric to use. Can be 'euclidean', 'manhattan', or any",
            "description": ""
          },
          "distance": {
            "type": "known to paired distance (see metric.pairwise).",
            "description": ""
          },
          "the": {
            "type": "two sets.",
            "description": "- \"complete\" or maximum linkage uses the maximum distances between"
          },
          "all": {
            "type": "observations of the two sets.",
            "description": "- \"single\" uses the minimum of the distances between all"
          },
          "observations": {
            "type": "of the two sets.",
            "description": ""
          },
          "affinity": {
            "type": "str or callable, default='euclidean'",
            "description": ""
          },
          "return_distance": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the distances between the clusters.",
            "description": "Returns\n-------"
          },
          "children": {
            "type": "ndarray of shape (n_nodes",
            "description": "1, 2)"
          },
          "The": {
            "type": "parent of each node. Only returned when a connectivity matrix",
            "description": ""
          },
          "correspond": {
            "type": "to leaves of the tree which are the original samples.",
            "description": ""
          },
          "A": {
            "type": "node `i` greater than or equal to `n_samples` is a non-leaf",
            "description": ""
          },
          "node": {
            "type": "and has children `children_[i - n_samples]`. Alternatively",
            "description": ""
          },
          "at": {
            "type": "the i-th iteration, children[i][0] and children[i][1]",
            "description": ""
          },
          "are": {
            "type": "merged to form node `n_samples + i`.",
            "description": ""
          },
          "n_connected_components": {
            "type": "int",
            "description": ""
          },
          "n_leaves": {
            "type": "int",
            "description": ""
          },
          "parents": {
            "type": "ndarray of shape (n_nodes, ) or None",
            "description": ""
          },
          "is": {
            "type": "specified, elsewhere 'None' is returned.",
            "description": ""
          },
          "distances": {
            "type": "ndarray of shape (n_nodes",
            "description": "1,)"
          },
          "Returned": {
            "type": "when `return_distance` is set to `True`.",
            "description": "distances[i] refers to the distance between children[i][0] and\nchildren[i][1] when they are merged."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "ward_tree": {
            "type": "Hierarchical clustering with ward linkage.",
            "description": ""
          }
        },
        "returns": "-------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`.\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Returned when `return_distance` is set to `True`.\n\n        distances[i] refers to the distance between children[i][0] and\n        children[i][1] when they are merged.\n\n    See Also\n    --------\n    ward_tree : Hierarchical clustering with ward linkage.",
        "raises": "",
        "see_also": "--------\n    ward_tree : Hierarchical clustering with ward linkage.",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "mean_shift",
      "signature": "mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs=None)",
      "documentation": {
        "description": "Perform mean shift clustering of data using a flat kernel.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    bandwidth : float, default=None\n        Kernel bandwidth. If not None, must be in the range [0, +inf).\n\n        If None, the bandwidth is determined using a heuristic based on\n        the median of all pairwise distances. This will take quadratic time in\n        the number of samples. The sklearn.cluster.estimate_bandwidth function\n        can be used to do this more efficiently.\n\n    seeds : array-like of shape (n_seeds, n_features) or None\n        Point used as initial kernel locations. If None and bin_seeding=False,\n        each data point is used as a seed. If None and bin_seeding=True,\n        see bin_seeding.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. The following tasks benefit\n        from the parallelization:\n\n        - The search of nearest neighbors for bandwidth estimation and label\n          assignments. See the details in the docstring of the\n          ``NearestNeighbors`` class.\n        - Hill-climbing optimization for all seeds.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.17\n           Parallel Execution using *n_jobs*.\n\n    Returns\n    -------\n\n    cluster_centers : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    Notes\n    -----\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Input": {
            "type": "data.",
            "description": ""
          },
          "bandwidth": {
            "type": "float, default=None",
            "description": ""
          },
          "Kernel": {
            "type": "bandwidth. If not None, must be in the range [0, +inf).",
            "description": ""
          },
          "If": {
            "type": "false, then orphans are given cluster label -1.",
            "description": ""
          },
          "the": {
            "type": "number of samples. The sklearn.cluster.estimate_bandwidth function",
            "description": ""
          },
          "can": {
            "type": "be used to do this more efficiently.",
            "description": ""
          },
          "seeds": {
            "type": "array",
            "description": "like of shape (n_seeds, n_features) or None"
          },
          "Point": {
            "type": "used as initial kernel locations. If None and bin_seeding=False,",
            "description": ""
          },
          "each": {
            "type": "data point is used as a seed. If None and bin_seeding=True,",
            "description": ""
          },
          "see": {
            "type": "bin_seeding.",
            "description": ""
          },
          "bin_seeding": {
            "type": "bool, default=False",
            "description": ""
          },
          "corresponds": {
            "type": "to the bandwidth. Setting this option to True will speed",
            "description": ""
          },
          "up": {
            "type": "the algorithm because fewer seeds will be initialized.",
            "description": ""
          },
          "Ignored": {
            "type": "if seeds argument is not None.",
            "description": ""
          },
          "min_bin_freq": {
            "type": "points as seeds.",
            "description": ""
          },
          "To": {
            "type": "speed up the algorithm, accept only those bins with at least",
            "description": ""
          },
          "cluster_all": {
            "type": "bool, default=True",
            "description": ""
          },
          "not": {
            "type": "within any kernel. Orphans are assigned to the nearest kernel.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations, per seed point before the clustering",
            "description": ""
          },
          "operation": {
            "type": "terminates (for that seed point), if has not converged yet.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of jobs to use for the computation. The following tasks benefit",
            "description": ""
          },
          "from": {
            "type": "the parallelization:",
            "description": "- The search of nearest neighbors for bandwidth estimation and label\nassignments. See the details in the docstring of the\n``NearestNeighbors`` class.\n- Hill-climbing optimization for all seeds."
          },
          "See": {
            "type": "term:`Glossary <n_jobs>` for more details.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ".. versionadded:: 0.17"
          },
          "Parallel": {
            "type": "Execution using *n_jobs*.",
            "description": "Returns\n-------"
          },
          "cluster_centers": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Coordinates": {
            "type": "of cluster centers.",
            "description": ""
          },
          "labels": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point.",
            "description": "Notes\n-----"
          },
          "For": {
            "type": "a usage example, see",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import mean_shift\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> cluster_centers, labels = mean_shift(X, bandwidth=2)\n>>> cluster_centers\narray([[3.33..., 6.     ],\n[1.33..., 0.66...]])\n>>> labels"
          },
          "array": {
            "type": "[1, 1, 1, 0, 0, 0]",
            "description": ""
          }
        },
        "returns": "-------\n\n    cluster_centers : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    Notes\n    -----\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import mean_shift\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> cluster_centers, labels = mean_shift(X, bandwidth=2)\n    >>> cluster_centers\n    array([[3.33..., 6.     ],\n           [1.33..., 0.66...]])\n    >>> labels\n    array([1, 1, 1, 0, 0, 0])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import mean_shift\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> cluster_centers, labels = mean_shift(X, bandwidth=2)\n    >>> cluster_centers\n    array([[3.33..., 6.     ],\n           [1.33..., 0.66...]])\n    >>> labels\n    array([1, 1, 1, 0, 0, 0])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import mean_shift\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> cluster_centers, labels = mean_shift(X, bandwidth=2)\n    >>> cluster_centers\n    array([[3.33..., 6.     ],\n           [1.33..., 0.66...]])\n    >>> labels\n    array([1, 1, 1, 0, 0, 0])"
      }
    },
    {
      "name": "spectral_clustering",
      "signature": "spectral_clustering(affinity, *, n_clusters=8, n_components=None, eigen_solver=None, random_state=None, n_init=10, eigen_tol='auto', assign_labels='kmeans', verbose=False)",
      "documentation": {
        "description": "Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster. For instance, when clusters are\n    nested circles on the 2D plane.\n\n    If affinity is the adjacency matrix of a graph, this method can be\n    used to find normalized graph cuts [1]_, [2]_.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    affinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n        The affinity matrix describing the relationship of the samples to\n        embed. **Must be symmetric**.\n\n        Possible examples:\n          - adjacency matrix of a graph,\n          - heat kernel of the pairwise distance matrix of the samples,\n          - symmetric k-nearest neighbours connectivity matrix of the samples.\n\n    n_clusters : int, default=None\n        Number of clusters to extract.\n\n    n_components : int, default=n_clusters\n        Number of eigenvectors to use for the spectral embedding.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition method. If None then ``'arpack'`` is used.\n        See [4]_ for more details regarding ``'lobpcg'``.\n        Eigensolver ``'amg'`` runs ``'lobpcg'`` with optional\n        Algebraic MultiGrid preconditioning and requires pyamg to be installed.\n        It can be faster on very large sparse problems [6]_ and [7]_.\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization\n        of the lobpcg eigenvectors decomposition when `eigen_solver ==\n        'amg'`, and for the K-Means initialization. Use an int to make\n        the results deterministic across calls (See\n        :term:`Glossary <random_state>`).\n\n        .. note::\n            When using `eigen_solver == 'amg'`,\n            it is necessary to also fix the global numpy seed with\n            `np.random.seed(int)` to get deterministic results. See\n            https://github.com/pyamg/pyamg/issues/139 for further\n            information.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    eigen_tol : float, default=\"auto\"\n        Stopping criterion for eigendecomposition of the Laplacian matrix.\n        If `eigen_tol=\"auto\"` then the passed tolerance will depend on the\n        `eigen_solver`:\n\n        - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n        - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n          `eigen_tol=None` which configures the underlying `lobpcg` solver to\n          automatically resolve the value according to their heuristics. See,\n          :func:`scipy.sparse.linalg.lobpcg` for details.\n\n        Note that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n        values of `tol<1e-5` may lead to convergence issues and should be\n        avoided.\n\n        .. versionadded:: 1.2\n           Added 'auto' option.\n\n    assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'\n        The strategy to use to assign labels in the embedding\n        space.  There are three ways to assign labels after the Laplacian\n        embedding.  k-means can be applied and is a popular choice. But it can\n        also be sensitive to initialization. Discretization is another\n        approach which is less sensitive to random initialization [3]_.\n        The cluster_qr method [5]_ directly extracts clusters from eigenvectors\n        in spectral clustering. In contrast to k-means and discretization, cluster_qr\n        has no tuning parameters and is not an iterative method, yet may outperform\n        k-means and discretization in terms of both quality and speed. For a detailed\n        comparison of clustering strategies, refer to the following example:\n        :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`.\n\n        .. versionchanged:: 1.1\n           Added new labeling method 'cluster_qr'.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    Notes\n    -----\n    The graph should contain only one connected component, elsewhere\n    the results make little sense.\n\n    This algorithm solves the normalized cut for `k=2`: it is a\n    normalized spectral clustering.\n\n    References\n    ----------\n\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`\n\n    .. [6] :doi:`Multiscale Spectral Image Segmentation Multiscale preconditioning\n           for computing eigenvalues of graph Laplacians in image segmentation, 2006\n           Andrew Knyazev\n           <10.13140/RG.2.2.35280.02565>`\n\n    .. [7] :doi:`Preconditioned spectral clustering for stochastic block partition\n           streaming graph challenge (Preliminary version at arXiv.)\n           David Zhuzhunashvili, Andrew Knyazev\n           <10.1109/HPEC.2017.8091045>`",
        "parameters": {
          "affinity": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_samples)"
          },
          "The": {
            "type": "graph should contain only one connected component, elsewhere",
            "description": ""
          },
          "Possible": {
            "type": "examples:",
            "description": "- adjacency matrix of a graph,\n- heat kernel of the pairwise distance matrix of the samples,\n- symmetric k-nearest neighbours connectivity matrix of the samples."
          },
          "n_clusters": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of time the k-means algorithm will be run with different",
            "description": ""
          },
          "n_components": {
            "type": "int, default=n_clusters",
            "description": ""
          },
          "eigen_solver": {
            "type": "{None, 'arpack', 'lobpcg', or 'amg'}",
            "description": ""
          },
          "See": {
            "type": "[4]_ for more details regarding ``'lobpcg'``.",
            "description": ""
          },
          "Eigensolver": {
            "type": "``'amg'`` runs ``'lobpcg'`` with optional",
            "description": ""
          },
          "Algebraic": {
            "type": "MultiGrid preconditioning and requires pyamg to be installed.",
            "description": ""
          },
          "It": {
            "type": "can be faster on very large sparse problems [6]_ and [7]_.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "A": {
            "type": "pseudo random number generator used for the initialization",
            "description": ""
          },
          "of": {
            "type": "the lobpcg eigenvectors decomposition when `eigen_solver ==",
            "description": "'amg'`, and for the K-Means initialization. Use an int to make"
          },
          "the": {
            "type": "results make little sense.",
            "description": ""
          },
          "When": {
            "type": "using `eigen_solver == 'amg'`,",
            "description": ""
          },
          "it": {
            "type": "is necessary to also fix the global numpy seed with",
            "description": "`np.random.seed(int)` to get deterministic results. See"
          },
          "https": {
            "type": "//github.com/pyamg/pyamg/issues/139 for further",
            "description": "information."
          },
          "n_init": {
            "type": "int, default=10",
            "description": ""
          },
          "centroid": {
            "type": "seeds. The final results will be the best output of n_init",
            "description": ""
          },
          "consecutive": {
            "type": "runs in terms of inertia. Only used if",
            "description": "``assign_labels='kmeans'``."
          },
          "eigen_tol": {
            "type": "float, default=\"auto\"",
            "description": ""
          },
          "Stopping": {
            "type": "criterion for eigendecomposition of the Laplacian matrix.",
            "description": ""
          },
          "If": {
            "type": "`eigen_tol=\"auto\"` then the passed tolerance will depend on the",
            "description": "`eigen_solver`:\n- If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n- If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n`eigen_tol=None` which configures the underlying `lobpcg` solver to"
          },
          "automatically": {
            "type": "resolve the value according to their heuristics. See,",
            "description": ":func:`scipy.sparse.linalg.lobpcg` for details."
          },
          "Note": {
            "type": "that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`",
            "description": ""
          },
          "values": {
            "type": "of `tol<1e-5` may lead to convergence issues and should be",
            "description": "avoided.\n.. versionadded:: 1.2"
          },
          "Added": {
            "type": "new labeling method 'cluster_qr'.",
            "description": ""
          },
          "assign_labels": {
            "type": "{'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'",
            "description": ""
          },
          "also": {
            "type": "be sensitive to initialization. Discretization is another",
            "description": ""
          },
          "approach": {
            "type": "which is less sensitive to random initialization [3]_.",
            "description": ""
          },
          "in": {
            "type": "spectral clustering. In contrast to k-means and discretization, cluster_qr",
            "description": ""
          },
          "has": {
            "type": "no tuning parameters and is not an iterative method, yet may outperform",
            "description": "k-means and discretization in terms of both quality and speed. For a detailed"
          },
          "comparison": {
            "type": "of clustering strategies, refer to the following example:",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`.\n.. versionchanged:: 1.1"
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Verbosity": {
            "type": "mode.",
            "description": ".. versionadded:: 0.24\nReturns\n-------"
          },
          "labels": {
            "type": "array of integers, shape: n_samples",
            "description": ""
          },
          "This": {
            "type": "algorithm solves the normalized cut for `k=2`: it is a",
            "description": ""
          },
          "normalized": {
            "type": "spectral clustering.",
            "description": "References\n----------\n.. [1] :doi:`Normalized cuts and image segmentation, 2000"
          },
          "Jianbo": {
            "type": "Shi, Jitendra Malik",
            "description": "<10.1109/34.868688>`\n.. [2] :doi:`A Tutorial on Spectral Clustering, 2007"
          },
          "Ulrike": {
            "type": "von Luxburg",
            "description": "<10.1007/s11222-007-9033-z>`\n.. [3] `Multiclass spectral clustering, 2003"
          },
          "Stella": {
            "type": "X. Yu, Jianbo Shi",
            "description": "<https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n.. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:"
          },
          "Locally": {
            "type": "Optimal Block Preconditioned Conjugate Gradient Method, 2001",
            "description": "A. V. Knyazev"
          },
          "SIAM": {
            "type": "Journal on Scientific Computing 23, no. 2, pp. 517-541.",
            "description": "<10.1137/S1064827500366124>`\n.. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019"
          },
          "Anil": {
            "type": "Damle, Victor Minden, Lexing Ying",
            "description": "<10.1093/imaiai/iay008>`\n.. [6] :doi:`Multiscale Spectral Image Segmentation Multiscale preconditioning"
          },
          "for": {
            "type": "computing eigenvalues of graph Laplacians in image segmentation, 2006",
            "description": ""
          },
          "Andrew": {
            "type": "Knyazev",
            "description": "<10.13140/RG.2.2.35280.02565>`\n.. [7] :doi:`Preconditioned spectral clustering for stochastic block partition"
          },
          "streaming": {
            "type": "graph challenge (Preliminary version at arXiv.)",
            "description": ""
          },
          "David": {
            "type": "Zhuzhunashvili, Andrew Knyazev",
            "description": "<10.1109/HPEC.2017.8091045>`\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics.pairwise import pairwise_kernels\n>>> from sklearn.cluster import spectral_clustering\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> affinity = pairwise_kernels(X, metric='rbf')\n>>> spectral_clustering(\n...     affinity=affinity, n_clusters=2, assign_labels=\"discretize\", random_state=0\n... )"
          },
          "array": {
            "type": "[1, 1, 1, 0, 0, 0]",
            "description": ""
          }
        },
        "returns": "-------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    Notes\n    -----\n    The graph should contain only one connected component, elsewhere\n    the results make little sense.\n\n    This algorithm solves the normalized cut for `k=2`: it is a\n    normalized spectral clustering.\n\n    References\n    ----------\n\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`\n\n    .. [6] :doi:`Multiscale Spectral Image Segmentation Multiscale preconditioning\n           for computing eigenvalues of graph Laplacians in image segmentation, 2006\n           Andrew Knyazev\n           <10.13140/RG.2.2.35280.02565>`\n\n    .. [7] :doi:`Preconditioned spectral clustering for stochastic block partition\n           streaming graph challenge (Preliminary version at arXiv.)\n           David Zhuzhunashvili, Andrew Knyazev\n           <10.1109/HPEC.2017.8091045>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> from sklearn.cluster import spectral_clustering\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> affinity = pairwise_kernels(X, metric='rbf')\n    >>> spectral_clustering(\n    ...     affinity=affinity, n_clusters=2, assign_labels=\"discretize\", random_state=0\n    ... )\n    array([1, 1, 1, 0, 0, 0])",
        "raises": "",
        "see_also": "",
        "notes": "that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n        values of `tol<1e-5` may lead to convergence issues and should be\n        avoided.\n\n        .. versionadded:: 1.2\n           Added 'auto' option.\n\n    assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'\n        The strategy to use to assign labels in the embedding\n        space.  There are three ways to assign labels after the Laplacian\n        embedding.  k-means can be applied and is a popular choice. But it can\n        also be sensitive to initialization. Discretization is another\n        approach which is less sensitive to random initialization [3]_.\n        The cluster_qr method [5]_ directly extracts clusters from eigenvectors\n        in spectral clustering. In contrast to k-means and discretization, cluster_qr\n        has no tuning parameters and is not an iterative method, yet may outperform\n        k-means and discretization in terms of both quality and speed. For a detailed\n        comparison of clustering strategies, refer to the following example:\n        :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`.\n\n        .. versionchanged:: 1.1\n           Added new labeling method 'cluster_qr'.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    Notes\n    -----\n    The graph should contain only one connected component, elsewhere\n    the results make little sense.\n\n    This algorithm solves the normalized cut for `k=2`: it is a\n    normalized spectral clustering.\n\n    References\n    ----------\n\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`\n\n    .. [6] :doi:`Multiscale Spectral Image Segmentation Multiscale preconditioning\n           for computing eigenvalues of graph Laplacians in image segmentation, 2006\n           Andrew Knyazev\n           <10.13140/RG.2.2.35280.02565>`\n\n    .. [7] :doi:`Preconditioned spectral clustering for stochastic block partition\n           streaming graph challenge (Preliminary version at arXiv.)\n           David Zhuzhunashvili, Andrew Knyazev\n           <10.1109/HPEC.2017.8091045>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> from sklearn.cluster import spectral_clustering\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> affinity = pairwise_kernels(X, metric='rbf')\n    >>> spectral_clustering(\n    ...     affinity=affinity, n_clusters=2, assign_labels=\"discretize\", random_state=0\n    ... )\n    array([1, 1, 1, 0, 0, 0])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> from sklearn.cluster import spectral_clustering\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> affinity = pairwise_kernels(X, metric='rbf')\n    >>> spectral_clustering(\n    ...     affinity=affinity, n_clusters=2, assign_labels=\"discretize\", random_state=0\n    ... )\n    array([1, 1, 1, 0, 0, 0])"
      }
    },
    {
      "name": "ward_tree",
      "signature": "ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False)",
      "documentation": {
        "description": "Ward clustering based on a Feature matrix.\n\n    Recursively merges the pair of clusters that minimally increases\n    within-cluster variance.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Feature matrix representing `n_samples` samples to be clustered.\n\n    connectivity : {array-like, sparse matrix}, default=None\n        Connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        `n_clusters` should be less than `n_samples`.  Stop early the\n        construction of the tree at `n_clusters.` This is useful to decrease\n        computation time if the number of clusters is not small compared to the\n        number of samples. In this case, the complete tree is not computed, thus\n        the 'children' output is of limited use, and the 'parents' output should\n        rather be used. This option is valid only when specifying a connectivity\n        matrix.\n\n    return_distance : bool, default=False\n        If `True`, return the distance between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`.\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes,) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Only returned if `return_distance` is set to `True` (for compatibility).\n        The distances between the centers of the nodes. `distances[i]`\n        corresponds to a weighted Euclidean distance between\n        the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n        leaves of the tree, then `distances[i]` is their unweighted Euclidean\n        distance. Distances are updated in the following way\n        (from scipy.hierarchy.linkage):\n\n        The new entry :math:`d(u,v)` is computed as follows,\n\n        .. math::\n\n           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\frac{|v|}\n                               {T}d(s,t)^2}\n\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Feature": {
            "type": "matrix representing `n_samples` samples to be clustered.",
            "description": ""
          },
          "connectivity": {
            "type": "{array",
            "description": "like, sparse matrix}, default=None"
          },
          "Connectivity": {
            "type": "matrix. Defines for each sample the neighboring samples",
            "description": ""
          },
          "following": {
            "type": "a given structure of the data. The matrix is assumed to",
            "description": ""
          },
          "be": {
            "type": "symmetric and only the upper triangular half is used.",
            "description": ""
          },
          "Default": {
            "type": "is None, i.e, the Ward algorithm is unstructured.",
            "description": ""
          },
          "n_clusters": {
            "type": "int, default=None",
            "description": "`n_clusters` should be less than `n_samples`.  Stop early the"
          },
          "construction": {
            "type": "of the tree at `n_clusters.` This is useful to decrease",
            "description": ""
          },
          "computation": {
            "type": "time if the number of clusters is not small compared to the",
            "description": ""
          },
          "number": {
            "type": "of samples. In this case, the complete tree is not computed, thus",
            "description": ""
          },
          "the": {
            "type": "nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to",
            "description": ""
          },
          "rather": {
            "type": "be used. This option is valid only when specifying a connectivity",
            "description": "matrix."
          },
          "return_distance": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "`True`, return the distance between the clusters.",
            "description": "Returns\n-------"
          },
          "children": {
            "type": "ndarray of shape (n_nodes",
            "description": "1, 2)"
          },
          "The": {
            "type": "new entry :math:`d(u,v)` is computed as follows,",
            "description": ".. math::"
          },
          "correspond": {
            "type": "to leaves of the tree which are the original samples.",
            "description": ""
          },
          "A": {
            "type": "node `i` greater than or equal to `n_samples` is a non-leaf",
            "description": ""
          },
          "node": {
            "type": "and has children `children_[i - n_samples]`. Alternatively",
            "description": ""
          },
          "at": {
            "type": "the i-th iteration, children[i][0] and children[i][1]",
            "description": ""
          },
          "are": {
            "type": "merged to form node `n_samples + i`.",
            "description": ""
          },
          "n_connected_components": {
            "type": "int",
            "description": ""
          },
          "n_leaves": {
            "type": "int",
            "description": ""
          },
          "parents": {
            "type": "ndarray of shape (n_nodes,) or None",
            "description": ""
          },
          "is": {
            "type": "specified, elsewhere 'None' is returned.",
            "description": ""
          },
          "distances": {
            "type": "ndarray of shape (n_nodes",
            "description": "1,)"
          },
          "Only": {
            "type": "returned if `return_distance` is set to `True` (for compatibility).",
            "description": ""
          },
          "corresponds": {
            "type": "to a weighted Euclidean distance between",
            "description": ""
          },
          "leaves": {
            "type": "of the tree, then `distances[i]` is their unweighted Euclidean",
            "description": "distance. Distances are updated in the following way\n(from scipy.hierarchy.linkage):"
          },
          "d": {
            "type": "u,v",
            "description": "= \\sqrt{\\frac{|v|+|s|}\n{T}d(v,s)^2\n+ \\frac{|v|+|t|}\n{T}d(v,t)^2\n- \\frac{|v|}\n{T}d(s,t)^2}"
          },
          "where": {
            "type": "math:`u` is the newly joined cluster consisting of",
            "description": ""
          },
          "clusters": {
            "type": "math:`s` and :math:`t`, :math:`v` is an unused",
            "description": ""
          },
          "cluster": {
            "type": "in the forest, :math:`T=|v|+|s|+|t|`, and",
            "description": ":math:`|*|` is the cardinality of its argument. This is also"
          },
          "known": {
            "type": "as the incremental algorithm.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import ward_tree\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> children, n_connected_components, n_leaves, parents = ward_tree(X)\n>>> children\narray([[0, 1],\n[3, 5],\n[2, 6],\n[4, 7],\n[8, 9]])\n>>> n_connected_components\n1\n>>> n_leaves\n6"
          }
        },
        "returns": "-------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`.\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes,) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Only returned if `return_distance` is set to `True` (for compatibility).\n        The distances between the centers of the nodes. `distances[i]`\n        corresponds to a weighted Euclidean distance between\n        the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n        leaves of the tree, then `distances[i]` is their unweighted Euclidean\n        distance. Distances are updated in the following way\n        (from scipy.hierarchy.linkage):\n\n        The new entry :math:`d(u,v)` is computed as follows,\n\n        .. math::\n\n           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\frac{|v|}\n                               {T}d(s,t)^2}\n\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import ward_tree\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> children, n_connected_components, n_leaves, parents = ward_tree(X)\n    >>> children\n    array([[0, 1],\n           [3, 5],\n           [2, 6],\n           [4, 7],\n           [8, 9]])\n    >>> n_connected_components\n    1\n    >>> n_leaves\n    6",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import ward_tree\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> children, n_connected_components, n_leaves, parents = ward_tree(X)\n    >>> children\n    array([[0, 1],\n           [3, 5],\n           [2, 6],\n           [4, 7],\n           [8, 9]])\n    >>> n_connected_components\n    1\n    >>> n_leaves\n    6"
      }
    }
  ],
  "classes": [
    {
      "name": "AffinityPropagation",
      "documentation": {
        "description": "Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, default=0.5\n        Damping factor in the range `[0.5, 1.0)` is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, default=200\n        Maximum number of iterations.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : bool, default=True\n        Make a copy of input data.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : {'euclidean', 'precomputed'}, default='euclidean'\n        Which affinity to use. At the moment 'precomputed' and\n        ``euclidean`` are supported. 'euclidean' uses the\n        negative squared euclidean distance between points.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : ndarray of shape (n_clusters,)\n        Indices of cluster centers.\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AgglomerativeClustering : Recursively merges the pair of\n        clusters that minimally increases a given linkage distance.\n    FeatureAgglomeration : Similar to AgglomerativeClustering,\n        but recursively merges features instead of samples.\n    KMeans : K-Means clustering.\n    MiniBatchKMeans : Mini-Batch K-Means clustering.\n    MeanShift : Mean shift clustering using a flat kernel.\n    SpectralClustering : Apply clustering to a projection\n        of the normalized Laplacian.\n\n    Notes\n    -----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When ``fit`` does not converge, ``cluster_centers_`` is still populated\n    however it may be degenerate. In such a case, proceed with caution.\n    If ``fit`` does not converge and fails to produce any ``cluster_centers_``\n    then ``predict`` will label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007",
        "parameters": {
          "damping": {
            "type": "float, default=0.5",
            "description": ""
          },
          "Damping": {
            "type": "factor in the range `[0.5, 1.0)` is the extent to",
            "description": ""
          },
          "which": {
            "type": "the current value is maintained relative to",
            "description": ""
          },
          "incoming": {
            "type": "values (weighted 1 - damping). This in order",
            "description": ""
          },
          "to": {
            "type": "avoid numerical oscillations when updating these",
            "description": ""
          },
          "values": {
            "type": "messages",
            "description": "."
          },
          "max_iter": {
            "type": "int, default=200",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations.",
            "description": ""
          },
          "convergence_iter": {
            "type": "int, default=15",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "of": {
            "type": "the normalized Laplacian.",
            "description": "Notes\n-----"
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "Make": {
            "type": "a copy of input data.",
            "description": ""
          },
          "preference": {
            "type": "array",
            "description": "like of shape (n_samples,) or float, default=None"
          },
          "Preferences": {
            "type": "for each point - points with larger values of",
            "description": ""
          },
          "preferences": {
            "type": "value. If the preferences are not passed as arguments,",
            "description": ""
          },
          "they": {
            "type": "will be set to the median of the input similarities.",
            "description": ""
          },
          "affinity": {
            "type": "{'euclidean', 'precomputed'}, default='euclidean'",
            "description": ""
          },
          "Which": {
            "type": "affinity to use. At the moment 'precomputed' and",
            "description": "``euclidean`` are supported. 'euclidean' uses the"
          },
          "negative": {
            "type": "squared euclidean distance between points.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to be verbose.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Pseudo-random number generator to control the starting state."
          },
          "Use": {
            "type": "an int for reproducible results across function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "this": {
            "type": "parameter was previously hardcoded as 0.",
            "description": "Attributes\n----------"
          },
          "cluster_centers_indices_": {
            "type": "ndarray of shape (n_clusters,)",
            "description": ""
          },
          "Indices": {
            "type": "of cluster centers.",
            "description": ""
          },
          "cluster_centers_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Cluster": {
            "type": "centers (if affinity != ``precomputed``).",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Labels": {
            "type": "of each point.",
            "description": ""
          },
          "affinity_matrix_": {
            "type": "ndarray of shape (n_samples, n_samples)",
            "description": ""
          },
          "Stores": {
            "type": "the affinity matrix used in ``fit``.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "AgglomerativeClustering": {
            "type": "Recursively merges the pair of",
            "description": ""
          },
          "clusters": {
            "type": "that minimally increases a given linkage distance.",
            "description": ""
          },
          "FeatureAgglomeration": {
            "type": "Similar to AgglomerativeClustering,",
            "description": ""
          },
          "but": {
            "type": "recursively merges features instead of samples.",
            "description": ""
          },
          "KMeans": {
            "type": "K",
            "description": "Means clustering."
          },
          "MiniBatchKMeans": {
            "type": "Mini",
            "description": "Batch K-Means clustering."
          },
          "MeanShift": {
            "type": "Mean shift clustering using a flat kernel.",
            "description": ""
          },
          "SpectralClustering": {
            "type": "Apply clustering to a projection",
            "description": ""
          },
          "For": {
            "type": "an example usage,",
            "description": ""
          },
          "see": {
            "type": "ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.",
            "description": ""
          },
          "The": {
            "type": "algorithmic complexity of affinity propagation is quadratic",
            "description": ""
          },
          "in": {
            "type": "the number of points.",
            "description": ""
          },
          "When": {
            "type": "all training samples have equal similarities and equal preferences,",
            "description": ""
          },
          "however": {
            "type": "it may be degenerate. In such a case, proceed with caution.",
            "description": ""
          },
          "If": {
            "type": "the preference is smaller than the similarities, ``fit`` will result in",
            "description": ""
          },
          "then": {
            "type": "``predict`` will label every sample as ``-1``.",
            "description": ""
          },
          "the": {
            "type": "assignment of cluster centers and labels depends on the preference.",
            "description": ""
          },
          "a": {
            "type": "single cluster center and label ``0`` for every sample. Otherwise, every",
            "description": ""
          },
          "training": {
            "type": "sample becomes its own cluster center and is assigned a unique",
            "description": "label.\nReferences\n----------"
          },
          "Brendan": {
            "type": "J. Frey and Delbert Dueck, \"Clustering by Passing Messages",
            "description": ""
          },
          "Between": {
            "type": "Data Points\", Science Feb. 2007",
            "description": "Examples\n--------\n>>> from sklearn.cluster import AffinityPropagation\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AffinityPropagation(random_state=5).fit(X)\n>>> clustering"
          },
          "AffinityPropagation": {
            "type": "random_state=5",
            "description": ">>> clustering.labels_"
          },
          "array": {
            "type": "[0, 1]",
            "description": ">>> clustering.cluster_centers_\narray([[1, 2],\n[4, 2]])"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AgglomerativeClustering : Recursively merges the pair of\n        clusters that minimally increases a given linkage distance.\n    FeatureAgglomeration : Similar to AgglomerativeClustering,\n        but recursively merges features instead of samples.\n    KMeans : K-Means clustering.\n    MiniBatchKMeans : Mini-Batch K-Means clustering.\n    MeanShift : Mean shift clustering using a flat kernel.\n    SpectralClustering : Apply clustering to a projection\n        of the normalized Laplacian.\n\n    Notes\n    -----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When ``fit`` does not converge, ``cluster_centers_`` is still populated\n    however it may be degenerate. In such a case, proceed with caution.\n    If ``fit`` does not converge and fails to produce any ``cluster_centers_``\n    then ``predict`` will label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation(random_state=5).fit(X)\n    >>> clustering\n    AffinityPropagation(random_state=5)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])",
        "notes": "-----\n    For an example usage,\n    see :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When the algorithm does not converge, it will still return a arrays of\n    ``cluster_center_indices`` and labels if there are any exemplars/clusters,\n    however they may be degenerate and should be used with caution.\n\n    When ``fit`` does not converge, ``cluster_centers_`` is still populated\n    however it may be degenerate. In such a case, proceed with caution.\n    If ``fit`` does not converge and fails to produce any ``cluster_centers_``\n    then ``predict`` will label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation(random_state=5).fit(X)\n    >>> clustering\n    AffinityPropagation(random_state=5)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])",
        "examples": "--------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation(random_state=5).fit(X)\n    >>> clustering\n    AffinityPropagation(random_state=5)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or similarities / affinities between",
                "description": ""
              },
              "instances": {
                "type": "if ``affinity='precomputed'``. If a sparse feature matrix",
                "description": ""
              },
              "is": {
                "type": "provided, it will be converted into a sparse ``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------\nself"
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "documentation": {
            "description": "Fit clustering from features/affinity matrix; return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or similarities / affinities between",
                "description": ""
              },
              "instances": {
                "type": "if ``affinity='precomputed'``. If a sparse feature matrix",
                "description": ""
              },
              "is": {
                "type": "provided, it will be converted into a sparse ``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to predict. If a sparse matrix is provided, it will be",
                "description": ""
              },
              "converted": {
                "type": "into a sparse ``csr_matrix``.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "AgglomerativeClustering",
      "documentation": {
        "description": "Agglomerative Clustering.\n\n    Recursively merges pair of clusters of sample data; uses linkage distance.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    metric : str or callable, default=\"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n        \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n        as input for the fit method. If connectivity is None, linkage is\n        \"single\" and affinity is not \"precomputed\" any valid pairwise distance\n        metric can be assigned.\n\n        .. versionadded:: 1.2\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like, sparse matrix, or callable, default=None\n        Connectivity matrix. Defines for each sample the neighboring\n        samples following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        `kneighbors_graph`. Default is ``None``, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n        For an example of connectivity matrix using\n        :class:`~sklearn.neighbors.kneighbors_graph`, see\n        :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at ``n_clusters``. This is\n        useful to decrease computation time if the number of clusters is not\n        small compared to the number of samples. This option is useful only\n        when specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of observation. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - 'ward' minimizes the variance of the clusters being merged.\n        - 'average' uses the average of the distances of each observation of\n          the two sets.\n        - 'complete' or 'maximum' linkage uses the maximum distances between\n          all observations of the two sets.\n        - 'single' uses the minimum of the distances between all observations\n          of the two sets.\n\n        .. versionadded:: 0.20\n            Added the 'single' option\n\n        For examples comparing different `linkage` criteria, see\n        :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`.\n\n    distance_threshold : float, default=None\n        The linkage distance threshold at or above which clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n        For an example of dendrogram visualization, see\n        :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`.\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : ndarray of shape (n_samples)\n        Cluster labels for each point.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    children_ : array-like of shape (n_samples-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`.\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    See Also\n    --------\n    FeatureAgglomeration : Agglomerative clustering but for features instead of\n        samples.\n    ward_tree : Hierarchical clustering with ward linkage.",
        "parameters": {
          "n_clusters": {
            "type": "int or None, default=2",
            "description": ""
          },
          "The": {
            "type": "children of each non-leaf node. Values less than `n_samples`",
            "description": ""
          },
          "metric": {
            "type": "can be assigned.",
            "description": ".. versionadded:: 1.2"
          },
          "Metric": {
            "type": "used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",",
            "description": "\"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n\"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed"
          },
          "as": {
            "type": "input for the fit method. If connectivity is None, linkage is",
            "description": "\"single\" and affinity is not \"precomputed\" any valid pairwise distance"
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": ""
          },
          "Used": {
            "type": "to cache the output of the computation of the tree.",
            "description": ""
          },
          "By": {
            "type": "default, no caching is done. If a string is given, it is the",
            "description": ""
          },
          "path": {
            "type": "to the caching directory.",
            "description": ""
          },
          "connectivity": {
            "type": "array",
            "description": "like, sparse matrix, or callable, default=None"
          },
          "Connectivity": {
            "type": "matrix. Defines for each sample the neighboring",
            "description": ""
          },
          "samples": {
            "type": "following a given structure of the data.",
            "description": ""
          },
          "This": {
            "type": "can be a connectivity matrix itself or a callable that transforms",
            "description": ""
          },
          "the": {
            "type": "two sets.",
            "description": "- 'complete' or 'maximum' linkage uses the maximum distances between"
          },
          "hierarchical": {
            "type": "clustering algorithm is unstructured.",
            "description": ""
          },
          "For": {
            "type": "an example of dendrogram visualization, see",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`.\nAttributes\n----------"
          },
          "compute_full_tree": {
            "type": "'auto' or bool, default='auto'",
            "description": ""
          },
          "Stop": {
            "type": "early the construction of the tree at ``n_clusters``. This is",
            "description": ""
          },
          "useful": {
            "type": "to decrease computation time if the number of clusters is not",
            "description": ""
          },
          "small": {
            "type": "compared to the number of samples. This option is useful only",
            "description": ""
          },
          "when": {
            "type": "specifying a connectivity matrix. Note also that when varying the",
            "description": ""
          },
          "number": {
            "type": "of clusters and using caching, it may be advantageous to compute",
            "description": ""
          },
          "to": {
            "type": "`True` when `distance_threshold` is not `None` or that `n_clusters`",
            "description": ""
          },
          "is": {
            "type": "set to `True`.",
            "description": ""
          },
          "linkage": {
            "type": "{'ward', 'complete', 'average', 'single'}, default='ward'",
            "description": ""
          },
          "Which": {
            "type": "linkage criterion to use. The linkage criterion determines which",
            "description": ""
          },
          "distance": {
            "type": "to use between sets of observation. The algorithm will merge",
            "description": ""
          },
          "all": {
            "type": "observations of the two sets.",
            "description": "- 'single' uses the minimum of the distances between all observations"
          },
          "of": {
            "type": "the two sets.",
            "description": ".. versionadded:: 0.20"
          },
          "Added": {
            "type": "the 'single' option",
            "description": ""
          },
          "distance_threshold": {
            "type": "float, default=None",
            "description": ""
          },
          "compute_distances": {
            "type": "bool, default=False",
            "description": ""
          },
          "Computes": {
            "type": "distances between clusters even if `distance_threshold` is not",
            "description": "used. This can be used to make dendrogram visualization, but introduces"
          },
          "a": {
            "type": "computational and memory overhead.",
            "description": ".. versionadded:: 0.24"
          },
          "n_clusters_": {
            "type": "int",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point.",
            "description": ""
          },
          "n_leaves_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_connected_components_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "children_": {
            "type": "array",
            "description": "like of shape (n_samples-1, 2)"
          },
          "correspond": {
            "type": "to leaves of the tree which are the original samples.",
            "description": ""
          },
          "A": {
            "type": "node `i` greater than or equal to `n_samples` is a non-leaf",
            "description": ""
          },
          "node": {
            "type": "and has children `children_[i - n_samples]`. Alternatively",
            "description": ""
          },
          "at": {
            "type": "the i-th iteration, children[i][0] and children[i][1]",
            "description": ""
          },
          "are": {
            "type": "merged to form node `n_samples + i`.",
            "description": ""
          },
          "distances_": {
            "type": "array",
            "description": "like of shape (n_nodes-1,)"
          },
          "Distances": {
            "type": "between nodes in the corresponding place in `children_`.",
            "description": ""
          },
          "Only": {
            "type": "computed if `distance_threshold` is used or `compute_distances`",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "FeatureAgglomeration": {
            "type": "Agglomerative clustering but for features instead of",
            "description": "samples."
          },
          "ward_tree": {
            "type": "Hierarchical clustering with ward linkage.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AgglomerativeClustering().fit(X)\n>>> clustering"
          },
          "AgglomerativeClustering": {
            "type": "",
            "description": ">>> clustering.labels_"
          },
          "array": {
            "type": "[1, 1, 1, 0, 0, 0]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    FeatureAgglomeration : Agglomerative clustering but for features instead of\n        samples.\n    ward_tree : Hierarchical clustering with ward linkage.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AgglomerativeClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AgglomerativeClustering().fit(X)\n    >>> clustering\n    AgglomerativeClustering()\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.cluster import AgglomerativeClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AgglomerativeClustering().fit(X)\n    >>> clustering\n    AgglomerativeClustering()\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the hierarchical clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or                 (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features) or                 (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or distances between instances if",
                "description": "``metric='precomputed'``."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the fitted instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "documentation": {
            "description": "Fit and return the result of each sample's clustering assignment.\n\n        In addition to fitting, this method also return the result of the\n        clustering assignment for each sample in the training set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features) or                 (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or distances between instances if",
                "description": "``affinity='precomputed'``."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Birch",
      "documentation": {
        "description": "Implements the BIRCH clustering algorithm.\n\n    It is a memory-efficient, online-learning algorithm provided as an\n    alternative to :class:`MiniBatchKMeans`. It constructs a tree\n    data structure with the cluster centroids being read off the leaf.\n    These can be either the final cluster centroids or can be provided as input\n    to another clustering algorithm such as :class:`AgglomerativeClustering`.\n\n    Read more in the :ref:`User Guide <birch>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    threshold : float, default=0.5\n        The radius of the subcluster obtained by merging a new sample and the\n        closest subcluster should be lesser than the threshold. Otherwise a new\n        subcluster is started. Setting this value to be very low promotes\n        splitting and vice-versa.\n\n    branching_factor : int, default=50\n        Maximum number of CF subclusters in each node. If a new samples enters\n        such that the number of subclusters exceed the branching_factor then\n        that node is split into two nodes with the subclusters redistributed\n        in each. The parent subcluster of that node is removed and two new\n        subclusters are added as parents of the 2 split nodes.\n\n    n_clusters : int, instance of sklearn.cluster model or None, default=3\n        Number of clusters after the final clustering step, which treats the\n        subclusters from the leaves as new samples.\n\n        - `None` : the final clustering step is not performed and the\n          subclusters are returned as they are.\n\n        - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n          is fit treating the subclusters as new samples and the initial data\n          is mapped to the label of the closest subcluster.\n\n        - `int` : the model fit is :class:`AgglomerativeClustering` with\n          `n_clusters` set to be equal to the int.\n\n    compute_labels : bool, default=True\n        Whether or not to compute labels for each fit.\n\n    copy : bool, default=True\n        Whether or not to make a copy of the given data. If set to False,\n        the initial data will be overwritten.\n\n        .. deprecated:: 1.6\n            `copy` was deprecated in 1.6 and will be removed in 1.8. It has no effect\n            as the estimator does not perform in-place operations on the input data.\n\n    Attributes\n    ----------\n    root_ : _CFNode\n        Root of the CFTree.\n\n    dummy_leaf_ : _CFNode\n        Start pointer to all the leaves.\n\n    subcluster_centers_ : ndarray\n        Centroids of all subclusters read directly from the leaves.\n\n    subcluster_labels_ : ndarray\n        Labels assigned to the centroids of the subclusters after\n        they are clustered globally.\n\n    labels_ : ndarray of shape (n_samples,)\n        Array of labels assigned to the input data.\n        if partial_fit is used instead of fit, they are assigned to the\n        last batch of data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative implementation that does incremental updates\n        of the centers' positions using mini-batches.\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.MiniBatchKMeans`.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch",
        "parameters": {
          "threshold": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "tree data structure consists of nodes with each node consisting of",
            "description": ""
          },
          "closest": {
            "type": "subcluster should be lesser than the threshold. Otherwise a new",
            "description": ""
          },
          "subcluster": {
            "type": "are updated. This is done recursively till the properties of",
            "description": ""
          },
          "splitting": {
            "type": "and vice-versa.",
            "description": ""
          },
          "branching_factor": {
            "type": "int, default=50",
            "description": ""
          },
          "Maximum": {
            "type": "number of CF subclusters in each node. If a new samples enters",
            "description": ""
          },
          "such": {
            "type": "that the number of subclusters exceed the branching_factor then",
            "description": ""
          },
          "that": {
            "type": "node is split into two nodes with the subclusters redistributed",
            "description": ""
          },
          "in": {
            "type": "each. The parent subcluster of that node is removed and two new",
            "description": ""
          },
          "subclusters": {
            "type": "are returned as they are.",
            "description": "- :mod:`sklearn.cluster` Estimator : If a model is provided, the model"
          },
          "n_clusters": {
            "type": "int, instance of sklearn.cluster model or None, default=3",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "is": {
            "type": "determined by the branching factor. Each subcluster maintains a",
            "description": ""
          },
          "compute_labels": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not to make a copy of the given data. If set to False,",
            "description": ""
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "the": {
            "type": "leaf node are updated.",
            "description": ""
          },
          "as": {
            "type": "the estimator does not perform in-place operations on the input data.",
            "description": "Attributes\n----------"
          },
          "root_": {
            "type": "_CFNode",
            "description": ""
          },
          "Root": {
            "type": "of the CFTree.",
            "description": ""
          },
          "dummy_leaf_": {
            "type": "_CFNode",
            "description": ""
          },
          "Start": {
            "type": "pointer to all the leaves.",
            "description": ""
          },
          "subcluster_centers_": {
            "type": "ndarray",
            "description": ""
          },
          "Centroids": {
            "type": "of all subclusters read directly from the leaves.",
            "description": ""
          },
          "subcluster_labels_": {
            "type": "ndarray",
            "description": ""
          },
          "Labels": {
            "type": "assigned to the centroids of the subclusters after",
            "description": ""
          },
          "they": {
            "type": "are clustered globally.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Array": {
            "type": "of labels assigned to the input data.",
            "description": ""
          },
          "if": {
            "type": "partial_fit is used instead of fit, they are assigned to the",
            "description": ""
          },
          "last": {
            "type": "batch of data.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a",
            "description": ""
          },
          "MiniBatchKMeans": {
            "type": "Alternative implementation that does incremental updates",
            "description": ""
          },
          "of": {
            "type": "the centers' positions using mini-batches.",
            "description": "Notes\n-----"
          },
          "a": {
            "type": "number of subclusters. The maximum number of subclusters in a node",
            "description": ""
          },
          "linear": {
            "type": "sum, squared sum and the number of samples in that subcluster.",
            "description": ""
          },
          "In": {
            "type": "addition, each subcluster can also have a node as its child, if the",
            "description": ""
          },
          "For": {
            "type": "a new point entering the root, it is merged with the subcluster closest",
            "description": ""
          },
          "to": {
            "type": "it and the linear sum, squared sum and the number of samples of that",
            "description": ""
          },
          "comparison": {
            "type": "with :class:`~sklearn.cluster.MiniBatchKMeans`.",
            "description": "References\n----------\n* Tian Zhang, Raghu Ramakrishnan, Maron Livny"
          },
          "BIRCH": {
            "type": "An efficient data clustering method for large databases.",
            "description": ""
          },
          "https": {
            "type": "//code.google.com/archive/p/jbirch",
            "description": "Examples\n--------\n>>> from sklearn.cluster import Birch\n>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n>>> brc = Birch(n_clusters=None)\n>>> brc.fit(X)"
          },
          "JBirch": {
            "type": "- Java implementation of BIRCH clustering algorithm",
            "description": ""
          },
          "Birch": {
            "type": "n_clusters=None",
            "description": ">>> brc.predict(X)"
          },
          "array": {
            "type": "[0, 0, 0, 1, 1, 1]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MiniBatchKMeans : Alternative implementation that does incremental updates\n        of the centers' positions using mini-batches.\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.MiniBatchKMeans`.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])",
        "notes": "-----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.MiniBatchKMeans`.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])",
        "examples": "--------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Build a CF Tree for the input data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------\nself"
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **kwargs)",
          "documentation": {
            "description": "Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**kwargs : dict"
              },
              "Arguments": {
                "type": "to be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,), dtype=np.int64",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              },
              "to": {
                "type": "be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X=None, y=None)",
          "documentation": {
            "description": "Online learning. Prevents rebuilding of CFTree from scratch.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n            Input data. If X is not provided, only the global clustering\n            step is done.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features),             default=None"
              },
              "Input": {
                "type": "data. If X is not provided, only the global clustering",
                "description": ""
              },
              "step": {
                "type": "is done.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------\nself"
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict data using the ``centroids_`` of subclusters.\n\n        Avoid computation of the row norms of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape(n_samples,)",
                "description": ""
              },
              "Labelled": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape(n_samples,)\n            Labelled data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform X into subcluster centroids dimension.\n\n        Each dimension represents the distance from the sample point to each\n        cluster centroid.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "X_trans": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_clusters)"
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BisectingKMeans",
      "documentation": {
        "description": "Bisecting K-Means clustering.\n\n    Read more in the :ref:`User Guide <bisect_k_means>`.\n\n    .. versionadded:: 1.1\n\n    Parameters\n    ----------\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'} or callable, default='random'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    n_init : int, default=1\n        Number of time the inner k-means algorithm will be run with different\n        centroid seeds in each bisection.\n        That will result producing for each bisection best output of n_init\n        consecutive runs in terms of inertia.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization\n        in inner K-Means. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the inner k-means algorithm at each\n        bisection.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations  to declare\n        convergence. Used in inner k-means algorithm at each bisection to pick\n        best possible clusters.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        Inner K-means algorithm used in bisection.\n        The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n    bisecting_strategy : {\"biggest_inertia\", \"largest_cluster\"},            default=\"biggest_inertia\"\n        Defines how bisection should be performed:\n\n        - \"biggest_inertia\" means that BisectingKMeans will always check\n          all calculated cluster for cluster with biggest SSE\n          (Sum of squared errors) and bisect it. This approach concentrates on\n          precision, but may be costly in terms of execution time (especially for\n          larger amount of data points).\n\n        - \"largest_cluster\" - BisectingKMeans will always split cluster with\n          largest amount of points assigned to it from all clusters\n          previously calculated. That should work faster than picking by SSE\n          ('biggest_inertia') and may produce similar results in most cases.\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center,\n        weighted by the sample weights if provided.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    KMeans : Original implementation of K-Means algorithm.\n\n    Notes\n    -----\n    It might be inefficient when n_cluster is less than 3, due to unnecessary\n    calculations for that case.",
        "parameters": {
          "n_clusters": {
            "type": "int, default=8",
            "description": ""
          },
          "The": {
            "type": "`\"elkan\"` variation can be more efficient on some datasets with",
            "description": "well-defined clusters, by using the triangle inequality. However it's"
          },
          "centroids": {
            "type": "to generate.",
            "description": ""
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'} or callable, default='random'"
          },
          "Method": {
            "type": "for initialization:",
            "description": "'k-means++' : selects initial cluster centers for k-mean"
          },
          "clustering": {
            "type": "in a smart way to speed up convergence. See section",
            "description": ""
          },
          "Notes": {
            "type": "in k_init for more details.",
            "description": "'random': choose `n_clusters` observations (rows) at random from data"
          },
          "for": {
            "type": "the initial centroids.",
            "description": ""
          },
          "If": {
            "type": "a callable is passed, it should take arguments X, n_clusters and a",
            "description": ""
          },
          "random": {
            "type": "state and return an initialization.",
            "description": ""
          },
          "n_init": {
            "type": "int, default=1",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ""
          },
          "centroid": {
            "type": "seeds in each bisection.",
            "description": ""
          },
          "That": {
            "type": "will result producing for each bisection best output of n_init",
            "description": ""
          },
          "consecutive": {
            "type": "runs in terms of inertia.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for centroid initialization",
            "description": ""
          },
          "in": {
            "type": "the cluster centers of two consecutive iterations  to declare",
            "description": "convergence. Used in inner k-means algorithm at each bisection to pick"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations of the inner k-means algorithm at each",
            "description": "bisection."
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Verbosity": {
            "type": "mode.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Relative": {
            "type": "tolerance with regards to Frobenius norm of the difference",
            "description": ""
          },
          "best": {
            "type": "possible clusters.",
            "description": ""
          },
          "copy_x": {
            "type": "is False. If the original data is sparse, but not in CSR format,",
            "description": ""
          },
          "When": {
            "type": "pre-computing distances it is more numerically accurate to center",
            "description": ""
          },
          "the": {
            "type": "original data is not C-contiguous, a copy will be made even if",
            "description": ""
          },
          "not": {
            "type": "modified. If False, the original data is modified, and put back",
            "description": ""
          },
          "before": {
            "type": "the function returns, but small numerical differences may be",
            "description": ""
          },
          "introduced": {
            "type": "by subtracting and then adding the data mean. Note that if",
            "description": ""
          },
          "a": {
            "type": "copy will be made even if copy_x is False.",
            "description": ""
          },
          "algorithm": {
            "type": "{\"lloyd\", \"elkan\"}, default=\"lloyd\"",
            "description": ""
          },
          "Inner": {
            "type": "K-means algorithm used in bisection.",
            "description": ""
          },
          "more": {
            "type": "memory intensive due to the allocation of an extra array of shape",
            "description": "`(n_samples, n_clusters)`."
          },
          "bisecting_strategy": {
            "type": "{\"biggest_inertia\", \"largest_cluster\"},            default=\"biggest_inertia\"",
            "description": ""
          },
          "Defines": {
            "type": "how bisection should be performed:",
            "description": "- \"biggest_inertia\" means that BisectingKMeans will always check"
          },
          "all": {
            "type": "calculated cluster for cluster with biggest SSE",
            "description": "(Sum of squared errors) and bisect it. This approach concentrates on\nprecision, but may be costly in terms of execution time (especially for"
          },
          "larger": {
            "type": "amount of data points).",
            "description": "- \"largest_cluster\" - BisectingKMeans will always split cluster with"
          },
          "largest": {
            "type": "amount of points assigned to it from all clusters",
            "description": ""
          },
          "previously": {
            "type": "calculated. That should work faster than picking by SSE",
            "description": "('biggest_inertia') and may produce similar results in most cases.\nAttributes\n----------"
          },
          "cluster_centers_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Coordinates": {
            "type": "of cluster centers. If the algorithm stops before fully",
            "description": ""
          },
          "converging": {
            "type": "see ``tol`` and ``max_iter``",
            "description": ", these will not be"
          },
          "consistent": {
            "type": "with ``labels_``.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Labels": {
            "type": "of each point.",
            "description": ""
          },
          "inertia_": {
            "type": "float",
            "description": ""
          },
          "Sum": {
            "type": "of squared distances of samples to their closest cluster center,",
            "description": ""
          },
          "weighted": {
            "type": "by the sample weights if provided.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ""
          },
          "KMeans": {
            "type": "Original implementation of K",
            "description": "Means algorithm.\nNotes\n-----"
          },
          "It": {
            "type": "might be inefficient when n_cluster is less than 3, due to unnecessary",
            "description": ""
          },
          "calculations": {
            "type": "for that case.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import BisectingKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 1], [10, 1], [3, 1],\n...               [10, 0], [2, 1], [10, 2],\n...               [10, 8], [10, 9], [10, 10]])\n>>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)\n>>> bisect_means.labels_"
          },
          "array": {
            "type": "[0, 2], dtype=int32",
            "description": ">>> bisect_means.cluster_centers_\narray([[ 2., 1.],\n[10., 9.],\n[10., 1.]])"
          },
          "For": {
            "type": "a comparison between BisectingKMeans and K-Means refer to example",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KMeans : Original implementation of K-Means algorithm.\n\n    Notes\n    -----\n    It might be inefficient when n_cluster is less than 3, due to unnecessary\n    calculations for that case.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import BisectingKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [10, 1], [3, 1],\n    ...               [10, 0], [2, 1], [10, 2],\n    ...               [10, 8], [10, 9], [10, 10]])\n    >>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)\n    >>> bisect_means.labels_\n    array([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)\n    >>> bisect_means.predict([[0, 0], [12, 3]])\n    array([0, 2], dtype=int32)\n    >>> bisect_means.cluster_centers_\n    array([[ 2., 1.],\n           [10., 9.],\n           [10., 1.]])\n\n    For a comparison between BisectingKMeans and K-Means refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`.",
        "notes": "in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    n_init : int, default=1\n        Number of time the inner k-means algorithm will be run with different\n        centroid seeds in each bisection.\n        That will result producing for each bisection best output of n_init\n        consecutive runs in terms of inertia.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization\n        in inner K-Means. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the inner k-means algorithm at each\n        bisection.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations  to declare\n        convergence. Used in inner k-means algorithm at each bisection to pick\n        best possible clusters.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        Inner K-means algorithm used in bisection.\n        The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n    bisecting_strategy : {\"biggest_inertia\", \"largest_cluster\"},            default=\"biggest_inertia\"\n        Defines how bisection should be performed:\n\n        - \"biggest_inertia\" means that BisectingKMeans will always check\n          all calculated cluster for cluster with biggest SSE\n          (Sum of squared errors) and bisect it. This approach concentrates on\n          precision, but may be costly in terms of execution time (especially for\n          larger amount of data points).\n\n        - \"largest_cluster\" - BisectingKMeans will always split cluster with\n          largest amount of points assigned to it from all clusters\n          previously calculated. That should work faster than picking by SSE\n          ('biggest_inertia') and may produce similar results in most cases.\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center,\n        weighted by the sample weights if provided.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    KMeans : Original implementation of K-Means algorithm.",
        "examples": "--------\n    >>> from sklearn.cluster import BisectingKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [10, 1], [3, 1],\n    ...               [10, 0], [2, 1], [10, 2],\n    ...               [10, 8], [10, 9], [10, 10]])\n    >>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)\n    >>> bisect_means.labels_\n    array([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)\n    >>> bisect_means.predict([[0, 0], [12, 3]])\n    array([0, 2], dtype=int32)\n    >>> bisect_means.cluster_centers_\n    array([[ 2., 1.],\n           [10., 9.],\n           [10., 1.]])\n\n    For a comparison between BisectingKMeans and K-Means refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute bisecting k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n\n            Training instances to cluster.\n\n            .. note:: The data will be converted to C ordering,\n                which will cause a memory copy\n                if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "instances to cluster.",
                "description": ".. note:: The data will be converted to C ordering,"
              },
              "which": {
                "type": "will cause a memory copy",
                "description": ""
              },
              "if": {
                "type": "the given data is not C-contiguous.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight. `sample_weight` is not used during",
                "description": ""
              },
              "initialization": {
                "type": "if `init` is a callable.",
                "description": "Returns\n-------\nself"
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict which cluster each sample in X belongs to.\n\n        Prediction is made by going down the hierarchical tree\n        in searching of closest leaf cluster.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to predict.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Opposite": {
                "type": "of the value of X on the K-means objective.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Opposite of the value of X on the K-means objective.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.cluster._bisect_k_means.BisectingKMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._bisect_k_means.BisectingKMeans",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.cluster._bisect_k_means.BisectingKMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._bisect_k_means.BisectingKMeans",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DBSCAN",
      "documentation": {
        "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    This implementation has a worst case memory complexity of :math:`O({n}^2)`,\n    which can occur when the `eps` param is large and `min_samples` is low,\n    while the original DBSCAN only uses linear memory.\n    For further details, see the Notes below.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point to\n        be considered as a core point. This includes the point itself. If\n        `min_samples` is set to a higher value, DBSCAN will find denser clusters,\n        whereas if it is set to a lower value, the found clusters will be more\n        sparse.\n\n    metric : str, or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=None\n        The power of the Minkowski metric to be used to calculate distance\n        between points. If None, then ``p=2`` (equivalent to the Euclidean\n        distance).\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    components_ : ndarray of shape (n_core_samples, n_features)\n        Copy of each core sample found by training.\n\n    labels_ : ndarray of shape (n_samples)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    OPTICS : A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.",
        "parameters": {
          "eps": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "number of parallel jobs to run.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "as": {
            "type": "in the neighborhood of the other. This is not a maximum bound",
            "description": ""
          },
          "on": {
            "type": "the ``algorithm``.",
            "description": ""
          },
          "important": {
            "type": "DBSCAN parameter to choose appropriately for your data set",
            "description": ""
          },
          "and": {
            "type": "Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996",
            "description": "Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n:doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n<10.1145/3068335>`"
          },
          "min_samples": {
            "type": "int, default=5",
            "description": ""
          },
          "be": {
            "type": "considered as a core point. This includes the point itself. If",
            "description": "`min_samples` is set to a higher value, DBSCAN will find denser clusters,"
          },
          "whereas": {
            "type": "if it is set to a lower value, the found clusters will be more",
            "description": "sparse."
          },
          "metric": {
            "type": "*precomputed* to accept precomputed sparse matrix.",
            "description": ""
          },
          "feature": {
            "type": "array. If metric is a string or callable, it must be one of",
            "description": ""
          },
          "the": {
            "type": "memory complexity to O(n.d) where d is the average number of neighbors,",
            "description": ""
          },
          "its": {
            "type": "metric parameter.",
            "description": ""
          },
          "If": {
            "type": "metric is \"precomputed\", X is assumed to be a distance matrix and",
            "description": ""
          },
          "must": {
            "type": "be square. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors for DBSCAN.",
            "description": ".. versionadded:: 0.17"
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ".. versionadded:: 0.19"
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "to": {
            "type": "store the tree. The optimal value depends",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or cKDTree. This can affect the speed",
            "description": ""
          },
          "of": {
            "type": "the construction and query, as well as the memory required",
            "description": ""
          },
          "p": {
            "type": "float, default=None",
            "description": ""
          },
          "between": {
            "type": "points. If None, then ``p=2`` (equivalent to the Euclidean",
            "description": "distance)."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": "Attributes\n----------"
          },
          "core_sample_indices_": {
            "type": "ndarray of shape (n_core_samples,)",
            "description": ""
          },
          "Indices": {
            "type": "of core samples.",
            "description": ""
          },
          "components_": {
            "type": "ndarray of shape (n_core_samples, n_features)",
            "description": ""
          },
          "Copy": {
            "type": "of each core sample found by training.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point in the dataset given to fit().",
            "description": ""
          },
          "Noisy": {
            "type": "samples are given the label -1.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "OPTICS": {
            "type": "A similar clustering at multiple values of eps. Our implementation",
            "description": ""
          },
          "is": {
            "type": "optimized for memory usage.",
            "description": "Notes\n-----"
          },
          "For": {
            "type": "an example, see",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`."
          },
          "This": {
            "type": "implementation bulk-computes all neighborhood queries, which increases",
            "description": ""
          },
          "while": {
            "type": "original DBSCAN had memory complexity O(n). It may attract a higher",
            "description": ""
          },
          "memory": {
            "type": "complexity when querying these nearest neighborhoods, depending",
            "description": ""
          },
          "One": {
            "type": "way to avoid the query complexity is to pre-compute sparse",
            "description": ""
          },
          "neighborhoods": {
            "type": "in chunks using",
            "description": ":func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here."
          },
          "Another": {
            "type": "way to reduce memory and computation time is to remove",
            "description": "(near-)duplicate points and use ``sample_weight`` instead.\n:class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\nusage.\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based"
          },
          "Algorithm": {
            "type": "for Discovering Clusters in Large Spatial Databases with Noise\"",
            "description": "<https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_."
          },
          "In": {
            "type": "Proceedings of the 2nd International Conference on Knowledge Discovery",
            "description": ""
          },
          "ACM": {
            "type": "Transactions on Database Systems (TODS), 42(3), 19.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n>>> clustering.labels_"
          },
          "array": {
            "type": "[ 0,  0,  0,  1,  1, -1]",
            "description": ">>> clustering"
          },
          "DBSCAN": {
            "type": "eps=3, min_samples=2",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    OPTICS : A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)",
        "notes": "-----\n    For an example, see\n    :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)",
        "examples": "--------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or distances between instances if",
                "description": "``metric='precomputed'``. If a sparse matrix is provided, it will"
              },
              "be": {
                "type": "converted into a sparse ``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Weight": {
                "type": "of each sample, such that a sample with a weight of at least",
                "description": "``min_samples`` is by itself a core sample; a sample with a"
              },
              "negative": {
                "type": "weight may inhibit its eps-neighbor from being core.",
                "description": ""
              },
              "Note": {
                "type": "that weights are absolute, and default to 1.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "a fitted instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "that weights are absolute, and default to 1.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of self.",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute clusters from a data or distance matrix and predict labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, or distances between instances if",
                "description": "``metric='precomputed'``. If a sparse matrix is provided, it will"
              },
              "be": {
                "type": "converted into a sparse ``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Weight": {
                "type": "of each sample, such that a sample with a weight of at least",
                "description": "``min_samples`` is by itself a core sample; a sample with a"
              },
              "negative": {
                "type": "weight may inhibit its eps-neighbor from being core.",
                "description": ""
              },
              "Note": {
                "type": "that weights are absolute, and default to 1.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels. Noisy samples are given the label -1.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.",
            "raises": "",
            "see_also": "",
            "notes": "that weights are absolute, and default to 1.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.cluster._dbscan.DBSCAN, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._dbscan.DBSCAN",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FeatureAgglomeration",
      "documentation": {
        "description": "Agglomerate features.\n\n    Recursively merges pair of clusters of features.\n\n    Refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`\n    for an example comparison of :class:`FeatureAgglomeration` strategy with a\n    univariate feature selection strategy (based on ANOVA).\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    metric : str or callable, default=\"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n        \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n        as input for the fit method.\n\n        .. versionadded:: 1.2\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like, sparse matrix, or callable, default=None\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        `kneighbors_graph`. Default is `None`, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at `n_clusters`. This is useful\n        to decrease computation time if the number of clusters is not small\n        compared to the number of features. This option is useful only when\n        specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - \"ward\" minimizes the variance of the clusters being merged.\n        - \"complete\" or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - \"average\" uses the average of the distances of each feature of\n          the two sets.\n        - \"single\" uses the minimum of the distances between all features\n          of the two sets.\n\n    pooling_func : callable, default=np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, default=None\n        The linkage distance threshold at or above which clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like of (n_features,)\n        Cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    children_ : array-like of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`.\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    See Also\n    --------\n    AgglomerativeClustering : Agglomerative clustering samples instead of\n        features.\n    ward_tree : Hierarchical clustering with ward linkage.",
        "parameters": {
          "n_clusters": {
            "type": "int or None, default=2",
            "description": ""
          },
          "The": {
            "type": "children of each non-leaf node. Values less than `n_features`",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default=\"euclidean\"",
            "description": ""
          },
          "Metric": {
            "type": "used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",",
            "description": "\"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n\"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed"
          },
          "as": {
            "type": "input for the fit method.",
            "description": ".. versionadded:: 1.2"
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": ""
          },
          "Used": {
            "type": "to cache the output of the computation of the tree.",
            "description": ""
          },
          "By": {
            "type": "default, no caching is done. If a string is given, it is the",
            "description": ""
          },
          "path": {
            "type": "to the caching directory.",
            "description": ""
          },
          "connectivity": {
            "type": "array",
            "description": "like, sparse matrix, or callable, default=None"
          },
          "Connectivity": {
            "type": "matrix. Defines for each feature the neighboring",
            "description": ""
          },
          "features": {
            "type": "following a given structure of the data.",
            "description": ""
          },
          "This": {
            "type": "combines the values of agglomerated features into a single",
            "description": "value, and should accept an array of shape [M, N] and the keyword"
          },
          "the": {
            "type": "two sets.",
            "description": "- \"single\" uses the minimum of the distances between all features"
          },
          "hierarchical": {
            "type": "clustering algorithm is unstructured.",
            "description": ""
          },
          "compute_full_tree": {
            "type": "'auto' or bool, default='auto'",
            "description": ""
          },
          "Stop": {
            "type": "early the construction of the tree at `n_clusters`. This is useful",
            "description": ""
          },
          "to": {
            "type": "`True` when `distance_threshold` is not `None` or that `n_clusters`",
            "description": ""
          },
          "compared": {
            "type": "to the number of features. This option is useful only when",
            "description": ""
          },
          "specifying": {
            "type": "a connectivity matrix. Note also that when varying the",
            "description": ""
          },
          "number": {
            "type": "of clusters and using caching, it may be advantageous to compute",
            "description": ""
          },
          "is": {
            "type": "set to `True`.",
            "description": ""
          },
          "linkage": {
            "type": "{\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"",
            "description": ""
          },
          "Which": {
            "type": "linkage criterion to use. The linkage criterion determines which",
            "description": ""
          },
          "distance": {
            "type": "to use between sets of features. The algorithm will merge",
            "description": ""
          },
          "all": {
            "type": "features of the two sets.",
            "description": "- \"average\" uses the average of the distances of each feature of"
          },
          "of": {
            "type": "the two sets.",
            "description": ""
          },
          "pooling_func": {
            "type": "callable, default=np.mean",
            "description": ""
          },
          "argument": {
            "type": "`axis=1`, and reduce it to an array of size [M].",
            "description": ""
          },
          "distance_threshold": {
            "type": "float, default=None",
            "description": ""
          },
          "compute_distances": {
            "type": "bool, default=False",
            "description": ""
          },
          "Computes": {
            "type": "distances between clusters even if `distance_threshold` is not",
            "description": "used. This can be used to make dendrogram visualization, but introduces"
          },
          "a": {
            "type": "computational and memory overhead.",
            "description": ".. versionadded:: 0.24\nAttributes\n----------"
          },
          "n_clusters_": {
            "type": "int",
            "description": ""
          },
          "labels_": {
            "type": "array",
            "description": "like of (n_features,)"
          },
          "Cluster": {
            "type": "labels for each feature.",
            "description": ""
          },
          "n_leaves_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_connected_components_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "children_": {
            "type": "array",
            "description": "like of shape (n_nodes-1, 2)"
          },
          "correspond": {
            "type": "to leaves of the tree which are the original samples.",
            "description": ""
          },
          "A": {
            "type": "node `i` greater than or equal to `n_features` is a non-leaf",
            "description": ""
          },
          "node": {
            "type": "and has children `children_[i - n_features]`. Alternatively",
            "description": ""
          },
          "at": {
            "type": "the i-th iteration, children[i][0] and children[i][1]",
            "description": ""
          },
          "are": {
            "type": "merged to form node `n_features + i`.",
            "description": ""
          },
          "distances_": {
            "type": "array",
            "description": "like of shape (n_nodes-1,)"
          },
          "Distances": {
            "type": "between nodes in the corresponding place in `children_`.",
            "description": ""
          },
          "Only": {
            "type": "computed if `distance_threshold` is used or `compute_distances`",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "AgglomerativeClustering": {
            "type": "Agglomerative clustering samples instead of",
            "description": "features."
          },
          "ward_tree": {
            "type": "Hierarchical clustering with ward linkage.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets, cluster\n>>> digits = datasets.load_digits()\n>>> images = digits.images\n>>> X = np.reshape(images, (len(images), -1))\n>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n>>> agglo.fit(X)"
          },
          "FeatureAgglomeration": {
            "type": "n_clusters=32",
            "description": ">>> X_reduced = agglo.transform(X)\n>>> X_reduced.shape\n(1797, 32)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AgglomerativeClustering : Agglomerative clustering samples instead of\n        features.\n    ward_tree : Hierarchical clustering with ward linkage.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the hierarchical clustering on the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the transformer.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X=None, *, Xt=None)",
          "documentation": {
            "description": "Inverse the transformation and return a vector of size `n_features`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n            The values to be assigned to each cluster of samples.\n\n            .. deprecated:: 1.5\n                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features) or (n_features,)",
                "description": ""
              },
              "The": {
                "type": "values to be assigned to each cluster of samples.",
                "description": ".. deprecated:: 1.5\n`Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.\nReturns\n-------"
              },
              "Xt": {
                "type": "array",
                "description": "like of shape (n_samples, n_clusters) or (n_clusters,)"
              },
              "A": {
                "type": "vector of size `n_samples` with the values of `Xred` assigned to",
                "description": ""
              },
              "each": {
                "type": "of the cluster of samples.",
                "description": ""
              }
            },
            "returns": "-------\n        X : ndarray of shape (n_samples, n_features) or (n_features,)\n            A vector of size `n_samples` with the values of `Xred` assigned to\n            each of the cluster of samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform a new matrix using the built clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n            A M by N array of M observations in N dimensions or a length\n            M array of M one-dimensional observations.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features) or                 (n_samples, n_samples)"
              },
              "A": {
                "type": "M by N array of M observations in N dimensions or a length",
                "description": ""
              },
              "M": {
                "type": "array of M one-dimensional observations.",
                "description": "Returns\n-------"
              },
              "Y": {
                "type": "ndarray of shape (n_samples, n_clusters) or (n_clusters,)",
                "description": ""
              },
              "The": {
                "type": "pooled values for each feature cluster.",
                "description": ""
              }
            },
            "returns": "-------\n        Y : ndarray of shape (n_samples, n_clusters) or (n_clusters,)\n            The pooled values for each feature cluster.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HDBSCAN",
      "documentation": {
        "description": "Cluster data using hierarchical density-based clustering.\n\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon\n    values and integrates the result to find a clustering that gives the best\n    stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike\n    :class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection.\n    Read more in the :ref:`User Guide <hdbscan>`.\n\n    For an example of how to use HDBSCAN, as well as a comparison to\n    :class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo\n    <sphx_glr_auto_examples_cluster_plot_hdbscan.py>`.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    min_cluster_size : int, default=5\n        The minimum number of samples in a group for that group to be\n        considered a cluster; groupings smaller than this size will be left\n        as noise.\n\n    min_samples : int, default=None\n        The parameter `k` used to calculate the distance between a point\n        `x_p` and its k-th nearest neighbor.\n        When `None`, defaults to `min_cluster_size`.\n\n    cluster_selection_epsilon : float, default=0.0\n        A distance threshold. Clusters below this value will be merged.\n        See [5]_ for more information.\n\n    max_cluster_size : int, default=None\n        A limit to the size of clusters returned by the `\"eom\"` cluster\n        selection algorithm. There is no limit when `max_cluster_size=None`.\n        Has no effect if `cluster_selection_method=\"leaf\"`.\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array.\n\n        - If metric is a string or callable, it must be one of\n          the options allowed by :func:`~sklearn.metrics.pairwise_distances`\n          for its metric parameter.\n\n        - If metric is \"precomputed\", X is assumed to be a distance matrix and\n          must be square.\n\n    metric_params : dict, default=None\n        Arguments passed to the distance metric.\n\n    alpha : float, default=1.0\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n\n    algorithm : {\"auto\", \"brute\", \"kd_tree\", \"ball_tree\"}, default=\"auto\"\n        Exactly which algorithm to use for computing core distances; By default\n        this is set to `\"auto\"` which attempts to use a\n        :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses\n        a :class:`~sklearn.neighbors.BallTree` tree. Both `\"kd_tree\"` and\n        `\"ball_tree\"` algorithms use the\n        :class:`~sklearn.neighbors.NearestNeighbors` estimator.\n\n        If the `X` passed during `fit` is sparse or `metric` is invalid for\n        both :class:`~sklearn.neighbors.KDTree` and\n        :class:`~sklearn.neighbors.BallTree`, then it resolves to use the\n        `\"brute\"` algorithm.\n\n    leaf_size : int, default=40\n        Leaf size for trees responsible for fast nearest neighbour queries when\n        a KDTree or a BallTree are used as core-distance algorithms. A large\n        dataset size and small `leaf_size` may induce excessive memory usage.\n        If you are running out of memory consider increasing the `leaf_size`\n        parameter. Ignored for `algorithm=\"brute\"`.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel to calculate distances.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    cluster_selection_method : {\"eom\", \"leaf\"}, default=\"eom\"\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass (`\"eom\"`)\n        algorithm to find the most persistent clusters. Alternatively you can\n        instead select the clusters at the leaves of the tree -- this provides\n        the most fine grained and homogeneous clusters.\n\n    allow_single_cluster : bool, default=False\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n\n    store_centers : str, default=None\n        Which, if any, cluster centers to compute and store. The options are:\n\n        - `None` which does not compute nor store any centers.\n        - `\"centroid\"` which calculates the center by taking the weighted\n          average of their positions. Note that the algorithm uses the\n          euclidean metric and does not guarantee that the output will be\n          an observed data point.\n        - `\"medoid\"` which calculates the center by taking the point in the\n          fitted data which minimizes the distance to all other points in\n          the cluster. This is slower than \"centroid\" since it requires\n          computing additional pairwise distances between points of the\n          same cluster but guarantees the output is an observed data point.\n          The medoid is also well-defined for arbitrary metrics, and does not\n          depend on a euclidean metric.\n        - `\"both\"` which computes and stores both forms of centers.\n\n    copy : bool, default=False\n        If `copy=True` then any time an in-place modifications would be made\n        that would overwrite data passed to :term:`fit`, a copy will first be\n        made, guaranteeing that the original data will be unchanged.\n        Currently, it only applies when `metric=\"precomputed\"`, when passing\n        a dense array or a CSR sparse matrix and when `algorithm=\"brute\"`.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to :term:`fit`.\n        Outliers are labeled as follows:\n\n        - Noisy samples are given the label -1.\n        - Samples with infinite elements (+/- np.inf) are given the label -2.\n        - Samples with missing data are given the label -3, even if they\n          also have infinite elements.\n\n    probabilities_ : ndarray of shape (n_samples,)\n        The strength with which each sample is a member of its assigned\n        cluster.\n\n        - Clustered samples have probabilities proportional to the degree that\n          they persist as part of the cluster.\n        - Noisy samples have probability zero.\n        - Samples with infinite elements (+/- np.inf) have probability 0.\n        - Samples with missing data have probability `np.nan`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    centroids_ : ndarray of shape (n_clusters, n_features)\n        A collection containing the centroid of each cluster calculated under\n        the standard euclidean metric. The centroids may fall \"outside\" their\n        respective clusters if the clusters themselves are non-convex.\n\n        Note that `n_clusters` only counts non-outlier clusters. That is to\n        say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\n    medoids_ : ndarray of shape (n_clusters, n_features)\n        A collection containing the medoid of each cluster calculated under\n        the whichever metric was passed to the `metric` parameter. The\n        medoids are points in the original cluster which minimize the average\n        distance to all other points in that cluster under the chosen metric.\n        These can be thought of as the result of projecting the `metric`-based\n        centroid back onto the cluster.\n\n        Note that `n_clusters` only counts non-outlier clusters. That is to\n        say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\n    See Also\n    --------\n    DBSCAN : Density-Based Spatial Clustering of Applications\n        with Noise.\n    OPTICS : Ordering Points To Identify the Clustering Structure.\n    Birch : Memory-efficient, online-learning algorithm.\n\n    Notes\n    -----\n    The `min_samples` parameter includes the point itself, whereas the implementation in\n    `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_\n    does not. To get the same results in both versions, the value of `min_samples` here\n    must be 1 greater than the value used in `scikit-learn-contrib/hdbscan\n    <https://github.com/scikit-learn-contrib/hdbscan>`_.\n\n    References\n    ----------\n\n    .. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering\n      based on hierarchical density estimates.\n      <10.1007/978-3-642-37456-2_14>`\n    .. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection.<10.1145/2733381>`\n\n    .. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the\n       cluster tree.\n       <https://papers.nips.cc/paper/2010/hash/\n       b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_\n\n    .. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J. Density-Based Clustering Validation.\n       <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_\n\n    .. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical\n       Density-based Cluster Selection.\"<1911.02282>`.",
        "parameters": {
          "min_cluster_size": {
            "type": "int, default=5",
            "description": ""
          },
          "The": {
            "type": "`min_samples` parameter includes the point itself, whereas the implementation in",
            "description": "`scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_"
          },
          "considered": {
            "type": "a cluster; groupings smaller than this size will be left",
            "description": ""
          },
          "as": {
            "type": "noise.",
            "description": ""
          },
          "min_samples": {
            "type": "int, default=None",
            "description": ""
          },
          "When": {
            "type": "`None`, defaults to `min_cluster_size`.",
            "description": ""
          },
          "cluster_selection_epsilon": {
            "type": "float, default=0.0",
            "description": ""
          },
          "A": {
            "type": "collection containing the medoid of each cluster calculated under",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "max_cluster_size": {
            "type": "int, default=None",
            "description": ""
          },
          "selection": {
            "type": "algorithm. There is no limit when `max_cluster_size=None`.",
            "description": ""
          },
          "Has": {
            "type": "no effect if `cluster_selection_method=\"leaf\"`.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='euclidean'",
            "description": ""
          },
          "feature": {
            "type": "array.",
            "description": "- If metric is a string or callable, it must be one of"
          },
          "the": {
            "type": "whichever metric was passed to the `metric` parameter. The",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "must": {
            "type": "be 1 greater than the value used in `scikit-learn-contrib/hdbscan",
            "description": "<https://github.com/scikit-learn-contrib/hdbscan>`_.\nReferences\n----------\n.. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering"
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Arguments": {
            "type": "passed to the distance metric.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "algorithm": {
            "type": "to find the most persistent clusters. Alternatively you can",
            "description": ""
          },
          "Exactly": {
            "type": "which algorithm to use for computing core distances; By default",
            "description": ""
          },
          "this": {
            "type": "is set to `\"auto\"` which attempts to use a",
            "description": ":class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses"
          },
          "a": {
            "type": "dense array or a CSR sparse matrix and when `algorithm=\"brute\"`.",
            "description": "Attributes\n----------"
          },
          "If": {
            "type": "`copy=True` then any time an in-place modifications would be made",
            "description": ""
          },
          "both": {
            "type": "class:`~sklearn.neighbors.KDTree` and",
            "description": ":class:`~sklearn.neighbors.BallTree`, then it resolves to use the\n`\"brute\"` algorithm."
          },
          "leaf_size": {
            "type": "int, default=40",
            "description": ""
          },
          "Leaf": {
            "type": "size for trees responsible for fast nearest neighbour queries when",
            "description": ""
          },
          "dataset": {
            "type": "size and small `leaf_size` may induce excessive memory usage.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ""
          },
          "cluster_selection_method": {
            "type": "{\"eom\", \"leaf\"}, default=\"eom\"",
            "description": ""
          },
          "standard": {
            "type": "approach for HDBSCAN* is to use an Excess of Mass (`\"eom\"`)",
            "description": ""
          },
          "instead": {
            "type": "select the clusters at the leaves of the tree -- this provides",
            "description": ""
          },
          "allow_single_cluster": {
            "type": "bool, default=False",
            "description": ""
          },
          "By": {
            "type": "default HDBSCAN* will not produce a single cluster, setting this",
            "description": ""
          },
          "to": {
            "type": "True will override this and allow single cluster results in",
            "description": ""
          },
          "store_centers": {
            "type": "str, default=None",
            "description": "Which, if any, cluster centers to compute and store. The options are:\n- `None` which does not compute nor store any centers.\n- `\"centroid\"` which calculates the center by taking the weighted"
          },
          "average": {
            "type": "of their positions. Note that the algorithm uses the",
            "description": ""
          },
          "euclidean": {
            "type": "metric and does not guarantee that the output will be",
            "description": ""
          },
          "an": {
            "type": "observed data point.",
            "description": "- `\"medoid\"` which calculates the center by taking the point in the"
          },
          "fitted": {
            "type": "data which minimizes the distance to all other points in",
            "description": ""
          },
          "computing": {
            "type": "additional pairwise distances between points of the",
            "description": ""
          },
          "same": {
            "type": "cluster but guarantees the output is an observed data point.",
            "description": ""
          },
          "depend": {
            "type": "on a euclidean metric.",
            "description": "- `\"both\"` which computes and stores both forms of centers."
          },
          "copy": {
            "type": "bool, default=False",
            "description": ""
          },
          "that": {
            "type": "would overwrite data passed to :term:`fit`, a copy will first be",
            "description": "made, guaranteeing that the original data will be unchanged.\nCurrently, it only applies when `metric=\"precomputed\"`, when passing"
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point in the dataset given to :term:`fit`.",
            "description": ""
          },
          "Outliers": {
            "type": "are labeled as follows:",
            "description": "- Noisy samples are given the label -1.\n- Samples with infinite elements (+/- np.inf) are given the label -2.\n- Samples with missing data are given the label -3, even if they"
          },
          "also": {
            "type": "have infinite elements.",
            "description": ""
          },
          "probabilities_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "they": {
            "type": "persist as part of the cluster.",
            "description": "- Noisy samples have probability zero.\n- Samples with infinite elements (+/- np.inf) have probability 0.\n- Samples with missing data have probability `np.nan`."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ""
          },
          "centroids_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "respective": {
            "type": "clusters if the clusters themselves are non-convex.",
            "description": ""
          },
          "Note": {
            "type": "that `n_clusters` only counts non-outlier clusters. That is to",
            "description": "say, the `-1, -2, -3` labels for the outlier clusters are excluded."
          },
          "medoids_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "medoids": {
            "type": "are points in the original cluster which minimize the average",
            "description": ""
          },
          "distance": {
            "type": "to all other points in that cluster under the chosen metric.",
            "description": ""
          },
          "These": {
            "type": "can be thought of as the result of projecting the `metric`-based",
            "description": ""
          },
          "centroid": {
            "type": "back onto the cluster.",
            "description": ""
          },
          "DBSCAN": {
            "type": "Density",
            "description": "Based Spatial Clustering of Applications"
          },
          "with": {
            "type": "Noise.",
            "description": ""
          },
          "OPTICS": {
            "type": "Ordering Points To Identify the Clustering Structure.",
            "description": ""
          },
          "Birch": {
            "type": "Memory",
            "description": "efficient, online-learning algorithm.\nNotes\n-----"
          },
          "does": {
            "type": "not. To get the same results in both versions, the value of `min_samples` here",
            "description": ""
          },
          "based": {
            "type": "on hierarchical density estimates.",
            "description": "<10.1007/978-3-642-37456-2_14>`\n.. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J."
          },
          "Hierarchical": {
            "type": "density estimates for data clustering, visualization,",
            "description": ""
          },
          "and": {
            "type": "outlier detection.<10.1145/2733381>`",
            "description": ".. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the"
          },
          "cluster": {
            "type": "tree.",
            "description": "<https://papers.nips.cc/paper/2010/hash/\nb534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_\n.. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\nSander, J. Density-Based Clustering Validation.\n<https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_\n.. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical\nDensity-based Cluster Selection.\"<1911.02282>`.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cluster import HDBSCAN\n>>> from sklearn.datasets import load_digits\n>>> X, _ = load_digits(return_X_y=True)\n>>> hdb = HDBSCAN(min_cluster_size=20)\n>>> hdb.fit(X)"
          },
          "HDBSCAN": {
            "type": "min_cluster_size=20",
            "description": ">>> hdb.labels_.shape == (X.shape[0],)\nTrue\n>>> np.unique(hdb.labels_).tolist()\n[-1, 0, 1, 2, 3, 4, 5, 6, 7]"
          },
          "passed": {
            "type": "to the distance metric.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DBSCAN : Density-Based Spatial Clustering of Applications\n        with Noise.\n    OPTICS : Ordering Points To Identify the Clustering Structure.\n    Birch : Memory-efficient, online-learning algorithm.\n\n    Notes\n    -----\n    The `min_samples` parameter includes the point itself, whereas the implementation in\n    `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_\n    does not. To get the same results in both versions, the value of `min_samples` here\n    must be 1 greater than the value used in `scikit-learn-contrib/hdbscan\n    <https://github.com/scikit-learn-contrib/hdbscan>`_.\n\n    References\n    ----------\n\n    .. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering\n      based on hierarchical density estimates.\n      <10.1007/978-3-642-37456-2_14>`\n    .. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection.<10.1145/2733381>`\n\n    .. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the\n       cluster tree.\n       <https://papers.nips.cc/paper/2010/hash/\n       b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_\n\n    .. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J. Density-Based Clustering Validation.\n       <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_\n\n    .. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical\n       Density-based Cluster Selection.\"<1911.02282>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import HDBSCAN\n    >>> from sklearn.datasets import load_digits\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> hdb = HDBSCAN(min_cluster_size=20)\n    >>> hdb.fit(X)\n    HDBSCAN(min_cluster_size=20)\n    >>> hdb.labels_.shape == (X.shape[0],)\n    True\n    >>> np.unique(hdb.labels_).tolist()\n    [-1, 0, 1, 2, 3, 4, 5, 6, 7]",
        "notes": "that `n_clusters` only counts non-outlier clusters. That is to\n        say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\n    medoids_ : ndarray of shape (n_clusters, n_features)\n        A collection containing the medoid of each cluster calculated under\n        the whichever metric was passed to the `metric` parameter. The\n        medoids are points in the original cluster which minimize the average\n        distance to all other points in that cluster under the chosen metric.\n        These can be thought of as the result of projecting the `metric`-based\n        centroid back onto the cluster.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.cluster import HDBSCAN\n    >>> from sklearn.datasets import load_digits\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> hdb = HDBSCAN(min_cluster_size=20)\n    >>> hdb.fit(X)\n    HDBSCAN(min_cluster_size=20)\n    >>> hdb.labels_.shape == (X.shape[0],)\n    True\n    >>> np.unique(hdb.labels_).tolist()\n    [-1, 0, 1, 2, 3, 4, 5, 6, 7]"
      },
      "methods": [
        {
          "name": "dbscan_clustering",
          "signature": "dbscan_clustering(self, cut_distance, min_cluster_size=5)",
          "documentation": {
            "description": "Return clustering given by DBSCAN without border points.",
            "parameters": {
              "cut_distance": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "mutual reachability distance cut value to use to generate a",
                "description": ""
              },
              "flat": {
                "type": "clustering.",
                "description": ""
              },
              "min_cluster_size": {
                "type": "int, default=5",
                "description": ""
              },
              "Clusters": {
                "type": "smaller than this value with be called 'noise' and remain",
                "description": ""
              },
              "unclustered": {
                "type": "in the resulting flat clustering.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "An": {
                "type": "array of cluster labels, one per datapoint.",
                "description": ""
              },
              "Outliers": {
                "type": "are labeled as follows:",
                "description": "- Noisy samples are given the label -1.\n- Samples with infinite elements (+/- np.inf) are given the label -2.\n- Samples with missing data are given the label -3, even if they"
              },
              "also": {
                "type": "have infinite elements.",
                "description": ""
              }
            },
            "returns": "clustering that would be equivalent to running DBSCAN* for a\n        particular cut_distance (or epsilon) DBSCAN* can be thought of as\n        DBSCAN without the border points.  As such these results may differ\n        slightly from `cluster.DBSCAN` due to the difference in implementation\n        over the non-core points.\n\n        This can also be thought of as a flat clustering derived from constant\n        height cut through the single linkage tree.\n\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n\n        Parameters\n        ----------\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a\n            flat clustering.\n\n        min_cluster_size : int, default=5\n            Clusters smaller than this value with be called 'noise' and remain\n            unclustered in the resulting flat clustering.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            An array of cluster labels, one per datapoint.\n            Outliers are labeled as follows:\n\n            - Noisy samples are given the label -1.\n            - Samples with infinite elements (+/- np.inf) are given the label -2.\n            - Samples with missing data are given the label -3, even if they\n              also have infinite elements.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Find clusters based on hierarchical density-based clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 ndarray of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            `metric='precomputed'`.\n\n        y : None\n            Ignored.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or                 ndarray of shape (n_samples, n_samples)"
              },
              "A": {
                "type": "feature array, or array of distances between samples if",
                "description": "`metric='precomputed'`."
              },
              "y": {
                "type": "None",
                "description": "Ignored.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "documentation": {
            "description": "Cluster X and return the associated cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or                 ndarray of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            `metric='precomputed'`.\n\n        y : None\n            Ignored.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), or                 ndarray of shape (n_samples, n_samples)"
              },
              "A": {
                "type": "feature array, or array of distances between samples if",
                "description": "`metric='precomputed'`."
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_samples,)\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KMeans",
      "documentation": {
        "description": "K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n        For an example of how to choose an optimal value for `n_clusters` refer to\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n        * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n        * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n        * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n        For an example of how to use the different `init` strategies, see\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\n        For an evaluation of the impact of initialization, see the example\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.\n\n    n_init : 'auto' or int, default='auto'\n        Number of times the k-means algorithm is run with different centroid\n        seeds. The final results is the best output of `n_init` consecutive runs\n        in terms of inertia. Several runs are recommended for sparse\n        high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center,\n        weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features.\n    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n    SoCG2006.<10.1145/1137856.1137880>` for more details.\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.",
        "parameters": {
          "n_clusters": {
            "type": "int, default=8",
            "description": ""
          },
          "The": {
            "type": "worst case complexity is given by O(n^(k+2/p)) with",
            "description": ""
          },
          "centroids": {
            "type": "to generate.",
            "description": ""
          },
          "For": {
            "type": "a comparison between K-Means and BisectingKMeans refer to example",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`."
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'"
          },
          "Method": {
            "type": "for initialization:",
            "description": "* 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n* 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n* If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n* If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization."
          },
          "n_init": {
            "type": "'auto' or int, default='auto'",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "in": {
            "type": "the cluster centers of two consecutive iterations to declare",
            "description": "convergence."
          },
          "When": {
            "type": "pre-computing distances it is more numerically accurate to center",
            "description": ""
          },
          "10": {
            "type": "if using `init='random'` or `init` is a callable;",
            "description": ""
          },
          "1": {
            "type": "if using `init='k-means++'` or `init` is an array-like.",
            "description": ".. versionadded:: 1.2"
          },
          "Added": {
            "type": "Elkan algorithm",
            "description": ".. versionchanged:: 1.1"
          },
          "Default": {
            "type": "value for `n_init` changed to `'auto'`.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations of the k-means algorithm for a",
            "description": ""
          },
          "single": {
            "type": "run.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Relative": {
            "type": "tolerance with regards to Frobenius norm of the difference",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Verbosity": {
            "type": "mode.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for centroid initialization. Use",
            "description": ""
          },
          "an": {
            "type": "int to make the randomness deterministic.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "copy_x": {
            "type": "is False. If the original data is sparse, but not in CSR format,",
            "description": ""
          },
          "the": {
            "type": "original data is not C-contiguous, a copy will be made even if",
            "description": ""
          },
          "not": {
            "type": "modified. If False, the original data is modified, and put back",
            "description": ""
          },
          "before": {
            "type": "the function returns, but small numerical differences may be",
            "description": ""
          },
          "introduced": {
            "type": "by subtracting and then adding the data mean. Note that if",
            "description": ""
          },
          "a": {
            "type": "copy will be made even if copy_x is False.",
            "description": ""
          },
          "algorithm": {
            "type": "{\"lloyd\", \"elkan\"}, default=\"lloyd\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`."
          },
          "more": {
            "type": "memory intensive due to the allocation of an extra array of shape",
            "description": "`(n_samples, n_clusters)`.\n.. versionchanged:: 0.18"
          },
          "Renamed": {
            "type": "\"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".",
            "description": ""
          },
          "Changed": {
            "type": "\"auto\" to use \"lloyd\" instead of \"elkan\".",
            "description": "Attributes\n----------"
          },
          "cluster_centers_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Coordinates": {
            "type": "of cluster centers. If the algorithm stops before fully",
            "description": ""
          },
          "converging": {
            "type": "see ``tol`` and ``max_iter``",
            "description": ", these will not be"
          },
          "consistent": {
            "type": "with ``labels_``.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Labels": {
            "type": "of each point",
            "description": ""
          },
          "inertia_": {
            "type": "float",
            "description": ""
          },
          "Sum": {
            "type": "of squared distances of samples to their closest cluster center,",
            "description": ""
          },
          "weighted": {
            "type": "by the sample weights if provided.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "MiniBatchKMeans": {
            "type": "Alternative online implementation that does incremental",
            "description": ""
          },
          "updates": {
            "type": "of the centers positions using mini-batches.",
            "description": ""
          },
          "probably": {
            "type": "much faster than the default batch implementation.",
            "description": "Notes\n-----"
          },
          "samples": {
            "type": "and T is the number of iteration.",
            "description": ""
          },
          "n": {
            "type": "= n_samples, p = n_features.",
            "description": ""
          },
          "Refer": {
            "type": "to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -",
            "description": "SoCG2006.<10.1145/1137856.1137880>` for more details."
          },
          "In": {
            "type": "practice, the k-means algorithm is very fast (one of the fastest",
            "description": ""
          },
          "clustering": {
            "type": "algorithms available), but it falls in local minima. That's why",
            "description": ""
          },
          "it": {
            "type": "can be useful to restart it several times.",
            "description": ""
          },
          "If": {
            "type": "the algorithm stops before fully converging (because of ``tol`` or",
            "description": "``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last"
          },
          "iteration": {
            "type": "to make ``labels_`` consistent with ``predict`` on the training",
            "description": "set.\nExamples\n--------\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_"
          },
          "array": {
            "type": "[1, 0], dtype=int32",
            "description": ">>> kmeans.cluster_centers_\narray([[10.,  2.],\n[ 1.,  2.]])"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features.\n    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n    SoCG2006.<10.1145/1137856.1137880>` for more details.\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n\n    For examples of common problems with K-Means and how to address them see\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\n    For a demonstration of how K-Means can be used to cluster text documents see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    For a comparison between K-Means and MiniBatchKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n\n    For a comparison between K-Means and BisectingKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`.",
        "notes": "-----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features.\n    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n    SoCG2006.<10.1145/1137856.1137880>` for more details.\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n\n    For examples of common problems with K-Means and how to address them see\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\n    For a demonstration of how K-Means can be used to cluster text documents see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    For a comparison between K-Means and MiniBatchKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n\n    For a comparison between K-Means and BisectingKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`.",
        "examples": "--------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n\n    For examples of common problems with K-Means and how to address them see\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\n    For a demonstration of how K-Means can be used to cluster text documents see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    For a comparison between K-Means and MiniBatchKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n\n    For a comparison between K-Means and BisectingKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "instances to cluster. It must be noted that the data",
                "description": ""
              },
              "will": {
                "type": "be converted to C ordering, which will cause a memory",
                "description": ""
              },
              "copy": {
                "type": "if the given data is not C-contiguous.",
                "description": ""
              },
              "If": {
                "type": "a sparse matrix is passed, a copy will be made if it's not in",
                "description": ""
              },
              "CSR": {
                "type": "format.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight. `sample_weight` is not used during",
                "description": ""
              },
              "initialization": {
                "type": "if `init` is a callable or a user provided array.",
                "description": ".. versionadded:: 0.20\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to predict.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Opposite": {
                "type": "of the value of X on the K-means objective.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Opposite of the value of X on the K-means objective.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.cluster._kmeans.KMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._kmeans.KMeans",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.cluster._kmeans.KMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._kmeans.KMeans",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MeanShift",
      "documentation": {
        "description": "Mean shift clustering using a flat kernel.\n\n    Mean shift clustering aims to discover \"blobs\" in a smooth density of\n    samples. It is a centroid-based algorithm, which works by updating\n    candidates for centroids to be the mean of the points within a given\n    region. These candidates are then filtered in a post-processing stage to\n    eliminate near-duplicates to form the final set of centroids.\n\n    Seeding is performed using a binning technique for scalability.\n\n    For an example of how to use MeanShift clustering, refer to:\n    :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n    bandwidth : float, default=None\n        Bandwidth used in the flat kernel.\n\n        If not given, the bandwidth is estimated using\n        sklearn.cluster.estimate_bandwidth; see the documentation for that\n        function for hints on scalability (see also the Notes, below).\n\n    seeds : array-like of shape (n_samples, n_features), default=None\n        Seeds used to initialize kernels. If not set,\n        the seeds are calculated by clustering.get_bin_seeds\n        with bandwidth as the grid size and default values for\n        other parameters.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        The default value is False.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. The following tasks benefit\n        from the parallelization:\n\n        - The search of nearest neighbors for bandwidth estimation and label\n          assignments. See the details in the docstring of the\n          ``NearestNeighbors`` class.\n        - Hill-climbing optimization for all seeds.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    n_iter_ : int\n        Maximum number of iterations performed on each seed.\n\n        .. versionadded:: 0.22\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : K-Means clustering.\n\n    Notes\n    -----\n\n    Scalability:\n\n    Because this implementation uses a flat kernel and\n    a Ball Tree to look up members of each kernel, the complexity will tend\n    towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n    and T the number of points. In higher dimensions the complexity will\n    tend towards O(T*n^2).\n\n    Scalability can be boosted by using fewer seeds, for example by using\n    a higher value of min_bin_freq in the get_bin_seeds function.\n\n    Note that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.",
        "parameters": {
          "bandwidth": {
            "type": "float, default=None",
            "description": ""
          },
          "Bandwidth": {
            "type": "used in the flat kernel.",
            "description": ""
          },
          "If": {
            "type": "false, then orphans are given cluster label -1.",
            "description": ""
          },
          "function": {
            "type": "for hints on scalability (see also the Notes, below).",
            "description": ""
          },
          "seeds": {
            "type": "array",
            "description": "like of shape (n_samples, n_features), default=None"
          },
          "Seeds": {
            "type": "used to initialize kernels. If not set,",
            "description": ""
          },
          "the": {
            "type": "seeds are calculated by clustering.get_bin_seeds",
            "description": ""
          },
          "with": {
            "type": "bandwidth as the grid size and default values for",
            "description": ""
          },
          "other": {
            "type": "parameters.",
            "description": ""
          },
          "bin_seeding": {
            "type": "bool, default=False",
            "description": ""
          },
          "corresponds": {
            "type": "to the bandwidth. Setting this option to True will speed",
            "description": ""
          },
          "up": {
            "type": "the algorithm because fewer seeds will be initialized.",
            "description": ""
          },
          "The": {
            "type": "number of jobs to use for the computation. The following tasks benefit",
            "description": ""
          },
          "Ignored": {
            "type": "if seeds argument is not None.",
            "description": ""
          },
          "min_bin_freq": {
            "type": "points as seeds.",
            "description": ""
          },
          "To": {
            "type": "speed up the algorithm, accept only those bins with at least",
            "description": ""
          },
          "cluster_all": {
            "type": "bool, default=True",
            "description": ""
          },
          "not": {
            "type": "within any kernel. Orphans are assigned to the nearest kernel.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "from": {
            "type": "the parallelization:",
            "description": "- The search of nearest neighbors for bandwidth estimation and label\nassignments. See the details in the docstring of the\n``NearestNeighbors`` class.\n- Hill-climbing optimization for all seeds."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations performed on each seed.",
            "description": ".. versionadded:: 0.22"
          },
          "operation": {
            "type": "terminates (for that seed point), if has not converged yet.",
            "description": ".. versionadded:: 0.22\nAttributes\n----------"
          },
          "cluster_centers_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Coordinates": {
            "type": "of cluster centers.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Labels": {
            "type": "of each point.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "KMeans": {
            "type": "K",
            "description": "Means clustering.\nNotes\n-----"
          },
          "Scalability": {
            "type": "can be boosted by using fewer seeds, for example by using",
            "description": ""
          },
          "Because": {
            "type": "this implementation uses a flat kernel and",
            "description": ""
          },
          "a": {
            "type": "higher value of min_bin_freq in the get_bin_seeds function.",
            "description": ""
          },
          "towards": {
            "type": "O(T*n*log(n)) in lower dimensions, with n the number of samples",
            "description": ""
          },
          "and": {
            "type": "T the number of points. In higher dimensions the complexity will",
            "description": ""
          },
          "tend": {
            "type": "towards O(T*n^2).",
            "description": ""
          },
          "Note": {
            "type": "that the estimate_bandwidth function is much less scalable than the",
            "description": ""
          },
          "mean": {
            "type": "shift algorithm and will be the bottleneck if it is used.",
            "description": "References\n----------"
          },
          "Dorin": {
            "type": "Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward",
            "description": ""
          },
          "feature": {
            "type": "space analysis\". IEEE Transactions on Pattern Analysis and",
            "description": ""
          },
          "Machine": {
            "type": "Intelligence. 2002. pp. 603-619.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import MeanShift\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = MeanShift(bandwidth=2).fit(X)\n>>> clustering.labels_"
          },
          "array": {
            "type": "[1, 0]",
            "description": ">>> clustering"
          },
          "MeanShift": {
            "type": "bandwidth=2",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KMeans : K-Means clustering.\n\n    Notes\n    -----\n\n    Scalability:\n\n    Because this implementation uses a flat kernel and\n    a Ball Tree to look up members of each kernel, the complexity will tend\n    towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n    and T the number of points. In higher dimensions the complexity will\n    tend towards O(T*n^2).\n\n    Scalability can be boosted by using fewer seeds, for example by using\n    a higher value of min_bin_freq in the get_bin_seeds function.\n\n    Note that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)",
        "notes": "that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)",
        "examples": "--------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Perform clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to cluster.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Samples": {
                "type": "to cluster.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n               Fitted instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **kwargs)",
          "documentation": {
            "description": "Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**kwargs : dict"
              },
              "Arguments": {
                "type": "to be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,), dtype=np.int64",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              },
              "to": {
                "type": "be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            New data to predict.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to predict.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MiniBatchKMeans",
      "documentation": {
        "description": "Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centroids using sampling based on\n        an empirical probability distribution of the points' contribution to the\n        overall inertia. This technique speeds up convergence. The algorithm\n        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n        by making several trials at each sampling step and choosing the best centroid\n        among them.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n        For an evaluation of the impact of initialization, see the example\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=1024\n        Size of the mini batches.\n        For faster computations, you can set the ``batch_size`` greater than\n        256 * number of cores to enable parallelism on all cores.\n\n        .. versionchanged:: 1.0\n           `batch_size` default changed from 100 to 1024.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, the heuristic is `init_size = 3 * batch_size` if\n        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the best of\n        the `n_init` initializations as measured by inertia. Several runs are\n        recommended for sparse high-dimensional problems (see\n        :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        3 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'` in version.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a center to\n        be reassigned. A higher value means that low count centers are more\n        easily reassigned, which means that the model will take longer to\n        converge, but should converge in a better clustering. However, too high\n        a value may cause convergence issues, especially with a small batch\n        size.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition if compute_labels is set to True. If compute_labels is set to\n        False, it's an approximation of the inertia based on an exponentially\n        weighted average of the batch inertiae.\n        The inertia is defined as the sum of square distances of samples to\n        their cluster center, weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of minibatches processed.\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.BIRCH`.",
        "parameters": {
          "n_clusters": {
            "type": "int, default=8",
            "description": ""
          },
          "The": {
            "type": "inertia is defined as the sum of square distances of samples to",
            "description": ""
          },
          "centroids": {
            "type": "to generate.",
            "description": ""
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'"
          },
          "Method": {
            "type": "for initialization:",
            "description": "'k-means++' : selects initial cluster centroids using sampling based on"
          },
          "an": {
            "type": "empirical probability distribution of the points' contribution to the",
            "description": ""
          },
          "overall": {
            "type": "inertia. This technique speeds up convergence. The algorithm",
            "description": ""
          },
          "implemented": {
            "type": "is \"greedy k-means++\". It differs from the vanilla k-means++",
            "description": ""
          },
          "by": {
            "type": "making several trials at each sampling step and choosing the best centroid",
            "description": ""
          },
          "among": {
            "type": "them.",
            "description": "'random': choose `n_clusters` observations (rows) at random from data"
          },
          "for": {
            "type": "the initial centroids.",
            "description": ""
          },
          "If": {
            "type": "`None`, the heuristic is `init_size = 3 * batch_size` if",
            "description": "`3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`."
          },
          "and": {
            "type": "gives the initial centers.",
            "description": ""
          },
          "random": {
            "type": "subset of the data. This needs to be larger than n_clusters.",
            "description": ""
          },
          "For": {
            "type": "faster computations, you can set the ``batch_size`` greater than",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations over the complete dataset before",
            "description": ""
          },
          "stopping": {
            "type": "independently of any early stopping criterion heuristics.",
            "description": ""
          },
          "batch_size": {
            "type": "int, default=1024",
            "description": ""
          },
          "Size": {
            "type": "of the mini batches.",
            "description": ""
          },
          "256": {
            "type": "* number of cores to enable parallelism on all cores.",
            "description": ".. versionchanged:: 1.0\n`batch_size` default changed from 100 to 1024."
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Verbosity": {
            "type": "mode.",
            "description": ""
          },
          "compute_labels": {
            "type": "bool, default=True",
            "description": ""
          },
          "Compute": {
            "type": "label assignment and inertia for the complete dataset",
            "description": ""
          },
          "once": {
            "type": "the minibatch optimization has converged in fit.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for centroid initialization and",
            "description": ""
          },
          "See": {
            "type": "ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a",
            "description": ""
          },
          "tol": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Control": {
            "type": "the fraction of the maximum number of counts for a center to",
            "description": ""
          },
          "measured": {
            "type": "by a smoothed, variance-normalized of the mean center",
            "description": ""
          },
          "squared": {
            "type": "position changes. This early stopping heuristics is",
            "description": ""
          },
          "closer": {
            "type": "to the one used for the batch variant of the algorithms",
            "description": ""
          },
          "but": {
            "type": "induces a slight computational and memory overhead over the",
            "description": ""
          },
          "inertia": {
            "type": "heuristic.",
            "description": ""
          },
          "To": {
            "type": "disable convergence detection based on inertia, set",
            "description": ""
          },
          "max_no_improvement": {
            "type": "to None.",
            "description": ""
          },
          "batches": {
            "type": "that does not yield an improvement on the smoothed inertia.",
            "description": ""
          },
          "init_size": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "initialization": {
            "type": "sometimes at the expense of accuracy",
            "description": "the"
          },
          "only": {
            "type": "algorithm is initialized by running a batch KMeans on a",
            "description": ""
          },
          "n_init": {
            "type": "'auto' or int, default=\"auto\"",
            "description": ""
          },
          "In": {
            "type": "contrast to KMeans, the algorithm is only run once, using the best of",
            "description": ""
          },
          "the": {
            "type": "`n_init` initializations as measured by inertia. Several runs are",
            "description": ""
          },
          "recommended": {
            "type": "for sparse high-dimensional problems (see",
            "description": ":ref:`kmeans_sparse_high_dim`)."
          },
          "When": {
            "type": "there are too few points in the dataset, some centers may be",
            "description": "duplicated, which means that a proper clustering in terms of the number"
          },
          "3": {
            "type": "if using `init='random'` or `init` is a callable;",
            "description": ""
          },
          "1": {
            "type": "if using `init='k-means++'` or `init` is an array-like.",
            "description": ".. versionadded:: 1.2"
          },
          "Added": {
            "type": "'auto' option for `n_init`.",
            "description": ".. versionchanged:: 1.4"
          },
          "Default": {
            "type": "value for `n_init` changed to `'auto'` in version.",
            "description": ""
          },
          "reassignment_ratio": {
            "type": "float, default=0.01",
            "description": ""
          },
          "be": {
            "type": "reassigned. A higher value means that low count centers are more",
            "description": ""
          },
          "easily": {
            "type": "reassigned, which means that the model will take longer to",
            "description": "converge, but should converge in a better clustering. However, too high"
          },
          "a": {
            "type": "value may cause convergence issues, especially with a small batch",
            "description": "size.\nAttributes\n----------"
          },
          "cluster_centers_": {
            "type": "ndarray of shape (n_clusters, n_features)",
            "description": ""
          },
          "Coordinates": {
            "type": "of cluster centers.",
            "description": ""
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Labels": {
            "type": "of each point (if compute_labels is set to True).",
            "description": ""
          },
          "inertia_": {
            "type": "float",
            "description": ""
          },
          "partition": {
            "type": "if compute_labels is set to True. If compute_labels is set to",
            "description": "False, it's an approximation of the inertia based on an exponentially"
          },
          "weighted": {
            "type": "average of the batch inertiae.",
            "description": ""
          },
          "their": {
            "type": "cluster center, weighted by the sample weights if provided.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_steps_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "KMeans": {
            "type": "The classic implementation of the clustering method based on the",
            "description": "Lloyd's algorithm. It consumes the whole set of input data at each\niteration.\nNotes\n-----"
          },
          "of": {
            "type": "requesting clusters and the number of returned clusters will not",
            "description": ""
          },
          "always": {
            "type": "match. One solution is to set `reassignment_ratio=0`, which",
            "description": ""
          },
          "prevents": {
            "type": "reassignments of clusters that are too small.",
            "description": ""
          },
          "comparison": {
            "type": "with :class:`~sklearn.cluster.BIRCH`.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 0], [4, 4],\n...               [4, 5], [0, 1], [2, 2],\n...               [3, 2], [5, 5], [1, -1]])\n>>> # manually fit on batches\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          n_init=\"auto\")\n>>> kmeans = kmeans.partial_fit(X[0:6,:])\n>>> kmeans = kmeans.partial_fit(X[6:12,:])\n>>> kmeans.cluster_centers_\narray([[3.375, 3.  ],\n[0.75 , 0.5 ]])\n>>> kmeans.predict([[0, 0], [4, 4]])"
          },
          "array": {
            "type": "[1, 0], dtype=int32",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.BIRCH`.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)",
        "notes": "-----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    See :ref:`sphx_glr_auto_examples_cluster_plot_birch_vs_minibatchkmeans.py` for a\n    comparison with :class:`~sklearn.cluster.BIRCH`.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)",
        "examples": "--------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "instances to cluster. It must be noted that the data",
                "description": ""
              },
              "will": {
                "type": "be converted to C ordering, which will cause a memory copy",
                "description": ""
              },
              "if": {
                "type": "the given data is not C-contiguous.",
                "description": ""
              },
              "If": {
                "type": "a sparse matrix is passed, a copy will be made if it's not in",
                "description": ""
              },
              "CSR": {
                "type": "format.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight. `sample_weight` is not used during",
                "description": ""
              },
              "initialization": {
                "type": "if `init` is a callable or a user provided array.",
                "description": ".. versionadded:: 0.20\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "instances to cluster. It must be noted that the data",
                "description": ""
              },
              "will": {
                "type": "be converted to C ordering, which will cause a memory copy",
                "description": ""
              },
              "if": {
                "type": "the given data is not C-contiguous.",
                "description": ""
              },
              "If": {
                "type": "a sparse matrix is passed, a copy will be made if it's not in",
                "description": ""
              },
              "CSR": {
                "type": "format.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight. `sample_weight` is not used during",
                "description": ""
              },
              "initialization": {
                "type": "if `init` is a callable or a user provided array.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Return": {
                "type": "updated estimator.",
                "description": ""
              }
            },
            "returns": "updated estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data to predict.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Index": {
                "type": "of the cluster each sample belongs to.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "New": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "The": {
                "type": "weights for each observation in X. If None, all observations",
                "description": ""
              },
              "are": {
                "type": "assigned equal weight.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Opposite": {
                "type": "of the value of X on the K-means objective.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Opposite of the value of X on the K-means objective.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.cluster._kmeans.MiniBatchKMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._kmeans.MiniBatchKMeans",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.cluster._kmeans.MiniBatchKMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._kmeans.MiniBatchKMeans",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.cluster._kmeans.MiniBatchKMeans, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.cluster._kmeans.MiniBatchKMeans",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.",
            "parameters": {
              "X": {
                "type": "transformed in the new space.",
                "description": ""
              },
              "New": {
                "type": "data to transform.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_clusters)",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OPTICS",
      "documentation": {
        "description": "Estimate clustering structure from vector array.\n\n    OPTICS (Ordering Points To Identify the Clustering Structure), closely\n    related to DBSCAN, finds core sample of high density and expands clusters\n    from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n    neighborhood radius. Better suited for usage on large datasets than the\n    current sklearn implementation of DBSCAN.\n\n    Clusters are then extracted using a DBSCAN-like method\n    (cluster_method = 'dbscan') or an automatic\n    technique proposed in [1]_ (cluster_method = 'xi').\n\n    This implementation deviates from the original OPTICS by first performing\n    k-nearest-neighborhood searches on all points to identify core sizes, then\n    computing only the distances to unprocessed points when constructing the\n    cluster order. Note that we do not employ a heap to manage the expansion\n    candidates, so the time complexity will be O(n^2).\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    min_samples : int > 1 or float between 0 and 1, default=5\n        The number of samples in a neighborhood for a point to be considered as\n        a core point. Also, up and down steep regions can't have more than\n        ``min_samples`` consecutive non-steep points. Expressed as an absolute\n        number or a fraction of the number of samples (rounded to be at least\n        2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", `X` is assumed to be a distance matrix and must be\n        square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        Sparse matrices are only supported by scikit-learn metrics.\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n        .. note::\n           `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The extraction method used to extract clusters using the calculated\n        reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\n    eps : float, default=None\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. By default it assumes the same value\n        as ``max_eps``.\n        Used only when ``cluster_method='dbscan'``.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n        Used only when ``cluster_method='xi'``.\n\n    predecessor_correction : bool, default=True\n        Correct clusters according to the predecessors calculated by OPTICS\n        [2]_. This parameter has minimal effect on most datasets.\n        Used only when ``cluster_method='xi'``.\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n        Used only when ``cluster_method='xi'``.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n        - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n        - 'brute' will use a brute-force search.\n        - 'auto' (default) will attempt to decide the most appropriate\n          algorithm based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n        :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n        construction and query, as well as the memory required to store the\n        tree. The optimal value depends on the nature of the problem.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples and points which are not included in a leaf cluster\n        of ``cluster_hierarchy_`` are labeled as -1.\n\n    reachability_ : ndarray of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    ordering_ : ndarray of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : ndarray of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : ndarray of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to\n        ``(end, -start)`` (ascending) so that larger clusters encompassing\n        smaller clusters come after those smaller ones. Since ``labels_`` does\n        not reflect the hierarchy, usually\n        ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n        note that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.",
        "parameters": {
          "min_samples": {
            "type": "int > 1 or float between 0 and 1, default=5",
            "description": ""
          },
          "The": {
            "type": "list of clusters in the form of ``[start, end]`` in each row, with",
            "description": ""
          },
          "a": {
            "type": "core point. Also, up and down steep regions can't have more than",
            "description": "``min_samples`` consecutive non-steep points. Expressed as an absolute"
          },
          "number": {
            "type": "or a fraction of the number of samples (rounded to be at least",
            "description": "2)."
          },
          "max_eps": {
            "type": "float, default=np.inf",
            "description": ""
          },
          "in": {
            "type": "the neighborhood of the other. By default it assumes the same value",
            "description": ""
          },
          "identify": {
            "type": "clusters across all scales; reducing ``max_eps`` will result",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Any metric from scikit-learn",
            "description": ""
          },
          "or": {
            "type": "scipy.spatial.distance can be used.",
            "description": ""
          },
          "If": {
            "type": "metric is a callable function, it is called on each",
            "description": ""
          },
          "pair": {
            "type": "of instances (rows) and the resulting value recorded. The callable",
            "description": ""
          },
          "should": {
            "type": "take two arrays as input and return one value indicating the",
            "description": ""
          },
          "distance": {
            "type": "between them. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string. If metric is",
            "description": "\"precomputed\", `X` is assumed to be a distance matrix and must be\nsquare."
          },
          "Valid": {
            "type": "values for metric are:",
            "description": "- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n'manhattan']\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n'yule']"
          },
          "Sparse": {
            "type": "matrices are only supported by scikit-learn metrics.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": ":class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is"
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "cluster_method": {
            "type": "str, default='xi'",
            "description": ""
          },
          "reachability": {
            "type": "plot is defined by the ratio from one point to its",
            "description": ""
          },
          "eps": {
            "type": "float, default=None",
            "description": ""
          },
          "as": {
            "type": "``max_eps``.",
            "description": ""
          },
          "Used": {
            "type": "to cache the output of the computation of the tree.",
            "description": ""
          },
          "xi": {
            "type": "float between 0 and 1, default=0.05",
            "description": ""
          },
          "Determines": {
            "type": "the minimum steepness on the reachability plot that",
            "description": ""
          },
          "constitutes": {
            "type": "a cluster boundary. For example, an upwards point in the",
            "description": ""
          },
          "successor": {
            "type": "being at most 1-xi.",
            "description": ""
          },
          "predecessor_correction": {
            "type": "bool, default=True",
            "description": ""
          },
          "Correct": {
            "type": "clusters according to the predecessors calculated by OPTICS",
            "description": "[2]_. This parameter has minimal effect on most datasets."
          },
          "min_cluster_size": {
            "type": "int > 1 or float between 0 and 1, default=None",
            "description": ""
          },
          "Minimum": {
            "type": "number of samples in an OPTICS cluster, expressed as an",
            "description": ""
          },
          "absolute": {
            "type": "number or a fraction of the number of samples (rounded to be",
            "description": ""
          },
          "at": {
            "type": "least 2). If ``None``, the value of ``min_samples`` is used instead.",
            "description": ""
          },
          "algorithm": {
            "type": "based on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n- 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n- 'brute' will use a brute-force search.\n- 'auto' (default) will attempt to decide the most appropriate"
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to :class:`~sklearn.neighbors.BallTree` or",
            "description": ":class:`~sklearn.neighbors.KDTree`. This can affect the speed of the"
          },
          "construction": {
            "type": "and query, as well as the memory required to store the",
            "description": "tree. The optimal value depends on the nature of the problem."
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": ""
          },
          "By": {
            "type": "default, no caching is done. If a string is given, it is the",
            "description": ""
          },
          "path": {
            "type": "to the caching directory.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": "Attributes\n----------"
          },
          "labels_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Cluster": {
            "type": "labels for each point in the dataset given to fit().",
            "description": ""
          },
          "Noisy": {
            "type": "samples and points which are not included in a leaf cluster",
            "description": ""
          },
          "of": {
            "type": "``cluster_hierarchy_`` are labeled as -1.",
            "description": ""
          },
          "reachability_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Reachability": {
            "type": "distances per sample, indexed by object order. Use",
            "description": "``clust.reachability_[clust.ordering_]`` to access in cluster order."
          },
          "ordering_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "core_distances_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Distance": {
            "type": "at which each sample becomes a core point, indexed by object",
            "description": "order. Points which will never be core have a distance of inf. Use\n``clust.core_distances_[clust.ordering_]`` to access in cluster order."
          },
          "predecessor_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Point": {
            "type": "that a sample was reached from, indexed by object order.",
            "description": ""
          },
          "Seed": {
            "type": "points have a predecessor of -1.",
            "description": ""
          },
          "cluster_hierarchy_": {
            "type": "ndarray of shape (n_clusters, 2)",
            "description": ""
          },
          "all": {
            "type": "indices inclusive. The clusters are ordered according to",
            "description": "``(end, -start)`` (ascending) so that larger clusters encompassing"
          },
          "smaller": {
            "type": "clusters come after those smaller ones. Since ``labels_`` does",
            "description": ""
          },
          "not": {
            "type": "reflect the hierarchy, usually",
            "description": "``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also"
          },
          "note": {
            "type": "that these indices are of the ``ordering_``, i.e.",
            "description": "``X[ordering_][start:end + 1]`` form a cluster."
          },
          "Only": {
            "type": "available when ``cluster_method='xi'``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "DBSCAN": {
            "type": "A similar clustering for a specified neighborhood radius (eps).",
            "description": ""
          },
          "Our": {
            "type": "implementation is optimized for runtime.",
            "description": "References\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,"
          },
          "and": {
            "type": "Jörg Sander. \"OPTICS: ordering points to identify the clustering",
            "description": "structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n.. [2] Schubert, Erich, Michael Gertz.\n\"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of"
          },
          "the": {
            "type": "Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.",
            "description": "Examples\n--------\n>>> from sklearn.cluster import OPTICS\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> clustering = OPTICS(min_samples=2).fit(X)\n>>> clustering.labels_"
          },
          "array": {
            "type": "[0, 0, 0, 1, 1, 1]",
            "description": ""
          },
          "For": {
            "type": "a more detailed example see",
            "description": ":ref:`sphx_glr_auto_examples_cluster_plot_optics.py`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n\n    For a more detailed example see\n    :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.",
        "notes": "that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n\n    For a more detailed example see\n    :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.",
        "examples": "--------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n\n    For a more detailed example see\n    :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric='precomputed'\n            A feature array, or array of distances between samples if\n            metric='precomputed'. If a sparse matrix is provided, it will be\n            converted into CSR format.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric='precomputed'",
                "description": ""
              },
              "A": {
                "type": "feature array, or array of distances between samples if",
                "description": "metric='precomputed'. If a sparse matrix is provided, it will be"
              },
              "converted": {
                "type": "into CSR format.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "a fitted instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **kwargs)",
          "documentation": {
            "description": "Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**kwargs : dict"
              },
              "Arguments": {
                "type": "to be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,), dtype=np.int64",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              },
              "to": {
                "type": "be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SpectralBiclustering",
      "documentation": {
        "description": "Spectral biclustering (Kluger, 2003).\n\n    Partitions rows and columns under the assumption that the data has\n    an underlying checkerboard structure. For instance, if there are\n    two row partitions and three column partitions, each row will\n    belong to three biclusters, and each column will belong to two\n    biclusters. The outer product of the corresponding row and column\n    label vectors gives this checkerboard structure.\n\n    Read more in the :ref:`User Guide <spectral_biclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n        The number of row and column clusters in the checkerboard\n        structure.\n\n    method : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n        Method of normalizing and converting singular vectors into\n        biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n        The authors recommend using 'log'. If the data is sparse,\n        however, log normalization will not work, which is why the\n        default is 'bistochastic'.\n\n        .. warning::\n           if `method='log'`, the data must not be sparse.\n\n    n_components : int, default=6\n        Number of singular vectors to check.\n\n    n_best : int, default=3\n        Number of best singular vectors to which to project the data\n        for clustering.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', uses\n        :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', uses\n        `scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'} or ndarray of shape (n_clusters, n_features),             default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        Row partition labels.\n\n    column_labels_ : array-like of shape (n_cols,)\n        Column partition labels.\n\n    biclusters_ : tuple of two ndarrays\n        The tuple contains the `rows_` and `columns_` arrays.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    References\n    ----------\n\n    * :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray\n      data: coclustering genes and conditions.\n      <10.1101/gr.648603>`",
        "parameters": {
          "n_clusters": {
            "type": "int or tuple (n_row_clusters, n_column_clusters), default=3",
            "description": ""
          },
          "The": {
            "type": "tuple contains the `rows_` and `columns_` arrays.",
            "description": ""
          },
          "method": {
            "type": "{'bistochastic', 'scale', 'log'}, default='bistochastic'",
            "description": ""
          },
          "Method": {
            "type": "for initialization of k-means algorithm; defaults to",
            "description": "'k-means++'."
          },
          "default": {
            "type": "is 'bistochastic'.",
            "description": ".. warning::"
          },
          "if": {
            "type": "`method='log'`, the data must not be sparse.",
            "description": ""
          },
          "n_components": {
            "type": "int, default=6",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_best": {
            "type": "int, default=3",
            "description": ""
          },
          "for": {
            "type": "large matrices. If 'arpack', uses",
            "description": "`scipy.sparse.linalg.svds`, which is more accurate, but"
          },
          "svd_method": {
            "type": "{'randomized', 'arpack'}, default='randomized'",
            "description": ""
          },
          "Selects": {
            "type": "the algorithm for finding singular vectors. May be",
            "description": "'randomized' or 'arpack'. If 'randomized', uses\n:func:`~sklearn.utils.extmath.randomized_svd`, which may be faster"
          },
          "possibly": {
            "type": "slower in some cases.",
            "description": ""
          },
          "n_svd_vecs": {
            "type": "int, default=None",
            "description": ""
          },
          "to": {
            "type": "`ncv` when `svd_method=arpack` and `n_oversamples` when",
            "description": "`svd_method` is 'randomized`."
          },
          "mini_batch": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to use mini-batch k-means, which is faster but may get",
            "description": ""
          },
          "different": {
            "type": "results.",
            "description": ""
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'} or ndarray of shape (n_clusters, n_features),             default='k-means++'"
          },
          "n_init": {
            "type": "int, default=10",
            "description": ""
          },
          "If": {
            "type": "mini-batch k-means is used, the best initialization is",
            "description": ""
          },
          "chosen": {
            "type": "and the algorithm runs once. Otherwise, the algorithm",
            "description": ""
          },
          "is": {
            "type": "run for each initialization and the best solution chosen.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "for randomizing the singular value decomposition and the k-means",
            "description": "initialization. Use an int to make the randomness deterministic."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "rows_": {
            "type": "array",
            "description": "like of shape (n_row_clusters, n_rows)"
          },
          "Results": {
            "type": "of the clustering, like `rows`.",
            "description": ""
          },
          "cluster": {
            "type": "`i` contains row `r`. Available only after calling ``fit``.",
            "description": ""
          },
          "columns_": {
            "type": "array",
            "description": "like of shape (n_column_clusters, n_columns)"
          },
          "row_labels_": {
            "type": "array",
            "description": "like of shape (n_rows,)"
          },
          "Row": {
            "type": "partition labels.",
            "description": ""
          },
          "column_labels_": {
            "type": "array",
            "description": "like of shape (n_cols,)"
          },
          "Column": {
            "type": "partition labels.",
            "description": ""
          },
          "biclusters_": {
            "type": "tuple of two ndarrays",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "SpectralCoclustering": {
            "type": "Spectral Co",
            "description": "Clustering algorithm (Dhillon, 2001).\nReferences\n----------\n* :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray"
          },
          "data": {
            "type": "coclustering genes and conditions.",
            "description": "<10.1101/gr.648603>`\nExamples\n--------\n>>> from sklearn.cluster import SpectralBiclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_"
          },
          "array": {
            "type": "[1, 0], dtype=int32",
            "description": ">>> clustering"
          },
          "SpectralBiclustering": {
            "type": "n_clusters=2, random_state=0",
            "description": ""
          },
          "For": {
            "type": "a more detailed example, see",
            "description": ":ref:`sphx_glr_auto_examples_bicluster_plot_spectral_biclustering.py`"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    References\n    ----------\n\n    * :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray\n      data: coclustering genes and conditions.\n      <10.1101/gr.648603>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([1, 0], dtype=int32)\n    >>> clustering\n    SpectralBiclustering(n_clusters=2, random_state=0)\n\n    For a more detailed example, see\n    :ref:`sphx_glr_auto_examples_bicluster_plot_spectral_biclustering.py`",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([1, 0], dtype=int32)\n    >>> clustering\n    SpectralBiclustering(n_clusters=2, random_state=0)\n\n    For a more detailed example, see\n    :ref:`sphx_glr_auto_examples_bicluster_plot_spectral_biclustering.py`"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Create a biclustering for X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "SpectralBiclustering": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            SpectralBiclustering instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_indices",
          "signature": "get_indices(self, i)",
          "documentation": {
            "description": "Row and column indices of the `i`'th bicluster.\n\n        Only works if ``rows_`` and ``columns_`` attributes exist.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "index of the cluster.",
                "description": "Returns\n-------"
              },
              "row_ind": {
                "type": "ndarray, dtype=np.intp",
                "description": ""
              },
              "Indices": {
                "type": "of columns in the dataset that belong to the bicluster.",
                "description": ""
              },
              "col_ind": {
                "type": "ndarray, dtype=np.intp",
                "description": ""
              }
            },
            "returns": "-------\n        row_ind : ndarray, dtype=np.intp\n            Indices of rows in the dataset that belong to the bicluster.\n        col_ind : ndarray, dtype=np.intp\n            Indices of columns in the dataset that belong to the bicluster.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_shape",
          "signature": "get_shape(self, i)",
          "documentation": {
            "description": "Shape of the `i`'th bicluster.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "index of the cluster.",
                "description": "Returns\n-------"
              },
              "n_rows": {
                "type": "int",
                "description": ""
              },
              "Number": {
                "type": "of columns in the bicluster.",
                "description": ""
              },
              "n_cols": {
                "type": "int",
                "description": ""
              }
            },
            "returns": "-------\n        n_rows : int\n            Number of rows in the bicluster.\n\n        n_cols : int\n            Number of columns in the bicluster.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submatrix",
          "signature": "get_submatrix(self, i, data)",
          "documentation": {
            "description": "Return the submatrix corresponding to bicluster `i`.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n        data : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "submatrix corresponding to bicluster `i`.",
                "description": "Notes\n-----"
              },
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "submatrix": {
                "type": "ndarray of shape (n_rows, n_cols)",
                "description": ""
              },
              "Works": {
                "type": "with sparse matrices. Only works if ``rows_`` and",
                "description": "``columns_`` attributes exist."
              }
            },
            "returns": "-------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SpectralClustering",
      "documentation": {
        "description": "Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex, or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster, such as when clusters are\n    nested circles on the 2D plane.\n\n    If the affinity matrix is the adjacency matrix of a graph, this method\n    can be used to find normalized graph cuts [1]_, [2]_.\n\n    When calling ``fit``, an affinity matrix is constructed using either\n    a kernel function such the Gaussian (aka RBF) kernel with Euclidean\n    distance ``d(X, X)``::\n\n            np.exp(-gamma * d(X,X) ** 2)\n\n    or a k-nearest neighbors connectivity matrix.\n\n    Alternatively, a user-provided affinity matrix can be specified by\n    setting ``affinity='precomputed'``.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=8\n        The dimension of the projection subspace.\n\n    eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used. See [4]_ for more details regarding `'lobpcg'`.\n\n    n_components : int, default=None\n        Number of eigenvectors to use for the spectral embedding. If None,\n        defaults to `n_clusters`.\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization\n        of the lobpcg eigenvectors decomposition when `eigen_solver ==\n        'amg'`, and for the K-Means initialization. Use an int to make\n        the results deterministic across calls (See\n        :term:`Glossary <random_state>`).\n\n        .. note::\n            When using `eigen_solver == 'amg'`,\n            it is necessary to also fix the global numpy seed with\n            `np.random.seed(int)` to get deterministic results. See\n            https://github.com/pyamg/pyamg/issues/139 for further\n            information.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    gamma : float, default=1.0\n        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n        Ignored for ``affinity='nearest_neighbors'``, ``affinity='precomputed'``\n        or ``affinity='precomputed_nearest_neighbors'``.\n\n    affinity : str or callable, default='rbf'\n        How to construct the affinity matrix.\n         - 'nearest_neighbors': construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf': construct the affinity matrix using a radial basis function\n           (RBF) kernel.\n         - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n           where larger values indicate greater similarity between instances.\n         - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n           of precomputed distances, and construct a binary affinity matrix\n           from the ``n_neighbors`` nearest neighbors of each instance.\n         - one of the kernels supported by\n           :func:`~sklearn.metrics.pairwise.pairwise_kernels`.\n\n        Only kernels that produce similarity scores (non-negative values that\n        increase with similarity) should be used. This property is not checked\n        by the clustering algorithm.\n\n    n_neighbors : int, default=10\n        Number of neighbors to use when constructing the affinity matrix using\n        the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\n    eigen_tol : float, default=\"auto\"\n        Stopping criterion for eigen decomposition of the Laplacian matrix.\n        If `eigen_tol=\"auto\"` then the passed tolerance will depend on the\n        `eigen_solver`:\n\n        - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n        - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n          `eigen_tol=None` which configures the underlying `lobpcg` solver to\n          automatically resolve the value according to their heuristics. See,\n          :func:`scipy.sparse.linalg.lobpcg` for details.\n\n        Note that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n        values of `tol<1e-5` may lead to convergence issues and should be\n        avoided.\n\n        .. versionadded:: 1.2\n           Added 'auto' option.\n\n    assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'\n        The strategy for assigning labels in the embedding space. There are two\n        ways to assign labels after the Laplacian embedding. k-means is a\n        popular choice, but it can be sensitive to initialization.\n        Discretization is another approach which is less sensitive to random\n        initialization [3]_.\n        The cluster_qr method [5]_ directly extract clusters from eigenvectors\n        in spectral clustering. In contrast to k-means and discretization, cluster_qr\n        has no tuning parameters and runs no iterations, yet may outperform\n        k-means and discretization in terms of both quality and speed.\n\n        .. versionchanged:: 1.1\n           Added new labeling method 'cluster_qr'.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict of str to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run when `affinity='nearest_neighbors'`\n        or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n        will be done in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    affinity_matrix_ : array-like of shape (n_samples, n_samples)\n        Affinity matrix used for clustering. Available only after calling\n        ``fit``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.cluster.KMeans : K-Means clustering.\n    sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of\n        Applications with Noise.\n\n    Notes\n    -----\n    A distance matrix for which 0 indicates identical elements and high values\n    indicate very dissimilar elements can be transformed into an affinity /\n    similarity matrix that is well-suited for the algorithm by\n    applying the Gaussian (aka RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    An alternative is to take a symmetric version of the k-nearest neighbors\n    connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`",
        "parameters": {
          "n_clusters": {
            "type": "int, default=8",
            "description": ""
          },
          "The": {
            "type": "cluster_qr method [5]_ directly extract clusters from eigenvectors",
            "description": ""
          },
          "eigen_solver": {
            "type": "{'arpack', 'lobpcg', 'amg'}, default=None",
            "description": ""
          },
          "to": {
            "type": "be installed. It can be faster on very large, sparse problems,",
            "description": ""
          },
          "but": {
            "type": "may also lead to instabilities. If None, then ``'arpack'`` is",
            "description": "used. See [4]_ for more details regarding `'lobpcg'`."
          },
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of neighbors to use when constructing the affinity matrix using",
            "description": ""
          },
          "defaults": {
            "type": "to `n_clusters`.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "A": {
            "type": "pseudo random number generator used for the initialization",
            "description": ""
          },
          "of": {
            "type": "precomputed distances, and construct a binary affinity matrix",
            "description": ""
          },
          "the": {
            "type": "nearest neighbors method. Ignored for ``affinity='rbf'``.",
            "description": ""
          },
          "When": {
            "type": "using `eigen_solver == 'amg'`,",
            "description": ""
          },
          "it": {
            "type": "is necessary to also fix the global numpy seed with",
            "description": "`np.random.seed(int)` to get deterministic results. See"
          },
          "https": {
            "type": "//github.com/pyamg/pyamg/issues/139 for further",
            "description": "information."
          },
          "n_init": {
            "type": "int, default=10",
            "description": ""
          },
          "centroid": {
            "type": "seeds. The final results will be the best output of n_init",
            "description": ""
          },
          "consecutive": {
            "type": "runs in terms of inertia. Only used if",
            "description": "``assign_labels='kmeans'``."
          },
          "gamma": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Kernel": {
            "type": "coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.",
            "description": ""
          },
          "Ignored": {
            "type": "by other kernels.",
            "description": ""
          },
          "or": {
            "type": "``affinity='precomputed_nearest_neighbors'``.",
            "description": ""
          },
          "affinity": {
            "type": "str or callable, default='rbf'",
            "description": ""
          },
          "How": {
            "type": "to construct the affinity matrix.",
            "description": "- 'nearest_neighbors': construct the affinity matrix by computing a"
          },
          "graph": {
            "type": "of nearest neighbors.",
            "description": "- 'rbf': construct the affinity matrix using a radial basis function\n(RBF) kernel.\n- 'precomputed': interpret ``X`` as a precomputed affinity matrix,"
          },
          "where": {
            "type": "larger values indicate greater similarity between instances.",
            "description": "- 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph"
          },
          "from": {
            "type": "the ``n_neighbors`` nearest neighbors of each instance.",
            "description": "- one of the kernels supported by\n:func:`~sklearn.metrics.pairwise.pairwise_kernels`."
          },
          "Only": {
            "type": "kernels that produce similarity scores (non-negative values that",
            "description": ""
          },
          "increase": {
            "type": "with similarity) should be used. This property is not checked",
            "description": ""
          },
          "by": {
            "type": "the clustering algorithm.",
            "description": ""
          },
          "n_neighbors": {
            "type": "int, default=10",
            "description": ""
          },
          "eigen_tol": {
            "type": "float, default=\"auto\"",
            "description": ""
          },
          "Stopping": {
            "type": "criterion for eigen decomposition of the Laplacian matrix.",
            "description": ""
          },
          "If": {
            "type": "`eigen_tol=\"auto\"` then the passed tolerance will depend on the",
            "description": "`eigen_solver`:\n- If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n- If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n`eigen_tol=None` which configures the underlying `lobpcg` solver to"
          },
          "automatically": {
            "type": "resolve the value according to their heuristics. See,",
            "description": ":func:`scipy.sparse.linalg.lobpcg` for details."
          },
          "Note": {
            "type": "that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`",
            "description": ""
          },
          "values": {
            "type": "of `tol<1e-5` may lead to convergence issues and should be",
            "description": "avoided.\n.. versionadded:: 1.2"
          },
          "Added": {
            "type": "new labeling method 'cluster_qr'.",
            "description": ""
          },
          "assign_labels": {
            "type": "{'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'",
            "description": ""
          },
          "ways": {
            "type": "to assign labels after the Laplacian embedding. k-means is a",
            "description": ""
          },
          "popular": {
            "type": "choice, but it can be sensitive to initialization.",
            "description": ""
          },
          "Discretization": {
            "type": "is another approach which is less sensitive to random",
            "description": ""
          },
          "initialization": {
            "type": "[3]_.",
            "description": ""
          },
          "in": {
            "type": "spectral clustering. In contrast to k-means and discretization, cluster_qr",
            "description": ""
          },
          "has": {
            "type": "no tuning parameters and runs no iterations, yet may outperform",
            "description": "k-means and discretization in terms of both quality and speed.\n.. versionchanged:: 1.1"
          },
          "degree": {
            "type": "float, default=3",
            "description": ""
          },
          "Degree": {
            "type": "of the polynomial kernel. Ignored by other kernels.",
            "description": ""
          },
          "coef0": {
            "type": "float, default=1",
            "description": ""
          },
          "Zero": {
            "type": "coefficient for polynomial and sigmoid kernels.",
            "description": ""
          },
          "kernel_params": {
            "type": "dict of str to any, default=None",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.cluster.KMeans : K-Means clustering.\n    sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of\n        Applications with Noise.\n\n    Notes\n    -----\n    A distance matrix for which 0 indicates identical elements and high values\n    indicate very dissimilar elements can be transformed into an affinity /\n    similarity matrix that is well-suited for the algorithm by\n    applying the Gaussian (aka RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    An alternative is to take a symmetric version of the k-nearest neighbors\n    connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels='discretize',\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)",
        "notes": "that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n        values of `tol<1e-5` may lead to convergence issues and should be\n        avoided.\n\n        .. versionadded:: 1.2\n           Added 'auto' option.\n\n    assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'\n        The strategy for assigning labels in the embedding space. There are two\n        ways to assign labels after the Laplacian embedding. k-means is a\n        popular choice, but it can be sensitive to initialization.\n        Discretization is another approach which is less sensitive to random\n        initialization [3]_.\n        The cluster_qr method [5]_ directly extract clusters from eigenvectors\n        in spectral clustering. In contrast to k-means and discretization, cluster_qr\n        has no tuning parameters and runs no iterations, yet may outperform\n        k-means and discretization in terms of both quality and speed.\n\n        .. versionchanged:: 1.1\n           Added new labeling method 'cluster_qr'.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict of str to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run when `affinity='nearest_neighbors'`\n        or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n        will be done in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    affinity_matrix_ : array-like of shape (n_samples, n_samples)\n        Affinity matrix used for clustering. Available only after calling\n        ``fit``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.cluster.KMeans : K-Means clustering.\n    sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of\n        Applications with Noise.\n\n    Notes\n    -----\n    A distance matrix for which 0 indicates identical elements and high values\n    indicate very dissimilar elements can be transformed into an affinity /\n    similarity matrix that is well-suited for the algorithm by\n    applying the Gaussian (aka RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    An alternative is to take a symmetric version of the k-nearest neighbors\n    connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n    .. [1] :doi:`Normalized cuts and image segmentation, 2000\n           Jianbo Shi, Jitendra Malik\n           <10.1109/34.868688>`\n\n    .. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n           Ulrike von Luxburg\n           <10.1007/s11222-007-9033-z>`\n\n    .. [3] `Multiclass spectral clustering, 2003\n           Stella X. Yu, Jianbo Shi\n           <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n    .. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n           Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n           A. V. Knyazev\n           SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n           <10.1137/S1064827500366124>`\n\n    .. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n           Anil Damle, Victor Minden, Lexing Ying\n           <10.1093/imaiai/iay008>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels='discretize',\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)",
        "examples": "--------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels='discretize',\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Perform spectral clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, similarities / affinities between",
                "description": ""
              },
              "instances": {
                "type": "if ``affinity='precomputed_nearest_neighbors``. If a",
                "description": ""
              },
              "sparse": {
                "type": "``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "A": {
                "type": "fitted instance of the estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            A fitted instance of the estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "documentation": {
            "description": "Perform spectral clustering on `X` and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)"
              },
              "Training": {
                "type": "instances to cluster, similarities / affinities between",
                "description": ""
              },
              "instances": {
                "type": "if ``affinity='precomputed_nearest_neighbors``. If a",
                "description": ""
              },
              "sparse": {
                "type": "``csr_matrix``.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present here for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "labels": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Cluster": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SpectralCoclustering",
      "documentation": {
        "description": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=3\n        The number of biclusters to find.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like of shape (n_cols,)\n        The bicluster label of each column.\n\n    biclusters_ : tuple of two ndarrays\n        The tuple contains the `rows_` and `columns_` arrays.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SpectralBiclustering : Partitions rows and columns under the assumption\n        that the data has an underlying checkerboard structure.\n\n    References\n    ----------\n    * :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using\n      bipartite spectral graph partitioning.\n      <10.1145/502512.502550>`",
        "parameters": {
          "n_clusters": {
            "type": "int, default=3",
            "description": ""
          },
          "The": {
            "type": "tuple contains the `rows_` and `columns_` arrays.",
            "description": ""
          },
          "svd_method": {
            "type": "{'randomized', 'arpack'}, default='randomized'",
            "description": ""
          },
          "Selects": {
            "type": "the algorithm for finding singular vectors. May be",
            "description": "'randomized' or 'arpack'. If 'randomized', use\n:func:`sklearn.utils.extmath.randomized_svd`, which may be faster"
          },
          "for": {
            "type": "large matrices. If 'arpack', use",
            "description": ":func:`scipy.sparse.linalg.svds`, which is more accurate, but"
          },
          "possibly": {
            "type": "slower in some cases.",
            "description": ""
          },
          "n_svd_vecs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "to": {
            "type": "`ncv` when `svd_method=arpack` and `n_oversamples` when",
            "description": "`svd_method` is 'randomized`."
          },
          "mini_batch": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to use mini-batch k-means, which is faster but may get",
            "description": ""
          },
          "different": {
            "type": "results.",
            "description": ""
          },
          "init": {
            "type": "{'k",
            "description": "means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'"
          },
          "Method": {
            "type": "for initialization of k-means algorithm; defaults to",
            "description": "'k-means++'."
          },
          "n_init": {
            "type": "int, default=10",
            "description": ""
          },
          "If": {
            "type": "mini-batch k-means is used, the best initialization is",
            "description": ""
          },
          "chosen": {
            "type": "and the algorithm runs once. Otherwise, the algorithm",
            "description": ""
          },
          "is": {
            "type": "run for each initialization and the best solution chosen.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "for randomizing the singular value decomposition and the k-means",
            "description": "initialization. Use an int to make the randomness deterministic."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "rows_": {
            "type": "array",
            "description": "like of shape (n_row_clusters, n_rows)"
          },
          "Results": {
            "type": "of the clustering, like `rows`.",
            "description": ""
          },
          "cluster": {
            "type": "`i` contains row `r`. Available only after calling ``fit``.",
            "description": ""
          },
          "columns_": {
            "type": "array",
            "description": "like of shape (n_column_clusters, n_columns)"
          },
          "row_labels_": {
            "type": "array",
            "description": "like of shape (n_rows,)"
          },
          "column_labels_": {
            "type": "array",
            "description": "like of shape (n_cols,)"
          },
          "biclusters_": {
            "type": "tuple of two ndarrays",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "SpectralBiclustering": {
            "type": "Partitions rows and columns under the assumption",
            "description": ""
          },
          "that": {
            "type": "the data has an underlying checkerboard structure.",
            "description": "References\n----------\n* :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using"
          },
          "bipartite": {
            "type": "spectral graph partitioning.",
            "description": "<10.1145/502512.502550>`\nExamples\n--------\n>>> from sklearn.cluster import SpectralCoclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_ #doctest: +SKIP"
          },
          "array": {
            "type": "[0, 0], dtype=int32",
            "description": ">>> clustering"
          },
          "SpectralCoclustering": {
            "type": "n_clusters=2, random_state=0",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SpectralBiclustering : Partitions rows and columns under the assumption\n        that the data has an underlying checkerboard structure.\n\n    References\n    ----------\n    * :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using\n      bipartite spectral graph partitioning.\n      <10.1145/502512.502550>`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Create a biclustering for X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "SpectralBiclustering": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            SpectralBiclustering instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_indices",
          "signature": "get_indices(self, i)",
          "documentation": {
            "description": "Row and column indices of the `i`'th bicluster.\n\n        Only works if ``rows_`` and ``columns_`` attributes exist.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "index of the cluster.",
                "description": "Returns\n-------"
              },
              "row_ind": {
                "type": "ndarray, dtype=np.intp",
                "description": ""
              },
              "Indices": {
                "type": "of columns in the dataset that belong to the bicluster.",
                "description": ""
              },
              "col_ind": {
                "type": "ndarray, dtype=np.intp",
                "description": ""
              }
            },
            "returns": "-------\n        row_ind : ndarray, dtype=np.intp\n            Indices of rows in the dataset that belong to the bicluster.\n        col_ind : ndarray, dtype=np.intp\n            Indices of columns in the dataset that belong to the bicluster.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_shape",
          "signature": "get_shape(self, i)",
          "documentation": {
            "description": "Shape of the `i`'th bicluster.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "index of the cluster.",
                "description": "Returns\n-------"
              },
              "n_rows": {
                "type": "int",
                "description": ""
              },
              "Number": {
                "type": "of columns in the bicluster.",
                "description": ""
              },
              "n_cols": {
                "type": "int",
                "description": ""
              }
            },
            "returns": "-------\n        n_rows : int\n            Number of rows in the bicluster.\n\n        n_cols : int\n            Number of columns in the bicluster.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_submatrix",
          "signature": "get_submatrix(self, i, data)",
          "documentation": {
            "description": "Return the submatrix corresponding to bicluster `i`.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n        data : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.",
            "parameters": {
              "i": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "submatrix corresponding to bicluster `i`.",
                "description": "Notes\n-----"
              },
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "submatrix": {
                "type": "ndarray of shape (n_rows, n_cols)",
                "description": ""
              },
              "Works": {
                "type": "with sparse matrices. Only works if ``rows_`` and",
                "description": "``columns_`` attributes exist."
              }
            },
            "returns": "-------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}