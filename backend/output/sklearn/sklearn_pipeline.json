{
  "description": "Utilities to build a composite estimator as a chain of transforms and estimators.",
  "functions": [
    {
      "name": "Bunch",
      "signature": "Bunch(**kwargs)",
      "docstring": {
        "description": "Container object exposing keys as attributes.\n\nBunch objects are sometimes used as an output for functions and methods.\nThey extend dictionaries by enabling values to be accessed by key,\n`bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.utils import Bunch\n>>> b = Bunch(a=1, b=2)\n>>> b['b']\n2\n>>> b.b\n2\n>>> b.a = 3\n>>> b['a']\n3\n>>> b.c = 6\n>>> b['c']\n6"
      }
    },
    {
      "name": "Counter",
      "signature": "Counter(iterable=None, /, **kwds)",
      "docstring": {
        "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "FeatureUnion",
      "signature": "FeatureUnion(transformer_list, *, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)",
      "docstring": {
        "description": "Concatenates results of multiple transformer objects.\n\nThis estimator applies a list of transformer objects in parallel to the\ninput data, then concatenates the results. This is useful to combine\nseveral feature extraction mechanisms into a single transformer.\n\nParameters of the transformers may be set using its name and the parameter\nname separated by a '__'. A transformer may be replaced entirely by\nsetting the parameter with its name to another transformer, removed by\nsetting to 'drop' or disabled by setting to 'passthrough' (features are\npassed without transformation).\n\nRead more in the :ref:`User Guide <feature_union>`.\n\n.. versionadded:: 0.13",
        "parameters": {
          "transformer_list": {
            "type": "list of (str, transformer) tuples",
            "description": "List of transformer objects to be applied to the data. The first\n    half of each tuple is the name of the transformer. The transformer can\n    be 'drop' for it to be ignored or can be 'passthrough' for features to\n    be passed unchanged.\n\n    .. versionadded:: 1.1\n       Added the option `\"passthrough\"`.\n\n    .. versionchanged:: 0.22\n       Deprecated `None` as a transformer in favor of 'drop'."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None"
          },
          "transformer_weights": {
            "type": "dict, default=None",
            "description": "Multiplicative weights for features per transformer.\n    Keys are transformer names, values the weights.\n    Raises ValueError if key not present in ``transformer_list``."
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each transformer will be\n    printed as it is completed."
          },
          "verbose_feature_names_out": {
            "type": "bool, default=True",
            "description": "If True, :meth:`get_feature_names_out` will prefix all feature names\n    with the name of the transformer that generated that feature.\n    If False, :meth:`get_feature_names_out` will not prefix any feature\n    names and will error if feature names are not unique.\n\n    .. versionadded:: 1.5\n\nAttributes\n----------"
          },
          "named_transformers": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Dictionary-like object, with the following attributes.\n    Read-only attribute to access any transformer parameter by user\n    given name. Keys are transformer names and values are\n    transformer parameters.\n\n    .. versionadded:: 1.2"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying first transformer in `transformer_list` exposes such an\n    attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when\n    `X` has feature names that are all strings.\n\n    .. versionadded:: 1.3"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "make_union : Convenience function for simplified feature union\n    construction.",
        "notes": "",
        "examples": ">>> from sklearn.pipeline import FeatureUnion\n>>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n...                       (\"svd\", TruncatedSVD(n_components=2))])\n>>> X = [[0., 1., 3], [2., 2., 5]]\n>>> union.fit_transform(X)\narray([[-1.5       ,  3.0..., -0.8...],\n       [ 1.5       ,  5.7...,  0.4...]])\n>>> # An estimator's parameter can be set using '__' syntax\n>>> union.set_params(svd__n_components=1).fit_transform(X)\narray([[-1.5       ,  3.0...],\n       [ 1.5       ,  5.7...]])\n\nFor a more detailed example of usage, see\n:ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`."
      }
    },
    {
      "name": "FunctionTransformer",
      "signature": "FunctionTransformer(func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, feature_names_out=None, kw_args=None, inv_kw_args=None)",
      "docstring": {
        "description": "Constructs a transformer from an arbitrary callable.\n\nA FunctionTransformer forwards its X (and optionally y) arguments to a\nuser-defined function or function object and returns the result of this\nfunction. This is useful for stateless transformations such as taking the\nlog of frequencies, doing custom scaling, etc.\n\nNote: If a lambda is used as the function, then the resulting\ntransformer will not be pickleable.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <function_transformer>`.",
        "parameters": {
          "func": {
            "type": "callable, default=None",
            "description": "The callable to use for the transformation. This will be passed\n    the same arguments as transform, with args and kwargs forwarded.\n    If func is None, then func will be the identity function."
          },
          "inverse_func": {
            "type": "callable, default=None",
            "description": "The callable to use for the inverse transformation. This will be\n    passed the same arguments as inverse transform, with args and\n    kwargs forwarded. If inverse_func is None, then inverse_func\n    will be the identity function."
          },
          "validate": {
            "type": "bool, default=False",
            "description": "Indicate that the input X array should be checked before calling\n    ``func``. The possibilities are:\n\n    - If False, there is no input validation.\n    - If True, then X will be converted to a 2-dimensional NumPy array or\n      sparse matrix. If the conversion is not possible an exception is\n      raised.\n\n    .. versionchanged:: 0.22\n       The default of ``validate`` changed from True to False."
          },
          "accept_sparse": {
            "type": "bool, default=False",
            "description": "Indicate that func accepts a sparse matrix as input. If validate is\n    False, this has no effect. Otherwise, if accept_sparse is false,\n    sparse matrix inputs will cause an exception to be raised."
          },
          "check_inverse": {
            "type": "bool, default=True",
            "description": "Whether to check that or ``func`` followed by ``inverse_func`` leads to\n   the original inputs. It can be used for a sanity check, raising a\n   warning when the condition is not fulfilled.\n\n   .. versionadded:: 0.20"
          },
          "feature_names_out": {
            "type": "callable, 'one-to-one' or None, default=None",
            "description": "Determines the list of feature names that will be returned by the\n    `get_feature_names_out` method. If it is 'one-to-one', then the output\n    feature names will be equal to the input feature names. If it is a\n    callable, then it must take two positional arguments: this\n    `FunctionTransformer` (`self`) and an array-like of input feature names\n    (`input_features`). It must return an array-like of output feature\n    names. The `get_feature_names_out` method is only defined if\n    `feature_names_out` is not None.\n\n    See ``get_feature_names_out`` for more details.\n\n    .. versionadded:: 1.1"
          },
          "kw_args": {
            "type": "dict, default=None",
            "description": "Dictionary of additional keyword arguments to pass to func.\n\n    .. versionadded:: 0.18"
          },
          "inv_kw_args": {
            "type": "dict, default=None",
            "description": "Dictionary of additional keyword arguments to pass to inverse_func.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X` has feature\n    names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "MaxAbsScaler : Scale each feature by its maximum absolute value.\nStandardScaler : Standardize features by removing the mean and\n    scaling to unit variance.\nLabelBinarizer : Binarize labels in a one-vs-all fashion.\nMultiLabelBinarizer : Transform between iterable of iterables\n    and a multilabel format.",
        "notes": "If `func` returns an output with a `columns` attribute, then the columns is enforced\nto be consistent with the output of `get_feature_names_out`.",
        "examples": ">>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> transformer.transform(X)\narray([[0.       , 0.6931...],\n       [1.0986..., 1.3862...]])"
      }
    },
    {
      "name": "HasMethods",
      "signature": "HasMethods(methods)",
      "docstring": {
        "description": "Constraint representing objects that expose specific methods.\n\nIt is useful for parameters following a protocol and where we don't want to impose\nan affiliation to a specific module or class.",
        "parameters": {
          "methods": {
            "type": "str or list of str",
            "description": "The method(s) that the object is expected to expose."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Hidden",
      "signature": "Hidden(constraint)",
      "docstring": {
        "description": "Class encapsulating a constraint not meant to be exposed to the user.",
        "parameters": {
          "constraint": {
            "type": "str or _Constraint instance",
            "description": "The constraint to be used internally."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "MetadataRouter",
      "signature": "MetadataRouter(owner)",
      "docstring": {
        "description": "Stores and handles metadata routing for a router object.\n\nThis class is used by router objects to store and handle metadata routing.\nRouting information is stored as a dictionary of the form ``{\"object_name\":\nRouteMappingPair(method_mapping, routing_info)}``, where ``method_mapping``\nis an instance of :class:`~sklearn.utils.metadata_routing.MethodMapping` and\n``routing_info`` is either a\n:class:`~sklearn.utils.metadata_routing.MetadataRequest` or a\n:class:`~sklearn.utils.metadata_routing.MetadataRouter` instance.\n\n.. versionadded:: 1.3",
        "parameters": {
          "owner": {
            "type": "str",
            "description": "The name of the object to which these requests belong."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "MethodMapping",
      "signature": "MethodMapping()",
      "docstring": {
        "description": "Stores the mapping between caller and callee methods for a router.\n\nThis class is primarily used in a ``get_metadata_routing()`` of a router\nobject when defining the mapping between the router's methods and a sub-object (a\nsub-estimator or a scorer).\n\nIterating through an instance of this class yields\n``MethodPair(caller, callee)`` instances.\n\n.. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "NotFittedError",
      "signature": "NotFittedError(...)",
      "docstring": {
        "description": "Exception class to raise if estimator is used before fitting.\n\nThis class inherits from both ValueError and AttributeError to help with\nexception handling and backward compatibility.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.svm import LinearSVC\n>>> from sklearn.exceptions import NotFittedError\n>>> try:\n...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])\n... except NotFittedError as e:\n...     print(repr(e))\nNotFittedError(\"This LinearSVC instance is not fitted yet. Call 'fit' with\nappropriate arguments before using this estimator.\"...)\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils.validation."
      }
    },
    {
      "name": "Parallel",
      "signature": "Parallel(n_jobs=default(None), backend=default(None), return_as='list', verbose=default(0), timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=default(None), max_nbytes=default('1M'), mmap_mode=default('r'), prefer=default(None), require=default(None))",
      "docstring": {
        "description": "Tweak of :class:`joblib.Parallel` that propagates the scikit-learn configuration.\n\nThis subclass of :class:`joblib.Parallel` ensures that the active configuration\n(thread-local) of scikit-learn is propagated to the parallel workers for the\nduration of the execution of the parallel tasks.\n\nThe API does not change and you can refer to :class:`joblib.Parallel`\ndocumentation for more details.\n\n.. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Pipeline",
      "signature": "Pipeline(steps, *, transform_input=None, memory=None, verbose=False)",
      "docstring": {
        "description": "A sequence of data transformers with an optional final predictor.\n\n`Pipeline` allows you to sequentially apply a list of transformers to\npreprocess the data and, if desired, conclude the sequence with a final\n:term:`predictor` for predictive modeling.\n\nIntermediate steps of the pipeline must be transformers, that is, they\nmust implement `fit` and `transform` methods.\nThe final :term:`estimator` only needs to implement `fit`.\nThe transformers in the pipeline can be cached using ``memory`` argument.\n\nThe purpose of the pipeline is to assemble several steps that can be\ncross-validated together while setting different parameters. For this, it\nenables setting parameters of the various steps using their names and the\nparameter name separated by a `'__'`, as in the example below. A step's\nestimator may be replaced entirely by setting the parameter with its name\nto another estimator, or a transformer removed by setting it to\n`'passthrough'` or `None`.\n\nFor an example use case of `Pipeline` combined with\n:class:`~sklearn.model_selection.GridSearchCV`, refer to\n:ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\nexample :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\nto grid search on a pipeline using `'__'` as a separator in the parameter names.\n\nRead more in the :ref:`User Guide <pipeline>`.\n\n.. versionadded:: 0.5",
        "parameters": {
          "steps": {
            "type": "list of tuples",
            "description": "List of (name of step, estimator) tuples that are to be chained in\n    sequential order. To be compatible with the scikit-learn API, all steps\n    must define `fit`. All non-last steps must also define `transform`. See\n    :ref:`Combining Estimators <combining_estimators>` for more details."
          },
          "transform_input": {
            "type": "list of str, default=None",
            "description": "The names of the :term:`metadata` parameters that should be transformed by the\n    pipeline before passing it to the step consuming it.\n\n    This enables transforming some input arguments to ``fit`` (other than ``X``)\n    to be transformed by the steps of the pipeline up to the step which requires\n    them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n    For instance, this can be used to pass a validation set through the pipeline.\n\n    You can only set this if metadata routing is enabled, which you\n    can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n    .. versionadded:: 1.6"
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": "Used to cache the fitted transformers of the pipeline. The last step\n    will never be cached, even if it is a transformer. By default, no\n    caching is performed. If a string is given, it is the path to the\n    caching directory. Enabling caching triggers a clone of the transformers\n    before fitting. Therefore, the transformer instance given to the\n    pipeline cannot be inspected directly. Use the attribute ``named_steps``\n    or ``steps`` to inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming. See\n    :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\n    for an example on how to enable caching."
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each step will be printed as it\n    is completed.\n\nAttributes\n----------"
          },
          "named_steps": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Dictionary-like object, with the following attributes.\n    Read-only attribute to access any step parameter by user given name.\n    Keys are step names and values are steps parameters."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only exist if the last step of the pipeline is a\n    classifier."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying first estimator in `steps` exposes such an attribute\n    when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "make_pipeline : Convenience function for simplified pipeline construction.",
        "notes": "",
        "examples": ">>> from sklearn.svm import SVC\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import Pipeline\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=0)\n>>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n>>> # The pipeline can be used as any other estimator\n>>> # and avoids leaking the test set into the train set\n>>> pipe.fit(X_train, y_train).score(X_test, y_test)\n0.88\n>>> # An estimator's parameter can be set using '__' syntax\n>>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n0.76"
      }
    },
    {
      "name": "TransformerMixin",
      "signature": "TransformerMixin()",
      "docstring": {
        "description": "Mixin class for all transformers in scikit-learn.\n\nThis mixin defines the following functionality:\n\n- a `fit_transform` method that delegates to `fit` and `transform`;\n- a `set_output` method to output `X` as a specific container type.\n\nIf :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\nautomatically wrap `transform` and `fit_transform` to follow the `set_output`\nAPI. See the :ref:`developer_api_set_output` for details.\n\n:class:`OneToOneFeatureMixin` and\n:class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\ndefining :term:`get_feature_names_out`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator, TransformerMixin\n>>> class MyTransformer(TransformerMixin, BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         return self\n...     def transform(self, X):\n...         return np.full(shape=len(X), fill_value=self.param)\n>>> transformer = MyTransformer()\n>>> X = [[1, 2], [2, 3], [3, 4]]\n>>> transformer.fit_transform(X)\narray([1, 1, 1])"
      }
    },
    {
      "name": "available_if",
      "signature": "available_if(check)",
      "docstring": {
        "description": "An attribute that is available only if check returns a truthy value.",
        "parameters": {
          "check": {
            "type": "callable",
            "description": "When passed the object with the decorated method, this should return\n    a truthy value if the attribute is available, and either return False\n    or raise an AttributeError if not available."
          }
        },
        "returns": "callable\n    Callable makes the decorated method available if `check` returns\n    a truthy value, otherwise the decorated method is unavailable.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.utils.metaestimators import available_if\n>>> class HelloIfEven:\n...    def __init__(self, x):\n...        self.x = x\n...\n...    def _x_is_even(self):\n...        return self.x % 2 == 0\n...\n...    @available_if(_x_is_even)\n...    def say_hello(self):\n...        print(\"Hello\")\n...\n>>> obj = HelloIfEven(1)\n>>> hasattr(obj, \"say_hello\")\nFalse\n>>> obj.x = 2\n>>> hasattr(obj, \"say_hello\")\nTrue\n>>> obj.say_hello()\nHello"
      }
    },
    {
      "name": "chain",
      "signature": "chain(...)",
      "docstring": {
        "description": "chain(*iterables) --> chain object\n\nReturn a chain object whose .__next__() method returns elements from the\nfirst iterable until it is exhausted, then elements from the next\niterable, until all of the iterables are exhausted.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "docstring": {
        "description": "Perform is_fitted validation for estimator.\n\nChecks if the estimator is fitted by verifying the presence of\nfitted attributes (ending with a trailing underscore) and otherwise\nraises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\nIf an estimator does not set any attributes with a trailing underscore, it\ncan define a ``__sklearn_is_fitted__`` method returning a boolean to\nspecify if the estimator is fitted or not. See\n:ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\nfor an example on how to use the API.\n\nIf no `attributes` are passed, this fuction will pass if an estimator is stateless.\nAn estimator can indicate it's stateless by setting the `requires_fit` tag. See\n:ref:`estimator_tags` for more information. Note that the `requires_fit` tag\nis ignored if `attributes` are passed.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": "Estimator instance for which the check is performed."
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": "Attribute name(s) given as string or a list/tuple of strings\n    Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n    If `None`, `estimator` is considered fitted if there exist an\n    attribute that ends with a underscore and does not start with double\n    underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": "The default error message is, \"This %(name)s instance is not fitted\n    yet. Call 'fit' with appropriate arguments before using this\n    estimator.\"\n\n    For custom messages if \"%(name)s\" is present in the message string,\n    it is substituted for the estimator name.\n\n    Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": "Specify whether all or any of the given attributes must exist."
          }
        },
        "returns": "",
        "raises": "TypeError\n    If the estimator is a class or not an estimator instance\n\nNotFittedError\n    If the attributes are not found.",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")\nModel is not fitted yet.\n>>> lr.fit([[1, 2], [1, 3]], [1, 0])\nLogisticRegression()\n>>> check_is_fitted(lr)"
      }
    },
    {
      "name": "check_memory",
      "signature": "check_memory(memory)",
      "docstring": {
        "description": "Check that ``memory`` is joblib.Memory-like.\n\njoblib.Memory-like means that ``memory`` can be converted into a\njoblib.Memory instance (typically a str denoting the ``location``)\nor has the same interface (has a ``cache`` method).",
        "parameters": {
          "memory": {
            "type": "None, str or object with the joblib.Memory interface",
            "description": "- If string, the location where to create the `joblib.Memory` interface.\n    - If None, no caching is done and the Memory object is completely transparent."
          }
        },
        "returns": "memory : object with the joblib.Memory interface\n    A correct joblib.Memory object.",
        "raises": "ValueError\n    If ``memory`` is not joblib.Memory-like.",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.utils.validation import check_memory\n>>> check_memory(\"caching_dir\")\nMemory(location=caching_dir/joblib)"
      }
    },
    {
      "name": "clone",
      "signature": "clone(estimator, *, safe=True)",
      "docstring": {
        "description": "Construct a new unfitted estimator with the same parameters.\n\nClone does a deep copy of the model in an estimator\nwithout actually copying attached data. It returns a new estimator\nwith the same parameters that has not been fitted on any data.\n\n.. versionchanged:: 1.3\n    Delegates to `estimator.__sklearn_clone__` if the method exists.",
        "parameters": {
          "estimator": {
            "type": "{list, tuple, set} of estimator instance or a single             estimator instance",
            "description": "The estimator or group of estimators to be cloned."
          },
          "safe": {
            "type": "bool, default=True",
            "description": "If safe is False, clone will fall back to a deep copy on objects\n    that are not estimators. Ignored if `estimator.__sklearn_clone__`\n    exists."
          }
        },
        "returns": "estimator : object\n    The deep copy of the input, an estimator if input is an estimator.",
        "raises": "",
        "see_also": "",
        "notes": "If the estimator's `random_state` parameter is an integer (or if the\nestimator doesn't have a `random_state` parameter), an *exact clone* is\nreturned: the clone and the original estimator will give the exact same\nresults. Otherwise, *statistical clone* is returned: the clone might\nreturn different results from the original estimator. More details can be\nfound in :ref:`randomness`.",
        "examples": ">>> from sklearn.base import clone\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n>>> y = [0, 0, 1, 1]\n>>> classifier = LogisticRegression().fit(X, y)\n>>> cloned_classifier = clone(classifier)\n>>> hasattr(classifier, \"classes_\")\nTrue\n>>> hasattr(cloned_classifier, \"classes_\")\nFalse\n>>> classifier is cloned_classifier\nFalse"
      }
    },
    {
      "name": "contextmanager",
      "signature": "contextmanager(func)",
      "docstring": {
        "description": "@contextmanager decorator.\n\nTypical usage:\n\n    @contextmanager\n    def some_generator(<arguments>):\n        <setup>\n        try:\n            yield <value>\n        finally:\n            <cleanup>\n\nThis makes this:\n\n    with some_generator(<arguments>) as <variable>:\n        <body>\n\nequivalent to this:\n\n    <setup>\n    try:\n        <variable> = <value>\n        <body>\n    finally:\n        <cleanup>",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "deepcopy",
      "signature": "deepcopy(x, memo=None, _nil=[])",
      "docstring": {
        "description": "Deep copy operation on arbitrary Python objects.\n\nSee the module's __doc__ string for more info.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "defaultdict",
      "signature": "defaultdict(...)",
      "docstring": {
        "description": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "delayed",
      "signature": "delayed(function)",
      "docstring": {
        "description": "Decorator used to capture the arguments of a function.\n\nThis alternative to `joblib.delayed` is meant to be used in conjunction\nwith `sklearn.utils.parallel.Parallel`. The latter captures the scikit-\nlearn configuration by calling `sklearn.get_config()` in the current\nthread, prior to dispatching the first task. The captured configuration is\nthen propagated and enabled for the duration of the execution of the\ndelayed function in the joblib workers.\n\n.. versionchanged:: 1.3\n   `delayed` was moved from `sklearn.utils.fixes` to `sklearn.utils.parallel`\n   in scikit-learn 1.3.",
        "parameters": {
          "function": {
            "type": "callable",
            "description": "The function to be delayed."
          }
        },
        "returns": "output: tuple\n    Tuple containing the delayed function, the positional arguments, and the\n    keyword arguments.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_routing_for_object",
      "signature": "get_routing_for_object(obj=None)",
      "docstring": {
        "description": "Get a ``Metadata{Router, Request}`` instance from the given object.\n\nThis function returns a\n:class:`~sklearn.utils.metadata_routing.MetadataRouter` or a\n:class:`~sklearn.utils.metadata_routing.MetadataRequest` from the given input.\n\nThis function always returns a copy or an instance constructed from the\ninput, such that changing the output of this function will not change the\noriginal object.\n\n.. versionadded:: 1.3",
        "parameters": {
          "obj": {
            "type": "object",
            "description": "- If the object provides a `get_metadata_routing` method, return a copy\n        of the output of that method.\n    - If the object is already a\n        :class:`~sklearn.utils.metadata_routing.MetadataRequest` or a\n        :class:`~sklearn.utils.metadata_routing.MetadataRouter`, return a copy\n        of that.\n    - Returns an empty :class:`~sklearn.utils.metadata_routing.MetadataRequest`\n        otherwise."
          }
        },
        "returns": "obj : MetadataRequest or MetadataRouting\n    A ``MetadataRequest`` or a ``MetadataRouting`` taken or created from\n    the given object.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "get_tags",
      "signature": "get_tags(estimator) -> 'Tags'",
      "docstring": {
        "description": "Get estimator tags.\n\n:class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\nHowever, if an estimator does not inherit from this base class, we should\nfall-back to the default tags.\n\nFor scikit-learn built-in estimators, we should still rely on\n`self.__sklearn_tags__()`. `get_tags(est)` should be used when we\nare not sure where `est` comes from: typically\n`get_tags(self.estimator)` where `self` is a meta-estimator, or in\nthe common checks.\n\n.. versionadded:: 1.6",
        "parameters": {
          "estimator": {
            "type": "estimator object",
            "description": "The estimator from which to get the tag."
          }
        },
        "returns": "tags : :class:`~.sklearn.utils.Tags`\n    The estimator tags.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "islice",
      "signature": "islice(...)",
      "docstring": {
        "description": "islice(iterable, stop) --> islice object\nislice(iterable, start, stop[, step]) --> islice object\n\nReturn an iterator whose next() method returns selected values from an\niterable.  If start is specified, will skip all preceding elements;\notherwise, start defaults to zero.  Step defaults to one.  If\nspecified as another value, step determines how many values are\nskipped between successive calls.  Works like a slice() on a list\nbut returns an iterator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "make_pipeline",
      "signature": "make_pipeline(*steps, memory=None, transform_input=None, verbose=False)",
      "docstring": {
        "description": "Construct a :class:`Pipeline` from the given estimators.\n\nThis is a shorthand for the :class:`Pipeline` constructor; it does not\nrequire, and does not permit, naming the estimators. Instead, their names\nwill be set to the lowercase of their types automatically.",
        "parameters": {
          "*steps": {
            "type": "list of Estimator objects",
            "description": "List of the scikit-learn estimators that are chained together."
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": "Used to cache the fitted transformers of the pipeline. The last step\n    will never be cached, even if it is a transformer. By default, no\n    caching is performed. If a string is given, it is the path to the\n    caching directory. Enabling caching triggers a clone of the transformers\n    before fitting. Therefore, the transformer instance given to the\n    pipeline cannot be inspected directly. Use the attribute ``named_steps``\n    or ``steps`` to inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming."
          },
          "transform_input": {
            "type": "list of str, default=None",
            "description": "This enables transforming some input arguments to ``fit`` (other than ``X``)\n    to be transformed by the steps of the pipeline up to the step which requires\n    them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n    This can be used to pass a validation set through the pipeline for instance.\n\n    You can only set this if metadata routing is enabled, which you\n    can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n    .. versionadded:: 1.6"
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each step will be printed as it\n    is completed."
          }
        },
        "returns": "p : Pipeline\n    Returns a scikit-learn :class:`Pipeline` object.",
        "raises": "",
        "see_also": "Pipeline : Class for creating a pipeline of transforms with a final\n    estimator.",
        "notes": "",
        "examples": ">>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('gaussiannb', GaussianNB())])"
      }
    },
    {
      "name": "make_union",
      "signature": "make_union(*transformers, n_jobs=None, verbose=False)",
      "docstring": {
        "description": "Construct a :class:`FeatureUnion` from the given transformers.\n\nThis is a shorthand for the :class:`FeatureUnion` constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting.",
        "parameters": {
          "*transformers": {
            "type": "list of estimators",
            "description": "One or more estimators."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None."
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each transformer will be\n    printed as it is completed."
          }
        },
        "returns": "f : FeatureUnion\n    A :class:`FeatureUnion` object for concatenating the results of multiple\n    transformer objects.",
        "raises": "",
        "see_also": "FeatureUnion : Class for concatenating the results of multiple transformer\n    objects.",
        "notes": "",
        "examples": ">>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> from sklearn.pipeline import make_union\n>>> make_union(PCA(), TruncatedSVD())\n FeatureUnion(transformer_list=[('pca', PCA()),\n                               ('truncatedsvd', TruncatedSVD())])"
      }
    },
    {
      "name": "process_routing",
      "signature": "process_routing(_obj, _method, /, **kwargs)",
      "docstring": {
        "description": "Validate and route input parameters.\n\nThis function is used inside a router's method, e.g. :term:`fit`,\nto validate the metadata and handle the routing.\n\nAssuming this signature of a router's fit method:\n``fit(self, X, y, sample_weight=None, **fit_params)``,\na call to this function would be:\n``process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params)``.\n\nNote that if routing is not enabled and ``kwargs`` is empty, then it\nreturns an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``\nis always an empty dictionary.\n\n.. versionadded:: 1.3",
        "parameters": {
          "_obj": {
            "type": "object",
            "description": "An object implementing ``get_metadata_routing``. Typically a\n    meta-estimator."
          },
          "_method": {
            "type": "str",
            "description": "The name of the router's method in which this function is called."
          },
          "**kwargs": {
            "type": "dict",
            "description": "Metadata to be routed."
          }
        },
        "returns": "routed_params : Bunch\n    A :class:`~utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n    {params: value}}}`` which can be used to pass the required metadata to\n    A :class:`~sklearn.utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n    {params: value}}}`` which can be used to pass the required metadata to\n    corresponding methods or corresponding child objects. The object names\n    are those defined in `obj.get_metadata_routing()`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "Bunch",
      "docstring": {
        "description": "Container object exposing keys as attributes.\n\nBunch objects are sometimes used as an output for functions and methods.\nThey extend dictionaries by enabling values to be accessed by key,\n`bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.utils import Bunch\n>>> b = Bunch(a=1, b=2)\n>>> b['b']\n2\n>>> b.b\n2\n>>> b.a = 3\n>>> b['a']\n3\n>>> b.c = 6\n>>> b['c']\n6"
      },
      "methods": [
        {
          "name": "clear",
          "signature": "clear(...)",
          "docstring": {
            "description": "D.clear() -> None.  Remove all items from D.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy",
          "signature": "copy(...)",
          "docstring": {
            "description": "D.copy() -> a shallow copy of D",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fromkeys",
          "signature": "fromkeys(iterable, value=None, /)",
          "docstring": {
            "description": "Create a new dictionary with keys from iterable and values set to value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self, key, default=None, /)",
          "docstring": {
            "description": "Return the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "items",
          "signature": "items(...)",
          "docstring": {
            "description": "D.items() -> a set-like object providing a view on D's items",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "keys",
          "signature": "keys(...)",
          "docstring": {
            "description": "D.keys() -> a set-like object providing a view on D's keys",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "pop",
          "signature": "pop(...)",
          "docstring": {
            "description": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\nIf the key is not found, return the default if given; otherwise,\nraise a KeyError.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "popitem",
          "signature": "popitem(self, /)",
          "docstring": {
            "description": "Remove and return a (key, value) pair as a 2-tuple.\n\nPairs are returned in LIFO (last-in, first-out) order.\nRaises KeyError if the dict is empty.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setdefault",
          "signature": "setdefault(self, key, default=None, /)",
          "docstring": {
            "description": "Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update",
          "signature": "update(...)",
          "docstring": {
            "description": "D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\nIf E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\nIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\nIn either case, this is followed by: for k in F:  D[k] = F[k]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "values",
          "signature": "values(...)",
          "docstring": {
            "description": "D.values() -> an object providing a view on D's values",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Counter",
      "docstring": {
        "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clear",
          "signature": "clear(...)",
          "docstring": {
            "description": "D.clear() -> None.  Remove all items from D.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy",
          "signature": "copy(self)",
          "docstring": {
            "description": "Return a shallow copy.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "elements",
          "signature": "elements(self)",
          "docstring": {
            "description": "Iterator over elements repeating each as many times as its count.\n\n>>> c = Counter('ABCABC')\n>>> sorted(c.elements())\n['A', 'A', 'B', 'B', 'C', 'C']\n\nKnuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n\n>>> import math\n>>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n>>> math.prod(prime_factors.elements())\n1836\n\nNote, if an element's count has been set to zero or is a negative\nnumber, elements() will ignore it.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fromkeys",
          "signature": "fromkeys(iterable, v=None)",
          "docstring": {
            "description": "Create a new dictionary with keys from iterable and values set to value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self, key, default=None, /)",
          "docstring": {
            "description": "Return the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "items",
          "signature": "items(...)",
          "docstring": {
            "description": "D.items() -> a set-like object providing a view on D's items",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "keys",
          "signature": "keys(...)",
          "docstring": {
            "description": "D.keys() -> a set-like object providing a view on D's keys",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "most_common",
          "signature": "most_common(self, n=None)",
          "docstring": {
            "description": "List the n most common elements and their counts from the most\ncommon to the least.  If n is None, then list all element counts.\n\n>>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "pop",
          "signature": "pop(...)",
          "docstring": {
            "description": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\nIf the key is not found, return the default if given; otherwise,\nraise a KeyError.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "popitem",
          "signature": "popitem(self, /)",
          "docstring": {
            "description": "Remove and return a (key, value) pair as a 2-tuple.\n\nPairs are returned in LIFO (last-in, first-out) order.\nRaises KeyError if the dict is empty.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setdefault",
          "signature": "setdefault(self, key, default=None, /)",
          "docstring": {
            "description": "Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "subtract",
          "signature": "subtract(self, iterable=None, /, **kwds)",
          "docstring": {
            "description": "Like dict.update() but subtracts counts instead of replacing them.\nCounts can be reduced below zero.  Both the inputs and outputs are\nallowed to contain zero and negative counts.\n\nSource can be an iterable, a dictionary, or another Counter instance.\n\n>>> c = Counter('which')\n>>> c.subtract('witch')             # subtract elements from another iterable\n>>> c.subtract(Counter('watch'))    # subtract elements from another counter\n>>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n0\n>>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n-1",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "total",
          "signature": "total(self)",
          "docstring": {
            "description": "Sum of the counts",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update",
          "signature": "update(self, iterable=None, /, **kwds)",
          "docstring": {
            "description": "Like dict.update() but add counts instead of replacing them.\n\nSource can be an iterable, a dictionary, or another Counter instance.\n\n>>> c = Counter('which')\n>>> c.update('witch')           # add elements from another iterable\n>>> d = Counter('watch')\n>>> c.update(d)                 # add elements from another counter\n>>> c['h']                      # four 'h' in which, witch, and watch\n4",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "values",
          "signature": "values(...)",
          "docstring": {
            "description": "D.values() -> an object providing a view on D's values",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FeatureUnion",
      "docstring": {
        "description": "Concatenates results of multiple transformer objects.\n\nThis estimator applies a list of transformer objects in parallel to the\ninput data, then concatenates the results. This is useful to combine\nseveral feature extraction mechanisms into a single transformer.\n\nParameters of the transformers may be set using its name and the parameter\nname separated by a '__'. A transformer may be replaced entirely by\nsetting the parameter with its name to another transformer, removed by\nsetting to 'drop' or disabled by setting to 'passthrough' (features are\npassed without transformation).\n\nRead more in the :ref:`User Guide <feature_union>`.\n\n.. versionadded:: 0.13",
        "parameters": {
          "transformer_list": {
            "type": "list of (str, transformer) tuples",
            "description": "List of transformer objects to be applied to the data. The first\n    half of each tuple is the name of the transformer. The transformer can\n    be 'drop' for it to be ignored or can be 'passthrough' for features to\n    be passed unchanged.\n\n    .. versionadded:: 1.1\n       Added the option `\"passthrough\"`.\n\n    .. versionchanged:: 0.22\n       Deprecated `None` as a transformer in favor of 'drop'."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": "Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None"
          },
          "transformer_weights": {
            "type": "dict, default=None",
            "description": "Multiplicative weights for features per transformer.\n    Keys are transformer names, values the weights.\n    Raises ValueError if key not present in ``transformer_list``."
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each transformer will be\n    printed as it is completed."
          },
          "verbose_feature_names_out": {
            "type": "bool, default=True",
            "description": "If True, :meth:`get_feature_names_out` will prefix all feature names\n    with the name of the transformer that generated that feature.\n    If False, :meth:`get_feature_names_out` will not prefix any feature\n    names and will error if feature names are not unique.\n\n    .. versionadded:: 1.5\n\nAttributes\n----------"
          },
          "named_transformers": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Dictionary-like object, with the following attributes.\n    Read-only attribute to access any transformer parameter by user\n    given name. Keys are transformer names and values are\n    transformer parameters.\n\n    .. versionadded:: 1.2"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying first transformer in `transformer_list` exposes such an\n    attribute when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when\n    `X` has feature names that are all strings.\n\n    .. versionadded:: 1.3"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "make_union : Convenience function for simplified feature union\n    construction.",
        "notes": "",
        "examples": ">>> from sklearn.pipeline import FeatureUnion\n>>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n...                       (\"svd\", TruncatedSVD(n_components=2))])\n>>> X = [[0., 1., 3], [2., 2., 5]]\n>>> union.fit_transform(X)\narray([[-1.5       ,  3.0..., -0.8...],\n       [ 1.5       ,  5.7...,  0.4...]])\n>>> # An estimator's parameter can be set using '__' syntax\n>>> union.set_params(svd__n_components=1).fit_transform(X)\narray([[-1.5       ,  3.0...],\n       [ 1.5       ,  5.7...]])\n\nFor a more detailed example of usage, see\n:ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit all transformers using X.",
            "parameters": {
              "X": {
                "type": "iterable or array-like, depending on transformers",
                "description": "Input data, used to fit transformers."
              },
              "y": {
                "type": "array-like of shape (n_samples, n_outputs), default=None",
                "description": "Targets for supervised learning."
              },
              "**fit_params": {
                "type": "dict, default=None",
                "description": "- If `enable_metadata_routing=False` (default):\n      Parameters directly passed to the `fit` methods of the\n      sub-transformers.\n\n    - If `enable_metadata_routing=True`:\n      Parameters safely routed to the `fit` methods of the\n      sub-transformers. See :ref:`Metadata Routing User Guide\n      <metadata_routing>` for more details.\n\n    .. versionchanged:: 1.5\n        `**fit_params` can be routed via metadata routing API."
              }
            },
            "returns": "self : object\n    FeatureUnion class instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **params)",
          "docstring": {
            "description": "Fit all transformers, transform the data and concatenate results.",
            "parameters": {
              "X": {
                "type": "iterable or array-like, depending on transformers",
                "description": "Input data to be transformed."
              },
              "y": {
                "type": "array-like of shape (n_samples, n_outputs), default=None",
                "description": "Targets for supervised learning."
              },
              "**params": {
                "type": "dict, default=None",
                "description": "- If `enable_metadata_routing=False` (default):\n      Parameters directly passed to the `fit` methods of the\n      sub-transformers.\n\n    - If `enable_metadata_routing=True`:\n      Parameters safely routed to the `fit` methods of the\n      sub-transformers. See :ref:`Metadata Routing User Guide\n      <metadata_routing>` for more details.\n\n    .. versionchanged:: 1.5\n        `**params` can now be routed via metadata routing API."
              }
            },
            "returns": "X_t : array-like or sparse matrix of                 shape (n_samples, sum_n_components)\n    The `hstack` of results of transformers. `sum_n_components` is the\n    sum of `n_components` (output dimension) over transformers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\n.. versionadded:: 1.5",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformer_list` of the\n`FeatureUnion`.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : mapping of string to any\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n`set_output` will set the output of all estimators in `transformer_list`.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged"
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **kwargs)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that\nyou can directly set the parameters of the estimators contained in\n`transformer_list`.",
            "parameters": {
              "**kwargs": {
                "type": "dict",
                "description": "Parameters of this estimator or parameters of estimators contained\n    in `transform_list`. Parameters of the transformers may be set\n    using its name and the parameter name separated by a '__'."
              }
            },
            "returns": "self : object\n    FeatureUnion class instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, **params)",
          "docstring": {
            "description": "Transform X separately by each transformer, concatenate results.",
            "parameters": {
              "X": {
                "type": "iterable or array-like, depending on transformers",
                "description": "Input data to be transformed."
              },
              "**params": {
                "type": "dict, default=None",
                "description": "Parameters routed to the `transform` method of the sub-transformers via the\n    metadata routing API. See :ref:`Metadata Routing User Guide\n    <metadata_routing>` for more details.\n\n    .. versionadded:: 1.5"
              }
            },
            "returns": "X_t : array-like or sparse matrix of shape (n_samples, sum_n_components)\n    The `hstack` of results of transformers. `sum_n_components` is the\n    sum of `n_components` (output dimension) over transformers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FunctionTransformer",
      "docstring": {
        "description": "Constructs a transformer from an arbitrary callable.\n\nA FunctionTransformer forwards its X (and optionally y) arguments to a\nuser-defined function or function object and returns the result of this\nfunction. This is useful for stateless transformations such as taking the\nlog of frequencies, doing custom scaling, etc.\n\nNote: If a lambda is used as the function, then the resulting\ntransformer will not be pickleable.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <function_transformer>`.",
        "parameters": {
          "func": {
            "type": "callable, default=None",
            "description": "The callable to use for the transformation. This will be passed\n    the same arguments as transform, with args and kwargs forwarded.\n    If func is None, then func will be the identity function."
          },
          "inverse_func": {
            "type": "callable, default=None",
            "description": "The callable to use for the inverse transformation. This will be\n    passed the same arguments as inverse transform, with args and\n    kwargs forwarded. If inverse_func is None, then inverse_func\n    will be the identity function."
          },
          "validate": {
            "type": "bool, default=False",
            "description": "Indicate that the input X array should be checked before calling\n    ``func``. The possibilities are:\n\n    - If False, there is no input validation.\n    - If True, then X will be converted to a 2-dimensional NumPy array or\n      sparse matrix. If the conversion is not possible an exception is\n      raised.\n\n    .. versionchanged:: 0.22\n       The default of ``validate`` changed from True to False."
          },
          "accept_sparse": {
            "type": "bool, default=False",
            "description": "Indicate that func accepts a sparse matrix as input. If validate is\n    False, this has no effect. Otherwise, if accept_sparse is false,\n    sparse matrix inputs will cause an exception to be raised."
          },
          "check_inverse": {
            "type": "bool, default=True",
            "description": "Whether to check that or ``func`` followed by ``inverse_func`` leads to\n   the original inputs. It can be used for a sanity check, raising a\n   warning when the condition is not fulfilled.\n\n   .. versionadded:: 0.20"
          },
          "feature_names_out": {
            "type": "callable, 'one-to-one' or None, default=None",
            "description": "Determines the list of feature names that will be returned by the\n    `get_feature_names_out` method. If it is 'one-to-one', then the output\n    feature names will be equal to the input feature names. If it is a\n    callable, then it must take two positional arguments: this\n    `FunctionTransformer` (`self`) and an array-like of input feature names\n    (`input_features`). It must return an array-like of output feature\n    names. The `get_feature_names_out` method is only defined if\n    `feature_names_out` is not None.\n\n    See ``get_feature_names_out`` for more details.\n\n    .. versionadded:: 1.1"
          },
          "kw_args": {
            "type": "dict, default=None",
            "description": "Dictionary of additional keyword arguments to pass to func.\n\n    .. versionadded:: 0.18"
          },
          "inv_kw_args": {
            "type": "dict, default=None",
            "description": "Dictionary of additional keyword arguments to pass to inverse_func.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X` has feature\n    names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "MaxAbsScaler : Scale each feature by its maximum absolute value.\nStandardScaler : Standardize features by removing the mean and\n    scaling to unit variance.\nLabelBinarizer : Binarize labels in a one-vs-all fashion.\nMultiLabelBinarizer : Transform between iterable of iterables\n    and a multilabel format.",
        "notes": "If `func` returns an output with a `columns` attribute, then the columns is enforced\nto be consistent with the output of `get_feature_names_out`.",
        "examples": ">>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> transformer.transform(X)\narray([[0.       , 0.6931...],\n       [1.0986..., 1.3862...]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "docstring": {
            "description": "Fit transformer by checking X.\n\nIf ``validate`` is ``True``, ``X`` will be checked.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse-matrix} of shape (n_samples, n_features)                 if `validate=True` else any object that `func` can handle",
                "description": "Input array."
              },
              "y": {
                "type": "Ignored",
                "description": "Not used, present here for API consistency by convention."
              }
            },
            "returns": "self : object\n    FunctionTransformer class instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.\n\nThis method is only defined if `feature_names_out` is not None.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input feature names.\n\n    - If `input_features` is None, then `feature_names_in_` is\n      used as the input feature names. If `feature_names_in_` is not\n      defined, then names are generated:\n      `[x0, x1, ..., x(n_features_in_ - 1)]`.\n    - If `input_features` is array-like, then `input_features` must\n      match `feature_names_in_` if `feature_names_in_` is defined."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.\n\n    - If `feature_names_out` is 'one-to-one', the input feature names\n      are returned (see `input_features` above). This requires\n      `feature_names_in_` and/or `n_features_in_` to be defined, which\n      is done automatically if `validate=True`. Alternatively, you can\n      set them in `func`.\n    - If `feature_names_out` is a callable, then it is called with two\n      arguments, `self` and `input_features`, and its return value is\n      returned by this method.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X)",
          "docstring": {
            "description": "Transform X using the inverse function.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse-matrix} of shape (n_samples, n_features)                 if `validate=True` else any object that `inverse_func` can handle",
                "description": "Input array."
              }
            },
            "returns": "X_out : array-like, shape (n_samples, n_features)\n    Transformed input.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "docstring": {
            "description": "Transform X using the forward function.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse-matrix} of shape (n_samples, n_features)                 if `validate=True` else any object that `func` can handle",
                "description": "Input array."
              }
            },
            "returns": "X_out : array-like, shape (n_samples, n_features)\n    Transformed input.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HasMethods",
      "docstring": {
        "description": "Constraint representing objects that expose specific methods.\n\nIt is useful for parameters following a protocol and where we don't want to impose\nan affiliation to a specific module or class.",
        "parameters": {
          "methods": {
            "type": "str or list of str",
            "description": "The method(s) that the object is expected to expose."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "docstring": {
            "description": "Whether or not a value satisfies the constraint.",
            "parameters": {
              "val": {
                "type": "object",
                "description": "The value to check."
              }
            },
            "returns": "is_satisfied : bool\n    Whether or not the constraint is satisfied by this value.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Hidden",
      "docstring": {
        "description": "Class encapsulating a constraint not meant to be exposed to the user.",
        "parameters": {
          "constraint": {
            "type": "str or _Constraint instance",
            "description": "The constraint to be used internally."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "MetadataRouter",
      "docstring": {
        "description": "Stores and handles metadata routing for a router object.\n\nThis class is used by router objects to store and handle metadata routing.\nRouting information is stored as a dictionary of the form ``{\"object_name\":\nRouteMappingPair(method_mapping, routing_info)}``, where ``method_mapping``\nis an instance of :class:`~sklearn.utils.metadata_routing.MethodMapping` and\n``routing_info`` is either a\n:class:`~sklearn.utils.metadata_routing.MetadataRequest` or a\n:class:`~sklearn.utils.metadata_routing.MetadataRouter` instance.\n\n.. versionadded:: 1.3",
        "parameters": {
          "owner": {
            "type": "str",
            "description": "The name of the object to which these requests belong."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self, *, method_mapping, **objs)",
          "docstring": {
            "description": "Add named objects with their corresponding method mapping.",
            "parameters": {
              "method_mapping": {
                "type": "MethodMapping",
                "description": "The mapping between the child and the parent's methods."
              },
              "**objs": {
                "type": "dict",
                "description": "A dictionary of objects from which metadata is extracted by calling\n    :func:`~sklearn.utils.metadata_routing.get_routing_for_object` on them."
              }
            },
            "returns": "self : MetadataRouter\n    Returns `self`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_self_request",
          "signature": "add_self_request(self, obj)",
          "docstring": {
            "description": "Add `self` (as a consumer) to the routing.\n\nThis method is used if the router is also a consumer, and hence the\nrouter itself needs to be included in the routing. The passed object\ncan be an estimator or a\n:class:`~sklearn.utils.metadata_routing.MetadataRequest`.\n\nA router should add itself using this method instead of `add` since it\nshould be treated differently than the other objects to which metadata\nis routed by the router.",
            "parameters": {
              "obj": {
                "type": "object",
                "description": "This is typically the router instance, i.e. `self` in a\n    ``get_metadata_routing()`` implementation. It can also be a\n    ``MetadataRequest`` instance."
              }
            },
            "returns": "self : MetadataRouter\n    Returns `self`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "consumes",
          "signature": "consumes(self, method, params)",
          "docstring": {
            "description": "Check whether the given parameters are consumed by the given method.\n\n.. versionadded:: 1.4",
            "parameters": {
              "method": {
                "type": "str",
                "description": "The name of the method to check."
              },
              "params": {
                "type": "iterable of str",
                "description": "An iterable of parameters to check."
              }
            },
            "returns": "consumed : set of str\n    A set of parameters which are consumed by the given method.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "route_params",
          "signature": "route_params(self, *, caller, params)",
          "docstring": {
            "description": "Return the input parameters requested by child objects.\n\nThe output of this method is a :class:`~sklearn.utils.Bunch`, which includes the\nmetadata for all methods of each child object that is used in the router's\n`caller` method.\n\nIf the router is also a consumer, it also checks for warnings of\n`self`'s/consumer's requested metadata.",
            "parameters": {
              "caller": {
                "type": "str",
                "description": "The name of the method for which the parameters are requested and\n    routed. If called inside the :term:`fit` method of a router, it\n    would be `\"fit\"`."
              },
              "params": {
                "type": "dict",
                "description": "A dictionary of provided metadata."
              }
            },
            "returns": "params : Bunch\n    A :class:`~sklearn.utils.Bunch` of the form\n    ``{\"object_name\": {\"method_name\": {params: value}}}`` which can be\n    used to pass the required metadata to corresponding methods or\n    corresponding child objects.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "validate_metadata",
          "signature": "validate_metadata(self, *, method, params)",
          "docstring": {
            "description": "Validate given metadata for a method.\n\nThis raises a ``TypeError`` if some of the passed metadata are not\nunderstood by child objects.",
            "parameters": {
              "method": {
                "type": "str",
                "description": "The name of the method for which the parameters are requested and\n    routed. If called inside the :term:`fit` method of a router, it\n    would be `\"fit\"`."
              },
              "params": {
                "type": "dict",
                "description": "A dictionary of provided metadata."
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MethodMapping",
      "docstring": {
        "description": "Stores the mapping between caller and callee methods for a router.\n\nThis class is primarily used in a ``get_metadata_routing()`` of a router\nobject when defining the mapping between the router's methods and a sub-object (a\nsub-estimator or a scorer).\n\nIterating through an instance of this class yields\n``MethodPair(caller, callee)`` instances.\n\n.. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self, *, caller, callee)",
          "docstring": {
            "description": "Add a method mapping.",
            "parameters": {
              "caller": {
                "type": "str",
                "description": "Parent estimator's method name in which the ``callee`` is called."
              },
              "callee": {
                "type": "str",
                "description": "Child object's method name. This method is called in ``caller``."
              }
            },
            "returns": "self : MethodMapping\n    Returns self.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NotFittedError",
      "docstring": {
        "description": "Exception class to raise if estimator is used before fitting.\n\nThis class inherits from both ValueError and AttributeError to help with\nexception handling and backward compatibility.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.svm import LinearSVC\n>>> from sklearn.exceptions import NotFittedError\n>>> try:\n...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])\n... except NotFittedError as e:\n...     print(repr(e))\nNotFittedError(\"This LinearSVC instance is not fitted yet. Call 'fit' with\nappropriate arguments before using this estimator.\"...)\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils.validation."
      },
      "methods": [
        {
          "name": "add_note",
          "signature": "add_note(...)",
          "docstring": {
            "description": "Exception.add_note(note) --\nadd a note to the exception",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_traceback",
          "signature": "with_traceback(...)",
          "docstring": {
            "description": "Exception.with_traceback(tb) --\nset self.__traceback__ to tb and return self.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Parallel",
      "docstring": {
        "description": "Tweak of :class:`joblib.Parallel` that propagates the scikit-learn configuration.\n\nThis subclass of :class:`joblib.Parallel` ensures that the active configuration\n(thread-local) of scikit-learn is propagated to the parallel workers for the\nduration of the execution of the parallel tasks.\n\nThe API does not change and you can refer to :class:`joblib.Parallel`\ndocumentation for more details.\n\n.. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "debug",
          "signature": "debug(self, msg)",
          "docstring": {}
        },
        {
          "name": "dispatch_next",
          "signature": "dispatch_next(self)",
          "docstring": {
            "description": "Dispatch more data for parallel processing\n\nThis method is meant to be called concurrently by the multiprocessing\ncallback. We rely on the thread-safety of dispatch_one_batch to protect\nagainst concurrent consumption of the unprotected iterator.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "dispatch_one_batch",
          "signature": "dispatch_one_batch(self, iterator)",
          "docstring": {
            "description": "Prefetch the tasks for the next batch and dispatch them.\n\nThe effective size of the batch is computed here.\nIf there are no more jobs to dispatch, return False, else return True.\n\nThe iterator consumption and dispatching is protected by the same\nlock so calling this function should be thread safe.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "format",
          "signature": "format(self, obj, indent=0)",
          "docstring": {
            "description": "Return the formatted representation of the object.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "info",
          "signature": "info(self, msg)",
          "docstring": {}
        },
        {
          "name": "print_progress",
          "signature": "print_progress(self)",
          "docstring": {
            "description": "Display the process of the parallel execution only a fraction\nof time, controlled by self.verbose.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "warn",
          "signature": "warn(self, msg)",
          "docstring": {}
        }
      ]
    },
    {
      "name": "Pipeline",
      "docstring": {
        "description": "A sequence of data transformers with an optional final predictor.\n\n`Pipeline` allows you to sequentially apply a list of transformers to\npreprocess the data and, if desired, conclude the sequence with a final\n:term:`predictor` for predictive modeling.\n\nIntermediate steps of the pipeline must be transformers, that is, they\nmust implement `fit` and `transform` methods.\nThe final :term:`estimator` only needs to implement `fit`.\nThe transformers in the pipeline can be cached using ``memory`` argument.\n\nThe purpose of the pipeline is to assemble several steps that can be\ncross-validated together while setting different parameters. For this, it\nenables setting parameters of the various steps using their names and the\nparameter name separated by a `'__'`, as in the example below. A step's\nestimator may be replaced entirely by setting the parameter with its name\nto another estimator, or a transformer removed by setting it to\n`'passthrough'` or `None`.\n\nFor an example use case of `Pipeline` combined with\n:class:`~sklearn.model_selection.GridSearchCV`, refer to\n:ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The\nexample :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how\nto grid search on a pipeline using `'__'` as a separator in the parameter names.\n\nRead more in the :ref:`User Guide <pipeline>`.\n\n.. versionadded:: 0.5",
        "parameters": {
          "steps": {
            "type": "list of tuples",
            "description": "List of (name of step, estimator) tuples that are to be chained in\n    sequential order. To be compatible with the scikit-learn API, all steps\n    must define `fit`. All non-last steps must also define `transform`. See\n    :ref:`Combining Estimators <combining_estimators>` for more details."
          },
          "transform_input": {
            "type": "list of str, default=None",
            "description": "The names of the :term:`metadata` parameters that should be transformed by the\n    pipeline before passing it to the step consuming it.\n\n    This enables transforming some input arguments to ``fit`` (other than ``X``)\n    to be transformed by the steps of the pipeline up to the step which requires\n    them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.\n    For instance, this can be used to pass a validation set through the pipeline.\n\n    You can only set this if metadata routing is enabled, which you\n    can enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n    .. versionadded:: 1.6"
          },
          "memory": {
            "type": "str or object with the joblib.Memory interface, default=None",
            "description": "Used to cache the fitted transformers of the pipeline. The last step\n    will never be cached, even if it is a transformer. By default, no\n    caching is performed. If a string is given, it is the path to the\n    caching directory. Enabling caching triggers a clone of the transformers\n    before fitting. Therefore, the transformer instance given to the\n    pipeline cannot be inspected directly. Use the attribute ``named_steps``\n    or ``steps`` to inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming. See\n    :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\n    for an example on how to enable caching."
          },
          "verbose": {
            "type": "bool, default=False",
            "description": "If True, the time elapsed while fitting each step will be printed as it\n    is completed.\n\nAttributes\n----------"
          },
          "named_steps": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Dictionary-like object, with the following attributes.\n    Read-only attribute to access any step parameter by user given name.\n    Keys are step names and values are steps parameters."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "The classes labels. Only exist if the last step of the pipeline is a\n    classifier."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`. Only defined if the\n    underlying first estimator in `steps` exposes such an attribute\n    when fit.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "make_pipeline : Convenience function for simplified pipeline construction.",
        "notes": "",
        "examples": ">>> from sklearn.svm import SVC\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import Pipeline\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=0)\n>>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n>>> # The pipeline can be used as any other estimator\n>>> # and avoids leaking the test set into the train set\n>>> pipe.fit(X_train, y_train).score(X_test, y_test)\n0.88\n>>> # An estimator's parameter can be set using '__' syntax\n>>> pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)\n0.76"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X, **params)",
          "docstring": {
            "description": "Transform the data, and apply `decision_function` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`decision_function` method. Only valid if the final estimator\nimplements `decision_function`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "**params": {
                "type": "dict of string -> object",
                "description": "Parameters requested and accepted by steps. Each step must have\n    requested certain metadata for these parameters to be forwarded to\n    them.\n\n    .. versionadded:: 1.4\n        Only available if `enable_metadata_routing=True`. See\n        :ref:`Metadata Routing User Guide <metadata_routing>` for more\n        details."
              }
            },
            "returns": "y_score : ndarray of shape (n_samples, n_classes)\n    Result of calling `decision_function` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "docstring": {
            "description": "Fit the model.\n\nFit all the transformers one after the other and sequentially transform the\ndata. Finally, fit the transformed data using the final estimator.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Training data. Must fulfill input requirements of first step of the\n    pipeline."
              },
              "y": {
                "type": "iterable, default=None",
                "description": "Training targets. Must fulfill label requirements for all steps of\n    the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters passed to the\n      ``fit`` method of each step, where each parameter name is prefixed such\n      that parameter ``p`` for step ``s`` has key ``s__p``.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True` is set via\n        :func:`~sklearn.set_config`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details."
              }
            },
            "returns": "self : object\n    Pipeline with fitted steps.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **params)",
          "docstring": {
            "description": "Transform the data, and apply `fit_predict` with the final estimator.\n\nCall `fit_transform` of each transformer in the pipeline. The\ntransformed data are finally passed to the final estimator that calls\n`fit_predict` method. Only valid if the final estimator implements\n`fit_predict`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Training data. Must fulfill input requirements of first step of\n    the pipeline."
              },
              "y": {
                "type": "iterable, default=None",
                "description": "Training targets. Must fulfill label requirements for all steps\n    of the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters to the\n      ``predict`` called at the end of all transformations in the pipeline.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details.\n\n    Note that while this may be used to return uncertainties from some\n    models with ``return_std`` or ``return_cov``, uncertainties that are\n    generated by the transformations in the pipeline are not propagated\n    to the final estimator."
              }
            },
            "returns": "y_pred : ndarray\n    Result of calling `fit_predict` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **params)",
          "docstring": {
            "description": "Fit the model and transform with the final estimator.\n\nFit all the transformers one after the other and sequentially transform\nthe data. Only valid if the final estimator either implements\n`fit_transform` or `fit` and `transform`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Training data. Must fulfill input requirements of first step of the\n    pipeline."
              },
              "y": {
                "type": "iterable, default=None",
                "description": "Training targets. Must fulfill label requirements for all steps of\n    the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters passed to the\n      ``fit`` method of each step, where each parameter name is prefixed such\n      that parameter ``p`` for step ``s`` has key ``s__p``.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details."
              }
            },
            "returns": "Xt : ndarray of shape (n_samples, n_transformed_features)\n    Transformed samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "docstring": {
            "description": "Get output feature names for transformation.\n\nTransform input features using the pipeline.",
            "parameters": {
              "input_features": {
                "type": "array-like of str or None, default=None",
                "description": "Input features."
              }
            },
            "returns": "feature_names_out : ndarray of str objects\n    Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRouter\n    A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `steps` of the `Pipeline`.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : mapping of string to any\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X=None, *, Xt=None, **params)",
          "docstring": {
            "description": "Apply `inverse_transform` for each step in a reverse order.\n\nAll estimators in the pipeline must support `inverse_transform`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_transformed_features)",
                "description": "Data samples, where ``n_samples`` is the number of samples and\n    ``n_features`` is the number of features. Must fulfill\n    input requirements of last step of pipeline's\n    ``inverse_transform`` method."
              },
              "Xt": {
                "type": "array-like of shape (n_samples, n_transformed_features)",
                "description": "Data samples, where ``n_samples`` is the number of samples and\n    ``n_features`` is the number of features. Must fulfill\n    input requirements of last step of pipeline's\n    ``inverse_transform`` method.\n\n    .. deprecated:: 1.5\n        `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "Parameters requested and accepted by steps. Each step must have\n    requested certain metadata for these parameters to be forwarded to\n    them.\n\n    .. versionadded:: 1.4\n        Only available if `enable_metadata_routing=True`. See\n        :ref:`Metadata Routing User Guide <metadata_routing>` for more\n        details."
              }
            },
            "returns": "Xt : ndarray of shape (n_samples, n_features)\n    Inverse transformed data, that is, data in the original feature\n    space.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, **params)",
          "docstring": {
            "description": "Transform the data, and apply `predict` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls `predict`\nmethod. Only valid if the final estimator implements `predict`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters to the\n      ``predict`` called at the end of all transformations in the pipeline.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True` is set via\n        :func:`~sklearn.set_config`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details.\n\n    Note that while this may be used to return uncertainties from some\n    models with ``return_std`` or ``return_cov``, uncertainties that are\n    generated by the transformations in the pipeline are not propagated\n    to the final estimator."
              }
            },
            "returns": "y_pred : ndarray\n    Result of calling `predict` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X, **params)",
          "docstring": {
            "description": "Transform the data, and apply `predict_log_proba` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`predict_log_proba` method. Only valid if the final estimator\nimplements `predict_log_proba`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters to the\n      `predict_log_proba` called at the end of all transformations in the\n      pipeline.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details."
              }
            },
            "returns": "y_log_proba : ndarray of shape (n_samples, n_classes)\n    Result of calling `predict_log_proba` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X, **params)",
          "docstring": {
            "description": "Transform the data, and apply `predict_proba` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`predict_proba` method. Only valid if the final estimator implements\n`predict_proba`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "- If `enable_metadata_routing=False` (default): Parameters to the\n      `predict_proba` called at the end of all transformations in the pipeline.\n\n    - If `enable_metadata_routing=True`: Parameters requested and accepted by\n      steps. Each step must have requested certain metadata for these parameters\n      to be forwarded to them.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 1.4\n        Parameters are now passed to the ``transform`` method of the\n        intermediate steps as well, if requested, and if\n        `enable_metadata_routing=True`.\n\n    See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n    details."
              }
            },
            "returns": "y_proba : ndarray of shape (n_samples, n_classes)\n    Result of calling `predict_proba` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None, sample_weight=None, **params)",
          "docstring": {
            "description": "Transform the data, and apply `score` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`score` method. Only valid if the final estimator implements `score`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "y": {
                "type": "iterable, default=None",
                "description": "Targets used for scoring. Must fulfill label requirements for all\n    steps of the pipeline."
              },
              "sample_weight": {
                "type": "array-like, default=None",
                "description": "If not None, this argument is passed as ``sample_weight`` keyword\n    argument to the ``score`` method of the final estimator."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "Parameters requested and accepted by steps. Each step must have\n    requested certain metadata for these parameters to be forwarded to\n    them.\n\n    .. versionadded:: 1.4\n        Only available if `enable_metadata_routing=True`. See\n        :ref:`Metadata Routing User Guide <metadata_routing>` for more\n        details."
              }
            },
            "returns": "score : float\n    Result of calling `score` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "docstring": {
            "description": "Transform the data, and apply `score_samples` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`score_samples` method. Only valid if the final estimator implements\n`score_samples`.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to predict on. Must fulfill input requirements of first step\n    of the pipeline."
              }
            },
            "returns": "y_score : ndarray of shape (n_samples,)\n    Result of calling `score_samples` on the final estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\nCalling `set_output` will set the output of all estimators in `steps`.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **kwargs)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that\nyou can directly set the parameters of the estimators contained in\n`steps`.",
            "parameters": {
              "**kwargs": {
                "type": "dict",
                "description": "Parameters of this estimator or parameters of estimators contained\n    in `steps`. Parameters of the steps may be set using its name and\n    the parameter name separated by a '__'."
              }
            },
            "returns": "self : object\n    Pipeline class instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.pipeline.Pipeline, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.pipeline.Pipeline",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, **params)",
          "docstring": {
            "description": "Transform the data, and apply `transform` with the final estimator.\n\nCall `transform` of each transformer in the pipeline. The transformed\ndata are finally passed to the final estimator that calls\n`transform` method. Only valid if the final estimator\nimplements `transform`.\n\nThis also works where final estimator is `None` in which case all prior\ntransformations are applied.",
            "parameters": {
              "X": {
                "type": "iterable",
                "description": "Data to transform. Must fulfill input requirements of first step\n    of the pipeline."
              },
              "**params": {
                "type": "dict of str -> object",
                "description": "Parameters requested and accepted by steps. Each step must have\n    requested certain metadata for these parameters to be forwarded to\n    them.\n\n    .. versionadded:: 1.4\n        Only available if `enable_metadata_routing=True`. See\n        :ref:`Metadata Routing User Guide <metadata_routing>` for more\n        details."
              }
            },
            "returns": "Xt : ndarray of shape (n_samples, n_transformed_features)\n    Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TransformerMixin",
      "docstring": {
        "description": "Mixin class for all transformers in scikit-learn.\n\nThis mixin defines the following functionality:\n\n- a `fit_transform` method that delegates to `fit` and `transform`;\n- a `set_output` method to output `X` as a specific container type.\n\nIf :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\nautomatically wrap `transform` and `fit_transform` to follow the `set_output`\nAPI. See the :ref:`developer_api_set_output` for details.\n\n:class:`OneToOneFeatureMixin` and\n:class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\ndefining :term:`get_feature_names_out`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator, TransformerMixin\n>>> class MyTransformer(TransformerMixin, BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         return self\n...     def transform(self, X):\n...         return np.full(shape=len(X), fill_value=self.param)\n>>> transformer = MyTransformer()\n>>> X = [[1, 2], [2, 3], [3, 4]]\n>>> transformer.fit_transform(X)\narray([1, 1, 1])"
      },
      "methods": [
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "docstring": {
            "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Input samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None",
                "description": "Target values (None for unsupervised transformations)."
              },
              "**fit_params": {
                "type": "dict",
                "description": "Additional fit parameters."
              }
            },
            "returns": "X_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "chain",
      "docstring": {
        "description": "chain(*iterables) --> chain object\n\nReturn a chain object whose .__next__() method returns elements from the\nfirst iterable until it is exhausted, then elements from the next\niterable, until all of the iterables are exhausted.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "from_iterable",
          "signature": "from_iterable(iterable, /)",
          "docstring": {
            "description": "Alternative chain() constructor taking a single iterable argument that evaluates lazily.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "defaultdict",
      "docstring": {
        "description": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "clear",
          "signature": "clear(...)",
          "docstring": {
            "description": "D.clear() -> None.  Remove all items from D.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy",
          "signature": "copy(...)",
          "docstring": {
            "description": "D.copy() -> a shallow copy of D.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fromkeys",
          "signature": "fromkeys(iterable, value=None, /)",
          "docstring": {
            "description": "Create a new dictionary with keys from iterable and values set to value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self, key, default=None, /)",
          "docstring": {
            "description": "Return the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "items",
          "signature": "items(...)",
          "docstring": {
            "description": "D.items() -> a set-like object providing a view on D's items",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "keys",
          "signature": "keys(...)",
          "docstring": {
            "description": "D.keys() -> a set-like object providing a view on D's keys",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "pop",
          "signature": "pop(...)",
          "docstring": {
            "description": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\nIf the key is not found, return the default if given; otherwise,\nraise a KeyError.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "popitem",
          "signature": "popitem(self, /)",
          "docstring": {
            "description": "Remove and return a (key, value) pair as a 2-tuple.\n\nPairs are returned in LIFO (last-in, first-out) order.\nRaises KeyError if the dict is empty.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setdefault",
          "signature": "setdefault(self, key, default=None, /)",
          "docstring": {
            "description": "Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update",
          "signature": "update(...)",
          "docstring": {
            "description": "D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\nIf E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\nIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\nIn either case, this is followed by: for k in F:  D[k] = F[k]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "values",
          "signature": "values(...)",
          "docstring": {
            "description": "D.values() -> an object providing a view on D's values",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "islice",
      "docstring": {
        "description": "islice(iterable, stop) --> islice object\nislice(iterable, start, stop[, step]) --> islice object\n\nReturn an iterator whose next() method returns selected values from an\niterable.  If start is specified, will skip all preceding elements;\notherwise, start defaults to zero.  Step defaults to one.  If\nspecified as another value, step determines how many values are\nskipped between successive calls.  Works like a slice() on a list\nbut returns an iterator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    }
  ],
  "constants": [
    {
      "name": "METHODS",
      "value": "['fit', 'partial_fit', 'predict', 'predict_proba', 'predict_log_proba', 'decision_function', 'score', 'split', 'transform', 'inverse_transform', 'fit_transform', 'fit_predict']",
      "docstring": {
        "description": "Built-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ]
}