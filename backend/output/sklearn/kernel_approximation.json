{
  "description": "Approximate kernel feature maps based on Fourier transforms and count sketches.",
  "functions": [
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "documentation": {
        "description": "Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "Estimator": {
            "type": "instance for which the check is performed.",
            "description": ""
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": ""
          },
          "Attribute": {
            "type": "name(s) given as string or a list/tuple of strings",
            "description": "Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"
          },
          "If": {
            "type": "the attributes are not found.",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")"
          },
          "attribute": {
            "type": "that ends with a underscore and does not start with double",
            "description": "underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": ""
          },
          "The": {
            "type": "default error message is, \"This %(name)s instance is not fitted",
            "description": "yet. Call 'fit' with appropriate arguments before using this\nestimator.\""
          },
          "For": {
            "type": "custom messages if \"%(name)s\" is present in the message string,",
            "description": ""
          },
          "it": {
            "type": "is substituted for the estimator name.",
            "description": "Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": ""
          },
          "Specify": {
            "type": "whether all or any of the given attributes must exist.",
            "description": "Raises\n------\nTypeError"
          },
          "Model": {
            "type": "is not fitted yet.",
            "description": ">>> lr.fit([[1, 2], [1, 3]], [1, 0])"
          },
          "LogisticRegression": {
            "type": "",
            "description": ">>> check_is_fitted(lr)"
          }
        },
        "returns": "",
        "raises": "a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)"
      }
    },
    {
      "name": "check_random_state",
      "signature": "check_random_state(seed)",
      "documentation": {
        "description": "Turn seed into a np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.",
        "parameters": {
          "seed": {
            "type": "None, int or instance of RandomState",
            "description": ""
          },
          "If": {
            "type": "seed is already a RandomState instance, return it.",
            "description": ""
          },
          "Otherwise": {
            "type": "raise ValueError.",
            "description": "Returns\n-------\n:class:`numpy:numpy.random.RandomState`"
          },
          "The": {
            "type": "random state object based on `seed` parameter.",
            "description": "Examples\n--------\n>>> from sklearn.utils.validation import check_random_state\n>>> check_random_state(42)"
          },
          "RandomState": {
            "type": "MT19937",
            "description": "at 0x..."
          }
        },
        "returns": "-------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x..."
      }
    },
    {
      "name": "fft",
      "signature": "fft(x, n=None, axis=-1, norm=None, overwrite_x=False, workers=None, *, plan=None)",
      "documentation": {
        "description": "Compute the 1-D discrete Fourier Transform.\n\n    This function computes the 1-D *n*-point discrete Fourier\n    Transform (DFT) with the efficient Fast Fourier Transform (FFT)\n    algorithm [1]_.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array, can be complex.\n    n : int, optional\n        Length of the transformed axis of the output.\n        If `n` is smaller than the length of the input, the input is cropped.\n        If it is larger, the input is padded with zeros. If `n` is not given,\n        the length of the input along the axis specified by `axis` is used.\n    axis : int, optional\n        Axis over which to compute the FFT. If not given, the last axis is\n        used.\n    norm : {\"backward\", \"ortho\", \"forward\"}, optional\n        Normalization mode. Default is \"backward\", meaning no normalization on\n        the forward transforms and scaling by ``1/n`` on the `ifft`.\n        \"forward\" instead applies the ``1/n`` factor on the forward transform.\n        For ``norm=\"ortho\"``, both directions are scaled by ``1/sqrt(n)``.\n\n        .. versionadded:: 1.6.0\n           ``norm={\"forward\", \"backward\"}`` options were added\n\n    overwrite_x : bool, optional\n        If True, the contents of `x` can be destroyed; the default is False.\n        See the notes below for more details.\n    workers : int, optional\n        Maximum number of workers to use for parallel computation. If negative,\n        the value wraps around from ``os.cpu_count()``. See below for more\n        details.\n    plan : object, optional\n        This argument is reserved for passing in a precomputed plan provided\n        by downstream FFT vendors. It is currently not used in SciPy.\n\n        .. versionadded:: 1.5.0\n\n    Returns\n    -------\n    out : complex ndarray\n        The truncated or zero-padded input, transformed along the axis\n        indicated by `axis`, or the last one if `axis` is not specified.\n\n    Raises\n    ------\n    IndexError\n        if `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    ifft : The inverse of `fft`.\n    fft2 : The 2-D FFT.\n    fftn : The N-D FFT.\n    rfftn : The N-D FFT of real input.\n    fftfreq : Frequency bins for given FFT parameters.\n    next_fast_len : Size to pad input to for most efficient transforms\n\n    Notes\n    -----\n    FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform\n    (DFT) can be calculated efficiently, by using symmetries in the calculated\n    terms. The symmetry is highest when `n` is a power of 2, and the transform\n    is therefore most efficient for these sizes. For poorly factorizable sizes,\n    `scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than\n    O(`n` log `n`). Further performance improvements may be seen by zero-padding\n    the input using `next_fast_len`.\n\n    If ``x`` is a 1d array, then the `fft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(-2j * np.pi * k * np.arange(n)/n))\n\n    The frequency term ``f=k/n`` is found at ``y[k]``. At ``y[n/2]`` we reach\n    the Nyquist frequency and wrap around to the negative-frequency terms. So,\n    for an 8-point transform, the frequencies of the result are\n    [0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\n    zero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],\n    use `fftshift`.\n\n    Transforms can be done in single, double, or extended precision (long\n    double) floating point. Half precision inputs will be converted to single\n    precision and non-floating-point inputs will be converted to double\n    precision.\n\n    If the data type of ``x`` is real, a \"real FFT\" algorithm is automatically\n    used, which roughly halves the computation time. To increase efficiency\n    a little further, use `rfft`, which does the same calculation, but only\n    outputs half of the symmetrical spectrum. If the data are both real and\n    symmetrical, the `dct` can again double the efficiency, by generating\n    half of the spectrum from half of the signal.\n\n    When ``overwrite_x=True`` is specified, the memory referenced by ``x`` may\n    be used by the implementation in any way. This may include reusing the\n    memory for the result, but this is in no way guaranteed. You should not\n    rely on the contents of ``x`` after the transform as this may change in\n    future without warning.\n\n    The ``workers`` argument specifies the maximum number of parallel jobs to\n    split the FFT computation into. This will execute independent 1-D\n    FFTs within ``x``. So, ``x`` must be at least 2-D and the\n    non-transformed axes must be large enough to split into chunks. If ``x`` is\n    too small, fewer jobs may be used than requested.\n\n    References\n    ----------\n    .. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the\n           machine calculation of complex Fourier series,\" *Math. Comput.*\n           19: 297-301.\n    .. [2] Bluestein, L., 1970, \"A linear filtering approach to the\n           computation of discrete Fourier transform\". *IEEE Transactions on\n           Audio and Electroacoustics.* 18 (4): 451-455.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": ""
          },
          "Input": {
            "type": "array, can be complex.",
            "description": ""
          },
          "n": {
            "type": "int, optional",
            "description": ""
          },
          "Length": {
            "type": "of the transformed axis of the output.",
            "description": ""
          },
          "If": {
            "type": "the data type of ``x`` is real, a \"real FFT\" algorithm is automatically",
            "description": "used, which roughly halves the computation time. To increase efficiency"
          },
          "the": {
            "type": "Nyquist frequency and wrap around to the negative-frequency terms. So,",
            "description": ""
          },
          "axis": {
            "type": "int, optional",
            "description": ""
          },
          "Axis": {
            "type": "over which to compute the FFT. If not given, the last axis is",
            "description": "used."
          },
          "norm": {
            "type": "{\"backward\", \"ortho\", \"forward\"}, optional",
            "description": ""
          },
          "Normalization": {
            "type": "mode. Default is \"backward\", meaning no normalization on",
            "description": ""
          },
          "For": {
            "type": "``norm=\"ortho\"``, both directions are scaled by ``1/sqrt(n)``.",
            "description": ".. versionadded:: 1.6.0\n``norm={\"forward\", \"backward\"}`` options were added"
          },
          "overwrite_x": {
            "type": "bool, optional",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "workers": {
            "type": "int, optional",
            "description": ""
          },
          "Maximum": {
            "type": "number of workers to use for parallel computation. If negative,",
            "description": ""
          },
          "plan": {
            "type": "object, optional",
            "description": ""
          },
          "This": {
            "type": "argument is reserved for passing in a precomputed plan provided",
            "description": ""
          },
          "by": {
            "type": "downstream FFT vendors. It is currently not used in SciPy.",
            "description": ".. versionadded:: 1.5.0\nReturns\n-------"
          },
          "out": {
            "type": "complex ndarray",
            "description": ""
          },
          "The": {
            "type": "``workers`` argument specifies the maximum number of parallel jobs to",
            "description": ""
          },
          "indicated": {
            "type": "by `axis`, or the last one if `axis` is not specified.",
            "description": "Raises\n------\nIndexError"
          },
          "if": {
            "type": "`axes` is larger than the last axis of `x`.",
            "description": ""
          },
          "ifft": {
            "type": "The inverse of `fft`.",
            "description": ""
          },
          "fft2": {
            "type": "The 2",
            "description": "D FFT."
          },
          "fftn": {
            "type": "The N",
            "description": "D FFT."
          },
          "rfftn": {
            "type": "The N",
            "description": "D FFT of real input."
          },
          "fftfreq": {
            "type": "Frequency bins for given FFT parameters.",
            "description": ""
          },
          "next_fast_len": {
            "type": "Size to pad input to for most efficient transforms",
            "description": "Notes\n-----"
          },
          "FFT": {
            "type": "Fast Fourier Transform",
            "description": "refers to a way the discrete Fourier Transform\n(DFT) can be calculated efficiently, by using symmetries in the calculated\nterms. The symmetry is highest when `n` is a power of 2, and the transform"
          },
          "is": {
            "type": "therefore most efficient for these sizes. For poorly factorizable sizes,",
            "description": "`scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than"
          },
          "O": {
            "type": "`n` log `n`",
            "description": ". Further performance improvements may be seen by zero-padding"
          },
          "for": {
            "type": "an 8-point transform, the frequencies of the result are",
            "description": "[0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\nzero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],"
          },
          "use": {
            "type": "`fftshift`.",
            "description": ""
          },
          "Transforms": {
            "type": "can be done in single, double, or extended precision (long",
            "description": "double) floating point. Half precision inputs will be converted to single"
          },
          "precision": {
            "type": "and non-floating-point inputs will be converted to double",
            "description": "precision."
          },
          "a": {
            "type": "little further, use `rfft`, which does the same calculation, but only",
            "description": ""
          },
          "outputs": {
            "type": "half of the symmetrical spectrum. If the data are both real and",
            "description": "symmetrical, the `dct` can again double the efficiency, by generating"
          },
          "half": {
            "type": "of the spectrum from half of the signal.",
            "description": ""
          },
          "When": {
            "type": "``overwrite_x=True`` is specified, the memory referenced by ``x`` may",
            "description": ""
          },
          "be": {
            "type": "used by the implementation in any way. This may include reusing the",
            "description": ""
          },
          "memory": {
            "type": "for the result, but this is in no way guaranteed. You should not",
            "description": ""
          },
          "rely": {
            "type": "on the contents of ``x`` after the transform as this may change in",
            "description": ""
          },
          "future": {
            "type": "without warning.",
            "description": ""
          },
          "split": {
            "type": "the FFT computation into. This will execute independent 1-D",
            "description": ""
          },
          "FFTs": {
            "type": "within ``x``. So, ``x`` must be at least 2-D and the",
            "description": "non-transformed axes must be large enough to split into chunks. If ``x`` is"
          },
          "too": {
            "type": "small, fewer jobs may be used than requested.",
            "description": "References\n----------\n.. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the"
          },
          "machine": {
            "type": "calculation of complex Fourier series,\" *Math. Comput.*",
            "description": ""
          },
          "19": {
            "type": "297",
            "description": "301.\n.. [2] Bluestein, L., 1970, \"A linear filtering approach to the"
          },
          "computation": {
            "type": "of discrete Fourier transform\". *IEEE Transactions on",
            "description": ""
          },
          "Audio": {
            "type": "and Electroacoustics.* 18 (4): 451-455.",
            "description": "Examples\n--------\n>>> import scipy.fft\n>>> import numpy as np\n>>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\narray([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n-1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])"
          },
          "In": {
            "type": "this example, real input has an FFT which is Hermitian, i.e., symmetric",
            "description": ""
          },
          "in": {
            "type": "the real part and anti-symmetric in the imaginary part:",
            "description": ">>> from scipy.fft import fft, fftfreq, fftshift\n>>> import matplotlib.pyplot as plt\n>>> t = np.arange(256)\n>>> sp = fftshift(fft(np.sin(t)))\n>>> freq = fftshift(fftfreq(t.shape[-1]))\n>>> plt.plot(freq, sp.real, freq, sp.imag)\n[<matplotlib.lines.Line2D object at 0x...>,\n<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.show()"
          }
        },
        "returns": "-------\n    out : complex ndarray\n        The truncated or zero-padded input, transformed along the axis\n        indicated by `axis`, or the last one if `axis` is not specified.\n\n    Raises\n    ------\n    IndexError\n        if `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    ifft : The inverse of `fft`.\n    fft2 : The 2-D FFT.\n    fftn : The N-D FFT.\n    rfftn : The N-D FFT of real input.\n    fftfreq : Frequency bins for given FFT parameters.\n    next_fast_len : Size to pad input to for most efficient transforms\n\n    Notes\n    -----\n    FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform\n    (DFT) can be calculated efficiently, by using symmetries in the calculated\n    terms. The symmetry is highest when `n` is a power of 2, and the transform\n    is therefore most efficient for these sizes. For poorly factorizable sizes,\n    `scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than\n    O(`n` log `n`). Further performance improvements may be seen by zero-padding\n    the input using `next_fast_len`.\n\n    If ``x`` is a 1d array, then the `fft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(-2j * np.pi * k * np.arange(n)/n))\n\n    The frequency term ``f=k/n`` is found at ``y[k]``. At ``y[n/2]`` we reach\n    the Nyquist frequency and wrap around to the negative-frequency terms. So,\n    for an 8-point transform, the frequencies of the result are\n    [0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\n    zero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],\n    use `fftshift`.\n\n    Transforms can be done in single, double, or extended precision (long\n    double) floating point. Half precision inputs will be converted to single\n    precision and non-floating-point inputs will be converted to double\n    precision.\n\n    If the data type of ``x`` is real, a \"real FFT\" algorithm is automatically\n    used, which roughly halves the computation time. To increase efficiency\n    a little further, use `rfft`, which does the same calculation, but only\n    outputs half of the symmetrical spectrum. If the data are both real and\n    symmetrical, the `dct` can again double the efficiency, by generating\n    half of the spectrum from half of the signal.\n\n    When ``overwrite_x=True`` is specified, the memory referenced by ``x`` may\n    be used by the implementation in any way. This may include reusing the\n    memory for the result, but this is in no way guaranteed. You should not\n    rely on the contents of ``x`` after the transform as this may change in\n    future without warning.\n\n    The ``workers`` argument specifies the maximum number of parallel jobs to\n    split the FFT computation into. This will execute independent 1-D\n    FFTs within ``x``. So, ``x`` must be at least 2-D and the\n    non-transformed axes must be large enough to split into chunks. If ``x`` is\n    too small, fewer jobs may be used than requested.\n\n    References\n    ----------\n    .. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the\n           machine calculation of complex Fourier series,\" *Math. Comput.*\n           19: 297-301.\n    .. [2] Bluestein, L., 1970, \"A linear filtering approach to the\n           computation of discrete Fourier transform\". *IEEE Transactions on\n           Audio and Electroacoustics.* 18 (4): 451-455.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n    array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n            2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n           -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n            1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n    In this example, real input has an FFT which is Hermitian, i.e., symmetric\n    in the real part and anti-symmetric in the imaginary part:\n\n    >>> from scipy.fft import fft, fftfreq, fftshift\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.arange(256)\n    >>> sp = fftshift(fft(np.sin(t)))\n    >>> freq = fftshift(fftfreq(t.shape[-1]))\n    >>> plt.plot(freq, sp.real, freq, sp.imag)\n    [<matplotlib.lines.Line2D object at 0x...>,\n     <matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()",
        "raises": "------\n    IndexError\n        if `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    ifft : The inverse of `fft`.\n    fft2 : The 2-D FFT.\n    fftn : The N-D FFT.\n    rfftn : The N-D FFT of real input.\n    fftfreq : Frequency bins for given FFT parameters.\n    next_fast_len : Size to pad input to for most efficient transforms\n\n    Notes\n    -----\n    FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform\n    (DFT) can be calculated efficiently, by using symmetries in the calculated\n    terms. The symmetry is highest when `n` is a power of 2, and the transform\n    is therefore most efficient for these sizes. For poorly factorizable sizes,\n    `scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than\n    O(`n` log `n`). Further performance improvements may be seen by zero-padding\n    the input using `next_fast_len`.\n\n    If ``x`` is a 1d array, then the `fft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(-2j * np.pi * k * np.arange(n)/n))\n\n    The frequency term ``f=k/n`` is found at ``y[k]``. At ``y[n/2]`` we reach\n    the Nyquist frequency and wrap around to the negative-frequency terms. So,\n    for an 8-point transform, the frequencies of the result are\n    [0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\n    zero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],\n    use `fftshift`.\n\n    Transforms can be done in single, double, or extended precision (long\n    double) floating point. Half precision inputs will be converted to single\n    precision and non-floating-point inputs will be converted to double\n    precision.\n\n    If the data type of ``x`` is real, a \"real FFT\" algorithm is automatically\n    used, which roughly halves the computation time. To increase efficiency\n    a little further, use `rfft`, which does the same calculation, but only\n    outputs half of the symmetrical spectrum. If the data are both real and\n    symmetrical, the `dct` can again double the efficiency, by generating\n    half of the spectrum from half of the signal.\n\n    When ``overwrite_x=True`` is specified, the memory referenced by ``x`` may\n    be used by the implementation in any way. This may include reusing the\n    memory for the result, but this is in no way guaranteed. You should not\n    rely on the contents of ``x`` after the transform as this may change in\n    future without warning.\n\n    The ``workers`` argument specifies the maximum number of parallel jobs to\n    split the FFT computation into. This will execute independent 1-D\n    FFTs within ``x``. So, ``x`` must be at least 2-D and the\n    non-transformed axes must be large enough to split into chunks. If ``x`` is\n    too small, fewer jobs may be used than requested.\n\n    References\n    ----------\n    .. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the\n           machine calculation of complex Fourier series,\" *Math. Comput.*\n           19: 297-301.\n    .. [2] Bluestein, L., 1970, \"A linear filtering approach to the\n           computation of discrete Fourier transform\". *IEEE Transactions on\n           Audio and Electroacoustics.* 18 (4): 451-455.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n    array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n            2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n           -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n            1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n    In this example, real input has an FFT which is Hermitian, i.e., symmetric\n    in the real part and anti-symmetric in the imaginary part:\n\n    >>> from scipy.fft import fft, fftfreq, fftshift\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.arange(256)\n    >>> sp = fftshift(fft(np.sin(t)))\n    >>> freq = fftshift(fftfreq(t.shape[-1]))\n    >>> plt.plot(freq, sp.real, freq, sp.imag)\n    [<matplotlib.lines.Line2D object at 0x...>,\n     <matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()",
        "see_also": "--------\n    ifft : The inverse of `fft`.\n    fft2 : The 2-D FFT.\n    fftn : The N-D FFT.\n    rfftn : The N-D FFT of real input.\n    fftfreq : Frequency bins for given FFT parameters.\n    next_fast_len : Size to pad input to for most efficient transforms\n\n    Notes\n    -----\n    FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform\n    (DFT) can be calculated efficiently, by using symmetries in the calculated\n    terms. The symmetry is highest when `n` is a power of 2, and the transform\n    is therefore most efficient for these sizes. For poorly factorizable sizes,\n    `scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than\n    O(`n` log `n`). Further performance improvements may be seen by zero-padding\n    the input using `next_fast_len`.\n\n    If ``x`` is a 1d array, then the `fft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(-2j * np.pi * k * np.arange(n)/n))\n\n    The frequency term ``f=k/n`` is found at ``y[k]``. At ``y[n/2]`` we reach\n    the Nyquist frequency and wrap around to the negative-frequency terms. So,\n    for an 8-point transform, the frequencies of the result are\n    [0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\n    zero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],\n    use `fftshift`.\n\n    Transforms can be done in single, double, or extended precision (long\n    double) floating point. Half precision inputs will be converted to single\n    precision and non-floating-point inputs will be converted to double\n    precision.\n\n    If the data type of ``x`` is real, a \"real FFT\" algorithm is automatically\n    used, which roughly halves the computation time. To increase efficiency\n    a little further, use `rfft`, which does the same calculation, but only\n    outputs half of the symmetrical spectrum. If the data are both real and\n    symmetrical, the `dct` can again double the efficiency, by generating\n    half of the spectrum from half of the signal.\n\n    When ``overwrite_x=True`` is specified, the memory referenced by ``x`` may\n    be used by the implementation in any way. This may include reusing the\n    memory for the result, but this is in no way guaranteed. You should not\n    rely on the contents of ``x`` after the transform as this may change in\n    future without warning.\n\n    The ``workers`` argument specifies the maximum number of parallel jobs to\n    split the FFT computation into. This will execute independent 1-D\n    FFTs within ``x``. So, ``x`` must be at least 2-D and the\n    non-transformed axes must be large enough to split into chunks. If ``x`` is\n    too small, fewer jobs may be used than requested.\n\n    References\n    ----------\n    .. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the\n           machine calculation of complex Fourier series,\" *Math. Comput.*\n           19: 297-301.\n    .. [2] Bluestein, L., 1970, \"A linear filtering approach to the\n           computation of discrete Fourier transform\". *IEEE Transactions on\n           Audio and Electroacoustics.* 18 (4): 451-455.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n    array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n            2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n           -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n            1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n    In this example, real input has an FFT which is Hermitian, i.e., symmetric\n    in the real part and anti-symmetric in the imaginary part:\n\n    >>> from scipy.fft import fft, fftfreq, fftshift\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.arange(256)\n    >>> sp = fftshift(fft(np.sin(t)))\n    >>> freq = fftshift(fftfreq(t.shape[-1]))\n    >>> plt.plot(freq, sp.real, freq, sp.imag)\n    [<matplotlib.lines.Line2D object at 0x...>,\n     <matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()",
        "notes": "-----\n    FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform\n    (DFT) can be calculated efficiently, by using symmetries in the calculated\n    terms. The symmetry is highest when `n` is a power of 2, and the transform\n    is therefore most efficient for these sizes. For poorly factorizable sizes,\n    `scipy.fft` uses Bluestein's algorithm [2]_ and so is never worse than\n    O(`n` log `n`). Further performance improvements may be seen by zero-padding\n    the input using `next_fast_len`.\n\n    If ``x`` is a 1d array, then the `fft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(-2j * np.pi * k * np.arange(n)/n))\n\n    The frequency term ``f=k/n`` is found at ``y[k]``. At ``y[n/2]`` we reach\n    the Nyquist frequency and wrap around to the negative-frequency terms. So,\n    for an 8-point transform, the frequencies of the result are\n    [0, 1, 2, 3, -4, -3, -2, -1]. To rearrange the fft output so that the\n    zero-frequency component is centered, like [-4, -3, -2, -1, 0, 1, 2, 3],\n    use `fftshift`.\n\n    Transforms can be done in single, double, or extended precision (long\n    double) floating point. Half precision inputs will be converted to single\n    precision and non-floating-point inputs will be converted to double\n    precision.\n\n    If the data type of ``x`` is real, a \"real FFT\" algorithm is automatically\n    used, which roughly halves the computation time. To increase efficiency\n    a little further, use `rfft`, which does the same calculation, but only\n    outputs half of the symmetrical spectrum. If the data are both real and\n    symmetrical, the `dct` can again double the efficiency, by generating\n    half of the spectrum from half of the signal.\n\n    When ``overwrite_x=True`` is specified, the memory referenced by ``x`` may\n    be used by the implementation in any way. This may include reusing the\n    memory for the result, but this is in no way guaranteed. You should not\n    rely on the contents of ``x`` after the transform as this may change in\n    future without warning.\n\n    The ``workers`` argument specifies the maximum number of parallel jobs to\n    split the FFT computation into. This will execute independent 1-D\n    FFTs within ``x``. So, ``x`` must be at least 2-D and the\n    non-transformed axes must be large enough to split into chunks. If ``x`` is\n    too small, fewer jobs may be used than requested.\n\n    References\n    ----------\n    .. [1] Cooley, James W., and John W. Tukey, 1965, \"An algorithm for the\n           machine calculation of complex Fourier series,\" *Math. Comput.*\n           19: 297-301.\n    .. [2] Bluestein, L., 1970, \"A linear filtering approach to the\n           computation of discrete Fourier transform\". *IEEE Transactions on\n           Audio and Electroacoustics.* 18 (4): 451-455.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n    array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n            2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n           -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n            1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n    In this example, real input has an FFT which is Hermitian, i.e., symmetric\n    in the real part and anti-symmetric in the imaginary part:\n\n    >>> from scipy.fft import fft, fftfreq, fftshift\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.arange(256)\n    >>> sp = fftshift(fft(np.sin(t)))\n    >>> freq = fftshift(fftfreq(t.shape[-1]))\n    >>> plt.plot(freq, sp.real, freq, sp.imag)\n    [<matplotlib.lines.Line2D object at 0x...>,\n     <matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()",
        "examples": "--------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n    array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n            2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n           -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n            1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n    In this example, real input has an FFT which is Hermitian, i.e., symmetric\n    in the real part and anti-symmetric in the imaginary part:\n\n    >>> from scipy.fft import fft, fftfreq, fftshift\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.arange(256)\n    >>> sp = fftshift(fft(np.sin(t)))\n    >>> freq = fftshift(fftfreq(t.shape[-1]))\n    >>> plt.plot(freq, sp.real, freq, sp.imag)\n    [<matplotlib.lines.Line2D object at 0x...>,\n     <matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()"
      }
    },
    {
      "name": "ifft",
      "signature": "ifft(x, n=None, axis=-1, norm=None, overwrite_x=False, workers=None, *, plan=None)",
      "documentation": {
        "description": "Compute the 1-D inverse discrete Fourier Transform.\n\n    This function computes the inverse of the 1-D *n*-point\n    discrete Fourier transform computed by `fft`.  In other words,\n    ``ifft(fft(x)) == x`` to within numerical accuracy.\n\n    The input should be ordered in the same way as is returned by `fft`,\n    i.e.,\n\n    * ``x[0]`` should contain the zero frequency term,\n    * ``x[1:n//2]`` should contain the positive-frequency terms,\n    * ``x[n//2 + 1:]`` should contain the negative-frequency terms, in\n      increasing order starting from the most negative frequency.\n\n    For an even number of input points, ``x[n//2]`` represents the sum of\n    the values at the positive and negative Nyquist frequencies, as the two\n    are aliased together. See `fft` for details.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array, can be complex.\n    n : int, optional\n        Length of the transformed axis of the output.\n        If `n` is smaller than the length of the input, the input is cropped.\n        If it is larger, the input is padded with zeros. If `n` is not given,\n        the length of the input along the axis specified by `axis` is used.\n        See notes about padding issues.\n    axis : int, optional\n        Axis over which to compute the inverse DFT. If not given, the last\n        axis is used.\n    norm : {\"backward\", \"ortho\", \"forward\"}, optional\n        Normalization mode (see `fft`). Default is \"backward\".\n    overwrite_x : bool, optional\n        If True, the contents of `x` can be destroyed; the default is False.\n        See :func:`fft` for more details.\n    workers : int, optional\n        Maximum number of workers to use for parallel computation. If negative,\n        the value wraps around from ``os.cpu_count()``.\n        See :func:`~scipy.fft.fft` for more details.\n    plan : object, optional\n        This argument is reserved for passing in a precomputed plan provided\n        by downstream FFT vendors. It is currently not used in SciPy.\n\n        .. versionadded:: 1.5.0\n\n    Returns\n    -------\n    out : complex ndarray\n        The truncated or zero-padded input, transformed along the axis\n        indicated by `axis`, or the last one if `axis` is not specified.\n\n    Raises\n    ------\n    IndexError\n        If `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    fft : The 1-D (forward) FFT, of which `ifft` is the inverse.\n    ifft2 : The 2-D inverse FFT.\n    ifftn : The N-D inverse FFT.\n\n    Notes\n    -----\n    If the input parameter `n` is larger than the size of the input, the input\n    is padded by appending zeros at the end. Even though this is the common\n    approach, it might lead to surprising results. If a different padding is\n    desired, it must be performed before calling `ifft`.\n\n    If ``x`` is a 1-D array, then the `ifft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)\n\n    As with `fft`, `ifft` has support for all floating point types and is\n    optimized for real input.",
        "parameters": {
          "x": {
            "type": "array_like",
            "description": ""
          },
          "Input": {
            "type": "array, can be complex.",
            "description": ""
          },
          "n": {
            "type": "int, optional",
            "description": ""
          },
          "Length": {
            "type": "of the transformed axis of the output.",
            "description": ""
          },
          "If": {
            "type": "``x`` is a 1-D array, then the `ifft` is equivalent to ::",
            "description": "y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)"
          },
          "the": {
            "type": "value wraps around from ``os.cpu_count()``.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "axis": {
            "type": "is used.",
            "description": ""
          },
          "Axis": {
            "type": "over which to compute the inverse DFT. If not given, the last",
            "description": ""
          },
          "norm": {
            "type": "{\"backward\", \"ortho\", \"forward\"}, optional",
            "description": ""
          },
          "Normalization": {
            "type": "mode (see `fft`). Default is \"backward\".",
            "description": ""
          },
          "overwrite_x": {
            "type": "bool, optional",
            "description": ""
          },
          "workers": {
            "type": "int, optional",
            "description": ""
          },
          "Maximum": {
            "type": "number of workers to use for parallel computation. If negative,",
            "description": ""
          },
          "plan": {
            "type": "object, optional",
            "description": ""
          },
          "This": {
            "type": "argument is reserved for passing in a precomputed plan provided",
            "description": ""
          },
          "by": {
            "type": "downstream FFT vendors. It is currently not used in SciPy.",
            "description": ".. versionadded:: 1.5.0\nReturns\n-------"
          },
          "out": {
            "type": "complex ndarray",
            "description": ""
          },
          "The": {
            "type": "truncated or zero-padded input, transformed along the axis",
            "description": ""
          },
          "indicated": {
            "type": "by `axis`, or the last one if `axis` is not specified.",
            "description": "Raises\n------\nIndexError"
          },
          "fft": {
            "type": "The 1",
            "description": "D (forward) FFT, of which `ifft` is the inverse."
          },
          "ifft2": {
            "type": "The 2",
            "description": "D inverse FFT."
          },
          "ifftn": {
            "type": "The N",
            "description": "D inverse FFT.\nNotes\n-----"
          },
          "is": {
            "type": "padded by appending zeros at the end. Even though this is the common",
            "description": "approach, it might lead to surprising results. If a different padding is\ndesired, it must be performed before calling `ifft`."
          },
          "As": {
            "type": "with `fft`, `ifft` has support for all floating point types and is",
            "description": ""
          },
          "optimized": {
            "type": "for real input.",
            "description": "Examples\n--------\n>>> import scipy.fft\n>>> import numpy as np\n>>> scipy.fft.ifft([0, 4, 0, 0])"
          },
          "array": {
            "type": "[ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]",
            "description": "# may vary"
          },
          "Create": {
            "type": "and plot a band-limited signal with random phases:",
            "description": ">>> import matplotlib.pyplot as plt\n>>> rng = np.random.default_rng()\n>>> t = np.arange(400)\n>>> n = np.zeros((400,), dtype=complex)\n>>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n>>> s = scipy.fft.ifft(n)\n>>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n[<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n>>> plt.legend(('real', 'imaginary'))\n<matplotlib.legend.Legend object at ...>\n>>> plt.show()"
          }
        },
        "returns": "-------\n    out : complex ndarray\n        The truncated or zero-padded input, transformed along the axis\n        indicated by `axis`, or the last one if `axis` is not specified.\n\n    Raises\n    ------\n    IndexError\n        If `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    fft : The 1-D (forward) FFT, of which `ifft` is the inverse.\n    ifft2 : The 2-D inverse FFT.\n    ifftn : The N-D inverse FFT.\n\n    Notes\n    -----\n    If the input parameter `n` is larger than the size of the input, the input\n    is padded by appending zeros at the end. Even though this is the common\n    approach, it might lead to surprising results. If a different padding is\n    desired, it must be performed before calling `ifft`.\n\n    If ``x`` is a 1-D array, then the `ifft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)\n\n    As with `fft`, `ifft` has support for all floating point types and is\n    optimized for real input.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.ifft([0, 4, 0, 0])\n    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n    Create and plot a band-limited signal with random phases:\n\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> t = np.arange(400)\n    >>> n = np.zeros((400,), dtype=complex)\n    >>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n    >>> s = scipy.fft.ifft(n)\n    >>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n    [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n    >>> plt.legend(('real', 'imaginary'))\n    <matplotlib.legend.Legend object at ...>\n    >>> plt.show()",
        "raises": "------\n    IndexError\n        If `axes` is larger than the last axis of `x`.\n\n    See Also\n    --------\n    fft : The 1-D (forward) FFT, of which `ifft` is the inverse.\n    ifft2 : The 2-D inverse FFT.\n    ifftn : The N-D inverse FFT.\n\n    Notes\n    -----\n    If the input parameter `n` is larger than the size of the input, the input\n    is padded by appending zeros at the end. Even though this is the common\n    approach, it might lead to surprising results. If a different padding is\n    desired, it must be performed before calling `ifft`.\n\n    If ``x`` is a 1-D array, then the `ifft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)\n\n    As with `fft`, `ifft` has support for all floating point types and is\n    optimized for real input.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.ifft([0, 4, 0, 0])\n    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n    Create and plot a band-limited signal with random phases:\n\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> t = np.arange(400)\n    >>> n = np.zeros((400,), dtype=complex)\n    >>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n    >>> s = scipy.fft.ifft(n)\n    >>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n    [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n    >>> plt.legend(('real', 'imaginary'))\n    <matplotlib.legend.Legend object at ...>\n    >>> plt.show()",
        "see_also": "--------\n    fft : The 1-D (forward) FFT, of which `ifft` is the inverse.\n    ifft2 : The 2-D inverse FFT.\n    ifftn : The N-D inverse FFT.\n\n    Notes\n    -----\n    If the input parameter `n` is larger than the size of the input, the input\n    is padded by appending zeros at the end. Even though this is the common\n    approach, it might lead to surprising results. If a different padding is\n    desired, it must be performed before calling `ifft`.\n\n    If ``x`` is a 1-D array, then the `ifft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)\n\n    As with `fft`, `ifft` has support for all floating point types and is\n    optimized for real input.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.ifft([0, 4, 0, 0])\n    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n    Create and plot a band-limited signal with random phases:\n\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> t = np.arange(400)\n    >>> n = np.zeros((400,), dtype=complex)\n    >>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n    >>> s = scipy.fft.ifft(n)\n    >>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n    [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n    >>> plt.legend(('real', 'imaginary'))\n    <matplotlib.legend.Legend object at ...>\n    >>> plt.show()",
        "notes": "-----\n    If the input parameter `n` is larger than the size of the input, the input\n    is padded by appending zeros at the end. Even though this is the common\n    approach, it might lead to surprising results. If a different padding is\n    desired, it must be performed before calling `ifft`.\n\n    If ``x`` is a 1-D array, then the `ifft` is equivalent to ::\n\n        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)\n\n    As with `fft`, `ifft` has support for all floating point types and is\n    optimized for real input.\n\n    Examples\n    --------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.ifft([0, 4, 0, 0])\n    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n    Create and plot a band-limited signal with random phases:\n\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> t = np.arange(400)\n    >>> n = np.zeros((400,), dtype=complex)\n    >>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n    >>> s = scipy.fft.ifft(n)\n    >>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n    [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n    >>> plt.legend(('real', 'imaginary'))\n    <matplotlib.legend.Legend object at ...>\n    >>> plt.show()",
        "examples": "--------\n    >>> import scipy.fft\n    >>> import numpy as np\n    >>> scipy.fft.ifft([0, 4, 0, 0])\n    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n    Create and plot a band-limited signal with random phases:\n\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> t = np.arange(400)\n    >>> n = np.zeros((400,), dtype=complex)\n    >>> n[40:60] = np.exp(1j*rng.uniform(0, 2*np.pi, (20,)))\n    >>> s = scipy.fft.ifft(n)\n    >>> plt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n    [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\n    >>> plt.legend(('real', 'imaginary'))\n    <matplotlib.legend.Legend object at ...>\n    >>> plt.show()"
      }
    },
    {
      "name": "pairwise_kernels",
      "signature": "pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)",
      "documentation": {
        "description": "Compute the kernel between arrays X and optional array Y.\n\n    This method takes either a vector array or a kernel matrix, and returns\n    a kernel matrix. If the input is a vector array, the kernels are\n    computed. If the input is a kernel matrix, it is returned instead.\n\n    This method provides a safe way to take a kernel matrix as input, while\n    preserving compatibility with many other algorithms that take a vector\n    array.\n\n    If Y is given (default is None), then the returned matrix is the pairwise\n    kernel between the arrays from both X and Y.\n\n    Valid values for metric are:\n        ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n        'laplacian', 'sigmoid', 'cosine']\n\n    Read more in the :ref:`User Guide <metrics>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n        Array of pairwise kernels between samples, or a feature array.\n        The shape of the array should be (n_samples_X, n_samples_X) if\n        metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\n    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n        A second feature array only if X has shape (n_samples_X, n_features).\n\n    metric : str or callable, default=\"linear\"\n        The metric to use when calculating kernel between instances in a\n        feature array. If metric is a string, it must be one of the metrics\n        in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.\n        If metric is \"precomputed\", X is assumed to be a kernel matrix.\n        Alternatively, if metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two rows from X as input and return the corresponding\n        kernel value as a single number. This means that callables from\n        :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n        matrices, not single samples. Use the string identifying the kernel\n        instead.\n\n    filter_params : bool, default=False\n        Whether to filter invalid parameters or not.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them\n        using multithreading.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    **kwds : optional keyword parameters\n        Any further parameters are passed directly to the kernel function.\n\n    Returns\n    -------\n    K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n        A kernel matrix K such that K_{i, j} is the kernel between the\n        ith and jth vectors of the given matrix X, if Y is None.\n        If Y is not None, then K_{i, j} is the kernel between the ith array\n        from X and the jth array from Y.\n\n    Notes\n    -----\n    If metric is 'precomputed', Y is ignored and X is returned.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)"
          },
          "Array": {
            "type": "of pairwise kernels between samples, or a feature array.",
            "description": ""
          },
          "The": {
            "type": "number of jobs to use for the computation. This works by breaking",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default=\"linear\"",
            "description": ""
          },
          "Y": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples_Y, n_features), default=None"
          },
          "A": {
            "type": "kernel matrix K such that K_{i, j} is the kernel between the",
            "description": ""
          },
          "feature": {
            "type": "array. If metric is a string, it must be one of the metrics",
            "description": ""
          },
          "in": {
            "type": "``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.",
            "description": ""
          },
          "If": {
            "type": "metric is 'precomputed', Y is ignored and X is returned.",
            "description": "Examples\n--------\n>>> from sklearn.metrics.pairwise import pairwise_kernels\n>>> X = [[0, 0, 0], [1, 1, 1]]\n>>> Y = [[1, 0, 0], [1, 1, 0]]\n>>> pairwise_kernels(X, Y, metric='linear')\narray([[0., 0.],\n[1., 2.]])"
          },
          "pair": {
            "type": "of instances (rows) and the resulting value recorded. The callable",
            "description": ""
          },
          "should": {
            "type": "take two rows from X as input and return the corresponding",
            "description": ""
          },
          "kernel": {
            "type": "value as a single number. This means that callables from",
            "description": ":mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\nmatrices, not single samples. Use the string identifying the kernel\ninstead."
          },
          "filter_params": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to filter invalid parameters or not.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "down": {
            "type": "the pairwise matrix into n_jobs even slices and computing them",
            "description": ""
          },
          "using": {
            "type": "multithreading.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": "**kwds : optional keyword parameters"
          },
          "Any": {
            "type": "further parameters are passed directly to the kernel function.",
            "description": "Returns\n-------"
          },
          "K": {
            "type": "ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)",
            "description": ""
          },
          "ith": {
            "type": "and jth vectors of the given matrix X, if Y is None.",
            "description": ""
          },
          "from": {
            "type": "X and the jth array from Y.",
            "description": "Notes\n-----"
          }
        },
        "returns": "-------\n    K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n        A kernel matrix K such that K_{i, j} is the kernel between the\n        ith and jth vectors of the given matrix X, if Y is None.\n        If Y is not None, then K_{i, j} is the kernel between the ith array\n        from X and the jth array from Y.\n\n    Notes\n    -----\n    If metric is 'precomputed', Y is ignored and X is returned.\n\n    Examples\n    --------\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> X = [[0, 0, 0], [1, 1, 1]]\n    >>> Y = [[1, 0, 0], [1, 1, 0]]\n    >>> pairwise_kernels(X, Y, metric='linear')\n    array([[0., 0.],\n           [1., 2.]])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    If metric is 'precomputed', Y is ignored and X is returned.\n\n    Examples\n    --------\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> X = [[0, 0, 0], [1, 1, 1]]\n    >>> Y = [[1, 0, 0], [1, 1, 0]]\n    >>> pairwise_kernels(X, Y, metric='linear')\n    array([[0., 0.],\n           [1., 2.]])",
        "examples": "--------\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> X = [[0, 0, 0], [1, 1, 1]]\n    >>> Y = [[1, 0, 0], [1, 1, 0]]\n    >>> pairwise_kernels(X, Y, metric='linear')\n    array([[0., 0.],\n           [1., 2.]])"
      }
    },
    {
      "name": "safe_sparse_dot",
      "signature": "safe_sparse_dot(a, b, *, dense_output=False)",
      "documentation": {
        "description": "Dot product that handle the sparse matrix case correctly.\n\n    Parameters\n    ----------\n    a : {ndarray, sparse matrix}\n    b : {ndarray, sparse matrix}\n    dense_output : bool, default=False\n        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n        When True, output will always be a dense array.\n\n    Returns\n    -------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.",
        "parameters": {
          "a": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "b": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "dense_output": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "True, output will always be a dense array.",
            "description": "Returns\n-------"
          },
          "dot_product": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "Sparse": {
            "type": "if ``a`` and ``b`` are sparse and ``dense_output=False``.",
            "description": "Examples\n--------\n>>> from scipy.sparse import csr_matrix\n>>> from sklearn.utils.extmath import safe_sparse_dot\n>>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n>>> dot_product = safe_sparse_dot(X, X.T)\n>>> dot_product.toarray()\narray([[ 5, 11, 17],\n[11, 25, 39],\n[17, 39, 61]])"
          }
        },
        "returns": "-------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.utils.extmath import safe_sparse_dot\n    >>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    >>> dot_product = safe_sparse_dot(X, X.T)\n    >>> dot_product.toarray()\n    array([[ 5, 11, 17],\n           [11, 25, 39],\n           [17, 39, 61]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.utils.extmath import safe_sparse_dot\n    >>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    >>> dot_product = safe_sparse_dot(X, X.T)\n    >>> dot_product.toarray()\n    array([[ 5, 11, 17],\n           [11, 25, 39],\n           [17, 39, 61]])"
      }
    },
    {
      "name": "svd",
      "signature": "svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd')",
      "documentation": {
        "description": "Singular Value Decomposition.\n\n    Factorizes the matrix `a` into two unitary matrices ``U`` and ``Vh``, and\n    a 1-D array ``s`` of singular values (real, non-negative) such that\n    ``a == U @ S @ Vh``, where ``S`` is a suitably shaped matrix of zeros with\n    main diagonal ``s``.\n\n    Parameters\n    ----------\n    a : (M, N) array_like\n        Matrix to decompose.\n    full_matrices : bool, optional\n        If True (default), `U` and `Vh` are of shape ``(M, M)``, ``(N, N)``.\n        If False, the shapes are ``(M, K)`` and ``(K, N)``, where\n        ``K = min(M, N)``.\n    compute_uv : bool, optional\n        Whether to compute also ``U`` and ``Vh`` in addition to ``s``.\n        Default is True.\n    overwrite_a : bool, optional\n        Whether to overwrite `a`; may improve performance.\n        Default is False.\n    check_finite : bool, optional\n        Whether to check that the input matrix contains only finite numbers.\n        Disabling may give a performance gain, but may result in problems\n        (crashes, non-termination) if the inputs do contain infinities or NaNs.\n    lapack_driver : {'gesdd', 'gesvd'}, optional\n        Whether to use the more efficient divide-and-conquer approach\n        (``'gesdd'``) or general rectangular approach (``'gesvd'``)\n        to compute the SVD. MATLAB and Octave use the ``'gesvd'`` approach.\n        Default is ``'gesdd'``.\n\n    Returns\n    -------\n    U : ndarray\n        Unitary matrix having left singular vectors as columns.\n        Of shape ``(M, M)`` or ``(M, K)``, depending on `full_matrices`.\n    s : ndarray\n        The singular values, sorted in non-increasing order.\n        Of shape (K,), with ``K = min(M, N)``.\n    Vh : ndarray\n        Unitary matrix having right singular vectors as rows.\n        Of shape ``(N, N)`` or ``(K, N)`` depending on `full_matrices`.\n\n    For ``compute_uv=False``, only ``s`` is returned.\n\n    Raises\n    ------\n    LinAlgError\n        If SVD computation does not converge.\n\n    See Also\n    --------\n    svdvals : Compute singular values of a matrix.\n    diagsvd : Construct the Sigma matrix, given the vector s.",
        "parameters": {
          "a": {
            "type": "(M, N) array_like",
            "description": ""
          },
          "Matrix": {
            "type": "to decompose.",
            "description": ""
          },
          "full_matrices": {
            "type": "bool, optional",
            "description": ""
          },
          "If": {
            "type": "SVD computation does not converge.",
            "description": ""
          },
          "compute_uv": {
            "type": "bool, optional",
            "description": ""
          },
          "Whether": {
            "type": "to use the more efficient divide-and-conquer approach",
            "description": "(``'gesdd'``) or general rectangular approach (``'gesvd'``)"
          },
          "Default": {
            "type": "is ``'gesdd'``.",
            "description": "Returns\n-------"
          },
          "overwrite_a": {
            "type": "bool, optional",
            "description": ""
          },
          "check_finite": {
            "type": "bool, optional",
            "description": ""
          },
          "Disabling": {
            "type": "may give a performance gain, but may result in problems",
            "description": "(crashes, non-termination) if the inputs do contain infinities or NaNs."
          },
          "lapack_driver": {
            "type": "{'gesdd', 'gesvd'}, optional",
            "description": ""
          },
          "to": {
            "type": "compute the SVD. MATLAB and Octave use the ``'gesvd'`` approach.",
            "description": ""
          },
          "U": {
            "type": "ndarray",
            "description": ""
          },
          "Unitary": {
            "type": "matrix having right singular vectors as rows.",
            "description": ""
          },
          "Of": {
            "type": "shape ``(N, N)`` or ``(K, N)`` depending on `full_matrices`.",
            "description": ""
          },
          "s": {
            "type": "ndarray",
            "description": ""
          },
          "The": {
            "type": "singular values, sorted in non-increasing order.",
            "description": ""
          },
          "Vh": {
            "type": "ndarray",
            "description": ""
          },
          "For": {
            "type": "``compute_uv=False``, only ``s`` is returned.",
            "description": "Raises\n------\nLinAlgError"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "svdvals": {
            "type": "Compute singular values of a matrix.",
            "description": ""
          },
          "diagsvd": {
            "type": "Construct the Sigma matrix, given the vector s.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from scipy import linalg\n>>> rng = np.random.default_rng()\n>>> m, n = 9, 6\n>>> a = rng.standard_normal((m, n)) + 1.j*rng.standard_normal((m, n))\n>>> U, s, Vh = linalg.svd(a)\n>>> U.shape,  s.shape, Vh.shape\n((9, 9), (6,), (6, 6))"
          },
          "Reconstruct": {
            "type": "the original matrix from the decomposition:",
            "description": ">>> sigma = np.zeros((m, n))\n>>> for i in range(min(m, n)):\n...     sigma[i, i] = s[i]\n>>> a1 = np.dot(U, np.dot(sigma, Vh))\n>>> np.allclose(a, a1)\nTrue\nAlternatively, use ``full_matrices=False`` (notice that the shape of\n``U`` is then ``(m, n)`` instead of ``(m, m)``):\n>>> U, s, Vh = linalg.svd(a, full_matrices=False)\n>>> U.shape, s.shape, Vh.shape\n((9, 6), (6,), (6, 6))\n>>> S = np.diag(s)\n>>> np.allclose(a, np.dot(U, np.dot(S, Vh)))\nTrue\n>>> s2 = linalg.svd(a, compute_uv=False)\n>>> np.allclose(s, s2)\nTrue"
          }
        },
        "returns": "-------\n    U : ndarray\n        Unitary matrix having left singular vectors as columns.\n        Of shape ``(M, M)`` or ``(M, K)``, depending on `full_matrices`.\n    s : ndarray\n        The singular values, sorted in non-increasing order.\n        Of shape (K,), with ``K = min(M, N)``.\n    Vh : ndarray\n        Unitary matrix having right singular vectors as rows.\n        Of shape ``(N, N)`` or ``(K, N)`` depending on `full_matrices`.\n\n    For ``compute_uv=False``, only ``s`` is returned.\n\n    Raises\n    ------\n    LinAlgError\n        If SVD computation does not converge.\n\n    See Also\n    --------\n    svdvals : Compute singular values of a matrix.\n    diagsvd : Construct the Sigma matrix, given the vector s.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> rng = np.random.default_rng()\n    >>> m, n = 9, 6\n    >>> a = rng.standard_normal((m, n)) + 1.j*rng.standard_normal((m, n))\n    >>> U, s, Vh = linalg.svd(a)\n    >>> U.shape,  s.shape, Vh.shape\n    ((9, 9), (6,), (6, 6))\n\n    Reconstruct the original matrix from the decomposition:\n\n    >>> sigma = np.zeros((m, n))\n    >>> for i in range(min(m, n)):\n    ...     sigma[i, i] = s[i]\n    >>> a1 = np.dot(U, np.dot(sigma, Vh))\n    >>> np.allclose(a, a1)\n    True\n\n    Alternatively, use ``full_matrices=False`` (notice that the shape of\n    ``U`` is then ``(m, n)`` instead of ``(m, m)``):\n\n    >>> U, s, Vh = linalg.svd(a, full_matrices=False)\n    >>> U.shape, s.shape, Vh.shape\n    ((9, 6), (6,), (6, 6))\n    >>> S = np.diag(s)\n    >>> np.allclose(a, np.dot(U, np.dot(S, Vh)))\n    True\n\n    >>> s2 = linalg.svd(a, compute_uv=False)\n    >>> np.allclose(s, s2)\n    True",
        "raises": "------\n    LinAlgError\n        If SVD computation does not converge.\n\n    See Also\n    --------\n    svdvals : Compute singular values of a matrix.\n    diagsvd : Construct the Sigma matrix, given the vector s.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> rng = np.random.default_rng()\n    >>> m, n = 9, 6\n    >>> a = rng.standard_normal((m, n)) + 1.j*rng.standard_normal((m, n))\n    >>> U, s, Vh = linalg.svd(a)\n    >>> U.shape,  s.shape, Vh.shape\n    ((9, 9), (6,), (6, 6))\n\n    Reconstruct the original matrix from the decomposition:\n\n    >>> sigma = np.zeros((m, n))\n    >>> for i in range(min(m, n)):\n    ...     sigma[i, i] = s[i]\n    >>> a1 = np.dot(U, np.dot(sigma, Vh))\n    >>> np.allclose(a, a1)\n    True\n\n    Alternatively, use ``full_matrices=False`` (notice that the shape of\n    ``U`` is then ``(m, n)`` instead of ``(m, m)``):\n\n    >>> U, s, Vh = linalg.svd(a, full_matrices=False)\n    >>> U.shape, s.shape, Vh.shape\n    ((9, 6), (6,), (6, 6))\n    >>> S = np.diag(s)\n    >>> np.allclose(a, np.dot(U, np.dot(S, Vh)))\n    True\n\n    >>> s2 = linalg.svd(a, compute_uv=False)\n    >>> np.allclose(s, s2)\n    True",
        "see_also": "--------\n    svdvals : Compute singular values of a matrix.\n    diagsvd : Construct the Sigma matrix, given the vector s.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> rng = np.random.default_rng()\n    >>> m, n = 9, 6\n    >>> a = rng.standard_normal((m, n)) + 1.j*rng.standard_normal((m, n))\n    >>> U, s, Vh = linalg.svd(a)\n    >>> U.shape,  s.shape, Vh.shape\n    ((9, 9), (6,), (6, 6))\n\n    Reconstruct the original matrix from the decomposition:\n\n    >>> sigma = np.zeros((m, n))\n    >>> for i in range(min(m, n)):\n    ...     sigma[i, i] = s[i]\n    >>> a1 = np.dot(U, np.dot(sigma, Vh))\n    >>> np.allclose(a, a1)\n    True\n\n    Alternatively, use ``full_matrices=False`` (notice that the shape of\n    ``U`` is then ``(m, n)`` instead of ``(m, m)``):\n\n    >>> U, s, Vh = linalg.svd(a, full_matrices=False)\n    >>> U.shape, s.shape, Vh.shape\n    ((9, 6), (6,), (6, 6))\n    >>> S = np.diag(s)\n    >>> np.allclose(a, np.dot(U, np.dot(S, Vh)))\n    True\n\n    >>> s2 = linalg.svd(a, compute_uv=False)\n    >>> np.allclose(s, s2)\n    True",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> rng = np.random.default_rng()\n    >>> m, n = 9, 6\n    >>> a = rng.standard_normal((m, n)) + 1.j*rng.standard_normal((m, n))\n    >>> U, s, Vh = linalg.svd(a)\n    >>> U.shape,  s.shape, Vh.shape\n    ((9, 9), (6,), (6, 6))\n\n    Reconstruct the original matrix from the decomposition:\n\n    >>> sigma = np.zeros((m, n))\n    >>> for i in range(min(m, n)):\n    ...     sigma[i, i] = s[i]\n    >>> a1 = np.dot(U, np.dot(sigma, Vh))\n    >>> np.allclose(a, a1)\n    True\n\n    Alternatively, use ``full_matrices=False`` (notice that the shape of\n    ``U`` is then ``(m, n)`` instead of ``(m, m)``):\n\n    >>> U, s, Vh = linalg.svd(a, full_matrices=False)\n    >>> U.shape, s.shape, Vh.shape\n    ((9, 6), (6,), (6, 6))\n    >>> S = np.diag(s)\n    >>> np.allclose(a, np.dot(U, np.dot(S, Vh)))\n    True\n\n    >>> s2 = linalg.svd(a, compute_uv=False)\n    >>> np.allclose(s, s2)\n    True"
      }
    },
    {
      "name": "validate_data",
      "signature": "validate_data(_estimator, /, X='no_validation', y='no_validation', reset=True, validate_separately=False, skip_check_array=False, **check_params)",
      "documentation": {
        "description": "Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape             (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.",
        "parameters": {
          "_estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "The": {
            "type": "targets.",
            "description": "- If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If"
          },
          "X": {
            "type": "{array",
            "description": "like, sparse matrix, dataframe} of shape             (n_samples, n_features), default='no validation'"
          },
          "If": {
            "type": "`True`, `X` and `y` are unchanged and only `feature_names_in_` and",
            "description": "`n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`"
          },
          "useful": {
            "type": "for meta-estimator which can delegate input validation to",
            "description": ""
          },
          "their": {
            "type": "underlying estimator(s). In that case `y` must be passed and",
            "description": ""
          },
          "the": {
            "type": "estimator's `requires_y` tag is True, then an error will be raised.",
            "description": "- If `'no_validation'`, :func:`~sklearn.utils.check_array` is called"
          },
          "y": {
            "type": "array",
            "description": "like of shape (n_samples,), default='no_validation'"
          },
          "on": {
            "type": "`X` and the estimator's `requires_y` tag is ignored. This is a default",
            "description": ""
          },
          "placeholder": {
            "type": "and is never meant to be explicitly set. In that case `X` must be",
            "description": "passed.\n- Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with"
          },
          "either": {
            "type": "func:`~sklearn.utils.check_array` or",
            "description": ":func:`~sklearn.utils.check_X_y` depending on `validate_separately`."
          },
          "reset": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to reset the `n_features_in_` attribute.",
            "description": ""
          },
          "provided": {
            "type": "when reset was last True.",
            "description": ".. note::"
          },
          "It": {
            "type": "is recommended to call `reset=True` in `fit` and in the first",
            "description": ""
          },
          "call": {
            "type": "to `partial_fit`. All other methods that validate `X`",
            "description": ""
          },
          "should": {
            "type": "set `reset=False`.",
            "description": ""
          },
          "validate_separately": {
            "type": "False or tuple of dicts, default=False",
            "description": ""
          },
          "Only": {
            "type": "used if `y` is not `None`.",
            "description": ""
          },
          "kwargs": {
            "type": "to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`",
            "description": "respectively.\n`estimator=self` is automatically added to these dicts to generate"
          },
          "more": {
            "type": "informative error message in case of invalid input data.",
            "description": ""
          },
          "skip_check_array": {
            "type": "bool, default=False",
            "description": ""
          },
          "is": {
            "type": "called on `X` and `y`.",
            "description": "**check_params : kwargs"
          }
        },
        "returns": "-------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "AdditiveChi2Sampler",
      "documentation": {
        "description": "Approximate feature map for additive chi2 kernel.\n\n    Uses sampling the fourier transform of the kernel characteristic\n    at regular intervals.\n\n    Since the kernel that is to be approximated is additive, the components of\n    the input vectors can be treated separately.  Each entry in the original\n    space is transformed into 2*sample_steps-1 features, where sample_steps is\n    a parameter of the method. Typical values of sample_steps include 1, 2 and\n    3.\n\n    Optimal choices for the sampling interval for certain data ranges can be\n    computed (see the reference). The default values should be reasonable.\n\n    Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.\n\n    Parameters\n    ----------\n    sample_steps : int, default=2\n        Gives the number of (complex) sampling points.\n\n    sample_interval : float, default=None\n        Sampling interval. Must be specified when sample_steps not in {1,2,3}.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of\n        the chi squared kernel.\n\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n\n    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi\n        squared kernel.\n\n    Notes\n    -----\n    This estimator approximates a slightly different version of the additive\n    chi squared kernel then ``metric.additive_chi2`` computes.\n\n    This estimator is stateless and does not need to be fitted. However, we\n    recommend to call :meth:`fit_transform` instead of :meth:`transform`, as\n    parameter validation is only performed in :meth:`fit`.\n\n    References\n    ----------\n    See `\"Efficient additive kernels via explicit feature maps\"\n    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\n    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n    2011",
        "parameters": {
          "sample_steps": {
            "type": "int, default=2",
            "description": ""
          },
          "Gives": {
            "type": "the number of (complex) sampling points.",
            "description": ""
          },
          "sample_interval": {
            "type": "float, default=None",
            "description": ""
          },
          "Sampling": {
            "type": "interval. Must be specified when sample_steps not in {1,2,3}.",
            "description": "Attributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "`\"Efficient additive kernels via explicit feature maps\"",
            "description": "<http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\nA. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n2011\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n>>> X, y = load_digits(return_X_y=True)\n>>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n>>> X_transformed = chi2sampler.fit_transform(X, y)\n>>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n>>> clf.fit(X_transformed, y)"
          },
          "SkewedChi2Sampler": {
            "type": "A Fourier",
            "description": "approximation to a non-additive variant of"
          },
          "the": {
            "type": "chi squared kernel.",
            "description": "sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\nsklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi"
          },
          "squared": {
            "type": "kernel.",
            "description": "Notes\n-----"
          },
          "This": {
            "type": "estimator is stateless and does not need to be fitted. However, we",
            "description": ""
          },
          "chi": {
            "type": "squared kernel then ``metric.additive_chi2`` computes.",
            "description": ""
          },
          "recommend": {
            "type": "to call :meth:`fit_transform` instead of :meth:`transform`, as",
            "description": ""
          },
          "parameter": {
            "type": "validation is only performed in :meth:`fit`.",
            "description": "References\n----------"
          },
          "SGDClassifier": {
            "type": "max_iter=5, random_state=0",
            "description": ">>> clf.score(X_transformed, y)\n0.9499..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of\n        the chi squared kernel.\n\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n\n    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi\n        squared kernel.\n\n    Notes\n    -----\n    This estimator approximates a slightly different version of the additive\n    chi squared kernel then ``metric.additive_chi2`` computes.\n\n    This estimator is stateless and does not need to be fitted. However, we\n    recommend to call :meth:`fit_transform` instead of :meth:`transform`, as\n    parameter validation is only performed in :meth:`fit`.\n\n    References\n    ----------\n    See `\"Efficient additive kernels via explicit feature maps\"\n    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\n    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n    2011\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n    >>> X, y = load_digits(return_X_y=True)\n    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n    >>> X_transformed = chi2sampler.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n    >>> clf.fit(X_transformed, y)\n    SGDClassifier(max_iter=5, random_state=0)\n    >>> clf.score(X_transformed, y)\n    0.9499...",
        "notes": "-----\n    This estimator approximates a slightly different version of the additive\n    chi squared kernel then ``metric.additive_chi2`` computes.\n\n    This estimator is stateless and does not need to be fitted. However, we\n    recommend to call :meth:`fit_transform` instead of :meth:`transform`, as\n    parameter validation is only performed in :meth:`fit`.\n\n    References\n    ----------\n    See `\"Efficient additive kernels via explicit feature maps\"\n    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\n    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n    2011\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n    >>> X, y = load_digits(return_X_y=True)\n    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n    >>> X_transformed = chi2sampler.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n    >>> clf.fit(X_transformed, y)\n    SGDClassifier(max_iter=5, random_state=0)\n    >>> clf.score(X_transformed, y)\n    0.9499...",
        "examples": "--------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n    >>> X, y = load_digits(return_X_y=True)\n    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n    >>> X_transformed = chi2sampler.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n    >>> clf.fit(X_transformed, y)\n    SGDClassifier(max_iter=5, random_state=0)\n    >>> clf.score(X_transformed, y)\n    0.9499..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Only validates estimator's parameters.\n\n        This method allows to: (i) validate the estimator's parameters and\n        (ii) be consistent with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like, shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the transformer.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in :meth:`fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in :meth:`fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply approximate feature map to X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "{ndarray, sparse matrix},                shape = (n_samples, n_features * (2*sample_steps",
                "description": "1))"
              },
              "Whether": {
                "type": "the return value is an array or sparse matrix depends on",
                "description": ""
              },
              "the": {
                "type": "type of the input X.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : {ndarray, sparse matrix},                shape = (n_samples, n_features * (2*sample_steps - 1))\n            Whether the return value is an array or sparse matrix depends on\n            the type of the input X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BaseEstimator",
      "documentation": {
        "description": "Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).",
        "parameters": {
          "array": {
            "type": "[3, 3, 3]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])"
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ClassNamePrefixFeaturesOutMixin",
      "documentation": {
        "description": "Mixin class for transformers that generate their own names by prefixing.\n\n    This mixin is useful when the transformer needs to generate its own feature\n    names out, such as :class:`~sklearn.decomposition.PCA`. For example, if\n    :class:`~sklearn.decomposition.PCA` outputs 3 features, then the generated feature\n    names out are: `[\"pca0\", \"pca1\", \"pca2\"]`.\n\n    This mixin assumes that a `_n_features_out` attribute is defined when the\n    transformer is fitted. `_n_features_out` is the number of output features\n    that the transformer will return in `transform` of `fit_transform`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import ClassNamePrefixFeaturesOutMixin, BaseEstimator\n    >>> class MyEstimator(ClassNamePrefixFeaturesOutMixin, BaseEstimator):\n    ...     def fit(self, X, y=None):\n    ...         self._n_features_out = X.shape[1]\n    ...         return self\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> MyEstimator().fit(X).get_feature_names_out()\n    array(['myestimator0', 'myestimator1'], dtype=object)"
      },
      "methods": [
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Integral",
      "documentation": {
        "description": "Integral adds methods that work on integral numbers.\n\n    In short, these are conversion to int, pow with modulus, and the\n    bit-string operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interval",
      "documentation": {
        "description": "Constraint representing a typed interval.\n\n    Parameters\n    ----------\n    type : {numbers.Integral, numbers.Real, RealNotInt}\n        The set of numbers in which to set the interval.\n\n        If RealNotInt, only reals that don't have the integer type\n        are allowed. For example 1.0 is allowed but 1 is not.\n\n    left : float or int or None\n        The left bound of the interval. None means left bound is -.\n\n    right : float, int or None\n        The right bound of the interval. None means right bound is +.\n\n    closed : {\"left\", \"right\", \"both\", \"neither\"}\n        Whether the interval is open or closed. Possible choices are:\n\n        - `\"left\"`: the interval is closed on the left and open on the right.\n          It is equivalent to the interval `[ left, right )`.\n        - `\"right\"`: the interval is closed on the right and open on the left.\n          It is equivalent to the interval `( left, right ]`.\n        - `\"both\"`: the interval is closed.\n          It is equivalent to the interval `[ left, right ]`.\n        - `\"neither\"`: the interval is open.\n          It is equivalent to the interval `( left, right )`.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": ""
          },
          "The": {
            "type": "right bound of the interval. None means right bound is +.",
            "description": ""
          },
          "If": {
            "type": "RealNotInt, only reals that don't have the integer type",
            "description": ""
          },
          "are": {
            "type": "allowed. For example 1.0 is allowed but 1 is not.",
            "description": ""
          },
          "left": {
            "type": "float or int or None",
            "description": ""
          },
          "right": {
            "type": "float, int or None",
            "description": ""
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": ""
          },
          "Whether": {
            "type": "the interval is open or closed. Possible choices are:",
            "description": "- `\"left\"`: the interval is closed on the left and open on the right."
          },
          "It": {
            "type": "is equivalent to the interval `( left, right )`.",
            "description": "Notes\n-----"
          },
          "Setting": {
            "type": "a bound to `None` and setting the interval closed is valid. For instance,",
            "description": ""
          },
          "strictly": {
            "type": "speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to",
            "description": "`[0, +) U {+}`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    Setting a bound to `None` and setting the interval closed is valid. For instance,\n    strictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n    `[0, +) U {+}`.",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Nystroem",
      "documentation": {
        "description": "Approximate a kernel map using a subset of the training data.\n\n    Constructs an approximate feature map for an arbitrary kernel\n    using a subset of the data as basis.\n\n    Read more in the :ref:`User Guide <nystroem_kernel_approx>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    kernel : str or callable, default='rbf'\n        Kernel map to be approximated. A callable should accept two arguments\n        and the keyword arguments passed to this object as `kernel_params`, and\n        should return a floating point number.\n\n    gamma : float, default=None\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n        and sigmoid kernels. Interpretation of the default value is left to\n        the kernel; see the documentation for sklearn.metrics.pairwise.\n        Ignored by other kernels.\n\n    coef0 : float, default=None\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    degree : float, default=None\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Additional parameters (keyword arguments) for kernel function passed\n        as callable object.\n\n    n_components : int, default=100\n        Number of features to construct.\n        How many data points will be used to construct the mapping.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the uniform sampling without\n        replacement of `n_components` of the training data to construct the\n        basis kernel.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the kernel matrix into `n_jobs` even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Subset of training points used to construct the feature map.\n\n    component_indices_ : ndarray of shape (n_components)\n        Indices of ``components_`` in the training set.\n\n    normalization_ : ndarray of shape (n_components, n_components)\n        Normalization matrix needed for embedding.\n        Square root of the kernel matrix on ``components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    * Williams, C.K.I. and Seeger, M.\n      \"Using the Nystroem method to speed up kernel machines\",\n      Advances in neural information processing systems 2001\n\n    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n      \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n      Comparison\",\n      Advances in Neural Information Processing Systems 2012",
        "parameters": {
          "kernel": {
            "type": "str or callable, default='rbf'",
            "description": ""
          },
          "Kernel": {
            "type": "map to be approximated. A callable should accept two arguments",
            "description": ""
          },
          "and": {
            "type": "sigmoid kernels. Interpretation of the default value is left to",
            "description": ""
          },
          "should": {
            "type": "return a floating point number.",
            "description": ""
          },
          "gamma": {
            "type": "float, default=None",
            "description": ""
          },
          "Gamma": {
            "type": "parameter for the RBF, laplacian, polynomial, exponential chi2",
            "description": ""
          },
          "the": {
            "type": "kernel; see the documentation for sklearn.metrics.pairwise.",
            "description": ""
          },
          "Ignored": {
            "type": "by other kernels.",
            "description": ""
          },
          "coef0": {
            "type": "float, default=None",
            "description": ""
          },
          "Zero": {
            "type": "coefficient for polynomial and sigmoid kernels.",
            "description": ""
          },
          "degree": {
            "type": "float, default=None",
            "description": ""
          },
          "Degree": {
            "type": "of the polynomial kernel. Ignored by other kernels.",
            "description": ""
          },
          "kernel_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "parameters (keyword arguments) for kernel function passed",
            "description": ""
          },
          "as": {
            "type": "callable object.",
            "description": ""
          },
          "n_components": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "How": {
            "type": "many data points will be used to construct the mapping.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Pseudo-random number generator to control the uniform sampling without"
          },
          "replacement": {
            "type": "of `n_components` of the training data to construct the",
            "description": ""
          },
          "basis": {
            "type": "kernel.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of jobs to use for the computation. This works by breaking",
            "description": ""
          },
          "down": {
            "type": "the kernel matrix into `n_jobs` even slices and computing them in",
            "description": "parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ".. versionadded:: 0.24\nAttributes\n----------"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "Subset": {
            "type": "of training points used to construct the feature map.",
            "description": ""
          },
          "component_indices_": {
            "type": "ndarray of shape (n_components)",
            "description": ""
          },
          "Indices": {
            "type": "of ``components_`` in the training set.",
            "description": ""
          },
          "normalization_": {
            "type": "ndarray of shape (n_components, n_components)",
            "description": ""
          },
          "Normalization": {
            "type": "matrix needed for embedding.",
            "description": ""
          },
          "Square": {
            "type": "root of the kernel matrix on ``components_``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "AdditiveChi2Sampler": {
            "type": "Approximate feature map for additive chi2 kernel.",
            "description": ""
          },
          "PolynomialCountSketch": {
            "type": "Polynomial kernel approximation via Tensor Sketch.",
            "description": ""
          },
          "RBFSampler": {
            "type": "Approximate a RBF kernel feature map using random Fourier",
            "description": "features."
          },
          "SkewedChi2Sampler": {
            "type": "Approximate feature map for \"skewed chi",
            "description": "squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\nReferences\n----------\n* Williams, C.K.I. and Seeger, M.\n\"Using the Nystroem method to speed up kernel machines\","
          },
          "Advances": {
            "type": "in Neural Information Processing Systems 2012",
            "description": "Examples\n--------\n>>> from sklearn import datasets, svm\n>>> from sklearn.kernel_approximation import Nystroem\n>>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n>>> data = X / 16.\n>>> clf = svm.LinearSVC()\n>>> feature_map_nystroem = Nystroem(gamma=.2,\n...                                 random_state=1,\n...                                 n_components=300)\n>>> data_transformed = feature_map_nystroem.fit_transform(data)\n>>> clf.fit(data_transformed, y)"
          },
          "LinearSVC": {
            "type": "",
            "description": ">>> clf.score(data_transformed, y)\n0.9987..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    * Williams, C.K.I. and Seeger, M.\n      \"Using the Nystroem method to speed up kernel machines\",\n      Advances in neural information processing systems 2001\n\n    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n      \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n      Comparison\",\n      Advances in Neural Information Processing Systems 2012\n\n    Examples\n    --------\n    >>> from sklearn import datasets, svm\n    >>> from sklearn.kernel_approximation import Nystroem\n    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n    >>> data = X / 16.\n    >>> clf = svm.LinearSVC()\n    >>> feature_map_nystroem = Nystroem(gamma=.2,\n    ...                                 random_state=1,\n    ...                                 n_components=300)\n    >>> data_transformed = feature_map_nystroem.fit_transform(data)\n    >>> clf.fit(data_transformed, y)\n    LinearSVC()\n    >>> clf.score(data_transformed, y)\n    0.9987...",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import datasets, svm\n    >>> from sklearn.kernel_approximation import Nystroem\n    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n    >>> data = X / 16.\n    >>> clf = svm.LinearSVC()\n    >>> feature_map_nystroem = Nystroem(gamma=.2,\n    ...                                 random_state=1,\n    ...                                 n_components=300)\n    >>> data_transformed = feature_map_nystroem.fit_transform(data)\n    >>> clf.fit(data_transformed, y)\n    LinearSVC()\n    >>> clf.score(data_transformed, y)\n    0.9987..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit estimator to data.\n\n        Samples a subset of training points, computes kernel\n        on these and computes normalization matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like, shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply feature map to X.\n\n        Computes an approximate feature map using the kernel\n        between some training points and X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to transform.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "to transform.",
                "description": "Returns\n-------"
              },
              "X_transformed": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        X_transformed : ndarray of shape (n_samples, n_components)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PolynomialCountSketch",
      "documentation": {
        "description": "Polynomial kernel approximation via Tensor Sketch.\n\n    Implements Tensor Sketch, which approximates the feature map\n    of the polynomial kernel::\n\n        K(X, Y) = (gamma * <X, Y> + coef0)^degree\n\n    by efficiently computing a Count Sketch of the outer product of a\n    vector with itself using Fast Fourier Transforms (FFT). Read more in the\n    :ref:`User Guide <polynomial_kernel_approx>`.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    gamma : float, default=1.0\n        Parameter of the polynomial kernel whose feature map\n        will be approximated.\n\n    degree : int, default=2\n        Degree of the polynomial kernel whose feature map\n        will be approximated.\n\n    coef0 : int, default=0\n        Constant term of the polynomial kernel whose feature map\n        will be approximated.\n\n    n_components : int, default=100\n        Dimensionality of the output feature space. Usually, `n_components`\n        should be greater than the number of features in input samples in\n        order to achieve good performance. The optimal score / run time\n        balance is typically achieved around `n_components` = 10 * `n_features`,\n        but this depends on the specific dataset being used.\n\n    random_state : int, RandomState instance, default=None\n        Determines random number generation for indexHash and bitHash\n        initialization. Pass an int for reproducible results across multiple\n        function calls. See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    indexHash_ : ndarray of shape (degree, n_features), dtype=int64\n        Array of indexes in range [0, n_components) used to represent\n        the 2-wise independent hash functions for Count Sketch computation.\n\n    bitHash_ : ndarray of shape (degree, n_features), dtype=float32\n        Array with random entries in {+1, -1}, used to represent\n        the 2-wise independent hash functions for Count Sketch computation.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.",
        "parameters": {
          "gamma": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Parameter": {
            "type": "of the polynomial kernel whose feature map",
            "description": ""
          },
          "will": {
            "type": "be approximated.",
            "description": ""
          },
          "degree": {
            "type": "int, default=2",
            "description": ""
          },
          "Degree": {
            "type": "of the polynomial kernel whose feature map",
            "description": ""
          },
          "coef0": {
            "type": "int, default=0",
            "description": ""
          },
          "Constant": {
            "type": "term of the polynomial kernel whose feature map",
            "description": ""
          },
          "n_components": {
            "type": "int, default=100",
            "description": ""
          },
          "Dimensionality": {
            "type": "of the output feature space. Usually, `n_components`",
            "description": ""
          },
          "should": {
            "type": "be greater than the number of features in input samples in",
            "description": ""
          },
          "order": {
            "type": "to achieve good performance. The optimal score / run time",
            "description": ""
          },
          "balance": {
            "type": "is typically achieved around `n_components` = 10 * `n_features`,",
            "description": ""
          },
          "but": {
            "type": "this depends on the specific dataset being used.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for indexHash and bitHash",
            "description": "initialization. Pass an int for reproducible results across multiple"
          },
          "function": {
            "type": "calls. See :term:`Glossary <random_state>`.",
            "description": "Attributes\n----------"
          },
          "indexHash_": {
            "type": "ndarray of shape (degree, n_features), dtype=int64",
            "description": ""
          },
          "Array": {
            "type": "with random entries in {+1, -1}, used to represent",
            "description": ""
          },
          "the": {
            "type": "2-wise independent hash functions for Count Sketch computation.",
            "description": ""
          },
          "bitHash_": {
            "type": "ndarray of shape (degree, n_features), dtype=float32",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "AdditiveChi2Sampler": {
            "type": "Approximate feature map for additive chi2 kernel.",
            "description": ""
          },
          "Nystroem": {
            "type": "Approximate a kernel map using a subset of the training data.",
            "description": ""
          },
          "RBFSampler": {
            "type": "Approximate a RBF kernel feature map using random Fourier",
            "description": "features."
          },
          "SkewedChi2Sampler": {
            "type": "Approximate feature map for \"skewed chi",
            "description": "squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\nExamples\n--------\n>>> from sklearn.kernel_approximation import PolynomialCountSketch\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> ps = PolynomialCountSketch(degree=3, random_state=1)\n>>> X_features = ps.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n>>> clf.fit(X_features, y)"
          },
          "SGDClassifier": {
            "type": "max_iter=10",
            "description": ">>> clf.score(X_features, y)\n1.0"
          },
          "For": {
            "type": "a more detailed example of usage, see",
            "description": ":ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import PolynomialCountSketch\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> ps = PolynomialCountSketch(degree=3, random_state=1)\n    >>> X_features = ps.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=10)\n    >>> clf.score(X_features, y)\n    1.0\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.kernel_approximation import PolynomialCountSketch\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> ps = PolynomialCountSketch(degree=3, random_state=1)\n    >>> X_features = ps.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=10)\n    >>> clf.score(X_features, y)\n    1.0\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X.\n\n        Initializes the internal variables. The method needs no information\n        about the distribution of data, so we only care about n_features in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Generate the feature map approximation for X.\n\n        Parameters\n        ----------\n        X : {array-like}, shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like}, shape (n_samples, n_features)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "array",
                "description": "like, shape (n_samples, n_components)"
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : array-like, shape (n_samples, n_components)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RBFSampler",
      "documentation": {
        "description": "Approximate a RBF kernel feature map using random Fourier features.\n\n    It implements a variant of Random Kitchen Sinks.[1]\n\n    Read more in the :ref:`User Guide <rbf_kernel_approx>`.\n\n    Parameters\n    ----------\n    gamma : 'scale' or float, default=1.0\n        Parameter of RBF kernel: exp(-gamma * x^2).\n        If ``gamma='scale'`` is passed then it uses\n        1 / (n_features * X.var()) as value of gamma.\n\n        .. versionadded:: 1.2\n           The option `\"scale\"` was added in 1.2.\n\n    n_components : int, default=100\n        Number of Monte Carlo samples per original feature.\n        Equals the dimensionality of the computed feature space.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the generation of the random\n        weights and random offset when fitting the training data.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    random_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n        Random offset used to compute the projection in the `n_components`\n        dimensions of the feature space.\n\n    random_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n        Random projection directions drawn from the Fourier transform\n        of the RBF kernel.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    SkewedChi2Sampler : Approximate feature map for\n        \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    Notes\n    -----\n    See \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\n    Benjamin Recht.\n\n    [1] \"Weighted Sums of Random Kitchen Sinks: Replacing\n    minimization with randomization in learning\" by A. Rahimi and\n    Benjamin Recht.\n    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)",
        "parameters": {
          "gamma": {
            "type": "'scale' or float, default=1.0",
            "description": ""
          },
          "Parameter": {
            "type": "of RBF kernel: exp(-gamma * x^2).",
            "description": ""
          },
          "If": {
            "type": "``gamma='scale'`` is passed then it uses",
            "description": ""
          },
          "1": {
            "type": "/ (n_features * X.var()) as value of gamma.",
            "description": ".. versionadded:: 1.2"
          },
          "The": {
            "type": "option `\"scale\"` was added in 1.2.",
            "description": ""
          },
          "n_components": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "Equals": {
            "type": "the dimensionality of the computed feature space.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Pseudo-random number generator to control the generation of the random"
          },
          "weights": {
            "type": "and random offset when fitting the training data.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "\"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and",
            "description": ""
          },
          "random_offset_": {
            "type": "ndarray of shape (n_components,), dtype={np.float64, np.float32}",
            "description": ""
          },
          "Random": {
            "type": "projection directions drawn from the Fourier transform",
            "description": ""
          },
          "dimensions": {
            "type": "of the feature space.",
            "description": ""
          },
          "random_weights_": {
            "type": "ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}",
            "description": ""
          },
          "of": {
            "type": "the RBF kernel.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "AdditiveChi2Sampler": {
            "type": "Approximate feature map for additive chi2 kernel.",
            "description": ""
          },
          "Nystroem": {
            "type": "Approximate a kernel map using a subset of the training data.",
            "description": ""
          },
          "PolynomialCountSketch": {
            "type": "Polynomial kernel approximation via Tensor Sketch.",
            "description": ""
          },
          "SkewedChi2Sampler": {
            "type": "Approximate feature map for",
            "description": "\"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\nNotes\n-----"
          },
          "Benjamin": {
            "type": "Recht.",
            "description": "(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\nExamples\n--------\n>>> from sklearn.kernel_approximation import RBFSampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n>>> X_features = rbf_feature.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n>>> clf.fit(X_features, y)"
          },
          "minimization": {
            "type": "with randomization in learning\" by A. Rahimi and",
            "description": ""
          },
          "SGDClassifier": {
            "type": "max_iter=5",
            "description": ">>> clf.score(X_features, y)\n1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    SkewedChi2Sampler : Approximate feature map for\n        \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    Notes\n    -----\n    See \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\n    Benjamin Recht.\n\n    [1] \"Weighted Sums of Random Kitchen Sinks: Replacing\n    minimization with randomization in learning\" by A. Rahimi and\n    Benjamin Recht.\n    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0",
        "notes": "-----\n    See \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\n    Benjamin Recht.\n\n    [1] \"Weighted Sums of Random Kitchen Sinks: Replacing\n    minimization with randomization in learning\" by A. Rahimi and\n    Benjamin Recht.\n    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0",
        "examples": "--------\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X.\n\n        Samples random projection according to n_features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like, shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply the approximate feature map to X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "array",
                "description": "like, shape (n_samples, n_components)"
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : array-like, shape (n_samples, n_components)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Real",
      "documentation": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\n    In short, those are: a conversion to float, trunc(), divmod,\n    %, <, <=, >, and >=.\n\n    Real also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SkewedChi2Sampler",
      "documentation": {
        "description": "Approximate feature map for \"skewed chi-squared\" kernel.\n\n    Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.\n\n    Parameters\n    ----------\n    skewedness : float, default=1.0\n        \"skewedness\" parameter of the kernel. Needs to be cross-validated.\n\n    n_components : int, default=100\n        Number of Monte Carlo samples per original feature.\n        Equals the dimensionality of the computed feature space.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the generation of the random\n        weights and random offset when fitting the training data.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    random_weights_ : ndarray of shape (n_features, n_components)\n        Weight array, sampled from a secant hyperbolic distribution, which will\n        be used to linearly transform the log of the data.\n\n    random_offset_ : ndarray of shape (n_features, n_components)\n        Bias term, which will be added to the data. It is uniformly distributed\n        between 0 and 2*pi.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    See \"Random Fourier Approximations for Skewed Multiplicative Histogram\n    Kernels\" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.",
        "parameters": {
          "skewedness": {
            "type": "float, default=1.0",
            "description": "\"skewedness\" parameter of the kernel. Needs to be cross-validated."
          },
          "n_components": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "Equals": {
            "type": "the dimensionality of the computed feature space.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": "Pseudo-random number generator to control the generation of the random"
          },
          "weights": {
            "type": "and random offset when fitting the training data.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "\"Random Fourier Approximations for Skewed Multiplicative Histogram",
            "description": "Kernels\" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.\nExamples\n--------\n>>> from sklearn.kernel_approximation import SkewedChi2Sampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> chi2_feature = SkewedChi2Sampler(skewedness=.01,\n...                                  n_components=10,\n...                                  random_state=0)\n>>> X_features = chi2_feature.fit_transform(X, y)\n>>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n>>> clf.fit(X_features, y)"
          },
          "random_weights_": {
            "type": "ndarray of shape (n_features, n_components)",
            "description": ""
          },
          "Weight": {
            "type": "array, sampled from a secant hyperbolic distribution, which will",
            "description": ""
          },
          "be": {
            "type": "used to linearly transform the log of the data.",
            "description": ""
          },
          "random_offset_": {
            "type": "ndarray of shape (n_features, n_components)",
            "description": ""
          },
          "Bias": {
            "type": "term, which will be added to the data. It is uniformly distributed",
            "description": ""
          },
          "between": {
            "type": "0 and 2*pi.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "AdditiveChi2Sampler": {
            "type": "Approximate feature map for additive chi2 kernel.",
            "description": ""
          },
          "Nystroem": {
            "type": "Approximate a kernel map using a subset of the training data.",
            "description": ""
          },
          "RBFSampler": {
            "type": "Approximate a RBF kernel feature map using random Fourier",
            "description": "features."
          },
          "SkewedChi2Sampler": {
            "type": "Approximate feature map for \"skewed chi",
            "description": "squared\" kernel.\nsklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\nReferences\n----------"
          },
          "SGDClassifier": {
            "type": "max_iter=10",
            "description": ">>> clf.score(X_features, y)\n1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    See \"Random Fourier Approximations for Skewed Multiplicative Histogram\n    Kernels\" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import SkewedChi2Sampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> chi2_feature = SkewedChi2Sampler(skewedness=.01,\n    ...                                  n_components=10,\n    ...                                  random_state=0)\n    >>> X_features = chi2_feature.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=10)\n    >>> clf.score(X_features, y)\n    1.0",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.kernel_approximation import SkewedChi2Sampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> chi2_feature = SkewedChi2Sampler(skewedness=.01,\n    ...                                  n_components=10,\n    ...                                  random_state=0)\n    >>> X_features = chi2_feature.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=10)\n    >>> clf.score(X_features, y)\n    1.0"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the model with X.\n\n        Samples random projection according to n_features.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like, shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply the approximate feature map to X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where `n_samples` is the number of samples\n            and `n_features` is the number of features. All values of X must be\n            strictly greater than \"-skewedness\".",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features)"
              },
              "New": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features. All values of X must be",
                "description": ""
              },
              "strictly": {
                "type": "greater than \"-skewedness\".",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "array",
                "description": "like, shape (n_samples, n_components)"
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : array-like, shape (n_samples, n_components)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StrOptions",
      "documentation": {
        "description": "Constraint representing a finite set of strings.",
        "parameters": {
          "options": {
            "type": "set of str",
            "description": ""
          },
          "The": {
            "type": "set of valid strings.",
            "description": ""
          },
          "deprecated": {
            "type": "set of str or None, default=None",
            "description": ""
          },
          "A": {
            "type": "subset of the `options` to mark as deprecated in the string",
            "description": ""
          },
          "representation": {
            "type": "of the constraint.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TransformerMixin",
      "documentation": {
        "description": "Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])"
      },
      "methods": [
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}