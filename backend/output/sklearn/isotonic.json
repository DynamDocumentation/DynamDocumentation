{
  "description": "Isotonic regression for obtaining monotonic fit to data.",
  "functions": [
    {
      "name": "check_array",
      "signature": "check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_writeable=False, force_all_finite='deprecated', ensure_all_finite=None, ensure_non_negative=False, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name='')",
      "documentation": {
        "description": "Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'} or None, default=None\n        Whether an array will be forced to be fortran or c-style.\n        When order is None (default), then if copy=False, nothing is ensured\n        about the memory layout of the output array; otherwise (copy=True)\n        the memory layout of the returned array is kept as close as possible\n        to the original array.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_writeable : bool, default=False\n        Whether to force the output array to be writeable. If True, the returned array\n        is guaranteed to be writeable, which may require a copy. Otherwise the\n        writeability of the input array is preserved.\n\n        .. versionadded:: 1.6\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n        .. deprecated:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite` and will be removed\n           in 1.8.\n\n    ensure_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite`.\n\n    ensure_non_negative : bool, default=False\n        Make sure the array has only non-negative values. If True, an array that\n        contains negative values will raise a ValueError.\n\n        .. versionadded:: 1.6\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if array is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow array.ndim > 2.\n\n    ensure_min_samples : int, default=1\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message. In particular\n        if `input_name` is \"X\" and the data has NaN values and\n        allow_nan is False, the error message will link to the imputer\n        documentation.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    array_converted : object\n        The converted and validated array.",
        "parameters": {
          "array": {
            "type": "[[1, 2, 3], [4, 5, 6]]",
            "description": ""
          },
          "Input": {
            "type": "object to check / convert.",
            "description": ""
          },
          "accept_sparse": {
            "type": "str, bool or list/tuple of str, default=False",
            "description": "String[s] representing allowed sparse matrix formats, such as 'csc',\n'csr', etc. If the input is sparse but not in the allowed format,"
          },
          "it": {
            "type": "will be converted to the first listed format. True allows the input",
            "description": ""
          },
          "to": {
            "type": "the original array.",
            "description": ""
          },
          "raise": {
            "type": "an error.",
            "description": ""
          },
          "accept_large_sparse": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "passed, include the name of the estimator in warning messages.",
            "description": ""
          },
          "only": {
            "type": "if its indices are stored with a 32-bit dtype.",
            "description": ".. versionadded:: 0.20"
          },
          "dtype": {
            "type": "'numeric', type, list of type or None, default='numeric'",
            "description": ""
          },
          "Data": {
            "type": "type of result. If None, the dtype of the input is preserved.",
            "description": ""
          },
          "performed": {
            "type": "if the dtype of the input is not in the list.",
            "description": ""
          },
          "order": {
            "type": "{'F', 'C'} or None, default=None",
            "description": ""
          },
          "Whether": {
            "type": "to allow array.ndim > 2.",
            "description": ""
          },
          "When": {
            "type": "order is None (default), then if copy=False, nothing is ensured",
            "description": ""
          },
          "about": {
            "type": "the memory layout of the output array; otherwise (copy=True)",
            "description": ""
          },
          "the": {
            "type": "memory layout of the returned array is kept as close as possible",
            "description": ""
          },
          "copy": {
            "type": "bool, default=False",
            "description": ""
          },
          "be": {
            "type": "triggered by a conversion.",
            "description": ""
          },
          "force_writeable": {
            "type": "bool, default=False",
            "description": ""
          },
          "is": {
            "type": "guaranteed to be writeable, which may require a copy. Otherwise the",
            "description": ""
          },
          "writeability": {
            "type": "of the input array is preserved.",
            "description": ".. versionadded:: 1.6"
          },
          "force_all_finite": {
            "type": "bool or 'allow",
            "description": "nan', default=True"
          },
          "possibilities": {
            "type": "are:",
            "description": "- True: Force all values of array to be finite.\n- False: accepts np.inf, np.nan, pd.NA in array.\n- 'allow-nan': accepts only np.nan and pd.NA values in array. Values"
          },
          "cannot": {
            "type": "be infinite.",
            "description": ".. versionadded:: 1.6\n`force_all_finite` was renamed to `ensure_all_finite`."
          },
          "Accepts": {
            "type": "`pd.NA` and converts it into `np.nan`",
            "description": ".. deprecated:: 1.6\n`force_all_finite` was renamed to `ensure_all_finite` and will be removed"
          },
          "in": {
            "type": "1.8.",
            "description": ""
          },
          "ensure_all_finite": {
            "type": "bool or 'allow",
            "description": "nan', default=True"
          },
          "ensure_non_negative": {
            "type": "bool, default=False",
            "description": ""
          },
          "Make": {
            "type": "sure that the 2D array has some minimum number of features",
            "description": "(columns). The default value of 1 rejects empty datasets."
          },
          "contains": {
            "type": "negative values will raise a ValueError.",
            "description": ".. versionadded:: 1.6"
          },
          "ensure_2d": {
            "type": "bool, default=True",
            "description": ""
          },
          "allow_nd": {
            "type": "bool, default=False",
            "description": ""
          },
          "ensure_min_samples": {
            "type": "int, default=1",
            "description": ""
          },
          "axis": {
            "type": "rows for a 2D array",
            "description": ". Setting to 0 disables this check."
          },
          "ensure_min_features": {
            "type": "int, default=1",
            "description": ""
          },
          "This": {
            "type": "check is only enforced when the input data has effectively 2",
            "description": ""
          },
          "dimensions": {
            "type": "or is originally 1D and ``ensure_2d`` is True. Setting to 0",
            "description": ""
          },
          "disables": {
            "type": "this check.",
            "description": ""
          },
          "estimator": {
            "type": "str or estimator instance, default=None",
            "description": ""
          },
          "input_name": {
            "type": "str, default=\"\"",
            "description": ""
          },
          "The": {
            "type": "converted and validated array.",
            "description": "Examples\n--------\n>>> from sklearn.utils.validation import check_array\n>>> X = [[1, 2, 3], [4, 5, 6]]\n>>> X_checked = check_array(X)\n>>> X_checked"
          },
          "if": {
            "type": "`input_name` is \"X\" and the data has NaN values and",
            "description": ""
          },
          "allow_nan": {
            "type": "is False, the error message will link to the imputer",
            "description": "documentation.\n.. versionadded:: 1.1.0\nReturns\n-------"
          },
          "array_converted": {
            "type": "object",
            "description": ""
          }
        },
        "returns": "-------\n    array_converted : object\n        The converted and validated array.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_array\n    >>> X = [[1, 2, 3], [4, 5, 6]]\n    >>> X_checked = check_array(X)\n    >>> X_checked\n    array([[1, 2, 3], [4, 5, 6]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.validation import check_array\n    >>> X = [[1, 2, 3], [4, 5, 6]]\n    >>> X_checked = check_array(X)\n    >>> X_checked\n    array([[1, 2, 3], [4, 5, 6]])"
      }
    },
    {
      "name": "check_consistent_length",
      "signature": "check_consistent_length(*arrays)",
      "documentation": {
        "description": "Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.",
        "parameters": {
          "Objects": {
            "type": "that will be checked for consistent length.",
            "description": "Examples\n--------\n>>> from sklearn.utils.validation import check_consistent_length\n>>> a = [1, 2, 3]\n>>> b = [2, 3, 4]\n>>> check_consistent_length(a, b)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.validation import check_consistent_length\n    >>> a = [1, 2, 3]\n    >>> b = [2, 3, 4]\n    >>> check_consistent_length(a, b)"
      }
    },
    {
      "name": "check_increasing",
      "signature": "check_increasing(x, y)",
      "documentation": {
        "description": "Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation",
        "parameters": {
          "x": {
            "type": "array",
            "description": "like of shape (n_samples,)"
          },
          "Training": {
            "type": "target.",
            "description": "Returns\n-------"
          },
          "y": {
            "type": "array",
            "description": "like of shape (n_samples,)"
          },
          "increasing_bool": {
            "type": "boolean",
            "description": ""
          },
          "Whether": {
            "type": "the relationship is increasing or decreasing.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "Spearman correlation coefficient is estimated from the data, and the",
            "description": ""
          },
          "sign": {
            "type": "of the resulting estimate is used as the result.",
            "description": ""
          },
          "In": {
            "type": "the event that the 95% confidence interval based on Fisher transform",
            "description": ""
          },
          "spans": {
            "type": "zero, a warning is raised.",
            "description": "References\n----------"
          },
          "Fisher": {
            "type": "transformation. Wikipedia.",
            "description": ""
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/Fisher_transformation",
            "description": "Examples\n--------\n>>> from sklearn.isotonic import check_increasing\n>>> x, y = [1, 2, 3, 4, 5], [2, 4, 6, 8, 10]\n>>> check_increasing(x, y)\nnp.True_\n>>> y = [10, 8, 6, 4, 2]\n>>> check_increasing(x, y)\nnp.False_"
          }
        },
        "returns": "-------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n\n    Examples\n    --------\n    >>> from sklearn.isotonic import check_increasing\n    >>> x, y = [1, 2, 3, 4, 5], [2, 4, 6, 8, 10]\n    >>> check_increasing(x, y)\n    np.True_\n    >>> y = [10, 8, 6, 4, 2]\n    >>> check_increasing(x, y)\n    np.False_",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n\n    Examples\n    --------\n    >>> from sklearn.isotonic import check_increasing\n    >>> x, y = [1, 2, 3, 4, 5], [2, 4, 6, 8, 10]\n    >>> check_increasing(x, y)\n    np.True_\n    >>> y = [10, 8, 6, 4, 2]\n    >>> check_increasing(x, y)\n    np.False_",
        "examples": "--------\n    >>> from sklearn.isotonic import check_increasing\n    >>> x, y = [1, 2, 3, 4, 5], [2, 4, 6, 8, 10]\n    >>> check_increasing(x, y)\n    np.True_\n    >>> y = [10, 8, 6, 4, 2]\n    >>> check_increasing(x, y)\n    np.False_"
      }
    },
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "documentation": {
        "description": "Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "Estimator": {
            "type": "instance for which the check is performed.",
            "description": ""
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": ""
          },
          "Attribute": {
            "type": "name(s) given as string or a list/tuple of strings",
            "description": "Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"
          },
          "If": {
            "type": "the attributes are not found.",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")"
          },
          "attribute": {
            "type": "that ends with a underscore and does not start with double",
            "description": "underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": ""
          },
          "The": {
            "type": "default error message is, \"This %(name)s instance is not fitted",
            "description": "yet. Call 'fit' with appropriate arguments before using this\nestimator.\""
          },
          "For": {
            "type": "custom messages if \"%(name)s\" is present in the message string,",
            "description": ""
          },
          "it": {
            "type": "is substituted for the estimator name.",
            "description": "Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": ""
          },
          "Specify": {
            "type": "whether all or any of the given attributes must exist.",
            "description": "Raises\n------\nTypeError"
          },
          "Model": {
            "type": "is not fitted yet.",
            "description": ">>> lr.fit([[1, 2], [1, 3]], [1, 0])"
          },
          "LogisticRegression": {
            "type": "",
            "description": ">>> check_is_fitted(lr)"
          }
        },
        "returns": "",
        "raises": "a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)"
      }
    },
    {
      "name": "isotonic_regression",
      "signature": "isotonic_regression(y, *, sample_weight=None, y_min=None, y_max=None, increasing=True)",
      "documentation": {
        "description": "Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : ndarray of shape (n_samples,)\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.",
        "parameters": {
          "y": {
            "type": "array",
            "description": "like of shape (n_samples,)"
          },
          "The": {
            "type": "data.",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Weights": {
            "type": "on each point of the regression.",
            "description": ""
          },
          "If": {
            "type": "None, weight is set to 1 (equal weights).",
            "description": ""
          },
          "y_min": {
            "type": "float, default=None",
            "description": ""
          },
          "Lower": {
            "type": "bound on the lowest predicted value (the minimum value may",
            "description": ""
          },
          "still": {
            "type": "be higher). If not set, defaults to -inf.",
            "description": ""
          },
          "y_max": {
            "type": "float, default=None",
            "description": ""
          },
          "Upper": {
            "type": "bound on the highest predicted value (the maximum may still be",
            "description": "lower). If not set, defaults to +inf."
          },
          "increasing": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to compute ``y_`` is increasing (if set to True) or decreasing",
            "description": "(if set to False).\nReturns\n-------"
          },
          "y_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Isotonic": {
            "type": "fit of y.",
            "description": "References\n----------\n\"Active set algorithms for isotonic regression; A unifying framework\""
          },
          "by": {
            "type": "Michael J. Best and Nilotpal Chakravarti, section 3.",
            "description": "Examples\n--------\n>>> from sklearn.isotonic import isotonic_regression\n>>> isotonic_regression([5, 3, 1, 2, 8, 10, 7, 9, 6, 4])\narray([2.75   , 2.75   , 2.75   , 2.75   , 7.33...,\n7.33..., 7.33..., 7.33..., 7.33..., 7.33...])"
          }
        },
        "returns": "-------\n    y_ : ndarray of shape (n_samples,)\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n\n    Examples\n    --------\n    >>> from sklearn.isotonic import isotonic_regression\n    >>> isotonic_regression([5, 3, 1, 2, 8, 10, 7, 9, 6, 4])\n    array([2.75   , 2.75   , 2.75   , 2.75   , 7.33...,\n           7.33..., 7.33..., 7.33..., 7.33..., 7.33...])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.isotonic import isotonic_regression\n    >>> isotonic_regression([5, 3, 1, 2, 8, 10, 7, 9, 6, 4])\n    array([2.75   , 2.75   , 2.75   , 2.75   , 7.33...,\n           7.33..., 7.33..., 7.33..., 7.33..., 7.33...])"
      }
    },
    {
      "name": "parse_version",
      "signature": "parse(version: str) -> Union[ForwardRef('LegacyVersion'), ForwardRef('Version')]",
      "documentation": {
        "description": "Parse the given version from a string to an appropriate class.\n\n    Parameters\n    ----------\n    version : str\n        Version in a string format, eg. \"0.9.1\" or \"1.2.dev0\".",
        "parameters": {
          "version": {
            "type": ":class:`Version` object or a :class:`LegacyVersion` object",
            "description": ""
          },
          "Version": {
            "type": "in a string format, eg. \"0.9.1\" or \"1.2.dev0\".",
            "description": "Returns\n-------"
          },
          "Returned": {
            "type": "class depends on the given version: if is a valid",
            "description": ""
          },
          "PEP": {
            "type": "440 version or a legacy version.",
            "description": ""
          }
        },
        "returns": "-------\n    version : :class:`Version` object or a :class:`LegacyVersion` object\n        Returned class depends on the given version: if is a valid\n        PEP 440 version or a legacy version.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "spearmanr",
      "signature": "spearmanr(a, b=None, axis=0, nan_policy='propagate', alternative='two-sided')",
      "documentation": {
        "description": "Calculate a Spearman correlation coefficient with associated p-value.\n\n    The Spearman rank-order correlation coefficient is a nonparametric measure\n    of the monotonicity of the relationship between two datasets.\n    Like other correlation coefficients,\n    this one varies between -1 and +1 with 0 implying no correlation.\n    Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n    correlations imply that as x increases, so does y. Negative correlations\n    imply that as x increases, y decreases.\n\n    The p-value roughly indicates the probability of an uncorrelated system\n    producing datasets that have a Spearman correlation at least as extreme\n    as the one computed from these datasets. Although calculation of the\n    p-value does not make strong assumptions about the distributions underlying\n    the samples, it is only accurate for very large samples (>500\n    observations). For smaller sample sizes, consider a permutation test (see",
        "parameters": {
          "One": {
            "type": "or two 1-D or 2-D arrays containing multiple variables and",
            "description": "observations. When these are 1-D, each represents a vector of"
          },
          "observations": {
            "type": "in sample ``x`` and ``y``), only one of the two inputs needs",
            "description": ""
          },
          "see": {
            "type": "under ``axis``, below.",
            "description": ""
          },
          "Both": {
            "type": "arrays need to have the same length in the ``axis`` dimension.",
            "description": ""
          },
          "axis": {
            "type": "int or None, optional",
            "description": ""
          },
          "If": {
            "type": "`axis` is not 0, 1 or None, or if the number of dimensions of `a`",
            "description": ""
          },
          "each": {
            "type": "row represents a variable, while the columns contain observations.",
            "description": ""
          },
          "nan_policy": {
            "type": "{'propagate', 'raise', 'omit'}, optional",
            "description": ""
          },
          "Defines": {
            "type": "the alternative hypothesis. Default is 'two-sided'.",
            "description": ""
          },
          "The": {
            "type": "Advanced Theory of Statistics, Volume 2: Inference and Relationship.",
            "description": "Griffin. 1973."
          },
          "alternative": {
            "type": "{'two",
            "description": "sided', 'less', 'greater'}, optional"
          },
          "res": {
            "type": "SignificanceResult",
            "description": ""
          },
          "An": {
            "type": "object containing attributes:",
            "description": ""
          },
          "statistic": {
            "type": "float or ndarray (2",
            "description": "D square)"
          },
          "Spearman": {
            "type": "correlation matrix or correlation coefficient (if only 2",
            "description": ""
          },
          "variables": {
            "type": "are given as parameters). Correlation matrix is square",
            "description": ""
          },
          "with": {
            "type": "length equal to total number of variables (columns or rows) in",
            "description": "``a`` and ``b`` combined."
          },
          "pvalue": {
            "type": "float",
            "description": ""
          },
          "is": {
            "type": "not defined in this case, so ``np.nan`` is returned.",
            "description": ""
          },
          "same": {
            "type": "shape as `statistic`.",
            "description": "Raises\n------\nValueError"
          },
          "Raised": {
            "type": "if an input is a constant array.  The correlation coefficient",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\n:ref:`hypothesis_spearmanr` : Extended example\nReferences\n----------\n.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard"
          },
          "Probability": {
            "type": "and Statistics Tables and Formulae. Chapman & Hall: New",
            "description": "York. 2000."
          },
          "Section": {
            "type": "31.18",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> res = stats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n>>> res.statistic\n0.8207826816681233\n>>> res.pvalue\n0.08858700531354381\n>>> rng = np.random.default_rng()\n>>> x2n = rng.standard_normal((100, 2))\n>>> y2n = rng.standard_normal((100, 2))\n>>> res = stats.spearmanr(x2n)\n>>> res.statistic, res.pvalue\n(-0.07960396039603959, 0.4311168705769747)\n>>> res = stats.spearmanr(x2n[:, 0], x2n[:, 1])\n>>> res.statistic, res.pvalue\n(-0.07960396039603959, 0.4311168705769747)\n>>> res = stats.spearmanr(x2n, y2n)\n>>> res.statistic\narray([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n[-0.07960396, 1. , -0.14448245, 0.16738074],\n[-0.08314431, -0.14448245, 1. , 0.03234323],\n[ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n>>> res.pvalue\narray([[0. , 0.43111687, 0.41084066, 0.33891628],\n[0.43111687, 0. , 0.15151618, 0.09600687],\n[0.41084066, 0.15151618, 0. , 0.74938561],\n[0.33891628, 0.09600687, 0.74938561, 0. ]])\n>>> res = stats.spearmanr(x2n.T, y2n.T, axis=1)\n>>> res.statistic\narray([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n[-0.07960396, 1. , -0.14448245, 0.16738074],\n[-0.08314431, -0.14448245, 1. , 0.03234323],\n[ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n>>> res = stats.spearmanr(x2n, y2n, axis=None)\n>>> res.statistic, res.pvalue\n(0.044981624540613524, 0.5270803651336189)\n>>> res = stats.spearmanr(x2n.ravel(), y2n.ravel())\n>>> res.statistic, res.pvalue\n(0.044981624540613524, 0.5270803651336189)\n>>> rng = np.random.default_rng()\n>>> xint = rng.integers(10, size=(100, 2))\n>>> res = stats.spearmanr(xint)\n>>> res.statistic, res.pvalue\n(0.09800224850707953, 0.3320271757932076)"
          },
          "For": {
            "type": "a more detailed example, see :ref:`hypothesis_spearmanr`.",
            "description": ""
          },
          "relying": {
            "type": "on the asymptotic p-value. Note that to calculate the null",
            "description": ""
          },
          "distribution": {
            "type": "of the statistic (for all possibly pairings between",
            "description": ""
          },
          "to": {
            "type": "be permuted.",
            "description": ">>> x = [1.76405235, 0.40015721, 0.97873798,\n... 2.2408932, 1.86755799, -0.97727788]\n>>> y = [2.71414076, 0.2488, 0.87551913,\n... 2.6514917, 2.01160156, 0.47699563]\n>>> def statistic(x): # permute only `x`\n...     return stats.spearmanr(x, y).statistic\n>>> res_exact = stats.permutation_test((x,), statistic,\n...     permutation_type='pairings')\n>>> res_asymptotic = stats.spearmanr(x, y)\n>>> res_exact.pvalue, res_asymptotic.pvalue # asymptotic pvalue is too low\n(0.10277777777777777, 0.07239650145772594)"
          }
        },
        "returns": "-------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float or ndarray (2-D square)\n            Spearman correlation matrix or correlation coefficient (if only 2\n            variables are given as parameters). Correlation matrix is square\n            with length equal to total number of variables (columns or rows) in\n            ``a`` and ``b`` combined.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis\n            is that two samples have no ordinal correlation. See\n            `alternative` above for alternative hypotheses. `pvalue` has the\n            same shape as `statistic`.\n\n    Raises\n    ------\n    ValueError\n        If `axis` is not 0, 1 or None, or if the number of dimensions of `a`\n        is greater than 2, or if `b` is None and the number of dimensions of\n        `a` is less than 2.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    :ref:`hypothesis_spearmanr` : Extended example\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n    .. [2] Kendall, M. G. and Stuart, A. (1973).\n       The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n       Griffin. 1973.\n       Section 31.18\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> res = stats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n    >>> res.statistic\n    0.8207826816681233\n    >>> res.pvalue\n    0.08858700531354381\n\n    >>> rng = np.random.default_rng()\n    >>> x2n = rng.standard_normal((100, 2))\n    >>> y2n = rng.standard_normal((100, 2))\n    >>> res = stats.spearmanr(x2n)\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n[:, 0], x2n[:, 1])\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n, y2n)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n    >>> res.pvalue\n    array([[0. , 0.43111687, 0.41084066, 0.33891628],\n           [0.43111687, 0. , 0.15151618, 0.09600687],\n           [0.41084066, 0.15151618, 0. , 0.74938561],\n           [0.33891628, 0.09600687, 0.74938561, 0. ]])\n\n    >>> res = stats.spearmanr(x2n.T, y2n.T, axis=1)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n\n    >>> res = stats.spearmanr(x2n, y2n, axis=None)\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> res = stats.spearmanr(x2n.ravel(), y2n.ravel())\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> rng = np.random.default_rng()\n    >>> xint = rng.integers(10, size=(100, 2))\n    >>> res = stats.spearmanr(xint)\n    >>> res.statistic, res.pvalue\n    (0.09800224850707953, 0.3320271757932076)\n\n    For small samples, consider performing a permutation test instead of\n    relying on the asymptotic p-value. Note that to calculate the null\n    distribution of the statistic (for all possibly pairings between\n    observations in sample ``x`` and ``y``), only one of the two inputs needs\n    to be permuted.\n\n    >>> x = [1.76405235, 0.40015721, 0.97873798,\n    ... 2.2408932, 1.86755799, -0.97727788]\n    >>> y = [2.71414076, 0.2488, 0.87551913,\n    ... 2.6514917, 2.01160156, 0.47699563]\n\n    >>> def statistic(x): # permute only `x`\n    ...     return stats.spearmanr(x, y).statistic\n    >>> res_exact = stats.permutation_test((x,), statistic,\n    ...     permutation_type='pairings')\n    >>> res_asymptotic = stats.spearmanr(x, y)\n    >>> res_exact.pvalue, res_asymptotic.pvalue # asymptotic pvalue is too low\n    (0.10277777777777777, 0.07239650145772594)\n\n    For a more detailed example, see :ref:`hypothesis_spearmanr`.",
        "raises": "------\n    ValueError\n        If `axis` is not 0, 1 or None, or if the number of dimensions of `a`\n        is greater than 2, or if `b` is None and the number of dimensions of\n        `a` is less than 2.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    :ref:`hypothesis_spearmanr` : Extended example\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n    .. [2] Kendall, M. G. and Stuart, A. (1973).\n       The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n       Griffin. 1973.\n       Section 31.18\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> res = stats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n    >>> res.statistic\n    0.8207826816681233\n    >>> res.pvalue\n    0.08858700531354381\n\n    >>> rng = np.random.default_rng()\n    >>> x2n = rng.standard_normal((100, 2))\n    >>> y2n = rng.standard_normal((100, 2))\n    >>> res = stats.spearmanr(x2n)\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n[:, 0], x2n[:, 1])\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n, y2n)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n    >>> res.pvalue\n    array([[0. , 0.43111687, 0.41084066, 0.33891628],\n           [0.43111687, 0. , 0.15151618, 0.09600687],\n           [0.41084066, 0.15151618, 0. , 0.74938561],\n           [0.33891628, 0.09600687, 0.74938561, 0. ]])\n\n    >>> res = stats.spearmanr(x2n.T, y2n.T, axis=1)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n\n    >>> res = stats.spearmanr(x2n, y2n, axis=None)\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> res = stats.spearmanr(x2n.ravel(), y2n.ravel())\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> rng = np.random.default_rng()\n    >>> xint = rng.integers(10, size=(100, 2))\n    >>> res = stats.spearmanr(xint)\n    >>> res.statistic, res.pvalue\n    (0.09800224850707953, 0.3320271757932076)\n\n    For small samples, consider performing a permutation test instead of\n    relying on the asymptotic p-value. Note that to calculate the null\n    distribution of the statistic (for all possibly pairings between\n    observations in sample ``x`` and ``y``), only one of the two inputs needs\n    to be permuted.\n\n    >>> x = [1.76405235, 0.40015721, 0.97873798,\n    ... 2.2408932, 1.86755799, -0.97727788]\n    >>> y = [2.71414076, 0.2488, 0.87551913,\n    ... 2.6514917, 2.01160156, 0.47699563]\n\n    >>> def statistic(x): # permute only `x`\n    ...     return stats.spearmanr(x, y).statistic\n    >>> res_exact = stats.permutation_test((x,), statistic,\n    ...     permutation_type='pairings')\n    >>> res_asymptotic = stats.spearmanr(x, y)\n    >>> res_exact.pvalue, res_asymptotic.pvalue # asymptotic pvalue is too low\n    (0.10277777777777777, 0.07239650145772594)\n\n    For a more detailed example, see :ref:`hypothesis_spearmanr`.",
        "see_also": "--------\n    :ref:`hypothesis_spearmanr` : Extended example\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n    .. [2] Kendall, M. G. and Stuart, A. (1973).\n       The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n       Griffin. 1973.\n       Section 31.18\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> res = stats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n    >>> res.statistic\n    0.8207826816681233\n    >>> res.pvalue\n    0.08858700531354381\n\n    >>> rng = np.random.default_rng()\n    >>> x2n = rng.standard_normal((100, 2))\n    >>> y2n = rng.standard_normal((100, 2))\n    >>> res = stats.spearmanr(x2n)\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n[:, 0], x2n[:, 1])\n    >>> res.statistic, res.pvalue\n    (-0.07960396039603959, 0.4311168705769747)\n\n    >>> res = stats.spearmanr(x2n, y2n)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n    >>> res.pvalue\n    array([[0. , 0.43111687, 0.41084066, 0.33891628],\n           [0.43111687, 0. , 0.15151618, 0.09600687],\n           [0.41084066, 0.15151618, 0. , 0.74938561],\n           [0.33891628, 0.09600687, 0.74938561, 0. ]])\n\n    >>> res = stats.spearmanr(x2n.T, y2n.T, axis=1)\n    >>> res.statistic\n    array([[ 1. , -0.07960396, -0.08314431, 0.09662166],\n           [-0.07960396, 1. , -0.14448245, 0.16738074],\n           [-0.08314431, -0.14448245, 1. , 0.03234323],\n           [ 0.09662166, 0.16738074, 0.03234323, 1. ]])\n\n    >>> res = stats.spearmanr(x2n, y2n, axis=None)\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> res = stats.spearmanr(x2n.ravel(), y2n.ravel())\n    >>> res.statistic, res.pvalue\n    (0.044981624540613524, 0.5270803651336189)\n\n    >>> rng = np.random.default_rng()\n    >>> xint = rng.integers(10, size=(100, 2))\n    >>> res = stats.spearmanr(xint)\n    >>> res.statistic, res.pvalue\n    (0.09800224850707953, 0.3320271757932076)\n\n    For small samples, consider performing a permutation test instead of\n    relying on the asymptotic p-value. Note that to calculate the null\n    distribution of the statistic (for all possibly pairings between\n    observations in sample ``x`` and ``y``), only one of the two inputs needs\n    to be permuted.\n\n    >>> x = [1.76405235, 0.40015721, 0.97873798,\n    ... 2.2408932, 1.86755799, -0.97727788]\n    >>> y = [2.71414076, 0.2488, 0.87551913,\n    ... 2.6514917, 2.01160156, 0.47699563]\n\n    >>> def statistic(x): # permute only `x`\n    ...     return stats.spearmanr(x, y).statistic\n    >>> res_exact = stats.permutation_test((x,), statistic,\n    ...     permutation_type='pairings')\n    >>> res_asymptotic = stats.spearmanr(x, y)\n    >>> res_exact.pvalue, res_asymptotic.pvalue # asymptotic pvalue is too low\n    (0.10277777777777777, 0.07239650145772594)\n\n    For a more detailed example, see :ref:`hypothesis_spearmanr`.",
        "notes": "",
        "examples": "section below).\n\n    Parameters\n    ----------\n    a, b : 1D or 2D array_like, b is optional\n        One or two 1-D or 2-D arrays containing multiple variables and\n        observations. When these are 1-D, each represents a vector of\n        observations of a single variable. For the behavior in the 2-D case,\n        see under ``axis``, below.\n        Both arrays need to have the same length in the ``axis`` dimension.\n    axis : int or None, optional\n        If axis=0 (default), then each column represents a variable, with\n        observations in the rows. If axis=1, the relationship is transposed:\n        each row represents a variable, while the columns contain observations.\n        If axis=None, then both arrays will be raveled.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the correlation is nonzero\n        * 'less': the correlation is negative (less than zero)\n        * 'greater':  the correlation is positive (greater than zero)\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float or ndarray (2-D square)\n            Spearman correlation matrix or correlation coefficient (if only 2\n            variables are given as parameters). Correlation matrix is square\n            with length equal to total number of variables (columns or rows) in\n            ``a`` and ``b`` combined.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis\n            is that two samples have no ordinal correlation. See\n            `alternative` above for alternative hypotheses. `pvalue` has the\n            same shape as `statistic`.\n\n    Raises\n    ------\n    ValueError\n        If `axis` is not 0, 1 or None, or if the number of dimensions of `a`\n        is greater than 2, or if `b` is None and the number of dimensions of\n        `a` is less than 2.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    :ref:`hypothesis_spearmanr` : Extended example\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n    .. [2] Kendall, M. G. and Stuart, A. (1973).\n       The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n       Griffin. 1973.\n       Section 31.18"
      }
    },
    {
      "name": "validate_params",
      "signature": "validate_params(parameter_constraints, *, prefer_skip_nested_validation)",
      "documentation": {
        "description": "Decorator to validate types and values of functions and methods.\n\n    Parameters\n    ----------\n    parameter_constraints : dict\n        A dictionary `param_name: list of constraints`. See the docstring of\n        `validate_parameter_constraints` for a description of the accepted constraints.",
        "parameters": {
          "parameter_constraints": {
            "type": "dict",
            "description": ""
          },
          "A": {
            "type": "dictionary `param_name: list of constraints`. See the docstring of",
            "description": "`validate_parameter_constraints` for a description of the accepted constraints."
          },
          "Note": {
            "type": "that the *args and **kwargs parameters are not validated and must not be",
            "description": ""
          },
          "present": {
            "type": "in the parameter_constraints dictionary.",
            "description": ""
          },
          "prefer_skip_nested_validation": {
            "type": "bool",
            "description": ""
          },
          "If": {
            "type": "True, the validation of parameters of inner estimators or functions",
            "description": ""
          },
          "called": {
            "type": "by the decorated function will be skipped.",
            "description": ""
          },
          "This": {
            "type": "is useful to avoid validating many times the parameters passed by the",
            "description": ""
          },
          "user": {
            "type": "from the public facing API. It's also useful to avoid validating",
            "description": ""
          }
        },
        "returns": "-------\n    decorated_function : function or method\n        The decorated function.",
        "raises": "",
        "see_also": "",
        "notes": "that the *args and **kwargs parameters are not validated and must not be\n        present in the parameter_constraints dictionary.\n\n    prefer_skip_nested_validation : bool\n        If True, the validation of parameters of inner estimators or functions\n        called by the decorated function will be skipped.\n\n        This is useful to avoid validating many times the parameters passed by the\n        user from the public facing API. It's also useful to avoid validating\n        parameters that we pass internally to inner functions that are guaranteed to\n        be valid by the test suite.\n\n        It should be set to True for most functions, except for those that receive\n        non-validated objects as parameters or that are just wrappers around classes\n        because they only perform a partial validation.\n\n    Returns\n    -------\n    decorated_function : function or method\n        The decorated function.",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "BaseEstimator",
      "documentation": {
        "description": "Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).",
        "parameters": {
          "array": {
            "type": "[3, 3, 3]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])"
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interval",
      "documentation": {
        "description": "Constraint representing a typed interval.\n\n    Parameters\n    ----------\n    type : {numbers.Integral, numbers.Real, RealNotInt}\n        The set of numbers in which to set the interval.\n\n        If RealNotInt, only reals that don't have the integer type\n        are allowed. For example 1.0 is allowed but 1 is not.\n\n    left : float or int or None\n        The left bound of the interval. None means left bound is -∞.\n\n    right : float, int or None\n        The right bound of the interval. None means right bound is +∞.\n\n    closed : {\"left\", \"right\", \"both\", \"neither\"}\n        Whether the interval is open or closed. Possible choices are:\n\n        - `\"left\"`: the interval is closed on the left and open on the right.\n          It is equivalent to the interval `[ left, right )`.\n        - `\"right\"`: the interval is closed on the right and open on the left.\n          It is equivalent to the interval `( left, right ]`.\n        - `\"both\"`: the interval is closed.\n          It is equivalent to the interval `[ left, right ]`.\n        - `\"neither\"`: the interval is open.\n          It is equivalent to the interval `( left, right )`.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": ""
          },
          "The": {
            "type": "right bound of the interval. None means right bound is +∞.",
            "description": ""
          },
          "If": {
            "type": "RealNotInt, only reals that don't have the integer type",
            "description": ""
          },
          "are": {
            "type": "allowed. For example 1.0 is allowed but 1 is not.",
            "description": ""
          },
          "left": {
            "type": "float or int or None",
            "description": ""
          },
          "right": {
            "type": "float, int or None",
            "description": ""
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": ""
          },
          "Whether": {
            "type": "the interval is open or closed. Possible choices are:",
            "description": "- `\"left\"`: the interval is closed on the left and open on the right."
          },
          "It": {
            "type": "is equivalent to the interval `( left, right )`.",
            "description": "Notes\n-----"
          },
          "Setting": {
            "type": "a bound to `None` and setting the interval closed is valid. For instance,",
            "description": ""
          },
          "strictly": {
            "type": "speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to",
            "description": "`[0, +∞) U {+∞}`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    Setting a bound to `None` and setting the interval closed is valid. For instance,\n    strictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n    `[0, +∞) U {+∞}`.",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "IsotonicRegression",
      "documentation": {
        "description": "Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977",
        "parameters": {
          "y_min": {
            "type": "float, default=None",
            "description": ""
          },
          "Lower": {
            "type": "bound on the lowest predicted value (the minimum value may",
            "description": ""
          },
          "still": {
            "type": "be higher). If not set, defaults to -inf.",
            "description": ""
          },
          "y_max": {
            "type": "float, default=None",
            "description": ""
          },
          "Upper": {
            "type": "bound on the highest predicted value (the maximum may still be",
            "description": "lower). If not set, defaults to +inf."
          },
          "increasing": {
            "type": "bool or 'auto', default=True",
            "description": ""
          },
          "Determines": {
            "type": "whether the predictions should be constrained to increase",
            "description": ""
          },
          "or": {
            "type": "decrease with `X`. 'auto' will decide based on the Spearman",
            "description": ""
          },
          "correlation": {
            "type": "estimate's sign.",
            "description": ""
          },
          "out_of_bounds": {
            "type": "{'nan', 'clip', 'raise'}, default='nan'",
            "description": ""
          },
          "Handles": {
            "type": "how `X` values outside of the training domain are handled",
            "description": ""
          },
          "during": {
            "type": "prediction.",
            "description": "- 'nan', predictions will be NaN.\n- 'clip', predictions will be set to the value corresponding to"
          },
          "the": {
            "type": "y = f(X) monotonic function.",
            "description": ".. versionadded:: 0.24"
          },
          "X_min_": {
            "type": "float",
            "description": ""
          },
          "Minimum": {
            "type": "value of input array `X_` for left bound.",
            "description": ""
          },
          "X_max_": {
            "type": "float",
            "description": ""
          },
          "Maximum": {
            "type": "value of input array `X_` for right bound.",
            "description": ""
          },
          "X_thresholds_": {
            "type": "ndarray of shape (n_thresholds,)",
            "description": ""
          },
          "Unique": {
            "type": "ascending `X` values used to interpolate",
            "description": ""
          },
          "y_thresholds_": {
            "type": "ndarray of shape (n_thresholds,)",
            "description": "De-duplicated `y` values suitable to interpolate the y = f(X)"
          },
          "monotonic": {
            "type": "function.",
            "description": ".. versionadded:: 0.24"
          },
          "f_": {
            "type": "function",
            "description": ""
          },
          "The": {
            "type": "stepwise interpolating function that covers the input domain ``X``.",
            "description": ""
          },
          "increasing_": {
            "type": "bool",
            "description": ""
          },
          "Inferred": {
            "type": "value for ``increasing``.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.linear_model.LinearRegression : Ordinary least squares Linear\nRegression.\nsklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that"
          },
          "is": {
            "type": "a non-parametric model accepting monotonicity constraints.",
            "description": ""
          },
          "isotonic_regression": {
            "type": "Function to solve the isotonic regression model.",
            "description": "Notes\n-----"
          },
          "Ties": {
            "type": "are broken using the secondary method from de Leeuw, 1977.",
            "description": "References\n----------"
          },
          "Isotonic": {
            "type": "Median Regression: A Linear Programming Approach",
            "description": ""
          },
          "Nilotpal": {
            "type": "Chakravarti",
            "description": ""
          },
          "Mathematics": {
            "type": "of Operations Research",
            "description": "Vol. 14, No. 2 (May, 1989), pp. 303-308"
          },
          "Isotone": {
            "type": "Optimization in R : Pool-Adjacent-Violators",
            "description": ""
          },
          "Algorithm": {
            "type": "PAVA",
            "description": "and Active Set Methods"
          },
          "de": {
            "type": "Leeuw, Psychometrica, 1977",
            "description": "Examples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.isotonic import IsotonicRegression\n>>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n>>> iso_reg = IsotonicRegression().fit(X, y)\n>>> iso_reg.predict([.1, .2])"
          },
          "Journal": {
            "type": "of Statistical Software 2009",
            "description": ""
          },
          "Correctness": {
            "type": "of Kruskal's algorithms for monotone regression with ties",
            "description": ""
          },
          "array": {
            "type": "[1.8628..., 3.7256...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])",
        "notes": "-----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])",
        "examples": "--------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.",
            "parameters": {
              "X": {
                "type": "is stored for future use, as :meth:`transform` needs X to interpolate",
                "description": ""
              },
              "Training": {
                "type": "target.",
                "description": ""
              },
              "Also": {
                "type": "accepts 2d array with 1 feature.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None\nWeights. If set to None, all weights will be set to 1 (equal\nweights).\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": "Notes\n-----"
              },
              "new": {
                "type": "input data.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None\nIgnored.\nReturns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "An": {
                "type": "ndarray with one string i.e. [\"isotonicregression0\"].",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, T)",
          "documentation": {
            "description": "Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.",
            "parameters": {
              "T": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, 1)"
              },
              "Data": {
                "type": "to transform.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.isotonic.IsotonicRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.isotonic.IsotonicRegression",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.isotonic.IsotonicRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.isotonic.IsotonicRegression",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, T)",
          "documentation": {
            "description": "Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.",
            "parameters": {
              "T": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, 1)"
              },
              "Data": {
                "type": "to transform.",
                "description": ".. versionchanged:: 0.24"
              },
              "Also": {
                "type": "accepts 2d array with 1 feature.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "The": {
                "type": "transformed data.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Real",
      "documentation": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\n    In short, those are: a conversion to float, trunc(), divmod,\n    %, <, <=, >, and >=.\n\n    Real also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RegressorMixin",
      "documentation": {
        "description": "Mixin class for all regression estimators in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - set estimator type to `\"regressor\"` through the `estimator_type` tag;\n    - `score` method that default to :func:`~sklearn.metrics.r2_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n      which is done by setting the regressor type tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, RegressorMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(RegressorMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=0)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([-1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([0, 0, 0])\n    >>> estimator.score(X, y)\n    0.0"
      },
      "methods": [
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StrOptions",
      "documentation": {
        "description": "Constraint representing a finite set of strings.",
        "parameters": {
          "options": {
            "type": "set of str",
            "description": ""
          },
          "The": {
            "type": "set of valid strings.",
            "description": ""
          },
          "deprecated": {
            "type": "set of str or None, default=None",
            "description": ""
          },
          "A": {
            "type": "subset of the `options` to mark as deprecated in the string",
            "description": ""
          },
          "representation": {
            "type": "of the constraint.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TransformerMixin",
      "documentation": {
        "description": "Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])"
      },
      "methods": [
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}