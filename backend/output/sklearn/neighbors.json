{
  "description": "The k-nearest neighbors algorithms.",
  "functions": [
    {
      "name": "kneighbors_graph",
      "signature": "kneighbors_graph(X, n_neighbors, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)",
      "documentation": {
        "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Sample data.\n\n    n_neighbors : int\n        Number of neighbors for each sample.\n\n    mode : {'connectivity', 'distance'}, default='connectivity'\n        Type of returned matrix: 'connectivity' will return the connectivity\n        matrix with ones and zeros, and 'distance' will return the distances\n        between neighbors according to the given metric.\n\n    metric : str, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is equivalent\n        to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n        For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n        to be positive.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    include_self : bool or 'auto', default=False\n        Whether or not to mark each sample as the first nearest neighbor to\n        itself. If 'auto', then True is used for mode='connectivity' and False\n        for mode='distance'.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    A : sparse matrix of shape (n_samples, n_samples)\n        Graph where A[i, j] is assigned the weight of edge that\n        connects i to j. The matrix is of CSR format.\n\n    See Also\n    --------\n    radius_neighbors_graph: Compute the (weighted) graph of Neighbors for points in X.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "Sample": {
            "type": "data.",
            "description": ""
          },
          "n_neighbors": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of neighbors for each sample.",
            "description": ""
          },
          "mode": {
            "type": "{'connectivity', 'distance'}, default='connectivity'",
            "description": ""
          },
          "Type": {
            "type": "of returned matrix: 'connectivity' will return the connectivity",
            "description": ""
          },
          "matrix": {
            "type": "with ones and zeros, and 'distance' will return the distances",
            "description": ""
          },
          "between": {
            "type": "neighbors according to the given metric.",
            "description": ""
          },
          "metric": {
            "type": "str, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is equivalent",
            "description": ""
          },
          "to": {
            "type": "be positive.",
            "description": ""
          },
          "For": {
            "type": "arbitrary p, minkowski_distance (l_p) is used. This parameter is expected",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "include_self": {
            "type": "bool or 'auto', default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to mark each sample as the first nearest neighbor to",
            "description": "itself. If 'auto', then True is used for mode='connectivity' and False"
          },
          "for": {
            "type": "more details.",
            "description": "Returns\n-------"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of parallel jobs to run for neighbors search.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "A": {
            "type": "sparse matrix of shape (n_samples, n_samples)",
            "description": ""
          },
          "Graph": {
            "type": "where A[i, j] is assigned the weight of edge that",
            "description": ""
          },
          "connects": {
            "type": "i to j. The matrix is of CSR format.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "radius_neighbors_graph": {
            "type": "Compute the (weighted) graph of Neighbors for points in X.",
            "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import kneighbors_graph\n>>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
          }
        },
        "returns": "-------\n    A : sparse matrix of shape (n_samples, n_samples)\n        Graph where A[i, j] is assigned the weight of edge that\n        connects i to j. The matrix is of CSR format.\n\n    See Also\n    --------\n    radius_neighbors_graph: Compute the (weighted) graph of Neighbors for points in X.\n\n    Examples\n    --------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import kneighbors_graph\n    >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 1.],\n           [1., 0., 1.]])",
        "raises": "",
        "see_also": "--------\n    radius_neighbors_graph: Compute the (weighted) graph of Neighbors for points in X.\n\n    Examples\n    --------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import kneighbors_graph\n    >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 1.],\n           [1., 0., 1.]])",
        "notes": "",
        "examples": "--------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import kneighbors_graph\n    >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 1.],\n           [1., 0., 1.]])"
      }
    },
    {
      "name": "radius_neighbors_graph",
      "signature": "radius_neighbors_graph(X, radius, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)",
      "documentation": {
        "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n    Neighborhoods are restricted the points at a distance lower than\n    radius.\n\n    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Sample data.\n\n    radius : float\n        Radius of neighborhoods.\n\n    mode : {'connectivity', 'distance'}, default='connectivity'\n        Type of returned matrix: 'connectivity' will return the connectivity\n        matrix with ones and zeros, and 'distance' will return the distances\n        between neighbors according to the given metric.\n\n    metric : str, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    include_self : bool or 'auto', default=False\n        Whether or not to mark each sample as the first nearest neighbor to\n        itself. If 'auto', then True is used for mode='connectivity' and False\n        for mode='distance'.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    A : sparse matrix of shape (n_samples, n_samples)\n        Graph where A[i, j] is assigned the weight of edge that connects\n        i to j. The matrix is of CSR format.\n\n    See Also\n    --------\n    kneighbors_graph: Compute the weighted graph of k-neighbors for points in X.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "Sample": {
            "type": "data.",
            "description": ""
          },
          "radius": {
            "type": "float",
            "description": ""
          },
          "Radius": {
            "type": "of neighborhoods.",
            "description": ""
          },
          "mode": {
            "type": "{'connectivity', 'distance'}, default='connectivity'",
            "description": ""
          },
          "Type": {
            "type": "of returned matrix: 'connectivity' will return the connectivity",
            "description": ""
          },
          "matrix": {
            "type": "with ones and zeros, and 'distance' will return the distances",
            "description": ""
          },
          "between": {
            "type": "neighbors according to the given metric.",
            "description": ""
          },
          "metric": {
            "type": "str, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is",
            "description": ""
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "include_self": {
            "type": "bool or 'auto', default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to mark each sample as the first nearest neighbor to",
            "description": "itself. If 'auto', then True is used for mode='connectivity' and False"
          },
          "for": {
            "type": "more details.",
            "description": "Returns\n-------"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of parallel jobs to run for neighbors search.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "A": {
            "type": "sparse matrix of shape (n_samples, n_samples)",
            "description": ""
          },
          "Graph": {
            "type": "where A[i, j] is assigned the weight of edge that connects",
            "description": ""
          },
          "i": {
            "type": "to j. The matrix is of CSR format.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "kneighbors_graph": {
            "type": "Compute the weighted graph of k",
            "description": "neighbors for points in X.\nExamples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import radius_neighbors_graph\n>>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n...                            include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 0.],\n[1., 0., 1.]])"
          }
        },
        "returns": "-------\n    A : sparse matrix of shape (n_samples, n_samples)\n        Graph where A[i, j] is assigned the weight of edge that connects\n        i to j. The matrix is of CSR format.\n\n    See Also\n    --------\n    kneighbors_graph: Compute the weighted graph of k-neighbors for points in X.\n\n    Examples\n    --------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import radius_neighbors_graph\n    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n    ...                            include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 0.],\n           [1., 0., 1.]])",
        "raises": "",
        "see_also": "--------\n    kneighbors_graph: Compute the weighted graph of k-neighbors for points in X.\n\n    Examples\n    --------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import radius_neighbors_graph\n    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n    ...                            include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 0.],\n           [1., 0., 1.]])",
        "notes": "",
        "examples": "--------\n    >>> X = [[0], [3], [1]]\n    >>> from sklearn.neighbors import radius_neighbors_graph\n    >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n    ...                            include_self=True)\n    >>> A.toarray()\n    array([[1., 0., 1.],\n           [0., 1., 0.],\n           [1., 0., 1.]])"
      }
    },
    {
      "name": "sort_graph_by_row_values",
      "signature": "sort_graph_by_row_values(graph, copy=False, warn_when_not_sorted=True)",
      "documentation": {
        "description": "Sort a sparse graph such that each row is stored with increasing values.\n\n    .. versionadded:: 1.2\n\n    Parameters\n    ----------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Distance matrix to other samples, where only non-zero elements are\n        considered neighbors. Matrix is converted to CSR format if not already.\n\n    copy : bool, default=False\n        If True, the graph is copied before sorting. If False, the sorting is\n        performed inplace. If the graph is not of CSR format, `copy` must be\n        True to allow the conversion to CSR format, otherwise an error is\n        raised.\n\n    warn_when_not_sorted : bool, default=True\n        If True, a :class:`~sklearn.exceptions.EfficiencyWarning` is raised\n        when the input graph is not sorted by row values.\n\n    Returns\n    -------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Distance matrix to other samples, where only non-zero elements are\n        considered neighbors. Matrix is in CSR format.",
        "parameters": {
          "graph": {
            "type": "sparse matrix of shape (n_samples, n_samples)",
            "description": ""
          },
          "Distance": {
            "type": "matrix to other samples, where only non-zero elements are",
            "description": ""
          },
          "considered": {
            "type": "neighbors. Matrix is in CSR format.",
            "description": "Examples\n--------\n>>> from scipy.sparse import csr_matrix\n>>> from sklearn.neighbors import sort_graph_by_row_values\n>>> X = csr_matrix(\n...     [[0., 3., 1.],\n...      [3., 0., 2.],\n...      [1., 2., 0.]])\n>>> X.data"
          },
          "copy": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "True, a :class:`~sklearn.exceptions.EfficiencyWarning` is raised",
            "description": ""
          },
          "performed": {
            "type": "inplace. If the graph is not of CSR format, `copy` must be",
            "description": ""
          },
          "True": {
            "type": "to allow the conversion to CSR format, otherwise an error is",
            "description": "raised."
          },
          "warn_when_not_sorted": {
            "type": "bool, default=True",
            "description": ""
          },
          "when": {
            "type": "the input graph is not sorted by row values.",
            "description": "Returns\n-------"
          },
          "array": {
            "type": "[1., 3., 2., 3., 1., 2.]",
            "description": ""
          }
        },
        "returns": "-------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Distance matrix to other samples, where only non-zero elements are\n        considered neighbors. Matrix is in CSR format.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.neighbors import sort_graph_by_row_values\n    >>> X = csr_matrix(\n    ...     [[0., 3., 1.],\n    ...      [3., 0., 2.],\n    ...      [1., 2., 0.]])\n    >>> X.data\n    array([3., 1., 3., 2., 1., 2.])\n    >>> X_ = sort_graph_by_row_values(X)\n    >>> X_.data\n    array([1., 3., 2., 3., 1., 2.])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from scipy.sparse import csr_matrix\n    >>> from sklearn.neighbors import sort_graph_by_row_values\n    >>> X = csr_matrix(\n    ...     [[0., 3., 1.],\n    ...      [3., 0., 2.],\n    ...      [1., 2., 0.]])\n    >>> X.data\n    array([3., 1., 3., 2., 1., 2.])\n    >>> X_ = sort_graph_by_row_values(X)\n    >>> X_.data\n    array([1., 3., 2., 3., 1., 2.])"
      }
    }
  ],
  "classes": [
    {
      "name": "BallTree",
      "documentation": {
        "description": "BallTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive int, default=40\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : str or DistanceMetric64 object, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for BallTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class.\nNote: Callable functions in the metric parameter are NOT supported for KDTree\nand Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------\ndata : memory view\n    The training data\nvalid_metrics: list of str\n    List of valid distance metrics.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "n_samples": {
            "type": "is the number of points in the data set, and",
            "description": ""
          },
          "n_features": {
            "type": "is the dimension of the parameter space.",
            "description": ""
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": ""
          },
          "not": {
            "type": "be copied. Otherwise, an internal copy will be made.",
            "description": ""
          },
          "leaf_size": {
            "type": "will not affect the results of a query, but can",
            "description": ""
          },
          "Number": {
            "type": "of points at which to switch to brute-force. Changing",
            "description": ""
          },
          "significantly": {
            "type": "impact the speed of a query and the memory required",
            "description": ""
          },
          "to": {
            "type": "store the constructed tree.  The amount of memory needed to",
            "description": ""
          },
          "store": {
            "type": "the tree scales as approximately n_samples / leaf_size.",
            "description": ""
          },
          "For": {
            "type": "a specified ``leaf_size``, a leaf node is guaranteed to",
            "description": ""
          },
          "satisfy": {
            "type": "``leaf_size <= n_points <= 2 * leaf_size``, except in",
            "description": ""
          },
          "the": {
            "type": "metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for",
            "description": ""
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2.",
            "description": ""
          },
          "A": {
            "type": "list of valid metrics for BallTree is given by the attribute",
            "description": "`valid_metrics`."
          },
          "See": {
            "type": "the documentation of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "more": {
            "type": "information on any distance metric.",
            "description": ""
          },
          "Additional": {
            "type": "keywords are passed to the distance metric class.",
            "description": ""
          },
          "and": {
            "type": "Ball Tree. Function call overhead will result in very poor performance.",
            "description": "Attributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": ""
          },
          "The": {
            "type": "training data",
            "description": ""
          },
          "valid_metrics": {
            "type": "list of str",
            "description": ""
          },
          "List": {
            "type": "of valid distance metrics.",
            "description": "Examples\n--------"
          },
          "Query": {
            "type": "for neighbors within a given radius",
            "description": ">>> import numpy as np\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n>>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n>>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n3\n>>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n>>> print(ind)  # indices of neighbors within distance 0.3\n[3 0 1]"
          },
          "Pickle": {
            "type": "and Unpickle a tree.  Note that the state of the tree is saved in the",
            "description": ""
          },
          "pickle": {
            "type": "operation: the tree needs not be rebuilt upon unpickling.",
            "description": ">>> import numpy as np\n>>> import pickle\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n>>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n>>> s = pickle.dumps(tree)                     # doctest: +SKIP\n>>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n>>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n>>> print(ind)  # indices of 3 closest neighbors\n[0 3 1]\n>>> print(dist)  # distances to 3 closest neighbors\n[ 0.          0.19662693  0.29473397]"
          },
          "Compute": {
            "type": "a two-point auto-correlation function",
            "description": ">>> import numpy as np\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((30, 3))\n>>> r = np.linspace(0, 1, 5)\n>>> tree = BallTree(X)                # doctest: +SKIP\n>>> tree.two_point_correlation(X, r)"
          },
          "array": {
            "type": "[ 30,  62, 278, 580, 820]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive int, default=40\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : str or DistanceMetric64 object, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for BallTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class.",
        "examples": "--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import BallTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      },
      "methods": [
        {
          "name": "get_arrays",
          "signature": "get_arrays(self)",
          "documentation": {
            "description": "get_arrays()\n\n        Get data and node arrays.",
            "parameters": {},
            "returns": "-------\n        arrays: tuple of array\n            Arrays for storing tree data, index, node data and node bounds.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_n_calls",
          "signature": "get_n_calls(self)",
          "documentation": {
            "description": "get_n_calls()\n\n        Get number of calls.",
            "parameters": {},
            "returns": "-------\n        n_calls: int\n            number of distance computation calls",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tree_stats",
          "signature": "get_tree_stats(self)",
          "documentation": {
            "description": "get_tree_stats()\n\n        Get tree status.",
            "parameters": {},
            "returns": "-------\n        tree_stats: tuple of int\n            (number of trims, number of leaves, number of splits)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kernel_density",
          "signature": "kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1e-08, breadth_first=True, return_log=False)",
          "documentation": {
            "description": "kernel_density(X, h, kernel='gaussian', atol=0, rtol=1E-8,\n                       breadth_first=True, return_log=False)\n\n        Compute the kernel density estimate at points X with the given kernel,\n        using the distance metric specified at tree creation.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data.\n        h : float\n            the bandwidth of the kernel\n        kernel : str, default=\"gaussian\"\n            specify the kernel to use.  Options are\n            - 'gaussian'\n            - 'tophat'\n            - 'epanechnikov'\n            - 'exponential'\n            - 'linear'\n            - 'cosine'\n            Default is kernel = 'gaussian'\n        atol : float, default=0\n            Specify the desired absolute tolerance of the result.\n            If the true result is `K_true`, then the returned result `K_ret`\n            satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n            The default is zero (i.e. machine precision).\n        rtol : float, default=1e-8\n            Specify the desired relative tolerance of the result.\n            If the true result is `K_true`, then the returned result `K_ret`\n            satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n            The default is `1e-8` (i.e. machine precision).\n        breadth_first : bool, default=False\n            If True, use a breadth-first search.  If False (default) use a\n            depth-first search.  Breadth-first is generally faster for\n            compact kernels and/or high tolerances.\n        return_log : bool, default=False",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query.  Last dimension should match dimension",
                "description": ""
              },
              "of": {
                "type": "training data.",
                "description": ""
              },
              "h": {
                "type": "float",
                "description": ""
              },
              "the": {
                "type": "bandwidth of the kernel",
                "description": ""
              },
              "kernel": {
                "type": "str, default=\"gaussian\"",
                "description": ""
              },
              "specify": {
                "type": "the kernel to use.  Options are",
                "description": "- 'gaussian'\n- 'tophat'\n- 'epanechnikov'\n- 'exponential'\n- 'linear'\n- 'cosine'"
              },
              "Default": {
                "type": "is kernel = 'gaussian'",
                "description": ""
              },
              "atol": {
                "type": "float, default=0",
                "description": ""
              },
              "Specify": {
                "type": "the desired relative tolerance of the result.",
                "description": ""
              },
              "If": {
                "type": "True, use a breadth-first search.  If False (default) use a",
                "description": "depth-first search.  Breadth-first is generally faster for"
              },
              "satisfies": {
                "type": "``abs(K_true - K_ret) < atol + rtol * K_ret``",
                "description": ""
              },
              "The": {
                "type": "array of (log)-density evaluations",
                "description": ""
              },
              "rtol": {
                "type": "float, default=1e",
                "description": "8"
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": ""
              },
              "compact": {
                "type": "kernels and/or high tolerances.",
                "description": ""
              },
              "return_log": {
                "type": "bool, default=False",
                "description": ""
              },
              "Return": {
                "type": "the logarithm of the result.  This can be more accurate",
                "description": ""
              },
              "than": {
                "type": "returning the result itself for narrow kernels.",
                "description": "Returns\n-------"
              },
              "density": {
                "type": "ndarray of shape X.shape[:",
                "description": "1]"
              }
            },
            "returns": "the logarithm of the result.  This can be more accurate\n            than returning the result itself for narrow kernels.\n\n        Returns\n        -------\n        density : ndarray of shape X.shape[:-1]\n            The array of (log)-density evaluations",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query",
          "signature": "query(self, X, k=1, return_distance=True, dualtree=False, breadth_first=False, sort_results=True)",
          "documentation": {
            "description": "query(X, k=1, return_distance=True,\n              dualtree=False, breadth_first=False)\n\n        query the tree for the k nearest neighbors\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query\n        k : int, default=1\n            The number of nearest neighbors to return\n        return_distance : bool, default=True\n            if True, return a tuple (d, i) of distances and indices\n            if False, return array i\n        dualtree : bool, default=False\n            if True, use the dual tree formalism for the query: a tree is\n            built for the query points, and the pair of trees is used to\n            efficiently search this space.  This can lead to better\n            performance as the number of points grows large.\n        breadth_first : bool, default=False\n            if True, then query the nodes in a breadth-first manner.\n            Otherwise, query the nodes in a depth-first manner.\n        sort_results : bool, default=True\n            if True, then distances and indices of each point are sorted\n            on return, so that the first column contains the closest points.\n            Otherwise, neighbors are returned in an arbitrary order.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query",
                "description": ""
              },
              "k": {
                "type": "int, default=1",
                "description": ""
              },
              "The": {
                "type": "number of nearest neighbors to return",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "if": {
                "type": "True, then distances and indices of each point are sorted",
                "description": ""
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": ""
              },
              "built": {
                "type": "for the query points, and the pair of trees is used to",
                "description": ""
              },
              "efficiently": {
                "type": "search this space.  This can lead to better",
                "description": ""
              },
              "performance": {
                "type": "as the number of points grows large.",
                "description": ""
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=True",
                "description": ""
              },
              "on": {
                "type": "return, so that the first column contains the closest points.",
                "description": "Otherwise, neighbors are returned in an arbitrary order.\nReturns\n-------"
              },
              "i": {
                "type": "ndarray of shape X.shape[:",
                "description": "1] + (k,), dtype=int"
              },
              "d": {
                "type": "ndarray of shape X.shape[:",
                "description": "1] + (k,), dtype=double"
              },
              "Each": {
                "type": "entry gives the list of indices of neighbors of the",
                "description": ""
              },
              "corresponding": {
                "type": "point.",
                "description": ""
              }
            },
            "returns": "-------\n        i    : if return_distance == False\n        (d,i) : if return_distance == True\n\n        d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n            Each entry gives the list of distances to the neighbors of the\n            corresponding point.\n\n        i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n            Each entry gives the list of indices of neighbors of the\n            corresponding point.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query_radius",
          "signature": "query_radius(self, X, r, return_distance=False, count_only=False, sort_results=False)",
          "documentation": {
            "description": "query_radius(X, r, return_distance=False,\n        count_only=False, sort_results=False)\n\n        query the tree for neighbors within a radius r\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query\n        r : distance within which neighbors are returned\n            r can be a single value, or an array of values of shape\n            x.shape[:-1] if different radii are desired for each point.\n        return_distance : bool, default=False\n            if True,  return distances to neighbors of each point\n            if False, return only neighbors",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query",
                "description": ""
              },
              "r": {
                "type": "can be a single value, or an array of values of shape",
                "description": "x.shape[:-1] if different radii are desired for each point."
              },
              "return_distance": {
                "type": "== False, setting sort_results = True will",
                "description": ""
              },
              "if": {
                "type": "True, the distances and indices will be sorted before being",
                "description": "returned.  If False, the results will not be sorted.  If"
              },
              "Note": {
                "type": "that unlike the query() method, setting return_distance=True",
                "description": ""
              },
              "here": {
                "type": "adds to the computation time.  Not all distances need to be",
                "description": ""
              },
              "calculated": {
                "type": "explicitly for return_distance=False.  Results are",
                "description": ""
              },
              "not": {
                "type": "sorted by default: see ``sort_results`` keyword.",
                "description": ""
              },
              "count_only": {
                "type": "bool, default=False",
                "description": ""
              },
              "If": {
                "type": "return_distance==True, setting count_only=True will",
                "description": ""
              },
              "result": {
                "type": "in an error.",
                "description": "Returns\n-------"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "count": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=int"
              },
              "ind": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=object"
              },
              "Each": {
                "type": "element is a numpy double array listing the distances",
                "description": ""
              },
              "corresponding": {
                "type": "to indices in i.",
                "description": ""
              },
              "neighbors": {
                "type": "of the corresponding point.  Note that unlike",
                "description": ""
              },
              "the": {
                "type": "results of a k-neighbors query, the returned neighbors",
                "description": ""
              },
              "are": {
                "type": "not sorted by distance by default.",
                "description": ""
              },
              "dist": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=object"
              }
            },
            "returns": "-------\n        count       : if count_only == True\n        ind         : if count_only == False and return_distance == False\n        (ind, dist) : if count_only == False and return_distance == True\n\n        count : ndarray of shape X.shape[:-1], dtype=int\n            Each entry gives the number of neighbors within a distance r of the\n            corresponding point.\n\n        ind : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy integer array listing the indices of\n            neighbors of the corresponding point.  Note that unlike\n            the results of a k-neighbors query, the returned neighbors\n            are not sorted by distance by default.\n\n        dist : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy double array listing the distances\n            corresponding to indices in i.",
            "raises": "",
            "see_also": "",
            "notes": "that unlike the query() method, setting return_distance=True\n            here adds to the computation time.  Not all distances need to be\n            calculated explicitly for return_distance=False.  Results are\n            not sorted by default: see ``sort_results`` keyword.\n        count_only : bool, default=False\n            if True,  return only the count of points within distance r\n            if False, return the indices of all points within distance r\n            If return_distance==True, setting count_only=True will\n            result in an error.\n        sort_results : bool, default=False\n            if True, the distances and indices will be sorted before being\n            returned.  If False, the results will not be sorted.  If\n            return_distance == False, setting sort_results = True will\n            result in an error.\n\n        Returns\n        -------\n        count       : if count_only == True\n        ind         : if count_only == False and return_distance == False\n        (ind, dist) : if count_only == False and return_distance == True\n\n        count : ndarray of shape X.shape[:-1], dtype=int\n            Each entry gives the number of neighbors within a distance r of the\n            corresponding point.\n\n        ind : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy integer array listing the indices of\n            neighbors of the corresponding point.  Note that unlike\n            the results of a k-neighbors query, the returned neighbors\n            are not sorted by distance by default.\n\n        dist : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy double array listing the distances\n            corresponding to indices in i.",
            "examples": ""
          }
        },
        {
          "name": "reset_n_calls",
          "signature": "reset_n_calls(self)",
          "documentation": {
            "description": "reset_n_calls()\n\n        Reset number of calls to 0.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "two_point_correlation",
          "signature": "two_point_correlation(self, X, r, dualtree=False)",
          "documentation": {
            "description": "two_point_correlation(X, r, dualtree=False)\n\n        Compute the two-point correlation function\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data.\n        r : array-like\n            A one-dimensional array of distances\n        dualtree : bool, default=False\n            If True, use a dualtree algorithm.  Otherwise, use a single-tree\n            algorithm.  Dual tree algorithms can have better scaling for\n            large N.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query.  Last dimension should match dimension",
                "description": ""
              },
              "of": {
                "type": "training data.",
                "description": ""
              },
              "r": {
                "type": "array",
                "description": "like"
              },
              "A": {
                "type": "one-dimensional array of distances",
                "description": ""
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": ""
              },
              "If": {
                "type": "True, use a dualtree algorithm.  Otherwise, use a single-tree",
                "description": "algorithm.  Dual tree algorithms can have better scaling for"
              },
              "large": {
                "type": "N.",
                "description": "Returns\n-------"
              },
              "counts": {
                "type": "ndarray",
                "description": "counts[i] contains the number of pairs of points with distance"
              },
              "less": {
                "type": "than or equal to r[i]",
                "description": ""
              }
            },
            "returns": "-------\n        counts : ndarray\n            counts[i] contains the number of pairs of points with distance\n            less than or equal to r[i]",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KDTree",
      "documentation": {
        "description": "KDTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive int, default=40\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : str or DistanceMetric64 object, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for KDTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class.\nNote: Callable functions in the metric parameter are NOT supported for KDTree\nand Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------\ndata : memory view\n    The training data\nvalid_metrics: list of str\n    List of valid distance metrics.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "n_samples": {
            "type": "is the number of points in the data set, and",
            "description": ""
          },
          "n_features": {
            "type": "is the dimension of the parameter space.",
            "description": ""
          },
          "Note": {
            "type": "Callable functions in the metric parameter are NOT supported for KDTree",
            "description": ""
          },
          "not": {
            "type": "be copied. Otherwise, an internal copy will be made.",
            "description": ""
          },
          "leaf_size": {
            "type": "will not affect the results of a query, but can",
            "description": ""
          },
          "Number": {
            "type": "of points at which to switch to brute-force. Changing",
            "description": ""
          },
          "significantly": {
            "type": "impact the speed of a query and the memory required",
            "description": ""
          },
          "to": {
            "type": "store the constructed tree.  The amount of memory needed to",
            "description": ""
          },
          "store": {
            "type": "the tree scales as approximately n_samples / leaf_size.",
            "description": ""
          },
          "For": {
            "type": "a specified ``leaf_size``, a leaf node is guaranteed to",
            "description": ""
          },
          "satisfy": {
            "type": "``leaf_size <= n_points <= 2 * leaf_size``, except in",
            "description": ""
          },
          "the": {
            "type": "metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for",
            "description": ""
          },
          "metric": {
            "type": "str or DistanceMetric64 object, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2.",
            "description": ""
          },
          "A": {
            "type": "list of valid metrics for KDTree is given by the attribute",
            "description": "`valid_metrics`."
          },
          "See": {
            "type": "the documentation of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "more": {
            "type": "information on any distance metric.",
            "description": ""
          },
          "Additional": {
            "type": "keywords are passed to the distance metric class.",
            "description": ""
          },
          "and": {
            "type": "Ball Tree. Function call overhead will result in very poor performance.",
            "description": "Attributes\n----------"
          },
          "data": {
            "type": "memory view",
            "description": ""
          },
          "The": {
            "type": "training data",
            "description": ""
          },
          "valid_metrics": {
            "type": "list of str",
            "description": ""
          },
          "List": {
            "type": "of valid distance metrics.",
            "description": "Examples\n--------"
          },
          "Query": {
            "type": "for neighbors within a given radius",
            "description": ">>> import numpy as np\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n>>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n>>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n3\n>>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n>>> print(ind)  # indices of neighbors within distance 0.3\n[3 0 1]"
          },
          "Pickle": {
            "type": "and Unpickle a tree.  Note that the state of the tree is saved in the",
            "description": ""
          },
          "pickle": {
            "type": "operation: the tree needs not be rebuilt upon unpickling.",
            "description": ">>> import numpy as np\n>>> import pickle\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n>>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n>>> s = pickle.dumps(tree)                     # doctest: +SKIP\n>>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n>>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n>>> print(ind)  # indices of 3 closest neighbors\n[0 3 1]\n>>> print(dist)  # distances to 3 closest neighbors\n[ 0.          0.19662693  0.29473397]"
          },
          "Compute": {
            "type": "a two-point auto-correlation function",
            "description": ">>> import numpy as np\n>>> rng = np.random.RandomState(0)\n>>> X = rng.random_sample((30, 3))\n>>> r = np.linspace(0, 1, 5)\n>>> tree = KDTree(X)                # doctest: +SKIP\n>>> tree.two_point_correlation(X, r)"
          },
          "array": {
            "type": "[ 30,  62, 278, 580, 820]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive int, default=40\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : str or DistanceMetric64 object, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for KDTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class.",
        "examples": "--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import KDTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])"
      },
      "methods": [
        {
          "name": "get_arrays",
          "signature": "get_arrays(self)",
          "documentation": {
            "description": "get_arrays()\n\n        Get data and node arrays.",
            "parameters": {},
            "returns": "-------\n        arrays: tuple of array\n            Arrays for storing tree data, index, node data and node bounds.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_n_calls",
          "signature": "get_n_calls(self)",
          "documentation": {
            "description": "get_n_calls()\n\n        Get number of calls.",
            "parameters": {},
            "returns": "-------\n        n_calls: int\n            number of distance computation calls",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_tree_stats",
          "signature": "get_tree_stats(self)",
          "documentation": {
            "description": "get_tree_stats()\n\n        Get tree status.",
            "parameters": {},
            "returns": "-------\n        tree_stats: tuple of int\n            (number of trims, number of leaves, number of splits)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kernel_density",
          "signature": "kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1e-08, breadth_first=True, return_log=False)",
          "documentation": {
            "description": "kernel_density(X, h, kernel='gaussian', atol=0, rtol=1E-8,\n                       breadth_first=True, return_log=False)\n\n        Compute the kernel density estimate at points X with the given kernel,\n        using the distance metric specified at tree creation.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data.\n        h : float\n            the bandwidth of the kernel\n        kernel : str, default=\"gaussian\"\n            specify the kernel to use.  Options are\n            - 'gaussian'\n            - 'tophat'\n            - 'epanechnikov'\n            - 'exponential'\n            - 'linear'\n            - 'cosine'\n            Default is kernel = 'gaussian'\n        atol : float, default=0\n            Specify the desired absolute tolerance of the result.\n            If the true result is `K_true`, then the returned result `K_ret`\n            satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n            The default is zero (i.e. machine precision).\n        rtol : float, default=1e-8\n            Specify the desired relative tolerance of the result.\n            If the true result is `K_true`, then the returned result `K_ret`\n            satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n            The default is `1e-8` (i.e. machine precision).\n        breadth_first : bool, default=False\n            If True, use a breadth-first search.  If False (default) use a\n            depth-first search.  Breadth-first is generally faster for\n            compact kernels and/or high tolerances.\n        return_log : bool, default=False",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query.  Last dimension should match dimension",
                "description": ""
              },
              "of": {
                "type": "training data.",
                "description": ""
              },
              "h": {
                "type": "float",
                "description": ""
              },
              "the": {
                "type": "bandwidth of the kernel",
                "description": ""
              },
              "kernel": {
                "type": "str, default=\"gaussian\"",
                "description": ""
              },
              "specify": {
                "type": "the kernel to use.  Options are",
                "description": "- 'gaussian'\n- 'tophat'\n- 'epanechnikov'\n- 'exponential'\n- 'linear'\n- 'cosine'"
              },
              "Default": {
                "type": "is kernel = 'gaussian'",
                "description": ""
              },
              "atol": {
                "type": "float, default=0",
                "description": ""
              },
              "Specify": {
                "type": "the desired relative tolerance of the result.",
                "description": ""
              },
              "If": {
                "type": "True, use a breadth-first search.  If False (default) use a",
                "description": "depth-first search.  Breadth-first is generally faster for"
              },
              "satisfies": {
                "type": "``abs(K_true - K_ret) < atol + rtol * K_ret``",
                "description": ""
              },
              "The": {
                "type": "array of (log)-density evaluations",
                "description": ""
              },
              "rtol": {
                "type": "float, default=1e",
                "description": "8"
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": ""
              },
              "compact": {
                "type": "kernels and/or high tolerances.",
                "description": ""
              },
              "return_log": {
                "type": "bool, default=False",
                "description": ""
              },
              "Return": {
                "type": "the logarithm of the result.  This can be more accurate",
                "description": ""
              },
              "than": {
                "type": "returning the result itself for narrow kernels.",
                "description": "Returns\n-------"
              },
              "density": {
                "type": "ndarray of shape X.shape[:",
                "description": "1]"
              }
            },
            "returns": "the logarithm of the result.  This can be more accurate\n            than returning the result itself for narrow kernels.\n\n        Returns\n        -------\n        density : ndarray of shape X.shape[:-1]\n            The array of (log)-density evaluations",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query",
          "signature": "query(self, X, k=1, return_distance=True, dualtree=False, breadth_first=False, sort_results=True)",
          "documentation": {
            "description": "query(X, k=1, return_distance=True,\n              dualtree=False, breadth_first=False)\n\n        query the tree for the k nearest neighbors\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query\n        k : int, default=1\n            The number of nearest neighbors to return\n        return_distance : bool, default=True\n            if True, return a tuple (d, i) of distances and indices\n            if False, return array i\n        dualtree : bool, default=False\n            if True, use the dual tree formalism for the query: a tree is\n            built for the query points, and the pair of trees is used to\n            efficiently search this space.  This can lead to better\n            performance as the number of points grows large.\n        breadth_first : bool, default=False\n            if True, then query the nodes in a breadth-first manner.\n            Otherwise, query the nodes in a depth-first manner.\n        sort_results : bool, default=True\n            if True, then distances and indices of each point are sorted\n            on return, so that the first column contains the closest points.\n            Otherwise, neighbors are returned in an arbitrary order.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query",
                "description": ""
              },
              "k": {
                "type": "int, default=1",
                "description": ""
              },
              "The": {
                "type": "number of nearest neighbors to return",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "if": {
                "type": "True, then distances and indices of each point are sorted",
                "description": ""
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": ""
              },
              "built": {
                "type": "for the query points, and the pair of trees is used to",
                "description": ""
              },
              "efficiently": {
                "type": "search this space.  This can lead to better",
                "description": ""
              },
              "performance": {
                "type": "as the number of points grows large.",
                "description": ""
              },
              "breadth_first": {
                "type": "bool, default=False",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=True",
                "description": ""
              },
              "on": {
                "type": "return, so that the first column contains the closest points.",
                "description": "Otherwise, neighbors are returned in an arbitrary order.\nReturns\n-------"
              },
              "i": {
                "type": "ndarray of shape X.shape[:",
                "description": "1] + (k,), dtype=int"
              },
              "d": {
                "type": "ndarray of shape X.shape[:",
                "description": "1] + (k,), dtype=double"
              },
              "Each": {
                "type": "entry gives the list of indices of neighbors of the",
                "description": ""
              },
              "corresponding": {
                "type": "point.",
                "description": ""
              }
            },
            "returns": "-------\n        i    : if return_distance == False\n        (d,i) : if return_distance == True\n\n        d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n            Each entry gives the list of distances to the neighbors of the\n            corresponding point.\n\n        i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n            Each entry gives the list of indices of neighbors of the\n            corresponding point.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "query_radius",
          "signature": "query_radius(self, X, r, return_distance=False, count_only=False, sort_results=False)",
          "documentation": {
            "description": "query_radius(X, r, return_distance=False,\n        count_only=False, sort_results=False)\n\n        query the tree for neighbors within a radius r\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query\n        r : distance within which neighbors are returned\n            r can be a single value, or an array of values of shape\n            x.shape[:-1] if different radii are desired for each point.\n        return_distance : bool, default=False\n            if True,  return distances to neighbors of each point\n            if False, return only neighbors",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query",
                "description": ""
              },
              "r": {
                "type": "can be a single value, or an array of values of shape",
                "description": "x.shape[:-1] if different radii are desired for each point."
              },
              "return_distance": {
                "type": "== False, setting sort_results = True will",
                "description": ""
              },
              "if": {
                "type": "True, the distances and indices will be sorted before being",
                "description": "returned.  If False, the results will not be sorted.  If"
              },
              "Note": {
                "type": "that unlike the query() method, setting return_distance=True",
                "description": ""
              },
              "here": {
                "type": "adds to the computation time.  Not all distances need to be",
                "description": ""
              },
              "calculated": {
                "type": "explicitly for return_distance=False.  Results are",
                "description": ""
              },
              "not": {
                "type": "sorted by default: see ``sort_results`` keyword.",
                "description": ""
              },
              "count_only": {
                "type": "bool, default=False",
                "description": ""
              },
              "If": {
                "type": "return_distance==True, setting count_only=True will",
                "description": ""
              },
              "result": {
                "type": "in an error.",
                "description": "Returns\n-------"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "count": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=int"
              },
              "ind": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=object"
              },
              "Each": {
                "type": "element is a numpy double array listing the distances",
                "description": ""
              },
              "corresponding": {
                "type": "to indices in i.",
                "description": ""
              },
              "neighbors": {
                "type": "of the corresponding point.  Note that unlike",
                "description": ""
              },
              "the": {
                "type": "results of a k-neighbors query, the returned neighbors",
                "description": ""
              },
              "are": {
                "type": "not sorted by distance by default.",
                "description": ""
              },
              "dist": {
                "type": "ndarray of shape X.shape[:",
                "description": "1], dtype=object"
              }
            },
            "returns": "-------\n        count       : if count_only == True\n        ind         : if count_only == False and return_distance == False\n        (ind, dist) : if count_only == False and return_distance == True\n\n        count : ndarray of shape X.shape[:-1], dtype=int\n            Each entry gives the number of neighbors within a distance r of the\n            corresponding point.\n\n        ind : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy integer array listing the indices of\n            neighbors of the corresponding point.  Note that unlike\n            the results of a k-neighbors query, the returned neighbors\n            are not sorted by distance by default.\n\n        dist : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy double array listing the distances\n            corresponding to indices in i.",
            "raises": "",
            "see_also": "",
            "notes": "that unlike the query() method, setting return_distance=True\n            here adds to the computation time.  Not all distances need to be\n            calculated explicitly for return_distance=False.  Results are\n            not sorted by default: see ``sort_results`` keyword.\n        count_only : bool, default=False\n            if True,  return only the count of points within distance r\n            if False, return the indices of all points within distance r\n            If return_distance==True, setting count_only=True will\n            result in an error.\n        sort_results : bool, default=False\n            if True, the distances and indices will be sorted before being\n            returned.  If False, the results will not be sorted.  If\n            return_distance == False, setting sort_results = True will\n            result in an error.\n\n        Returns\n        -------\n        count       : if count_only == True\n        ind         : if count_only == False and return_distance == False\n        (ind, dist) : if count_only == False and return_distance == True\n\n        count : ndarray of shape X.shape[:-1], dtype=int\n            Each entry gives the number of neighbors within a distance r of the\n            corresponding point.\n\n        ind : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy integer array listing the indices of\n            neighbors of the corresponding point.  Note that unlike\n            the results of a k-neighbors query, the returned neighbors\n            are not sorted by distance by default.\n\n        dist : ndarray of shape X.shape[:-1], dtype=object\n            Each element is a numpy double array listing the distances\n            corresponding to indices in i.",
            "examples": ""
          }
        },
        {
          "name": "reset_n_calls",
          "signature": "reset_n_calls(self)",
          "documentation": {
            "description": "reset_n_calls()\n\n        Reset number of calls to 0.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "two_point_correlation",
          "signature": "two_point_correlation(self, X, r, dualtree=False)",
          "documentation": {
            "description": "two_point_correlation(X, r, dualtree=False)\n\n        Compute the two-point correlation function\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data.\n        r : array-like\n            A one-dimensional array of distances\n        dualtree : bool, default=False\n            If True, use a dualtree algorithm.  Otherwise, use a single-tree\n            algorithm.  Dual tree algorithms can have better scaling for\n            large N.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query.  Last dimension should match dimension",
                "description": ""
              },
              "of": {
                "type": "training data.",
                "description": ""
              },
              "r": {
                "type": "array",
                "description": "like"
              },
              "A": {
                "type": "one-dimensional array of distances",
                "description": ""
              },
              "dualtree": {
                "type": "bool, default=False",
                "description": ""
              },
              "If": {
                "type": "True, use a dualtree algorithm.  Otherwise, use a single-tree",
                "description": "algorithm.  Dual tree algorithms can have better scaling for"
              },
              "large": {
                "type": "N.",
                "description": "Returns\n-------"
              },
              "counts": {
                "type": "ndarray",
                "description": "counts[i] contains the number of pairs of points with distance"
              },
              "less": {
                "type": "than or equal to r[i]",
                "description": ""
              }
            },
            "returns": "-------\n        counts : ndarray\n            counts[i] contains the number of pairs of points with distance\n            less than or equal to r[i]",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsClassifier",
      "documentation": {
        "description": "Classifier implementing the k-nearest neighbors vote.\n\n    Read more in the :ref:`User Guide <classification>`.\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n        Weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Refer to the example entitled\n        :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n        showing the impact of the `weights` parameter on the decision\n        boundary.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is equivalent\n        to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n        For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n        to be positive.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        Class labels known to the classifier\n\n    effective_metric_ : str or callble\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    See Also\n    --------\n    RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n    KNeighborsRegressor: Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n    NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances\n       but different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": ""
          },
          "Weight": {
            "type": "function used in prediction.  Possible values:",
            "description": "- 'uniform' : uniform weights.  All points in each neighborhood"
          },
          "are": {
            "type": "weighted equally.",
            "description": "- 'distance' : weight points by the inverse of their distance."
          },
          "in": {
            "type": "this case, closer neighbors of a query point will have a",
            "description": ""
          },
          "greater": {
            "type": "influence than neighbors which are further away.",
            "description": "- [callable] : a user-defined function which accepts an"
          },
          "array": {
            "type": "of distances, and returns an array of the same shape",
            "description": ""
          },
          "containing": {
            "type": "the weights.",
            "description": ""
          },
          "Refer": {
            "type": "to the example entitled",
            "description": ":ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`"
          },
          "showing": {
            "type": "the impact of the `weights` parameter on the decision",
            "description": "boundary."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is equivalent",
            "description": ""
          },
          "to": {
            "type": "be positive.",
            "description": ""
          },
          "For": {
            "type": "arbitrary p, minkowski_distance (l_p) is used. This parameter is expected",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "metric is a callable function, it takes two arrays representing 1D",
            "description": ""
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "distance metric used. It will be same as the `metric` parameter",
            "description": ""
          },
          "for": {
            "type": "a discussion of the choice of ``algorithm`` and ``leaf_size``.",
            "description": ".. warning::"
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": ""
          },
          "Class": {
            "type": "labels known to the classifier",
            "description": ""
          },
          "effective_metric_": {
            "type": "str or callble",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "outputs_2d_": {
            "type": "bool",
            "description": ""
          },
          "False": {
            "type": "when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit",
            "description": ""
          },
          "otherwise": {
            "type": "True.",
            "description": ""
          },
          "See": {
            "type": "ref:`Nearest Neighbors <neighbors>` in the online documentation",
            "description": ""
          },
          "RadiusNeighborsClassifier": {
            "type": "Classifier based on neighbors within a fixed radius.",
            "description": ""
          },
          "KNeighborsRegressor": {
            "type": "Regression based on k",
            "description": "nearest neighbors."
          },
          "RadiusNeighborsRegressor": {
            "type": "Regression based on neighbors within a fixed radius.",
            "description": ""
          },
          "NearestNeighbors": {
            "type": "Unsupervised learner for implementing neighbor searches.",
            "description": "Notes\n-----"
          },
          "Regarding": {
            "type": "the Nearest Neighbors algorithms, if it is found that two",
            "description": "neighbors, neighbor `k+1` and `k`, have identical distances"
          },
          "but": {
            "type": "different labels, the results will depend on the ordering of the",
            "description": ""
          },
          "training": {
            "type": "data.",
            "description": ""
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/K",
            "description": "nearest_neighbor_algorithm\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> neigh = KNeighborsClassifier(n_neighbors=3)\n>>> neigh.fit(X, y)"
          },
          "KNeighborsClassifier": {
            "type": "...",
            "description": ">>> print(neigh.predict([[1.1]]))\n[0]\n>>> print(neigh.predict_proba([[0.9]]))\n[[0.666... 0.333...]]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n    KNeighborsRegressor: Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n    NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances\n       but different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n    >>> neigh.fit(X, y)\n    KNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.1]]))\n    [0]\n    >>> print(neigh.predict_proba([[0.9]]))\n    [[0.666... 0.333...]]",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is equivalent\n        to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n        For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n        to be positive.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        Class labels known to the classifier\n\n    effective_metric_ : str or callble\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    See Also\n    --------\n    RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n    KNeighborsRegressor: Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n    NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances\n       but different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n    >>> neigh.fit(X, y)\n    KNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.1]]))\n    [0]\n    >>> print(neigh.predict_proba([[0.9]]))\n    [[0.666... 0.333...]]",
        "examples": "--------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n    >>> neigh.fit(X, y)\n    KNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.1]]))\n    [0]\n    >>> print(neigh.predict_proba([[0.9]]))\n    [[0.666... 0.333...]]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the k-nearest neighbors classifier from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n            Target values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "KNeighborsClassifier",
                "description": ""
              },
              "The": {
                "type": "fitted k-nearest neighbors classifier.",
                "description": ""
              }
            },
            "returns": "-------\n        self : KNeighborsClassifier\n            The fitted k-nearest neighbors classifier.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "documentation": {
            "description": "Find the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True.\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "query point or points.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NearestNeighbors",
                "description": ""
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors required for each sample. The default is the",
                "description": ""
              },
              "value": {
                "type": "passed to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": "Returns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Array": {
                "type": "representing the lengths to points, only present if",
                "description": "return_distance=True."
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Indices": {
                "type": "of the nearest points in the population matrix.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1,1,1]",
                "description": ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "n_neighbors=1",
                "description": ">>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))"
              },
              "As": {
                "type": "you can see, it returns [[0.5]], and [[2]], which means that the",
                "description": ""
              },
              "element": {
                "type": "is at distance 0.5 and is the third element of samples",
                "description": "(indexes start at 0). You can also query for multiple points:\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n[2]]...)"
              }
            },
            "returns": "indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "documentation": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "For": {
                "type": "``metric='precomputed'`` the shape should be",
                "description": "(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors for each sample. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "n_neighbors=2",
                "description": ">>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------\nNearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph"
              },
              "of": {
                "type": "Neighbors for points in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "y": {
                "type": "ndarray of shape (n_queries,) or (n_queries, n_outputs)",
                "description": ""
              },
              "Class": {
                "type": "labels for each data sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n            Class labels for each data sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "p": {
                "type": "ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.",
                "description": ""
              },
              "The": {
                "type": "class probabilities of the input samples. Classes are ordered",
                "description": ""
              },
              "by": {
                "type": "lexicographic order.",
                "description": ""
              }
            },
            "returns": "-------\n        p : ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.\n            The class probabilities of the input samples. Classes are ordered\n            by lexicographic order.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features), or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "used; in this case, points are not considered their own\nneighbors. This means that `knn.fit(X, y).score(None, y)`"
              },
              "implicitly": {
                "type": "performs a leave-one-out cross-validation procedure",
                "description": ""
              },
              "and": {
                "type": "is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`",
                "description": ""
              },
              "but": {
                "type": "typically much faster.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), or None\n            Test samples. If `None`, predictions for all indexed points are\n            used; in this case, points are not considered their own\n            neighbors. This means that `knn.fit(X, y).score(None, y)`\n            implicitly performs a leave-one-out cross-validation procedure\n            and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n            but typically much faster.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._classification.KNeighborsClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._classification.KNeighborsClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsRegressor",
      "documentation": {
        "description": "Regression based on k-nearest neighbors.\n\n    The target is predicted by local interpolation of the targets\n    associated of the nearest neighbors in the training set.\n\n    Read more in the :ref:`User Guide <regression>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n        Weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Uniform weights are used by default.\n\n        See the following example for a demonstration of the impact of\n        different weighting schemes on predictions:\n        :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str, DistanceMetric object or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        If metric is a DistanceMetric object, it will be passed directly to\n        the underlying computation routines.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric to use. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n    RadiusNeighborsClassifier : Classifier implementing\n        a vote among neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances but\n       different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": ""
          },
          "Weight": {
            "type": "function used in prediction.  Possible values:",
            "description": "- 'uniform' : uniform weights.  All points in each neighborhood"
          },
          "are": {
            "type": "weighted equally.",
            "description": "- 'distance' : weight points by the inverse of their distance."
          },
          "in": {
            "type": "this case, closer neighbors of a query point will have a",
            "description": ""
          },
          "greater": {
            "type": "influence than neighbors which are further away.",
            "description": "- [callable] : a user-defined function which accepts an"
          },
          "array": {
            "type": "of distances, and returns an array of the same shape",
            "description": ""
          },
          "containing": {
            "type": "the weights.",
            "description": ""
          },
          "Uniform": {
            "type": "weights are used by default.",
            "description": ""
          },
          "See": {
            "type": "ref:`Nearest Neighbors <neighbors>` in the online documentation",
            "description": ""
          },
          "different": {
            "type": "labels, the results will depend on the ordering of the",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is",
            "description": ""
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str, DistanceMetric object or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "underlying computation routines.",
            "description": ""
          },
          "If": {
            "type": "metric is a DistanceMetric object, it will be passed directly to",
            "description": ""
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "distance metric to use. It will be same as the `metric` parameter",
            "description": ""
          },
          "for": {
            "type": "a discussion of the choice of ``algorithm`` and ``leaf_size``.",
            "description": ".. warning::"
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "NearestNeighbors": {
            "type": "Unsupervised learner for implementing neighbor searches.",
            "description": ""
          },
          "RadiusNeighborsRegressor": {
            "type": "Regression based on neighbors within a fixed radius.",
            "description": ""
          },
          "KNeighborsClassifier": {
            "type": "Classifier implementing the k",
            "description": "nearest neighbors vote."
          },
          "RadiusNeighborsClassifier": {
            "type": "Classifier implementing",
            "description": ""
          },
          "a": {
            "type": "vote among neighbors within a given radius.",
            "description": "Notes\n-----"
          },
          "Regarding": {
            "type": "the Nearest Neighbors algorithms, if it is found that two",
            "description": "neighbors, neighbor `k+1` and `k`, have identical distances but"
          },
          "training": {
            "type": "data.",
            "description": ""
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/K",
            "description": "nearest_neighbors_algorithm\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> neigh = KNeighborsRegressor(n_neighbors=2)\n>>> neigh.fit(X, y)"
          },
          "KNeighborsRegressor": {
            "type": "...",
            "description": ">>> print(neigh.predict([[1.5]]))\n[0.5]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n    RadiusNeighborsClassifier : Classifier implementing\n        a vote among neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances but\n       different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsRegressor\n    >>> neigh = KNeighborsRegressor(n_neighbors=2)\n    >>> neigh.fit(X, y)\n    KNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str, DistanceMetric object or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        If metric is a DistanceMetric object, it will be passed directly to\n        the underlying computation routines.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric to use. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n    RadiusNeighborsClassifier : Classifier implementing\n        a vote among neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances but\n       different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsRegressor\n    >>> neigh = KNeighborsRegressor(n_neighbors=2)\n    >>> neigh.fit(X, y)\n    KNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]",
        "examples": "--------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsRegressor\n    >>> neigh = KNeighborsRegressor(n_neighbors=2)\n    >>> neigh.fit(X, y)\n    KNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the k-nearest neighbors regressor from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n            Target values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "KNeighborsRegressor",
                "description": ""
              },
              "The": {
                "type": "fitted k-nearest neighbors regressor.",
                "description": ""
              }
            },
            "returns": "-------\n        self : KNeighborsRegressor\n            The fitted k-nearest neighbors regressor.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "documentation": {
            "description": "Find the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True.\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "query point or points.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NearestNeighbors",
                "description": ""
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors required for each sample. The default is the",
                "description": ""
              },
              "value": {
                "type": "passed to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": "Returns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Array": {
                "type": "representing the lengths to points, only present if",
                "description": "return_distance=True."
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Indices": {
                "type": "of the nearest points in the population matrix.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1,1,1]",
                "description": ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "n_neighbors=1",
                "description": ">>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))"
              },
              "As": {
                "type": "you can see, it returns [[0.5]], and [[2]], which means that the",
                "description": ""
              },
              "element": {
                "type": "is at distance 0.5 and is the third element of samples",
                "description": "(indexes start at 0). You can also query for multiple points:\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n[2]]...)"
              }
            },
            "returns": "indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "documentation": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "For": {
                "type": "``metric='precomputed'`` the shape should be",
                "description": "(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors for each sample. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "n_neighbors=2",
                "description": ">>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------\nNearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph"
              },
              "of": {
                "type": "Neighbors for points in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the target for the provided data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "y": {
                "type": "ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n            Target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._regression.KNeighborsRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._regression.KNeighborsRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KNeighborsTransformer",
      "documentation": {
        "description": "Transform X into a (weighted) graph of k nearest neighbors.\n\n    The transformed data is a sparse graph as returned by kneighbors_graph.\n\n    Read more in the :ref:`User Guide <neighbors_transformer>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    mode : {'distance', 'connectivity'}, default='distance'\n        Type of returned matrix: 'connectivity' will return the connectivity\n        matrix with ones and zeros, and 'distance' will return the distances\n        between neighbors according to the given metric.\n\n    n_neighbors : int, default=5\n        Number of neighbors for each sample in the transformed sparse graph.\n        For compatibility reasons, as each sample is considered as its own\n        neighbor, one extra neighbor will be computed when mode == 'distance'.\n        In this case, the sparse graph contains (n_neighbors + 1) neighbors.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    RadiusNeighborsTransformer : Transform X into a weighted graph of\n        neighbors nearer than a radius.\n\n    Notes\n    -----\n    For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\n    in combination with :class:`~sklearn.manifold.TSNE` see\n    :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": ""
          },
          "Type": {
            "type": "of returned matrix: 'connectivity' will return the connectivity",
            "description": ""
          },
          "matrix": {
            "type": "with ones and zeros, and 'distance' will return the distances",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "n_neighbors": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "For": {
            "type": "an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`",
            "description": ""
          },
          "In": {
            "type": "this case, the sparse graph contains (n_neighbors + 1) neighbors.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "``-1``, then the number of jobs is set to the number of CPU cores.",
            "description": "Attributes\n----------"
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "Distance": {
            "type": "matrices are not supported.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": "sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is"
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "This": {
            "type": "parameter is expected to be positive.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "distance metric used. It will be same as the `metric` parameter",
            "description": ""
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "kneighbors_graph": {
            "type": "Compute the weighted graph of k",
            "description": "neighbors for"
          },
          "points": {
            "type": "in X.",
            "description": ""
          },
          "RadiusNeighborsTransformer": {
            "type": "Transform X into a weighted graph of",
            "description": ""
          },
          "neighbors": {
            "type": "nearer than a radius.",
            "description": "Notes\n-----"
          },
          "in": {
            "type": "combination with :class:`~sklearn.manifold.TSNE` see",
            "description": ":ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.\nExamples\n--------\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> X, _ = load_wine(return_X_y=True)\n>>> X.shape\n(178, 13)\n>>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n>>> X_dist_graph = transformer.fit_transform(X)\n>>> X_dist_graph.shape\n(178, 178)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    RadiusNeighborsTransformer : Transform X into a weighted graph of\n        neighbors nearer than a radius.\n\n    Notes\n    -----\n    For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\n    in combination with :class:`~sklearn.manifold.TSNE` see\n    :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.neighbors import KNeighborsTransformer\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> X.shape\n    (178, 13)\n    >>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n    >>> X_dist_graph = transformer.fit_transform(X)\n    >>> X_dist_graph.shape\n    (178, 178)",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    RadiusNeighborsTransformer : Transform X into a weighted graph of\n        neighbors nearer than a radius.\n\n    Notes\n    -----\n    For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\n    in combination with :class:`~sklearn.manifold.TSNE` see\n    :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.neighbors import KNeighborsTransformer\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> X.shape\n    (178, 13)\n    >>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n    >>> X_dist_graph = transformer.fit_transform(X)\n    >>> X_dist_graph.shape\n    (178, 178)",
        "examples": "--------\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.neighbors import KNeighborsTransformer\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> X.shape\n    (178, 13)\n    >>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n    >>> X_dist_graph = transformer.fit_transform(X)\n    >>> X_dist_graph.shape\n    (178, 178)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the k-nearest neighbors transformer from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "KNeighborsTransformer",
                "description": ""
              },
              "The": {
                "type": "fitted k-nearest neighbors transformer.",
                "description": ""
              }
            },
            "returns": "-------\n        self : KNeighborsTransformer\n            The fitted k-nearest neighbors transformer.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training set.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "set.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "Xt": {
                "type": "sparse matrix of shape (n_samples, n_samples)",
                "description": "Xt[i, j] is assigned the weight of edge that connects i to j."
              },
              "Only": {
                "type": "the neighbors have an explicit value.",
                "description": ""
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              }
            },
            "returns": "-------\n        Xt : sparse matrix of shape (n_samples, n_samples)\n            Xt[i, j] is assigned the weight of edge that connects i to j.\n            Only the neighbors have an explicit value.\n            The diagonal is always explicit.\n            The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "documentation": {
            "description": "Find the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True.\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "query point or points.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NearestNeighbors",
                "description": ""
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors required for each sample. The default is the",
                "description": ""
              },
              "value": {
                "type": "passed to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": "Returns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Array": {
                "type": "representing the lengths to points, only present if",
                "description": "return_distance=True."
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Indices": {
                "type": "of the nearest points in the population matrix.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1,1,1]",
                "description": ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "n_neighbors=1",
                "description": ">>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))"
              },
              "As": {
                "type": "you can see, it returns [[0.5]], and [[2]], which means that the",
                "description": ""
              },
              "element": {
                "type": "is at distance 0.5 and is the third element of samples",
                "description": "(indexes start at 0). You can also query for multiple points:\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n[2]]...)"
              }
            },
            "returns": "indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "documentation": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "For": {
                "type": "``metric='precomputed'`` the shape should be",
                "description": "(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors for each sample. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "n_neighbors=2",
                "description": ">>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------\nNearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph"
              },
              "of": {
                "type": "Neighbors for points in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_transform, n_features)\n            Sample data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples_transform, n_features)"
              },
              "Sample": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "Xt": {
                "type": "sparse matrix of shape (n_samples_transform, n_samples_fit)",
                "description": "Xt[i, j] is assigned the weight of edge that connects i to j."
              },
              "Only": {
                "type": "the neighbors have an explicit value.",
                "description": ""
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              }
            },
            "returns": "-------\n        Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n            Xt[i, j] is assigned the weight of edge that connects i to j.\n            Only the neighbors have an explicit value.\n            The diagonal is always explicit.\n            The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "KernelDensity",
      "documentation": {
        "description": "Kernel Density Estimation.\n\n    Read more in the :ref:`User Guide <kernel_density>`.\n\n    Parameters\n    ----------\n    bandwidth : float or {\"scott\", \"silverman\"}, default=1.0\n        The bandwidth of the kernel. If bandwidth is a float, it defines the\n        bandwidth of the kernel. If bandwidth is a string, one of the estimation\n        methods is implemented.\n\n    algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'\n        The tree algorithm to use.\n\n    kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'\n        The kernel to use.\n\n    metric : str, default='euclidean'\n        Metric to use for distance computation. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        Not all metrics are valid with all algorithms: refer to the\n        documentation of :class:`BallTree` and :class:`KDTree`. Note that the\n        normalization of the density output is correct only for the Euclidean\n        distance metric.\n\n    atol : float, default=0\n        The desired absolute tolerance of the result.  A larger tolerance will\n        generally lead to faster execution.\n\n    rtol : float, default=0\n        The desired relative tolerance of the result.  A larger tolerance will\n        generally lead to faster execution.\n\n    breadth_first : bool, default=True\n        If true (default), use a breadth-first approach to the problem.\n        Otherwise use a depth-first approach.\n\n    leaf_size : int, default=40\n        Specify the leaf size of the underlying tree.  See :class:`BallTree`\n        or :class:`KDTree` for details.\n\n    metric_params : dict, default=None\n        Additional parameters to be passed to the tree for use with the\n        metric.  For more information, see the documentation of\n        :class:`BallTree` or :class:`KDTree`.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    tree_ : ``BinaryTree`` instance\n        The tree algorithm for fast generalized N-point problems.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    bandwidth_ : float\n        Value of the bandwidth, given directly by the bandwidth parameter or\n        estimated using the 'scott' or 'silverman' method.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n        problems.\n    sklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n        problems.",
        "parameters": {
          "bandwidth": {
            "type": "of the kernel. If bandwidth is a string, one of the estimation",
            "description": ""
          },
          "The": {
            "type": "tree algorithm for fast generalized N-point problems.",
            "description": ""
          },
          "methods": {
            "type": "is implemented.",
            "description": ""
          },
          "algorithm": {
            "type": "{'kd_tree', 'ball_tree', 'auto'}, default='auto'",
            "description": ""
          },
          "kernel": {
            "type": "{'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'",
            "description": ""
          },
          "metric": {
            "type": "str, default='euclidean'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. See the",
            "description": ""
          },
          "documentation": {
            "type": "of :class:`BallTree` and :class:`KDTree`. Note that the",
            "description": ""
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "Not": {
            "type": "all metrics are valid with all algorithms: refer to the",
            "description": ""
          },
          "normalization": {
            "type": "of the density output is correct only for the Euclidean",
            "description": ""
          },
          "distance": {
            "type": "metric.",
            "description": ""
          },
          "atol": {
            "type": "float, default=0",
            "description": ""
          },
          "generally": {
            "type": "lead to faster execution.",
            "description": ""
          },
          "rtol": {
            "type": "float, default=0",
            "description": ""
          },
          "breadth_first": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "true (default), use a breadth-first approach to the problem.",
            "description": ""
          },
          "Otherwise": {
            "type": "use a depth-first approach.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=40",
            "description": ""
          },
          "Specify": {
            "type": "the leaf size of the underlying tree.  See :class:`BallTree`",
            "description": ""
          },
          "or": {
            "type": "class:`KDTree` for details.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "parameters to be passed to the tree for use with the",
            "description": "metric.  For more information, see the documentation of\n:class:`BallTree` or :class:`KDTree`.\nAttributes\n----------"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "tree_": {
            "type": "``BinaryTree`` instance",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ""
          },
          "bandwidth_": {
            "type": "float",
            "description": ""
          },
          "Value": {
            "type": "of the bandwidth, given directly by the bandwidth parameter or",
            "description": ""
          },
          "estimated": {
            "type": "using the 'scott' or 'silverman' method.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\nproblems.\nsklearn.neighbors.BallTree : Ball tree for fast generalized N-point\nproblems.\nExamples\n--------"
          },
          "Compute": {
            "type": "a gaussian kernel density estimate with a fixed bandwidth.",
            "description": ">>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> rng = np.random.RandomState(42)\n>>> X = rng.random_sample((100, 3))\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n>>> log_density = kde.score_samples(X[:3])\n>>> log_density"
          },
          "array": {
            "type": "[-1.52955942, -1.51462041, -1.60244657]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n        problems.\n    sklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n        problems.\n\n    Examples\n    --------\n    Compute a gaussian kernel density estimate with a fixed bandwidth.\n\n    >>> from sklearn.neighbors import KernelDensity\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n    >>> log_density = kde.score_samples(X[:3])\n    >>> log_density\n    array([-1.52955942, -1.51462041, -1.60244657])",
        "notes": "",
        "examples": "--------\n    Compute a gaussian kernel density estimate with a fixed bandwidth.\n\n    >>> from sklearn.neighbors import KernelDensity\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n    >>> log_density = kde.score_samples(X[:3])\n    >>> log_density\n    array([-1.52955942, -1.51462041, -1.60244657])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Fit the Kernel Density model on the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            List of sample weights attached to the data X.\n\n            .. versionadded:: 0.20",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "List": {
                "type": "of sample weights attached to the data X.",
                "description": ".. versionadded:: 0.20\nReturns\n-------"
              },
              "corresponds": {
                "type": "to a single data point.",
                "description": ""
              },
              "y": {
                "type": "None",
                "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "sample",
          "signature": "sample(self, n_samples=1, random_state=None)",
          "documentation": {
            "description": "Generate random samples from the model.\n\n        Currently, this is implemented only for gaussian and tophat kernels.\n\n        Parameters\n        ----------\n        n_samples : int, default=1\n            Number of samples to generate.\n\n        random_state : int, RandomState instance or None, default=None\n            Determines random number generation used to generate\n            random samples. Pass an int for reproducible results\n            across multiple function calls.\n            See :term:`Glossary <random_state>`.",
            "parameters": {
              "n_samples": {
                "type": "int, default=1",
                "description": ""
              },
              "Number": {
                "type": "of samples to generate.",
                "description": ""
              },
              "random_state": {
                "type": "int, RandomState instance or None, default=None",
                "description": ""
              },
              "Determines": {
                "type": "random number generation used to generate",
                "description": ""
              },
              "random": {
                "type": "samples. Pass an int for reproducible results",
                "description": ""
              },
              "across": {
                "type": "multiple function calls.",
                "description": ""
              },
              "See": {
                "type": "term:`Glossary <random_state>`.",
                "description": "Returns\n-------"
              },
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "List": {
                "type": "of samples.",
                "description": ""
              }
            },
            "returns": "-------\n        X : array-like of shape (n_samples, n_features)\n            List of samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y=None)",
          "documentation": {
            "description": "Compute the total log-likelihood under the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "List": {
                "type": "of n_features-dimensional data points.  Each row",
                "description": ""
              },
              "corresponds": {
                "type": "to a single data point.",
                "description": ""
              },
              "y": {
                "type": "None",
                "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`.\nReturns\n-------"
              },
              "logprob": {
                "type": "float",
                "description": ""
              },
              "Total": {
                "type": "log-likelihood of the data in X. This is normalized to be a",
                "description": ""
              },
              "probability": {
                "type": "density, so the value will be low for high-dimensional",
                "description": "data."
              }
            },
            "returns": "-------\n        logprob : float\n            Total log-likelihood of the data in X. This is normalized to be a\n            probability density, so the value will be low for high-dimensional\n            data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Compute the log-likelihood of each sample under the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data (n_features).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "An": {
                "type": "array of points to query.  Last dimension should match dimension",
                "description": ""
              },
              "of": {
                "type": "training data (n_features).",
                "description": "Returns\n-------"
              },
              "density": {
                "type": "ndarray of shape (n_samples,)",
                "description": "Log-likelihood of each sample in `X`. These are normalized to be"
              },
              "probability": {
                "type": "densities, so values will be low for high-dimensional",
                "description": "data."
              }
            },
            "returns": "-------\n        density : ndarray of shape (n_samples,)\n            Log-likelihood of each sample in `X`. These are normalized to be\n            probability densities, so values will be low for high-dimensional\n            data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.neighbors._kde.KernelDensity, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._kde.KernelDensity",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LocalOutlierFactor",
      "documentation": {
        "description": "Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n\n    The anomaly score of each sample is called the Local Outlier Factor.\n    It measures the local deviation of the density of a given sample with respect\n    to its neighbors.\n    It is local in that the anomaly score depends on how isolated the object\n    is with respect to the surrounding neighborhood.\n    More precisely, locality is given by k-nearest neighbors, whose distance\n    is used to estimate the local density.\n    By comparing the local density of a sample to the local densities of its\n    neighbors, one can identify samples that have a substantially lower density\n    than their neighbors. These are considered outliers.\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    n_neighbors : int, default=20\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n        If n_neighbors is larger than the number of samples provided,\n        all samples will be used.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :func:`sklearn.metrics.pairwise_distances`. When p = 1, this\n        is equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    contamination : 'auto' or float, default='auto'\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. When fitting this is used to define the\n        threshold on the scores of the samples.\n\n        - if 'auto', the threshold is determined as in the\n          original paper,\n        - if a float, the contamination should be in the range (0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    novelty : bool, default=False\n        By default, LocalOutlierFactor is only meant to be used for outlier\n        detection (novelty=False). Set novelty to True if you want to use\n        LocalOutlierFactor for novelty detection. In this case be aware that\n        you should only use predict, decision_function and score_samples\n        on new unseen data and not on the training set; and note that the\n        results obtained this way may differ from the standard LOF results.\n\n        .. versionadded:: 0.20\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    negative_outlier_factor_ : ndarray of shape (n_samples,)\n        The opposite LOF of the training samples. The higher, the more normal.\n        Inliers tend to have a LOF score close to 1\n        (``negative_outlier_factor_`` close to -1), while outliers tend to have\n        a larger LOF score.\n\n        The local outlier factor (LOF) of a sample captures its\n        supposed 'degree of abnormality'.\n        It is the average of the ratio of the local reachability density of\n        a sample and those of its k-nearest neighbors.\n\n    n_neighbors_ : int\n        The actual number of neighbors used for :meth:`kneighbors` queries.\n\n    offset_ : float\n        Offset used to obtain binary labels from the raw scores.\n        Observations having a negative_outlier_factor smaller than `offset_`\n        are detected as abnormal.\n        The offset is set to -1.5 (inliers score around -1), except when a\n        contamination parameter different than \"auto\" is provided. In that\n        case, the offset is defined in such a way we obtain the expected\n        number of outliers in training.\n\n        .. versionadded:: 0.20\n\n    effective_metric_ : str\n        The effective metric used for the distance computation.\n\n    effective_metric_params_ : dict\n        The effective additional keyword arguments for the metric function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        It is the number of samples in the fitted data.\n\n    See Also\n    --------\n    sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n        Support Vector Machine.\n\n    References\n    ----------\n    .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n           LOF: identifying density-based local outliers. In ACM sigmod record.",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=20",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "If": {
            "type": "metric is a callable function, it takes two arrays representing 1D",
            "description": ""
          },
          "all": {
            "type": "samples will be used.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "is size passed to :class:`BallTree` or :class:`KDTree`. This can",
            "description": ""
          },
          "affect": {
            "type": "the speed of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree. The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "obtained this way may differ from the standard LOF results.",
            "description": ".. versionadded:: 0.20"
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": ":func:`sklearn.metrics.pairwise_distances`. When p = 1, this"
          },
          "is": {
            "type": "equivalent to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "contamination": {
            "type": "parameter different than \"auto\" is provided. In that",
            "description": "case, the offset is defined in such a way we obtain the expected"
          },
          "The": {
            "type": "effective additional keyword arguments for the metric function.",
            "description": ""
          },
          "of": {
            "type": "outliers in the data set. When fitting this is used to define the",
            "description": ""
          },
          "threshold": {
            "type": "on the scores of the samples.",
            "description": "- if 'auto', the threshold is determined as in the"
          },
          "original": {
            "type": "paper,",
            "description": "- if a float, the contamination should be in the range (0, 0.5].\n.. versionchanged:: 0.22"
          },
          "to": {
            "type": "``'auto'``.",
            "description": ""
          },
          "novelty": {
            "type": "bool, default=False",
            "description": ""
          },
          "By": {
            "type": "default, LocalOutlierFactor is only meant to be used for outlier",
            "description": ""
          },
          "detection": {
            "type": "novelty=False",
            "description": ". Set novelty to True if you want to use"
          },
          "LocalOutlierFactor": {
            "type": "for novelty detection. In this case be aware that",
            "description": ""
          },
          "you": {
            "type": "should only use predict, decision_function and score_samples",
            "description": ""
          },
          "on": {
            "type": "new unseen data and not on the training set; and note that the",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": "Attributes\n----------"
          },
          "negative_outlier_factor_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Inliers": {
            "type": "tend to have a LOF score close to 1",
            "description": "(``negative_outlier_factor_`` close to -1), while outliers tend to have"
          },
          "a": {
            "type": "sample and those of its k-nearest neighbors.",
            "description": ""
          },
          "supposed": {
            "type": "'degree of abnormality'.",
            "description": ""
          },
          "It": {
            "type": "is the number of samples in the fitted data.",
            "description": ""
          },
          "n_neighbors_": {
            "type": "int",
            "description": ""
          },
          "offset_": {
            "type": "float",
            "description": ""
          },
          "Offset": {
            "type": "used to obtain binary labels from the raw scores.",
            "description": ""
          },
          "Observations": {
            "type": "having a negative_outlier_factor smaller than `offset_`",
            "description": ""
          },
          "are": {
            "type": "detected as abnormal.",
            "description": ""
          },
          "number": {
            "type": "of outliers in training.",
            "description": ".. versionadded:: 0.20"
          },
          "effective_metric_": {
            "type": "str",
            "description": ""
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.svm.OneClassSVM: Unsupervised Outlier Detection using"
          },
          "Support": {
            "type": "Vector Machine.",
            "description": "References\n----------\n.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May)."
          },
          "LOF": {
            "type": "identifying density",
            "description": "based local outliers. In ACM sigmod record.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.neighbors import LocalOutlierFactor\n>>> X = [[-1.1], [0.2], [101.1], [0.3]]\n>>> clf = LocalOutlierFactor(n_neighbors=2)\n>>> clf.fit_predict(X)"
          },
          "array": {
            "type": "[ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n        Support Vector Machine.\n\n    References\n    ----------\n    .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n           LOF: identifying density-based local outliers. In ACM sigmod record.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import LocalOutlierFactor\n    >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n    >>> clf = LocalOutlierFactor(n_neighbors=2)\n    >>> clf.fit_predict(X)\n    array([ 1,  1, -1,  1])\n    >>> clf.negative_outlier_factor_\n    array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        :func:`sklearn.metrics.pairwise_distances`. When p = 1, this\n        is equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    contamination : 'auto' or float, default='auto'\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. When fitting this is used to define the\n        threshold on the scores of the samples.\n\n        - if 'auto', the threshold is determined as in the\n          original paper,\n        - if a float, the contamination should be in the range (0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    novelty : bool, default=False\n        By default, LocalOutlierFactor is only meant to be used for outlier\n        detection (novelty=False). Set novelty to True if you want to use\n        LocalOutlierFactor for novelty detection. In this case be aware that\n        you should only use predict, decision_function and score_samples\n        on new unseen data and not on the training set; and note that the\n        results obtained this way may differ from the standard LOF results.\n\n        .. versionadded:: 0.20\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    negative_outlier_factor_ : ndarray of shape (n_samples,)\n        The opposite LOF of the training samples. The higher, the more normal.\n        Inliers tend to have a LOF score close to 1\n        (``negative_outlier_factor_`` close to -1), while outliers tend to have\n        a larger LOF score.\n\n        The local outlier factor (LOF) of a sample captures its\n        supposed 'degree of abnormality'.\n        It is the average of the ratio of the local reachability density of\n        a sample and those of its k-nearest neighbors.\n\n    n_neighbors_ : int\n        The actual number of neighbors used for :meth:`kneighbors` queries.\n\n    offset_ : float\n        Offset used to obtain binary labels from the raw scores.\n        Observations having a negative_outlier_factor smaller than `offset_`\n        are detected as abnormal.\n        The offset is set to -1.5 (inliers score around -1), except when a\n        contamination parameter different than \"auto\" is provided. In that\n        case, the offset is defined in such a way we obtain the expected\n        number of outliers in training.\n\n        .. versionadded:: 0.20\n\n    effective_metric_ : str\n        The effective metric used for the distance computation.\n\n    effective_metric_params_ : dict\n        The effective additional keyword arguments for the metric function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        It is the number of samples in the fitted data.\n\n    See Also\n    --------\n    sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n        Support Vector Machine.\n\n    References\n    ----------\n    .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n           LOF: identifying density-based local outliers. In ACM sigmod record.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import LocalOutlierFactor\n    >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n    >>> clf = LocalOutlierFactor(n_neighbors=2)\n    >>> clf.fit_predict(X)\n    array([ 1,  1, -1,  1])\n    >>> clf.negative_outlier_factor_\n    array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import LocalOutlierFactor\n    >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n    >>> clf = LocalOutlierFactor(n_neighbors=2)\n    >>> clf.fit_predict(X)\n    array([ 1,  1, -1,  1])\n    >>> clf.negative_outlier_factor_\n    array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Shifted opposite of the Local Outlier Factor of X.\n\n        Bigger is better, i.e. large values correspond to inliers.\n\n        **Only available for novelty detection (when novelty is set to True).**\n        The shift offset allows a zero threshold for being an outlier.\n        The argument X is supposed to contain *new data*: if X contains a\n        point from training, it considers the later in its own neighborhood.\n        Also, the samples in X are not considered in the neighborhood of any\n        point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The query sample or samples to compute the Local Outlier Factor\n            w.r.t. the training samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "shifted opposite of the Local Outlier Factor of each input",
                "description": "samples. The lower, the more abnormal. Negative scores represent\noutliers, positive scores represent inliers."
              },
              "shifted_opposite_lof_scores": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              }
            },
            "returns": "-------\n        shifted_opposite_lof_scores : ndarray of shape (n_samples,)\n            The shifted opposite of the Local Outlier Factor of each input\n            samples. The lower, the more abnormal. Negative scores represent\n            outliers, positive scores represent inliers.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the local outlier factor detector from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "LocalOutlierFactor",
                "description": ""
              },
              "The": {
                "type": "fitted local outlier factor detector.",
                "description": ""
              }
            },
            "returns": "-------\n        self : LocalOutlierFactor\n            The fitted local outlier factor detector.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None)",
          "documentation": {
            "description": "Fit the model to the training set X and return the labels.\n\n        **Not available for novelty detection (when novelty is set to True).**\n        Label is 1 for an inlier and -1 for an outlier according to the LOF\n        score and the contamination parameter.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), default=None\n            The query sample or samples to compute the Local Outlier Factor\n            w.r.t. the training samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), default=None"
              },
              "The": {
                "type": "query sample or samples to compute the Local Outlier Factor",
                "description": "w.r.t. the training samples."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "is_inlier": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "-1 for anomalies/outliers and 1 for inliers.",
                "description": ""
              }
            },
            "returns": "-------\n        is_inlier : ndarray of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "documentation": {
            "description": "Find the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True.\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "query point or points.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NearestNeighbors",
                "description": ""
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors required for each sample. The default is the",
                "description": ""
              },
              "value": {
                "type": "passed to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": "Returns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Array": {
                "type": "representing the lengths to points, only present if",
                "description": "return_distance=True."
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Indices": {
                "type": "of the nearest points in the population matrix.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1,1,1]",
                "description": ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "n_neighbors=1",
                "description": ">>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))"
              },
              "As": {
                "type": "you can see, it returns [[0.5]], and [[2]], which means that the",
                "description": ""
              },
              "element": {
                "type": "is at distance 0.5 and is the third element of samples",
                "description": "(indexes start at 0). You can also query for multiple points:\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n[2]]...)"
              }
            },
            "returns": "indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "documentation": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "For": {
                "type": "``metric='precomputed'`` the shape should be",
                "description": "(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors for each sample. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "n_neighbors=2",
                "description": ">>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------\nNearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph"
              },
              "of": {
                "type": "Neighbors for points in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X=None)",
          "documentation": {
            "description": "Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n\n        **Only available for novelty detection (when novelty is set to True).**\n        This method allows to generalize prediction to *new observations* (not\n        in the training set). Note that the result of ``clf.fit(X)`` then\n        ``clf.predict(X)`` with ``novelty=True`` may differ from the result\n        obtained by ``clf.fit_predict(X)`` with ``novelty=False``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The query sample or samples to compute the Local Outlier Factor\n            w.r.t. the training samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "query sample or samples to compute the Local Outlier Factor",
                "description": "w.r.t. the training samples.\nReturns\n-------"
              },
              "is_inlier": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "-1 for anomalies/outliers and +1 for inliers.",
                "description": ""
              }
            },
            "returns": "-------\n        is_inlier : ndarray of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Opposite of the Local Outlier Factor of X.\n\n        It is the opposite as bigger is better, i.e. large values correspond\n        to inliers.\n\n        **Only available for novelty detection (when novelty is set to True).**\n        The argument X is supposed to contain *new data*: if X contains a\n        point from training, it considers the later in its own neighborhood.\n        Also, the samples in X are not considered in the neighborhood of any\n        point. Because of this, the scores obtained via ``score_samples`` may\n        differ from the standard LOF scores.\n        The standard LOF scores for the training data is available via the\n        ``negative_outlier_factor_`` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The query sample or samples to compute the Local Outlier Factor\n            w.r.t. the training samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "lower, the more abnormal.",
                "description": ""
              },
              "opposite_lof_scores": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              }
            },
            "returns": "-------\n        opposite_lof_scores : ndarray of shape (n_samples,)\n            The opposite of the Local Outlier Factor of each input samples.\n            The lower, the more abnormal.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NearestCentroid",
      "documentation": {
        "description": "Nearest centroid classifier.\n\n    Each class is represented by its centroid, with test samples classified to\n    the class with the nearest centroid.\n\n    Read more in the :ref:`User Guide <nearest_centroid_classifier>`.\n\n    Parameters\n    ----------\n    metric : {\"euclidean\", \"manhattan\"}, default=\"euclidean\"\n        Metric to use for distance computation.\n\n        If `metric=\"euclidean\"`, the centroid for the samples corresponding to each\n        class is the arithmetic mean, which minimizes the sum of squared L1 distances.\n        If `metric=\"manhattan\"`, the centroid is the feature-wise median, which\n        minimizes the sum of L1 distances.\n\n        .. versionchanged:: 1.5\n            All metrics but `\"euclidean\"` and `\"manhattan\"` were deprecated and\n            now raise an error.\n\n        .. versionchanged:: 0.19\n            `metric='precomputed'` was deprecated and now raises an error\n\n    shrink_threshold : float, default=None\n        Threshold for shrinking centroids to remove features.\n\n    priors : {\"uniform\", \"empirical\"} or array-like of shape (n_classes,),         default=\"uniform\"\n        The class prior probabilities. By default, the class proportions are\n        inferred from the training data.\n\n        .. versionadded:: 1.6\n\n    Attributes\n    ----------\n    centroids_ : array-like of shape (n_classes, n_features)\n        Centroid of each class.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    deviations_ : ndarray of shape (n_classes, n_features)\n        Deviations (or shrinkages) of the centroids of each class from the\n        overall centroid. Equal to eq. (18.4) if `shrink_threshold=None`,\n        else (18.5) p. 653 of [2]. Can be used to identify features used\n        for classification.\n\n        .. versionadded:: 1.6\n\n    within_class_std_dev_ : ndarray of shape (n_features,)\n        Pooled or within-class standard deviation of input data.\n\n        .. versionadded:: 1.6\n\n    class_prior_ : ndarray of shape (n_classes,)\n        The class prior probabilities.\n\n        .. versionadded:: 1.6\n\n    See Also\n    --------\n    KNeighborsClassifier : Nearest neighbors classifier.\n\n    Notes\n    -----\n    When used for text classification with tf-idf vectors, this classifier is\n    also known as the Rocchio classifier.\n\n    References\n    ----------\n    [1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\n    of the National Academy of Sciences of the United States of America,\n    99(10), 6567-6572. The National Academy of Sciences.\n\n    [2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical\n    Learning Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.",
        "parameters": {
          "metric": {
            "type": "{\"euclidean\", \"manhattan\"}, default=\"euclidean\"",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation.",
            "description": ""
          },
          "If": {
            "type": "`metric=\"manhattan\"`, the centroid is the feature-wise median, which",
            "description": ""
          },
          "class": {
            "type": "is the arithmetic mean, which minimizes the sum of squared L1 distances.",
            "description": ""
          },
          "minimizes": {
            "type": "the sum of L1 distances.",
            "description": ".. versionchanged:: 1.5"
          },
          "All": {
            "type": "metrics but `\"euclidean\"` and `\"manhattan\"` were deprecated and",
            "description": ""
          },
          "now": {
            "type": "raise an error.",
            "description": ".. versionchanged:: 0.19\n`metric='precomputed'` was deprecated and now raises an error"
          },
          "shrink_threshold": {
            "type": "float, default=None",
            "description": ""
          },
          "Threshold": {
            "type": "for shrinking centroids to remove features.",
            "description": ""
          },
          "priors": {
            "type": "{\"uniform\", \"empirical\"} or array",
            "description": "like of shape (n_classes,),         default=\"uniform\""
          },
          "The": {
            "type": "class prior probabilities.",
            "description": ".. versionadded:: 1.6"
          },
          "inferred": {
            "type": "from the training data.",
            "description": ".. versionadded:: 1.6\nAttributes\n----------"
          },
          "centroids_": {
            "type": "array",
            "description": "like of shape (n_classes, n_features)"
          },
          "Centroid": {
            "type": "of each class.",
            "description": ""
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "deviations_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": ""
          },
          "Deviations": {
            "type": "or shrinkages",
            "description": "of the centroids of each class from the"
          },
          "overall": {
            "type": "centroid. Equal to eq. (18.4) if `shrink_threshold=None`,",
            "description": ""
          },
          "else": {
            "type": "18.5",
            "description": "p. 653 of [2]. Can be used to identify features used"
          },
          "for": {
            "type": "classification.",
            "description": ".. versionadded:: 1.6"
          },
          "within_class_std_dev_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Pooled": {
            "type": "or within-class standard deviation of input data.",
            "description": ".. versionadded:: 1.6"
          },
          "class_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "KNeighborsClassifier": {
            "type": "Nearest neighbors classifier.",
            "description": "Notes\n-----"
          },
          "When": {
            "type": "used for text classification with tf-idf vectors, this classifier is",
            "description": ""
          },
          "also": {
            "type": "known as the Rocchio classifier.",
            "description": "References\n----------\n[1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of"
          },
          "multiple": {
            "type": "cancer types by shrunken centroids of gene expression. Proceedings",
            "description": ""
          },
          "of": {
            "type": "the National Academy of Sciences of the United States of America,",
            "description": ""
          },
          "99": {
            "type": "10",
            "description": ", 6567-6572. The National Academy of Sciences.\n[2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical"
          },
          "Learning": {
            "type": "Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.",
            "description": "Examples\n--------\n>>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)"
          },
          "NearestCentroid": {
            "type": "",
            "description": ">>> print(clf.predict([[-0.8, -1]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KNeighborsClassifier : Nearest neighbors classifier.\n\n    Notes\n    -----\n    When used for text classification with tf-idf vectors, this classifier is\n    also known as the Rocchio classifier.\n\n    References\n    ----------\n    [1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\n    of the National Academy of Sciences of the United States of America,\n    99(10), 6567-6572. The National Academy of Sciences.\n\n    [2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical\n    Learning Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import NearestCentroid\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = NearestCentroid()\n    >>> clf.fit(X, y)\n    NearestCentroid()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]",
        "notes": "-----\n    When used for text classification with tf-idf vectors, this classifier is\n    also known as the Rocchio classifier.\n\n    References\n    ----------\n    [1] Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\n    of the National Academy of Sciences of the United States of America,\n    99(10), 6567-6572. The National Academy of Sciences.\n\n    [2] Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical\n    Learning Data Mining, Inference, and Prediction. 2nd Edition. New York, Springer.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import NearestCentroid\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = NearestCentroid()\n    >>> clf.fit(X, y)\n    NearestCentroid()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]",
        "examples": "--------\n    >>> from sklearn.neighbors import NearestCentroid\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = NearestCentroid()\n    >>> clf.fit(X, y)\n    NearestCentroid()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Apply decision function to an array of samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array of samples (test vectors).",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples (test vectors).",
                "description": "Returns\n-------"
              },
              "y_scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Decision": {
                "type": "function values related to each class, per sample.",
                "description": ""
              },
              "In": {
                "type": "the two-class case, the shape is `(n_samples,)`, giving the",
                "description": ""
              },
              "log": {
                "type": "likelihood ratio of the positive class.",
                "description": ""
              }
            },
            "returns": "-------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the NearestCentroid model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "Note": {
                "type": "that centroid shrinking cannot be used with sparse matrices.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "that centroid shrinking cannot be used with sparse matrices.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Perform classification on an array of test vectors `X`.\n\n        The predicted class `C` for each sample in `X` is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "The": {
                "type": "predicted classes.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted classes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Estimate log class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y_log_proba": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Estimated": {
                "type": "log probabilities.",
                "description": ""
              }
            },
            "returns": "-------\n        y_log_proba : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Estimate class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y_proba": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Probability": {
                "type": "estimate of the sample for each class in the",
                "description": "model, where classes are ordered as they are in `self.classes_`."
              }
            },
            "returns": "-------\n        y_proba : ndarray of shape (n_samples, n_classes)\n            Probability estimate of the sample for each class in the\n            model, where classes are ordered as they are in `self.classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._nearest_centroid.NearestCentroid, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._nearest_centroid.NearestCentroid",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NearestNeighbors",
      "documentation": {
        "description": "Unsupervised learner for implementing neighbor searches.\n\n    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    radius : float, default=1.0\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    p : float (positive), default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    effective_metric_ : str\n        Metric used to compute distances to neighbors.\n\n    effective_metric_params_ : dict\n        Parameters for the metric used to compute distances to neighbors.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n        within a given radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed\n        radius.\n    BallTree : Space partitioning data structure for organizing points in a\n        multi-dimensional space, used for nearest neighbor search.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "parameters": {
          "n_neighbors": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of neighbors to use by default for :meth:`kneighbors` queries.",
            "description": ""
          },
          "radius": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Range": {
            "type": "of parameter space to use by default for :meth:`radius_neighbors`",
            "description": "queries."
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "used to compute distances to neighbors.",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "metric is a callable function, it takes two arrays representing 1D",
            "description": ""
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "p": {
            "type": "float (positive), default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": "sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is"
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of parallel jobs to run for neighbors search.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": "Attributes\n----------"
          },
          "effective_metric_": {
            "type": "str",
            "description": ""
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n        within a given radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed\n        radius.\n    BallTree : Space partitioning data structure for organizing points in a\n        multi-dimensional space, used for nearest neighbor search.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n    >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n    >>> neigh.fit(samples)\n    NearestNeighbors(...)\n    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n    array([[2, 0]]...)\n    >>> nbrs = neigh.radius_neighbors(\n    ...    [[0, 0, 1.3]], 0.4, return_distance=False\n    ... )\n    >>> np.asarray(nbrs[0][0])\n    array(2)",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    p : float (positive), default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    effective_metric_ : str\n        Metric used to compute distances to neighbors.\n\n    effective_metric_params_ : dict\n        Parameters for the metric used to compute distances to neighbors.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n        within a given radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    RadiusNeighborsRegressor : Regression based on neighbors within a fixed\n        radius.\n    BallTree : Space partitioning data structure for organizing points in a\n        multi-dimensional space, used for nearest neighbor search.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n    >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n    >>> neigh.fit(samples)\n    NearestNeighbors(...)\n    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n    array([[2, 0]]...)\n    >>> nbrs = neigh.radius_neighbors(\n    ...    [[0, 0, 1.3]], 0.4, return_distance=False\n    ... )\n    >>> np.asarray(nbrs[0][0])\n    array(2)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n    >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n    >>> neigh.fit(samples)\n    NearestNeighbors(...)\n    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n    array([[2, 0]]...)\n    >>> nbrs = neigh.radius_neighbors(\n    ...    [[0, 0, 1.3]], 0.4, return_distance=False\n    ... )\n    >>> np.asarray(nbrs[0][0])\n    array(2)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the nearest neighbors estimator from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "NearestNeighbors",
                "description": ""
              },
              "The": {
                "type": "fitted nearest neighbors estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : NearestNeighbors\n            The fitted nearest neighbors estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "kneighbors",
          "signature": "kneighbors(self, X=None, n_neighbors=None, return_distance=True)",
          "documentation": {
            "description": "Find the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True.\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "query point or points.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NearestNeighbors",
                "description": ""
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors required for each sample. The default is the",
                "description": ""
              },
              "value": {
                "type": "passed to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": "Returns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Array": {
                "type": "representing the lengths to points, only present if",
                "description": "return_distance=True."
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_queries, n_neighbors)",
                "description": ""
              },
              "Indices": {
                "type": "of the nearest points in the population matrix.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1,1,1]",
                "description": ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "n_neighbors=1",
                "description": ">>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))"
              },
              "As": {
                "type": "you can see, it returns [[0.5]], and [[2]], which means that the",
                "description": ""
              },
              "element": {
                "type": "is at distance 0.5 and is the third element of samples",
                "description": "(indexes start at 0). You can also query for multiple points:\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n[2]]...)"
              }
            },
            "returns": "indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": "--------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)"
          }
        },
        {
          "name": "kneighbors_graph",
          "signature": "kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')",
          "documentation": {
            "description": "Compute the (weighted) graph of k-Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "not provided, neighbors of each indexed point are returned.",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "For": {
                "type": "``metric='precomputed'`` the shape should be",
                "description": "(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
              },
              "n_neighbors": {
                "type": "int, default=None",
                "description": ""
              },
              "Number": {
                "type": "of neighbors for each sample. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "n_neighbors=2",
                "description": ">>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 1.],\n[1., 0., 1.]])"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------\nNearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph"
              },
              "of": {
                "type": "Neighbors for points in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n            of Neighbors for points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "documentation": {
            "description": "Find the neighbors within a given radius of a point or points.\n\n        Return the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of (n_samples, n_features), default=None"
              },
              "The": {
                "type": "first array returned contains the distances to all points which",
                "description": ""
              },
              "If": {
                "type": "True, the distances and indices will be sorted by increasing",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NeighborsClassifier",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Limiting": {
                "type": "distance of neighbors to return. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "distances": {
                "type": "before being returned. If False, the results may not",
                "description": ""
              },
              "be": {
                "type": "sorted. If `return_distance=False`, setting `sort_results=True`",
                "description": ""
              },
              "will": {
                "type": "result in an error.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "Array": {
                "type": "representing the distances to each point, only present if",
                "description": "`return_distance=True`. The distance values are computed according"
              },
              "to": {
                "type": "the ``metric`` constructor parameter.",
                "description": ""
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "An": {
                "type": "array of arrays of indices of the approximate nearest points",
                "description": ""
              },
              "from": {
                "type": "the population matrix that lie within a ball of size",
                "description": "``radius`` around the query points.\nNotes\n-----"
              },
              "Because": {
                "type": "the number of neighbors of each point is not necessarily",
                "description": "equal, the results for multiple query points cannot be fit in a"
              },
              "standard": {
                "type": "data array.",
                "description": ""
              },
              "For": {
                "type": "efficiency, `radius_neighbors` returns arrays of objects, where",
                "description": ""
              },
              "each": {
                "type": "object is a 1D array of indices or distances.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1, 1, 1]:",
                "description": ">>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "radius=1.6",
                "description": ">>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]"
              },
              "are": {
                "type": "closer than 1.6, while the second array returned contains their",
                "description": "indices.  In general, multiple points can be queried at the same time."
              }
            },
            "returns": "the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "examples": "--------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Neighborhoods are restricted the points at a distance lower than\n        radius.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Radius of neighborhoods. The default is the value passed to the\n            constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        sort_results : bool, default=False\n            If True, in each row of the result, the non-zero entries will be\n            sorted by increasing distances. If False, the non-zero entries may\n            not be sorted. Only used with mode='distance'.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "True, in each row of the result, the non-zero entries will be",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Radius": {
                "type": "of neighborhoods. The default is the value passed to the",
                "description": "constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "radius=1.5",
                "description": ">>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 0.],\n[1., 0., 1.]])"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "sorted": {
                "type": "by increasing distances. If False, the non-zero entries may",
                "description": ""
              },
              "not": {
                "type": "be sorted. Only used with mode='distance'.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "kneighbors_graph": {
                "type": "Compute the (weighted) graph of k",
                "description": "Neighbors for"
              },
              "points": {
                "type": "in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NeighborhoodComponentsAnalysis",
      "documentation": {
        "description": "Neighborhood Components Analysis.\n\n    Neighborhood Component Analysis (NCA) is a machine learning algorithm for\n    metric learning. It learns a linear transformation in a supervised fashion\n    to improve the classification accuracy of a stochastic nearest neighbors\n    rule in the transformed space.\n\n    Read more in the :ref:`User Guide <nca>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Preferred dimensionality of the projected space.\n        If None it will be set to `n_features`.\n\n    init : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'\n        Initialization of the linear transformation. Possible options are\n        `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy\n        array of shape `(n_features_a, n_features_b)`.\n\n        - `'auto'`\n            Depending on `n_components`, the most reasonable initialization\n            is chosen. If `n_components <= min(n_features, n_classes - 1)`\n            we use `'lda'`, as it uses labels information. If not, but\n            `n_components < min(n_features, n_samples)`, we use `'pca'`, as\n            it projects data in meaningful directions (those of higher\n            variance). Otherwise, we just use `'identity'`.\n\n        - `'pca'`\n            `n_components` principal components of the inputs passed\n            to :meth:`fit` will be used to initialize the transformation.\n            (See :class:`~sklearn.decomposition.PCA`)\n\n        - `'lda'`\n            `min(n_components, n_classes)` most discriminative\n            components of the inputs passed to :meth:`fit` will be used to\n            initialize the transformation. (If `n_components > n_classes`,\n            the rest of the components will be zero.) (See\n            :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n\n        - `'identity'`\n            If `n_components` is strictly smaller than the\n            dimensionality of the inputs passed to :meth:`fit`, the identity\n            matrix will be truncated to the first `n_components` rows.\n\n        - `'random'`\n            The initial transformation will be a random array of shape\n            `(n_components, n_features)`. Each value is sampled from the\n            standard normal distribution.\n\n        - numpy array\n            `n_features_b` must match the dimensionality of the inputs passed\n            to :meth:`fit` and n_features_a must be less than or equal to that.\n            If `n_components` is not `None`, `n_features_a` must match it.\n\n    warm_start : bool, default=False\n        If `True` and :meth:`fit` has been called before, the solution of the\n        previous call to :meth:`fit` is used as the initial linear\n        transformation (`n_components` and `init` will be ignored).\n\n    max_iter : int, default=50\n        Maximum number of iterations in the optimization.\n\n    tol : float, default=1e-5\n        Convergence tolerance for the optimization.\n\n    callback : callable, default=None\n        If not `None`, this function is called after every iteration of the\n        optimizer, taking as arguments the current solution (flattened\n        transformation matrix) and the number of iterations. This might be\n        useful in case one wants to examine or store the transformation\n        found after each iteration.\n\n    verbose : int, default=0\n        If 0, no progress messages will be printed.\n        If 1, progress messages will be printed to stdout.\n        If > 1, progress messages will be printed and the `disp`\n        parameter of :func:`scipy.optimize.minimize` will be set to\n        `verbose - 2`.\n\n    random_state : int or numpy.RandomState, default=None\n        A pseudo random number generator object or a seed for it if int. If\n        `init='random'`, `random_state` is used to initialize the random\n        transformation. If `init='pca'`, `random_state` is passed as an\n        argument to PCA when initializing the transformation. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The linear transformation learned during fitting.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    n_iter_ : int\n        Counts the number of iterations performed by the optimizer.\n\n    random_state_ : numpy.RandomState\n        Pseudo random number generator object used during initialization.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n        Discriminant Analysis.\n    sklearn.decomposition.PCA : Principal component analysis (PCA).\n\n    References\n    ----------\n    .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n           \"Neighbourhood Components Analysis\". Advances in Neural Information\n           Processing Systems. 17, 513-520, 2005.\n           http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n    .. [2] Wikipedia entry on Neighborhood Components Analysis\n           https://en.wikipedia.org/wiki/Neighbourhood_components_analysis",
        "parameters": {
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Preferred": {
            "type": "dimensionality of the projected space.",
            "description": ""
          },
          "If": {
            "type": "> 1, progress messages will be printed and the `disp`",
            "description": ""
          },
          "init": {
            "type": "{'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'",
            "description": ""
          },
          "Initialization": {
            "type": "of the linear transformation. Possible options are",
            "description": "`'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy"
          },
          "array": {
            "type": "of shape `(n_features_a, n_features_b)`.",
            "description": "- `'auto'`"
          },
          "Depending": {
            "type": "on `n_components`, the most reasonable initialization",
            "description": ""
          },
          "is": {
            "type": "chosen. If `n_components <= min(n_features, n_classes - 1)`",
            "description": ""
          },
          "we": {
            "type": "use `'lda'`, as it uses labels information. If not, but",
            "description": "`n_components < min(n_features, n_samples)`, we use `'pca'`, as"
          },
          "it": {
            "type": "projects data in meaningful directions (those of higher",
            "description": "variance). Otherwise, we just use `'identity'`.\n- `'pca'`\n`n_components` principal components of the inputs passed"
          },
          "to": {
            "type": "meth:`fit` and n_features_a must be less than or equal to that.",
            "description": ""
          },
          "components": {
            "type": "of the inputs passed to :meth:`fit` will be used to",
            "description": ""
          },
          "initialize": {
            "type": "the transformation. (If `n_components > n_classes`,",
            "description": ""
          },
          "the": {
            "type": "rest of the components will be zero.) (See",
            "description": ":class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n- `'identity'`"
          },
          "dimensionality": {
            "type": "of the inputs passed to :meth:`fit`, the identity",
            "description": ""
          },
          "matrix": {
            "type": "will be truncated to the first `n_components` rows.",
            "description": "- `'random'`"
          },
          "The": {
            "type": "linear transformation learned during fitting.",
            "description": ""
          },
          "standard": {
            "type": "normal distribution.",
            "description": "- numpy array\n`n_features_b` must match the dimensionality of the inputs passed"
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "previous": {
            "type": "call to :meth:`fit` is used as the initial linear",
            "description": ""
          },
          "transformation": {
            "type": "matrix) and the number of iterations. This might be",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=50",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations in the optimization.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "5"
          },
          "Convergence": {
            "type": "tolerance for the optimization.",
            "description": ""
          },
          "callback": {
            "type": "callable, default=None",
            "description": ""
          },
          "useful": {
            "type": "in case one wants to examine or store the transformation",
            "description": ""
          },
          "found": {
            "type": "after each iteration.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "parameter": {
            "type": "of :func:`scipy.optimize.minimize` will be set to",
            "description": "`verbose - 2`."
          },
          "random_state": {
            "type": "int or numpy.RandomState, default=None",
            "description": ""
          },
          "A": {
            "type": "pseudo random number generator object or a seed for it if int. If",
            "description": "`init='random'`, `random_state` is used to initialize the random\ntransformation. If `init='pca'`, `random_state` is passed as an"
          },
          "argument": {
            "type": "to PCA when initializing the transformation. Pass an int",
            "description": ""
          },
          "for": {
            "type": "reproducible results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear"
          },
          "components_": {
            "type": "ndarray of shape (n_components, n_features)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Counts": {
            "type": "the number of iterations performed by the optimizer.",
            "description": ""
          },
          "random_state_": {
            "type": "numpy.RandomState",
            "description": ""
          },
          "Pseudo": {
            "type": "random number generator object used during initialization.",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "Discriminant": {
            "type": "Analysis.",
            "description": "sklearn.decomposition.PCA : Principal component analysis (PCA).\nReferences\n----------\n.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n\"Neighbourhood Components Analysis\". Advances in Neural Information"
          },
          "Processing": {
            "type": "Systems. 17, 513-520, 2005.",
            "description": ""
          },
          "http": {
            "type": "//www.cs.nyu.edu/~roweis/papers/ncanips.pdf",
            "description": ".. [2] Wikipedia entry on Neighborhood Components Analysis"
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/Neighbourhood_components_analysis",
            "description": "Examples\n--------\n>>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> nca.fit(X_train, y_train)"
          },
          "NeighborhoodComponentsAnalysis": {
            "type": "...",
            "description": ">>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> knn.fit(X_train, y_train)"
          },
          "KNeighborsClassifier": {
            "type": "...",
            "description": ">>> print(knn.score(nca.transform(X_test), y_test))\n0.961904..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n        Discriminant Analysis.\n    sklearn.decomposition.PCA : Principal component analysis (PCA).\n\n    References\n    ----------\n    .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n           \"Neighbourhood Components Analysis\". Advances in Neural Information\n           Processing Systems. 17, 513-520, 2005.\n           http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n    .. [2] Wikipedia entry on Neighborhood Components Analysis\n           https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ... stratify=y, test_size=0.7, random_state=42)\n    >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n    >>> nca.fit(X_train, y_train)\n    NeighborhoodComponentsAnalysis(...)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> knn.fit(X_train, y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(X_test, y_test))\n    0.933333...\n    >>> knn.fit(nca.transform(X_train), y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(nca.transform(X_test), y_test))\n    0.961904...",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ... stratify=y, test_size=0.7, random_state=42)\n    >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n    >>> nca.fit(X_train, y_train)\n    NeighborhoodComponentsAnalysis(...)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> knn.fit(X_train, y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(X_test, y_test))\n    0.933333...\n    >>> knn.fit(nca.transform(X_train), y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(nca.transform(X_test), y_test))\n    0.961904..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like of shape (n_samples,)\n            The corresponding training labels.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "corresponding training labels.",
                "description": "Returns\n-------"
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Apply the learned transformation to the given data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data samples.\n\n        Returns\n        -------\n        X_embedded: ndarray of shape (n_samples, n_components)\n            The data samples transformed.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "samples.",
                "description": "Returns\n-------"
              },
              "X_embedded": {
                "type": "ndarray of shape (n_samples, n_components)",
                "description": ""
              },
              "The": {
                "type": "data samples transformed.",
                "description": "Raises\n------\nNotFittedError"
              },
              "If": {
                "type": "meth:`fit` has not been called before.",
                "description": ""
              }
            },
            "returns": "-------\n        X_embedded: ndarray of shape (n_samples, n_components)\n            The data samples transformed.\n\n        Raises\n        ------\n        NotFittedError\n            If :meth:`fit` has not been called before.",
            "raises": "------\n        NotFittedError\n            If :meth:`fit` has not been called before.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsClassifier",
      "documentation": {
        "description": "Classifier implementing a vote among neighbors within a given radius.\n\n    Read more in the :ref:`User Guide <classification>`.\n\n    Parameters\n    ----------\n    radius : float, default=1.0\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n        Weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Uniform weights are used by default.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    outlier_label : {manual label, 'most_frequent'}, default=None\n        Label for outlier samples (samples with no neighbors in given radius).\n\n        - manual label: str or int label (should be the same type as y)\n          or list of manual labels if multi-output is used.\n        - 'most_frequent' : assign the most frequent label of y to outliers.\n        - None : when any outlier is detected, ValueError will be raised.\n\n        The outlier label should be selected from among the unique 'Y' labels.\n        If it is specified with a different value a warning will be raised and\n        all class probabilities of outliers will be assigned to be 0.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outlier_label_ : int or array-like of shape (n_class,)\n        Label which is given for outlier samples (samples with no neighbors\n        on given radius).\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    See Also\n    --------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsRegressor : Regression based on neighbors within a\n        fixed radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    NearestNeighbors : Unsupervised learner for implementing neighbor\n        searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Range": {
            "type": "of parameter space to use by default for :meth:`radius_neighbors`",
            "description": "queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": ""
          },
          "Weight": {
            "type": "function used in prediction.  Possible values:",
            "description": "- 'uniform' : uniform weights.  All points in each neighborhood"
          },
          "are": {
            "type": "weighted equally.",
            "description": "- 'distance' : weight points by the inverse of their distance."
          },
          "in": {
            "type": "this case, closer neighbors of a query point will have a",
            "description": ""
          },
          "greater": {
            "type": "influence than neighbors which are further away.",
            "description": "- [callable] : a user-defined function which accepts an"
          },
          "array": {
            "type": "of distances, and returns an array of the same shape",
            "description": ""
          },
          "containing": {
            "type": "the weights.",
            "description": ""
          },
          "Uniform": {
            "type": "weights are used by default.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is",
            "description": ""
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "This": {
            "type": "parameter is expected to be positive.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "it is specified with a different value a warning will be raised and",
            "description": ""
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "outlier_label": {
            "type": "{manual label, 'most_frequent'}, default=None",
            "description": ""
          },
          "Label": {
            "type": "which is given for outlier samples (samples with no neighbors",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "The": {
            "type": "distance metric used. It will be same as the `metric` parameter",
            "description": ""
          },
          "all": {
            "type": "class probabilities of outliers will be assigned to be 0.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "a discussion of the choice of ``algorithm`` and ``leaf_size``.",
            "description": ""
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "Class": {
            "type": "labels known to the classifier.",
            "description": ""
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": ""
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "outlier_label_": {
            "type": "int or array",
            "description": "like of shape (n_class,)"
          },
          "on": {
            "type": "given radius).",
            "description": ""
          },
          "outputs_2d_": {
            "type": "bool",
            "description": ""
          },
          "False": {
            "type": "when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit",
            "description": ""
          },
          "otherwise": {
            "type": "True.",
            "description": ""
          },
          "See": {
            "type": "ref:`Nearest Neighbors <neighbors>` in the online documentation",
            "description": ""
          },
          "KNeighborsClassifier": {
            "type": "Classifier implementing the k",
            "description": "nearest neighbors\nvote."
          },
          "RadiusNeighborsRegressor": {
            "type": "Regression based on neighbors within a",
            "description": ""
          },
          "fixed": {
            "type": "radius.",
            "description": ""
          },
          "KNeighborsRegressor": {
            "type": "Regression based on k",
            "description": "nearest neighbors."
          },
          "NearestNeighbors": {
            "type": "Unsupervised learner for implementing neighbor",
            "description": "searches.\nNotes\n-----"
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/K",
            "description": "nearest_neighbor_algorithm\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsClassifier\n>>> neigh = RadiusNeighborsClassifier(radius=1.0)\n>>> neigh.fit(X, y)"
          },
          "RadiusNeighborsClassifier": {
            "type": "...",
            "description": ">>> print(neigh.predict([[1.5]]))\n[0]\n>>> print(neigh.predict_proba([[1.0]]))\n[[0.66666667 0.33333333]]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsRegressor : Regression based on neighbors within a\n        fixed radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    NearestNeighbors : Unsupervised learner for implementing neighbor\n        searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0]\n    >>> print(neigh.predict_proba([[1.0]]))\n    [[0.66666667 0.33333333]]",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    outlier_label : {manual label, 'most_frequent'}, default=None\n        Label for outlier samples (samples with no neighbors in given radius).\n\n        - manual label: str or int label (should be the same type as y)\n          or list of manual labels if multi-output is used.\n        - 'most_frequent' : assign the most frequent label of y to outliers.\n        - None : when any outlier is detected, ValueError will be raised.\n\n        The outlier label should be selected from among the unique 'Y' labels.\n        If it is specified with a different value a warning will be raised and\n        all class probabilities of outliers will be assigned to be 0.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outlier_label_ : int or array-like of shape (n_class,)\n        Label which is given for outlier samples (samples with no neighbors\n        on given radius).\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    See Also\n    --------\n    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n        vote.\n    RadiusNeighborsRegressor : Regression based on neighbors within a\n        fixed radius.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    NearestNeighbors : Unsupervised learner for implementing neighbor\n        searches.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0]\n    >>> print(neigh.predict_proba([[1.0]]))\n    [[0.66666667 0.33333333]]",
        "examples": "--------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0]\n    >>> print(neigh.predict_proba([[1.0]]))\n    [[0.66666667 0.33333333]]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the radius neighbors classifier from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n            Target values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "RadiusNeighborsClassifier",
                "description": ""
              },
              "The": {
                "type": "fitted radius neighbors classifier.",
                "description": ""
              }
            },
            "returns": "-------\n        self : RadiusNeighborsClassifier\n            The fitted radius neighbors classifier.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "y": {
                "type": "ndarray of shape (n_queries,) or (n_queries, n_outputs)",
                "description": ""
              },
              "Class": {
                "type": "labels for each data sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n            Class labels for each data sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "p": {
                "type": "ndarray of shape (n_queries, n_classes), or a list of                 n_outputs of such arrays if n_outputs > 1.",
                "description": ""
              },
              "The": {
                "type": "class probabilities of the input samples. Classes are ordered",
                "description": ""
              },
              "by": {
                "type": "lexicographic order.",
                "description": ""
              }
            },
            "returns": "-------\n        p : ndarray of shape (n_queries, n_classes), or a list of                 n_outputs of such arrays if n_outputs > 1.\n            The class probabilities of the input samples. Classes are ordered\n            by lexicographic order.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "documentation": {
            "description": "Find the neighbors within a given radius of a point or points.\n\n        Return the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of (n_samples, n_features), default=None"
              },
              "The": {
                "type": "first array returned contains the distances to all points which",
                "description": ""
              },
              "If": {
                "type": "True, the distances and indices will be sorted by increasing",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NeighborsClassifier",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Limiting": {
                "type": "distance of neighbors to return. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "distances": {
                "type": "before being returned. If False, the results may not",
                "description": ""
              },
              "be": {
                "type": "sorted. If `return_distance=False`, setting `sort_results=True`",
                "description": ""
              },
              "will": {
                "type": "result in an error.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "Array": {
                "type": "representing the distances to each point, only present if",
                "description": "`return_distance=True`. The distance values are computed according"
              },
              "to": {
                "type": "the ``metric`` constructor parameter.",
                "description": ""
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "An": {
                "type": "array of arrays of indices of the approximate nearest points",
                "description": ""
              },
              "from": {
                "type": "the population matrix that lie within a ball of size",
                "description": "``radius`` around the query points.\nNotes\n-----"
              },
              "Because": {
                "type": "the number of neighbors of each point is not necessarily",
                "description": "equal, the results for multiple query points cannot be fit in a"
              },
              "standard": {
                "type": "data array.",
                "description": ""
              },
              "For": {
                "type": "efficiency, `radius_neighbors` returns arrays of objects, where",
                "description": ""
              },
              "each": {
                "type": "object is a 1D array of indices or distances.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1, 1, 1]:",
                "description": ">>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "radius=1.6",
                "description": ">>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]"
              },
              "are": {
                "type": "closer than 1.6, while the second array returned contains their",
                "description": "indices.  In general, multiple points can be queried at the same time."
              }
            },
            "returns": "the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "examples": "--------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Neighborhoods are restricted the points at a distance lower than\n        radius.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Radius of neighborhoods. The default is the value passed to the\n            constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        sort_results : bool, default=False\n            If True, in each row of the result, the non-zero entries will be\n            sorted by increasing distances. If False, the non-zero entries may\n            not be sorted. Only used with mode='distance'.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "True, in each row of the result, the non-zero entries will be",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Radius": {
                "type": "of neighborhoods. The default is the value passed to the",
                "description": "constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "radius=1.5",
                "description": ">>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 0.],\n[1., 0., 1.]])"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "sorted": {
                "type": "by increasing distances. If False, the non-zero entries may",
                "description": ""
              },
              "not": {
                "type": "be sorted. Only used with mode='distance'.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "kneighbors_graph": {
                "type": "Compute the (weighted) graph of k",
                "description": "Neighbors for"
              },
              "points": {
                "type": "in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features), or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "used; in this case, points are not considered their own\nneighbors. This means that `knn.fit(X, y).score(None, y)`"
              },
              "implicitly": {
                "type": "performs a leave-one-out cross-validation procedure",
                "description": ""
              },
              "and": {
                "type": "is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`",
                "description": ""
              },
              "but": {
                "type": "typically much faster.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), or None\n            Test samples. If `None`, predictions for all indexed points are\n            used; in this case, points are not considered their own\n            neighbors. This means that `knn.fit(X, y).score(None, y)`\n            implicitly performs a leave-one-out cross-validation procedure\n            and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n            but typically much faster.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._classification.RadiusNeighborsClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._classification.RadiusNeighborsClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsRegressor",
      "documentation": {
        "description": "Regression based on neighbors within a fixed radius.\n\n    The target is predicted by local interpolation of the targets\n    associated of the nearest neighbors in the training set.\n\n    Read more in the :ref:`User Guide <regression>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    radius : float, default=1.0\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n        Weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Uniform weights are used by default.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric to use. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    KNeighborsClassifier : Classifier based on the k-nearest neighbors.\n    RadiusNeighborsClassifier : Classifier based on neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm",
        "parameters": {
          "radius": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Range": {
            "type": "of parameter space to use by default for :meth:`radius_neighbors`",
            "description": "queries."
          },
          "weights": {
            "type": "{'uniform', 'distance'}, callable or None, default='uniform'",
            "description": ""
          },
          "Weight": {
            "type": "function used in prediction.  Possible values:",
            "description": "- 'uniform' : uniform weights.  All points in each neighborhood"
          },
          "are": {
            "type": "weighted equally.",
            "description": "- 'distance' : weight points by the inverse of their distance."
          },
          "in": {
            "type": "this case, closer neighbors of a query point will have a",
            "description": ""
          },
          "greater": {
            "type": "influence than neighbors which are further away.",
            "description": "- [callable] : a user-defined function which accepts an"
          },
          "array": {
            "type": "of distances, and returns an array of the same shape",
            "description": ""
          },
          "containing": {
            "type": "the weights.",
            "description": ""
          },
          "Uniform": {
            "type": "weights are used by default.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Power": {
            "type": "parameter for the Minkowski metric. When p = 1, this is",
            "description": ""
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "metric is a callable function, it takes two arrays representing 1D",
            "description": ""
          },
          "must": {
            "type": "be square during fit. X may be a :term:`sparse graph`, in which",
            "description": ""
          },
          "case": {
            "type": "only \"nonzero\" elements may be considered neighbors.",
            "description": ""
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "distance metric to use. It will be same as the `metric` parameter",
            "description": ""
          },
          "for": {
            "type": "a discussion of the choice of ``algorithm`` and ``leaf_size``.",
            "description": ""
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "See": {
            "type": "ref:`Nearest Neighbors <neighbors>` in the online documentation",
            "description": ""
          },
          "NearestNeighbors": {
            "type": "Unsupervised learner for implementing neighbor searches.",
            "description": ""
          },
          "KNeighborsRegressor": {
            "type": "Regression based on k",
            "description": "nearest neighbors."
          },
          "KNeighborsClassifier": {
            "type": "Classifier based on the k",
            "description": "nearest neighbors."
          },
          "RadiusNeighborsClassifier": {
            "type": "Classifier based on neighbors within a given radius.",
            "description": "Notes\n-----"
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/K",
            "description": "nearest_neighbor_algorithm\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsRegressor\n>>> neigh = RadiusNeighborsRegressor(radius=1.0)\n>>> neigh.fit(X, y)"
          },
          "RadiusNeighborsRegressor": {
            "type": "...",
            "description": ">>> print(neigh.predict([[1.5]]))\n[0.5]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    KNeighborsClassifier : Classifier based on the k-nearest neighbors.\n    RadiusNeighborsClassifier : Classifier based on neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsRegressor\n    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : float, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric to use. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n    KNeighborsRegressor : Regression based on k-nearest neighbors.\n    KNeighborsClassifier : Classifier based on the k-nearest neighbors.\n    RadiusNeighborsClassifier : Classifier based on neighbors within a given radius.\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsRegressor\n    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]",
        "examples": "--------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsRegressor\n    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the radius neighbors regressor from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n            Target values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "RadiusNeighborsRegressor",
                "description": ""
              },
              "The": {
                "type": "fitted radius neighbors regressor.",
                "description": ""
              }
            },
            "returns": "-------\n        self : RadiusNeighborsRegressor\n            The fitted radius neighbors regressor.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the target for the provided data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n            Test samples. If `None`, predictions for all indexed points are\n            returned; in this case, points are not considered their own\n            neighbors.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None"
              },
              "Test": {
                "type": "samples. If `None`, predictions for all indexed points are",
                "description": "returned; in this case, points are not considered their own\nneighbors.\nReturns\n-------"
              },
              "y": {
                "type": "ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double\n            Target values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "documentation": {
            "description": "Find the neighbors within a given radius of a point or points.\n\n        Return the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of (n_samples, n_features), default=None"
              },
              "The": {
                "type": "first array returned contains the distances to all points which",
                "description": ""
              },
              "If": {
                "type": "True, the distances and indices will be sorted by increasing",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NeighborsClassifier",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Limiting": {
                "type": "distance of neighbors to return. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "distances": {
                "type": "before being returned. If False, the results may not",
                "description": ""
              },
              "be": {
                "type": "sorted. If `return_distance=False`, setting `sort_results=True`",
                "description": ""
              },
              "will": {
                "type": "result in an error.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "Array": {
                "type": "representing the distances to each point, only present if",
                "description": "`return_distance=True`. The distance values are computed according"
              },
              "to": {
                "type": "the ``metric`` constructor parameter.",
                "description": ""
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "An": {
                "type": "array of arrays of indices of the approximate nearest points",
                "description": ""
              },
              "from": {
                "type": "the population matrix that lie within a ball of size",
                "description": "``radius`` around the query points.\nNotes\n-----"
              },
              "Because": {
                "type": "the number of neighbors of each point is not necessarily",
                "description": "equal, the results for multiple query points cannot be fit in a"
              },
              "standard": {
                "type": "data array.",
                "description": ""
              },
              "For": {
                "type": "efficiency, `radius_neighbors` returns arrays of objects, where",
                "description": ""
              },
              "each": {
                "type": "object is a 1D array of indices or distances.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1, 1, 1]:",
                "description": ">>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "radius=1.6",
                "description": ">>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]"
              },
              "are": {
                "type": "closer than 1.6, while the second array returned contains their",
                "description": "indices.  In general, multiple points can be queried at the same time."
              }
            },
            "returns": "the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "examples": "--------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Neighborhoods are restricted the points at a distance lower than\n        radius.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Radius of neighborhoods. The default is the value passed to the\n            constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        sort_results : bool, default=False\n            If True, in each row of the result, the non-zero entries will be\n            sorted by increasing distances. If False, the non-zero entries may\n            not be sorted. Only used with mode='distance'.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "True, in each row of the result, the non-zero entries will be",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Radius": {
                "type": "of neighborhoods. The default is the value passed to the",
                "description": "constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "radius=1.5",
                "description": ">>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 0.],\n[1., 0., 1.]])"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "sorted": {
                "type": "by increasing distances. If False, the non-zero entries may",
                "description": ""
              },
              "not": {
                "type": "be sorted. Only used with mode='distance'.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "kneighbors_graph": {
                "type": "Compute the (weighted) graph of k",
                "description": "Neighbors for"
              },
              "points": {
                "type": "in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.neighbors._regression.RadiusNeighborsRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._regression.RadiusNeighborsRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RadiusNeighborsTransformer",
      "documentation": {
        "description": "Transform X into a (weighted) graph of neighbors nearer than a radius.\n\n    The transformed data is a sparse graph as returned by\n    `radius_neighbors_graph`.\n\n    Read more in the :ref:`User Guide <neighbors_transformer>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    mode : {'distance', 'connectivity'}, default='distance'\n        Type of returned matrix: 'connectivity' will return the connectivity\n        matrix with ones and zeros, and 'distance' will return the distances\n        between neighbors according to the given metric.\n\n    radius : float, default=1.0\n        Radius of neighborhood in the transformed sparse graph.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    KNeighborsTransformer : Transform X into a weighted graph of k\n        nearest neighbors.",
        "parameters": {
          "mode": {
            "type": "{'distance', 'connectivity'}, default='distance'",
            "description": ""
          },
          "Type": {
            "type": "of returned matrix: 'connectivity' will return the connectivity",
            "description": ""
          },
          "matrix": {
            "type": "with ones and zeros, and 'distance' will return the distances",
            "description": ""
          },
          "between": {
            "type": "those vectors. This works for Scipy's metrics, but is less",
            "description": ""
          },
          "radius": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Radius": {
            "type": "of neighborhood in the transformed sparse graph.",
            "description": ""
          },
          "algorithm": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'",
            "description": ""
          },
          "Algorithm": {
            "type": "used to compute the nearest neighbors:",
            "description": "- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm"
          },
          "based": {
            "type": "on the values passed to :meth:`fit` method.",
            "description": ""
          },
          "Note": {
            "type": "fitting on sparse input will override the setting of",
            "description": ""
          },
          "this": {
            "type": "parameter, using brute force.",
            "description": ""
          },
          "leaf_size": {
            "type": "int, default=30",
            "description": ""
          },
          "Leaf": {
            "type": "size passed to BallTree or KDTree.  This can affect the",
            "description": ""
          },
          "speed": {
            "type": "of the construction and query, as well as the memory",
            "description": ""
          },
          "required": {
            "type": "to store the tree.  The optimal value depends on the",
            "description": ""
          },
          "nature": {
            "type": "of the problem.",
            "description": ""
          },
          "metric": {
            "type": "str or callable, default='minkowski'",
            "description": ""
          },
          "Metric": {
            "type": "to use for distance computation. Default is \"minkowski\", which",
            "description": ""
          },
          "results": {
            "type": "in the standard Euclidean distance when p = 2. See the",
            "description": ""
          },
          "documentation": {
            "type": "of `scipy.spatial.distance",
            "description": "<https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and"
          },
          "the": {
            "type": "metrics listed in",
            "description": ":class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues."
          },
          "If": {
            "type": "``-1``, then the number of jobs is set to the number of CPU cores.",
            "description": "Attributes\n----------"
          },
          "vectors": {
            "type": "as inputs and must return one value indicating the distance",
            "description": ""
          },
          "efficient": {
            "type": "than passing the metric name as a string.",
            "description": ""
          },
          "Distance": {
            "type": "matrices are not supported.",
            "description": ""
          },
          "p": {
            "type": "float, default=2",
            "description": ""
          },
          "Parameter": {
            "type": "for the Minkowski metric from",
            "description": "sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is"
          },
          "equivalent": {
            "type": "to using manhattan_distance (l1), and euclidean_distance",
            "description": "(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "This": {
            "type": "parameter is expected to be positive.",
            "description": ""
          },
          "metric_params": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "keyword arguments for the metric function. For most metrics",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "distance metric used. It will be same as the `metric` parameter",
            "description": ""
          },
          "effective_metric_": {
            "type": "str or callable",
            "description": ""
          },
          "or": {
            "type": "a synonym of it, e.g. 'euclidean' if the `metric` parameter set to",
            "description": "'minkowski' and `p` parameter set to 2."
          },
          "effective_metric_params_": {
            "type": "dict",
            "description": ""
          },
          "will": {
            "type": "be same with `metric_params` parameter, but may also contain the",
            "description": "`p` parameter value if the `effective_metric_` attribute is set to\n'minkowski'."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of samples in the fitted data.",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_fit_": {
            "type": "int",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "kneighbors_graph": {
            "type": "Compute the weighted graph of k",
            "description": "neighbors for"
          },
          "points": {
            "type": "in X.",
            "description": ""
          },
          "KNeighborsTransformer": {
            "type": "Transform X into a weighted graph of k",
            "description": ""
          },
          "nearest": {
            "type": "neighbors.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.cluster import DBSCAN\n>>> from sklearn.neighbors import RadiusNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> X, _ = load_wine(return_X_y=True)\n>>> estimator = make_pipeline(\n...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n...     DBSCAN(eps=25.0, metric='precomputed'))\n>>> X_clustered = estimator.fit_predict(X)\n>>> clusters, counts = np.unique(X_clustered, return_counts=True)\n>>> print(counts)\n[ 29  15 111  11  12]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    KNeighborsTransformer : Transform X into a weighted graph of k\n        nearest neighbors.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.cluster import DBSCAN\n    >>> from sklearn.neighbors import RadiusNeighborsTransformer\n    >>> from sklearn.pipeline import make_pipeline\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> estimator = make_pipeline(\n    ...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n    ...     DBSCAN(eps=25.0, metric='precomputed'))\n    >>> X_clustered = estimator.fit_predict(X)\n    >>> clusters, counts = np.unique(X_clustered, return_counts=True)\n    >>> print(counts)\n    [ 29  15 111  11  12]",
        "notes": "fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n        If metric is a callable function, it takes two arrays representing 1D\n        vectors as inputs and must return one value indicating the distance\n        between those vectors. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n        This parameter is expected to be positive.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Attributes\n    ----------\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    See Also\n    --------\n    kneighbors_graph : Compute the weighted graph of k-neighbors for\n        points in X.\n    KNeighborsTransformer : Transform X into a weighted graph of k\n        nearest neighbors.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.cluster import DBSCAN\n    >>> from sklearn.neighbors import RadiusNeighborsTransformer\n    >>> from sklearn.pipeline import make_pipeline\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> estimator = make_pipeline(\n    ...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n    ...     DBSCAN(eps=25.0, metric='precomputed'))\n    >>> X_clustered = estimator.fit_predict(X)\n    >>> clusters, counts = np.unique(X_clustered, return_counts=True)\n    >>> print(counts)\n    [ 29  15 111  11  12]",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import load_wine\n    >>> from sklearn.cluster import DBSCAN\n    >>> from sklearn.neighbors import RadiusNeighborsTransformer\n    >>> from sklearn.pipeline import make_pipeline\n    >>> X, _ = load_wine(return_X_y=True)\n    >>> estimator = make_pipeline(\n    ...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n    ...     DBSCAN(eps=25.0, metric='precomputed'))\n    >>> X_clustered = estimator.fit_predict(X)\n    >>> clusters, counts = np.unique(X_clustered, return_counts=True)\n    >>> print(counts)\n    [ 29  15 111  11  12]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the radius neighbors transformer from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "RadiusNeighborsTransformer",
                "description": ""
              },
              "The": {
                "type": "fitted radius neighbors transformer.",
                "description": ""
              }
            },
            "returns": "-------\n        self : RadiusNeighborsTransformer\n            The fitted radius neighbors transformer.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training set.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "set.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "Xt": {
                "type": "sparse matrix of shape (n_samples, n_samples)",
                "description": "Xt[i, j] is assigned the weight of edge that connects i to j."
              },
              "Only": {
                "type": "the neighbors have an explicit value.",
                "description": ""
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              }
            },
            "returns": "-------\n        Xt : sparse matrix of shape (n_samples, n_samples)\n            Xt[i, j] is assigned the weight of edge that connects i to j.\n            Only the neighbors have an explicit value.\n            The diagonal is always explicit.\n            The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "radius_neighbors",
          "signature": "radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)",
          "documentation": {
            "description": "Find the neighbors within a given radius of a point or points.\n\n        Return the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of (n_samples, n_features), default=None"
              },
              "The": {
                "type": "first array returned contains the distances to all points which",
                "description": ""
              },
              "If": {
                "type": "True, the distances and indices will be sorted by increasing",
                "description": ""
              },
              "In": {
                "type": "the following example, we construct a NeighborsClassifier",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Limiting": {
                "type": "distance of neighbors to return. The default is the value",
                "description": ""
              },
              "passed": {
                "type": "to the constructor.",
                "description": ""
              },
              "return_distance": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "or not to return the distances.",
                "description": ""
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "distances": {
                "type": "before being returned. If False, the results may not",
                "description": ""
              },
              "be": {
                "type": "sorted. If `return_distance=False`, setting `sort_results=True`",
                "description": ""
              },
              "will": {
                "type": "result in an error.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "neigh_dist": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "Array": {
                "type": "representing the distances to each point, only present if",
                "description": "`return_distance=True`. The distance values are computed according"
              },
              "to": {
                "type": "the ``metric`` constructor parameter.",
                "description": ""
              },
              "neigh_ind": {
                "type": "ndarray of shape (n_samples,) of arrays",
                "description": ""
              },
              "An": {
                "type": "array of arrays of indices of the approximate nearest points",
                "description": ""
              },
              "from": {
                "type": "the population matrix that lie within a ball of size",
                "description": "``radius`` around the query points.\nNotes\n-----"
              },
              "Because": {
                "type": "the number of neighbors of each point is not necessarily",
                "description": "equal, the results for multiple query points cannot be fit in a"
              },
              "standard": {
                "type": "data array.",
                "description": ""
              },
              "For": {
                "type": "efficiency, `radius_neighbors` returns arrays of objects, where",
                "description": ""
              },
              "each": {
                "type": "object is a 1D array of indices or distances.",
                "description": "Examples\n--------"
              },
              "class": {
                "type": "from an array representing our data set and ask who's",
                "description": ""
              },
              "the": {
                "type": "closest point to [1, 1, 1]:",
                "description": ">>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)"
              },
              "NearestNeighbors": {
                "type": "radius=1.6",
                "description": ">>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]"
              },
              "are": {
                "type": "closer than 1.6, while the second array returned contains their",
                "description": "indices.  In general, multiple points can be queried at the same time."
              }
            },
            "returns": "the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.",
            "examples": "--------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time."
          }
        },
        {
          "name": "radius_neighbors_graph",
          "signature": "radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Neighborhoods are restricted the points at a distance lower than\n        radius.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Radius of neighborhoods. The default is the value passed to the\n            constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are distances between points, type of distance\n            depends on the selected metric parameter in\n            NearestNeighbors class.\n\n        sort_results : bool, default=False\n            If True, in each row of the result, the non-zero entries will be\n            sorted by increasing distances. If False, the non-zero entries may\n            not be sorted. Only used with mode='distance'.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features), default=None"
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              },
              "If": {
                "type": "True, in each row of the result, the non-zero entries will be",
                "description": ""
              },
              "In": {
                "type": "this case, the query point is not considered its own neighbor.",
                "description": ""
              },
              "radius": {
                "type": "float, default=None",
                "description": ""
              },
              "Radius": {
                "type": "of neighborhoods. The default is the value passed to the",
                "description": "constructor."
              },
              "mode": {
                "type": "{'connectivity', 'distance'}, default='connectivity'",
                "description": ""
              },
              "Type": {
                "type": "of returned matrix: 'connectivity' will return the",
                "description": ""
              },
              "connectivity": {
                "type": "matrix with ones and zeros, in 'distance' the",
                "description": ""
              },
              "edges": {
                "type": "are distances between points, type of distance",
                "description": ""
              },
              "depends": {
                "type": "on the selected metric parameter in",
                "description": ""
              },
              "NearestNeighbors": {
                "type": "radius=1.5",
                "description": ">>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n[0., 1., 0.],\n[1., 0., 1.]])"
              },
              "sort_results": {
                "type": "bool, default=False",
                "description": ""
              },
              "sorted": {
                "type": "by increasing distances. If False, the non-zero entries may",
                "description": ""
              },
              "not": {
                "type": "be sorted. Only used with mode='distance'.",
                "description": ".. versionadded:: 0.22\nReturns\n-------"
              },
              "A": {
                "type": "sparse",
                "description": "matrix of shape (n_queries, n_samples_fit)\n`n_samples_fit` is the number of samples in the fitted data.\n`A[i, j]` gives the weight of the edge connecting `i` to `j`."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "kneighbors_graph": {
                "type": "Compute the (weighted) graph of k",
                "description": "Neighbors for"
              },
              "points": {
                "type": "in X.",
                "description": "Examples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)"
              }
            },
            "returns": "-------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        See Also\n        --------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "raises": "",
            "see_also": "--------\n        kneighbors_graph : Compute the (weighted) graph of k-Neighbors for\n            points in X.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])",
            "notes": "",
            "examples": "--------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])"
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Compute the (weighted) graph of Neighbors for points in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_transform, n_features)\n            Sample data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples_transform, n_features)"
              },
              "Sample": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "Xt": {
                "type": "sparse matrix of shape (n_samples_transform, n_samples_fit)",
                "description": "Xt[i, j] is assigned the weight of edge that connects i to j."
              },
              "Only": {
                "type": "the neighbors have an explicit value.",
                "description": ""
              },
              "The": {
                "type": "matrix is of CSR format.",
                "description": ""
              }
            },
            "returns": "-------\n        Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n            Xt[i, j] is assigned the weight of edge that connects i to j.\n            Only the neighbors have an explicit value.\n            The diagonal is always explicit.\n            The matrix is of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}