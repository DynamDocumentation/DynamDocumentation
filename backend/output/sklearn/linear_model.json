{
  "description": "A variety of linear models.",
  "functions": [
    {
      "name": "enet_path",
      "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
      "documentation": {
        "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "Training": {
            "type": "data. Pass directly as Fortran-contiguous data to avoid",
            "description": ""
          },
          "unnecessary": {
            "type": "memory duplication. If ``y`` is mono-output then ``X``",
            "description": ""
          },
          "can": {
            "type": "be sparse.",
            "description": ""
          },
          "y": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
          },
          "Target": {
            "type": "values.",
            "description": ""
          },
          "l1_ratio": {
            "type": "float, default=0.5",
            "description": ""
          },
          "Number": {
            "type": "of alphas along the regularization path.",
            "description": ""
          },
          "l1": {
            "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
            "description": ""
          },
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "alphas": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "set to False, the input validation checks are skipped (including the",
            "description": ""
          },
          "precompute": {
            "type": "'auto', bool or array",
            "description": "like of shape             (n_features, n_features), default='auto'"
          },
          "Whether": {
            "type": "to return the number of iterations or not.",
            "description": ""
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "Xy": {
            "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
            "description": ""
          },
          "only": {
            "type": "when the Gram matrix is precomputed.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "coef_init": {
            "type": "array",
            "description": "like of shape (n_features, ), default=None"
          },
          "The": {
            "type": "number of iterations taken by the coordinate descent optimizer to",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "check_input": {
            "type": "bool, default=True",
            "description": ""
          },
          "Gram": {
            "type": "matrix when provided). It is assumed that they are handled",
            "description": ""
          },
          "by": {
            "type": "the caller.",
            "description": "**params : kwargs"
          },
          "Keyword": {
            "type": "arguments passed to the coordinate descent solver.",
            "description": "Returns\n-------"
          },
          "coefs": {
            "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
            "description": ""
          },
          "Coefficients": {
            "type": "along the path.",
            "description": ""
          },
          "dual_gaps": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "n_iters": {
            "type": "list of int",
            "description": ""
          },
          "reach": {
            "type": "the specified tolerance for each alpha.",
            "description": "(Is returned when ``return_n_iter`` is set to True)."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "MultiTaskElasticNet": {
            "type": "Multi",
            "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
          },
          "MultiTaskElasticNetCV": {
            "type": "Multi",
            "description": "task L1/L2 ElasticNet with built-in cross-validation."
          },
          "ElasticNet": {
            "type": "Linear regression with combined L1 and L2 priors as regularizer.",
            "description": ""
          },
          "ElasticNetCV": {
            "type": "Elastic Net model with iterative fitting along a regularization path.",
            "description": "Notes\n-----"
          },
          "For": {
            "type": "an example, see",
            "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
          },
          "array": {
            "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
            "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
          }
        },
        "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
        "raises": "",
        "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
        "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
        "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
      }
    },
    {
      "name": "lars_path",
      "signature": "lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)",
      "documentation": {
        "description": "Compute Least Angle Regression or Lasso path using the LARS algorithm.\n\n    The optimization objective for the case method='lasso' is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    in the case of method='lar', the objective function is only known in\n    the form of an implicit equation (see discussion in [1]_).\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    X : None or ndarray of shape (n_samples, n_features)\n        Input data. If X is `None`, Gram must also be `None`.\n        If only the Gram matrix is available, use `lars_path_gram` instead.\n\n    y : None or ndarray of shape (n_samples,)\n        Input targets.\n\n    Xy : array-like of shape (n_features,), default=None\n        `Xy = X.T @ y` that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    Gram : None, 'auto', bool, ndarray of shape (n_features, n_features),             default=None\n        Precomputed Gram matrix `X.T @ X`, if `'auto'`, the Gram\n        matrix is precomputed from the given X, if there are more samples\n        than features.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform, set to infinity for no limit.\n\n    alpha_min : float, default=0\n        Minimum correlation along the path. It corresponds to the\n        regularization parameter `alpha` in the Lasso.\n\n    method : {'lar', 'lasso'}, default='lar'\n        Specifies the returned model. Select `'lar'` for Least Angle\n        Regression, `'lasso'` for the Lasso.\n\n    copy_X : bool, default=True\n        If `False`, `X` is overwritten.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the `tol` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_Gram : bool, default=True\n        If `False`, `Gram` is overwritten.\n\n    verbose : int, default=0\n        Controls output verbosity.\n\n    return_path : bool, default=True\n        If `True`, returns the entire path, else returns only the\n        last point of the path.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0.\n        This option is only allowed with method 'lasso'. Note that the model\n        coefficients will not converge to the ordinary-least-squares solution\n        for small values of alpha. Only coefficients up to the smallest alpha\n        value (`alphas_[alphas_ > 0.].min()` when fit_path=True) reached by\n        the stepwise Lars-Lasso algorithm are typically in congruence with the\n        solution of the coordinate descent `lasso_path` function.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas + 1,)\n        Maximum of covariances (in absolute value) at each iteration.\n        `n_alphas` is either `max_iter`, `n_features`, or the\n        number of nodes in the path with `alpha >= alpha_min`, whichever\n        is smaller.\n\n    active : ndarray of shape (n_alphas,)\n        Indices of active variables at the end of the path.\n\n    coefs : ndarray of shape (n_features, n_alphas + 1)\n        Coefficients along the path.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is set\n        to True.\n\n    See Also\n    --------\n    lars_path_gram : Compute LARS path in the sufficient stats mode.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_",
        "parameters": {
          "X": {
            "type": "None or ndarray of shape (n_samples, n_features)",
            "description": ""
          },
          "Input": {
            "type": "targets.",
            "description": ""
          },
          "If": {
            "type": "`True`, returns the entire path, else returns only the",
            "description": ""
          },
          "y": {
            "type": "None or ndarray of shape (n_samples,)",
            "description": ""
          },
          "Xy": {
            "type": "array",
            "description": "like of shape (n_features,), default=None\n`Xy = X.T @ y` that can be precomputed. It is useful"
          },
          "only": {
            "type": "when the Gram matrix is precomputed.",
            "description": ""
          },
          "Gram": {
            "type": "None, 'auto', bool, ndarray of shape (n_features, n_features),             default=None",
            "description": ""
          },
          "Precomputed": {
            "type": "Gram matrix `X.T @ X`, if `'auto'`, the Gram",
            "description": ""
          },
          "matrix": {
            "type": "is precomputed from the given X, if there are more samples",
            "description": ""
          },
          "than": {
            "type": "features.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "of covariances (in absolute value) at each iteration.",
            "description": "`n_alphas` is either `max_iter`, `n_features`, or the"
          },
          "alpha_min": {
            "type": "float, default=0",
            "description": ""
          },
          "Minimum": {
            "type": "correlation along the path. It corresponds to the",
            "description": ""
          },
          "regularization": {
            "type": "parameter `alpha` in the Lasso.",
            "description": ""
          },
          "method": {
            "type": "{'lar', 'lasso'}, default='lar'",
            "description": ""
          },
          "Specifies": {
            "type": "the returned model. Select `'lar'` for Least Angle",
            "description": "Regression, `'lasso'` for the Lasso."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "The": {
            "type": "machine-precision regularization in the computation of the",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the `tol` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "stepwise Lars-Lasso algorithm are typically in congruence with the",
            "description": ""
          },
          "copy_Gram": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Controls": {
            "type": "output verbosity.",
            "description": ""
          },
          "return_path": {
            "type": "bool, default=True",
            "description": ""
          },
          "last": {
            "type": "point of the path.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to return the number of iterations.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Restrict": {
            "type": "coefficients to be >= 0.",
            "description": ""
          },
          "This": {
            "type": "option is only allowed with method 'lasso'. Note that the model",
            "description": ""
          },
          "coefficients": {
            "type": "will not converge to the ordinary-least-squares solution",
            "description": ""
          },
          "for": {
            "type": "small values of alpha. Only coefficients up to the smallest alpha",
            "description": ""
          },
          "value": {
            "type": "`alphas_[alphas_ > 0.].min(",
            "description": "` when fit_path=True) reached by"
          },
          "solution": {
            "type": "of the coordinate descent `lasso_path` function.",
            "description": "Returns\n-------"
          },
          "alphas": {
            "type": "ndarray of shape (n_alphas + 1,)",
            "description": ""
          },
          "number": {
            "type": "of nodes in the path with `alpha >= alpha_min`, whichever",
            "description": ""
          },
          "is": {
            "type": "smaller.",
            "description": ""
          },
          "active": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "coefs": {
            "type": "ndarray of shape (n_features, n_alphas + 1)",
            "description": ""
          },
          "Coefficients": {
            "type": "along the path.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of iterations run. Returned only if `return_n_iter` is set",
            "description": ""
          },
          "to": {
            "type": "True.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path_gram": {
            "type": "Compute LARS path in the sufficient stats mode.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "Lars": {
            "type": "Least Angle Regression model a.k.a. LAR.",
            "description": ""
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso, using the LARS algorithm."
          },
          "LarsCV": {
            "type": "Cross",
            "description": "validated Least Angle Regression model.\nsklearn.decomposition.sparse_encode : Sparse coding.\nReferences\n----------\n.. [1] \"Least Angle Regression\", Efron et al."
          },
          "http": {
            "type": "//statweb.stanford.edu/~tibs/ftp/lars.pdf",
            "description": ".. [2] `Wikipedia entry on the Least-angle regression\n<https://en.wikipedia.org/wiki/Least-angle_regression>`_\n.. [3] `Wikipedia entry on the Lasso\n<https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\nExamples\n--------\n>>> from sklearn.linear_model import lars_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
          },
          "array": {
            "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
            "description": ">>> alphas, _, estimated_coef = lars_path(X, y)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.     ,  0.     ,  0.     ],\n[ 0.     ,  0.     ,  0.     ],\n[ 0.     ,  0.     ,  0.     ],\n[ 0.     , 46.96..., 97.99...],\n[ 0.     ,  0.     , 45.70...]])"
          }
        },
        "returns": "-------\n    alphas : ndarray of shape (n_alphas + 1,)\n        Maximum of covariances (in absolute value) at each iteration.\n        `n_alphas` is either `max_iter`, `n_features`, or the\n        number of nodes in the path with `alpha >= alpha_min`, whichever\n        is smaller.\n\n    active : ndarray of shape (n_alphas,)\n        Indices of active variables at the end of the path.\n\n    coefs : ndarray of shape (n_features, n_alphas + 1)\n        Coefficients along the path.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is set\n        to True.\n\n    See Also\n    --------\n    lars_path_gram : Compute LARS path in the sufficient stats mode.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import lars_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path(X, y)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])",
        "raises": "",
        "see_also": "--------\n    lars_path_gram : Compute LARS path in the sufficient stats mode.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import lars_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path(X, y)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import lars_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path(X, y)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])"
      }
    },
    {
      "name": "lars_path_gram",
      "signature": "lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)",
      "documentation": {
        "description": "The lars_path in the sufficient stats mode.\n\n    The optimization objective for the case method='lasso' is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    in the case of method='lar', the objective function is only known in\n    the form of an implicit equation (see discussion in [1]_).\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    Xy : ndarray of shape (n_features,)\n        `Xy = X.T @ y`.\n\n    Gram : ndarray of shape (n_features, n_features)\n        `Gram = X.T @ X`.\n\n    n_samples : int\n        Equivalent size of sample.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform, set to infinity for no limit.\n\n    alpha_min : float, default=0\n        Minimum correlation along the path. It corresponds to the\n        regularization parameter alpha parameter in the Lasso.\n\n    method : {'lar', 'lasso'}, default='lar'\n        Specifies the returned model. Select `'lar'` for Least Angle\n        Regression, ``'lasso'`` for the Lasso.\n\n    copy_X : bool, default=True\n        If `False`, `X` is overwritten.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the `tol` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_Gram : bool, default=True\n        If `False`, `Gram` is overwritten.\n\n    verbose : int, default=0\n        Controls output verbosity.\n\n    return_path : bool, default=True\n        If `return_path==True` returns the entire path, else returns only the\n        last point of the path.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0.\n        This option is only allowed with method 'lasso'. Note that the model\n        coefficients will not converge to the ordinary-least-squares solution\n        for small values of alpha. Only coefficients up to the smallest alpha\n        value (`alphas_[alphas_ > 0.].min()` when `fit_path=True`) reached by\n        the stepwise Lars-Lasso algorithm are typically in congruence with the\n        solution of the coordinate descent lasso_path function.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas + 1,)\n        Maximum of covariances (in absolute value) at each iteration.\n        `n_alphas` is either `max_iter`, `n_features` or the\n        number of nodes in the path with `alpha >= alpha_min`, whichever\n        is smaller.\n\n    active : ndarray of shape (n_alphas,)\n        Indices of active variables at the end of the path.\n\n    coefs : ndarray of shape (n_features, n_alphas + 1)\n        Coefficients along the path.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is set\n        to True.\n\n    See Also\n    --------\n    lars_path_gram : Compute LARS path.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_",
        "parameters": {
          "Xy": {
            "type": "ndarray of shape (n_features,)",
            "description": "`Xy = X.T @ y`."
          },
          "Gram": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": "`Gram = X.T @ X`."
          },
          "n_samples": {
            "type": "int",
            "description": ""
          },
          "Equivalent": {
            "type": "size of sample.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "of covariances (in absolute value) at each iteration.",
            "description": "`n_alphas` is either `max_iter`, `n_features` or the"
          },
          "alpha_min": {
            "type": "float, default=0",
            "description": ""
          },
          "Minimum": {
            "type": "correlation along the path. It corresponds to the",
            "description": ""
          },
          "regularization": {
            "type": "parameter alpha parameter in the Lasso.",
            "description": ""
          },
          "method": {
            "type": "{'lar', 'lasso'}, default='lar'",
            "description": ""
          },
          "Specifies": {
            "type": "the returned model. Select `'lar'` for Least Angle",
            "description": "Regression, ``'lasso'`` for the Lasso."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "`return_path==True` returns the entire path, else returns only the",
            "description": ""
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "The": {
            "type": "machine-precision regularization in the computation of the",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the `tol` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "stepwise Lars-Lasso algorithm are typically in congruence with the",
            "description": ""
          },
          "copy_Gram": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Controls": {
            "type": "output verbosity.",
            "description": ""
          },
          "return_path": {
            "type": "bool, default=True",
            "description": ""
          },
          "last": {
            "type": "point of the path.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "to return the number of iterations.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Restrict": {
            "type": "coefficients to be >= 0.",
            "description": ""
          },
          "This": {
            "type": "option is only allowed with method 'lasso'. Note that the model",
            "description": ""
          },
          "coefficients": {
            "type": "will not converge to the ordinary-least-squares solution",
            "description": ""
          },
          "for": {
            "type": "small values of alpha. Only coefficients up to the smallest alpha",
            "description": ""
          },
          "value": {
            "type": "`alphas_[alphas_ > 0.].min(",
            "description": "` when `fit_path=True`) reached by"
          },
          "solution": {
            "type": "of the coordinate descent lasso_path function.",
            "description": "Returns\n-------"
          },
          "alphas": {
            "type": "ndarray of shape (n_alphas + 1,)",
            "description": ""
          },
          "number": {
            "type": "of nodes in the path with `alpha >= alpha_min`, whichever",
            "description": ""
          },
          "is": {
            "type": "smaller.",
            "description": ""
          },
          "active": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "coefs": {
            "type": "ndarray of shape (n_features, n_alphas + 1)",
            "description": ""
          },
          "Coefficients": {
            "type": "along the path.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of iterations run. Returned only if `return_n_iter` is set",
            "description": ""
          },
          "to": {
            "type": "True.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path_gram": {
            "type": "Compute LARS path.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "Lars": {
            "type": "Least Angle Regression model a.k.a. LAR.",
            "description": ""
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso, using the LARS algorithm."
          },
          "LarsCV": {
            "type": "Cross",
            "description": "validated Least Angle Regression model.\nsklearn.decomposition.sparse_encode : Sparse coding.\nReferences\n----------\n.. [1] \"Least Angle Regression\", Efron et al."
          },
          "http": {
            "type": "//statweb.stanford.edu/~tibs/ftp/lars.pdf",
            "description": ".. [2] `Wikipedia entry on the Least-angle regression\n<https://en.wikipedia.org/wiki/Least-angle_regression>`_\n.. [3] `Wikipedia entry on the Lasso\n<https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\nExamples\n--------\n>>> from sklearn.linear_model import lars_path_gram\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
          },
          "array": {
            "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
            "description": ">>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.     ,  0.     ,  0.     ],\n[ 0.     ,  0.     ,  0.     ],\n[ 0.     ,  0.     ,  0.     ],\n[ 0.     , 46.96..., 97.99...],\n[ 0.     ,  0.     , 45.70...]])"
          }
        },
        "returns": "-------\n    alphas : ndarray of shape (n_alphas + 1,)\n        Maximum of covariances (in absolute value) at each iteration.\n        `n_alphas` is either `max_iter`, `n_features` or the\n        number of nodes in the path with `alpha >= alpha_min`, whichever\n        is smaller.\n\n    active : ndarray of shape (n_alphas,)\n        Indices of active variables at the end of the path.\n\n    coefs : ndarray of shape (n_features, n_alphas + 1)\n        Coefficients along the path.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is set\n        to True.\n\n    See Also\n    --------\n    lars_path_gram : Compute LARS path.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import lars_path_gram\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])",
        "raises": "",
        "see_also": "--------\n    lars_path_gram : Compute LARS path.\n    lasso_path : Compute Lasso path with coordinate descent.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Efron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import lars_path_gram\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import lars_path_gram\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n    array([[ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     ,  0.     ,  0.     ],\n           [ 0.     , 46.96..., 97.99...],\n           [ 0.     ,  0.     , 45.70...]])"
      }
    },
    {
      "name": "lasso_path",
      "signature": "lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)",
      "documentation": {
        "description": "Compute Lasso path with coordinate descent.\n\n    The Lasso optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "Training": {
            "type": "data. Pass directly as Fortran-contiguous data to avoid",
            "description": ""
          },
          "unnecessary": {
            "type": "memory duplication. If ``y`` is mono-output then ``X``",
            "description": ""
          },
          "can": {
            "type": "be sparse.",
            "description": ""
          },
          "y": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
          },
          "Target": {
            "type": "values.",
            "description": ""
          },
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of alphas along the regularization path.",
            "description": ""
          },
          "alphas": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "set to True, forces coefficients to be positive.",
            "description": "(Only allowed when ``y.ndim == 1``).\n**params : kwargs"
          },
          "precompute": {
            "type": "'auto', bool or array",
            "description": "like of shape             (n_features, n_features), default='auto'"
          },
          "Whether": {
            "type": "to return the number of iterations or not.",
            "description": ""
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "Xy": {
            "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
            "description": ""
          },
          "only": {
            "type": "when the Gram matrix is precomputed.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "coef_init": {
            "type": "array",
            "description": "like of shape (n_features, ), default=None"
          },
          "The": {
            "type": "number of iterations taken by the coordinate descent optimizer to",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Keyword": {
            "type": "arguments passed to the coordinate descent solver.",
            "description": "Returns\n-------"
          },
          "coefs": {
            "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
            "description": ""
          },
          "Coefficients": {
            "type": "along the path.",
            "description": ""
          },
          "dual_gaps": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "n_iters": {
            "type": "list of int",
            "description": ""
          },
          "reach": {
            "type": "the specified tolerance for each alpha.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using LARS",
            "description": "algorithm."
          },
          "Lasso": {
            "type": "The Lasso is a linear model that estimates sparse coefficients.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting along a regularization",
            "description": "path."
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso using the LARS algorithm.\nsklearn.decomposition.sparse_encode : Estimator that can be used to"
          },
          "transform": {
            "type": "signals into sparse linear combination of atoms from a fixed.",
            "description": "Notes\n-----"
          },
          "For": {
            "type": "an example, see",
            "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`."
          },
          "To": {
            "type": "avoid unnecessary memory duplication the X argument of the fit method",
            "description": ""
          },
          "should": {
            "type": "be directly passed as a Fortran-contiguous numpy array.",
            "description": ""
          },
          "Note": {
            "type": "that in certain cases, the Lars solver may be significantly",
            "description": ""
          },
          "faster": {
            "type": "to implement this functionality. In particular, linear",
            "description": ""
          },
          "interpolation": {
            "type": "can be used to retrieve model coefficients between the",
            "description": ""
          },
          "values": {
            "type": "output by lars_path",
            "description": "Examples\n--------"
          },
          "Comparing": {
            "type": "lasso_path and lars_path with interpolation:",
            "description": ">>> import numpy as np\n>>> from sklearn.linear_model import lasso_path\n>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n>>> y = np.array([1, 2, 3.1])\n>>> # Use lasso_path to compute a coefficient path\n>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n>>> print(coef_path)\n[[0.         0.         0.46874778]\n[0.2159048  0.4425765  0.23689075]]\n>>> # Now use lars_path and 1D linear interpolation to compute the\n>>> # same path\n>>> from sklearn.linear_model import lars_path\n>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n>>> from scipy import interpolate\n>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n...                                             coef_path_lars[:, ::-1])\n>>> print(coef_path_continuous([5., 1., .5]))\n[[0.         0.         0.46915237]\n[0.2159048  0.4425765  0.23668876]]"
          }
        },
        "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
        "notes": "that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
        "examples": "--------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]"
      }
    },
    {
      "name": "orthogonal_mp",
      "signature": "orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)",
      "documentation": {
        "description": "Orthogonal Matching Pursuit (OMP).\n\n    Solves n_targets Orthogonal Matching Pursuit problems.\n    An instance of the problem has the form:\n\n    When parametrized by the number of non-zero coefficients using\n    `n_nonzero_coefs`:\n    argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n\n    When parametrized by error using the parameter `tol`:\n    argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data. Columns are assumed to have unit norm.\n\n    y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n        Input targets.\n\n    n_nonzero_coefs : int, default=None\n        Desired number of non-zero entries in the solution. If None (by\n        default) this value is set to 10% of n_features.\n\n    tol : float, default=None\n        Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n\n    precompute : 'auto' or bool, default=False\n        Whether to perform precomputations. Improves performance when n_targets\n        or n_samples is very large.\n\n    copy_X : bool, default=True\n        Whether the design matrix X must be copied by the algorithm. A false\n        value is only helpful if X is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    return_path : bool, default=False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        (n_features, n_features) or (n_features, n_targets, n_features) and\n        iterating over the last axis generates coefficients in increasing order\n        of active features.\n\n    n_iters : array-like or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See Also\n    --------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model.\n    orthogonal_mp_gram : Solve OMP problems using Gram matrix and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Input": {
            "type": "targets.",
            "description": ""
          },
          "y": {
            "type": "ndarray of shape (n_samples,) or (n_samples, n_targets)",
            "description": ""
          },
          "n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "Desired": {
            "type": "number of non-zero entries in the solution. If None (by",
            "description": "default) this value is set to 10% of n_features."
          },
          "tol": {
            "type": "float, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "squared norm of the residual. If not None, overrides n_nonzero_coefs.",
            "description": ""
          },
          "precompute": {
            "type": "'auto' or bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": "Returns\n-------"
          },
          "or": {
            "type": "n_samples is very large.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "value": {
            "type": "is only helpful if X is already Fortran-ordered, otherwise a",
            "description": ""
          },
          "copy": {
            "type": "is made anyway.",
            "description": ""
          },
          "return_path": {
            "type": "bool, default=False",
            "description": ""
          },
          "forward": {
            "type": "path. Useful for cross-validation.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "coef": {
            "type": "ndarray of shape (n_features,) or (n_features, n_targets)",
            "description": ""
          },
          "Coefficients": {
            "type": "of the OMP solution. If `return_path=True`, this contains",
            "description": ""
          },
          "the": {
            "type": "whole coefficient path. In this case its shape is",
            "description": "(n_features, n_features) or (n_features, n_targets, n_features) and"
          },
          "iterating": {
            "type": "over the last axis generates coefficients in increasing order",
            "description": ""
          },
          "of": {
            "type": "active features.",
            "description": ""
          },
          "n_iters": {
            "type": "array",
            "description": "like or int"
          },
          "Number": {
            "type": "of active features across every target. Returned only if",
            "description": "`return_n_iter` is set to True."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "OrthogonalMatchingPursuit": {
            "type": "Orthogonal Matching Pursuit model.",
            "description": ""
          },
          "orthogonal_mp_gram": {
            "type": "Solve OMP problems using Gram matrix and the product X.T * y.",
            "description": ""
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using LARS algorithm.",
            "description": "sklearn.decomposition.sparse_encode : Sparse coding.\nNotes\n-----"
          },
          "Orthogonal": {
            "type": "matching pursuit was introduced in S. Mallat, Z. Zhang,",
            "description": ""
          },
          "Matching": {
            "type": "Pursuit Technical Report - CS Technion, April 2008.",
            "description": ""
          },
          "Signal": {
            "type": "Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.",
            "description": "(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)"
          },
          "This": {
            "type": "implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,",
            "description": "M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal"
          },
          "https": {
            "type": "//www.cs.technion.ac.il/~ronrubin/Publications/KSVD",
            "description": "OMP-v2.pdf\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.linear_model import orthogonal_mp\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> coef = orthogonal_mp(X, y)\n>>> coef.shape\n(100,)\n>>> X[:1,] @ coef"
          },
          "array": {
            "type": "[-78.68...]",
            "description": ""
          }
        },
        "returns": "-------\n    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        (n_features, n_features) or (n_features, n_targets, n_features) and\n        iterating over the last axis generates coefficients in increasing order\n        of active features.\n\n    n_iters : array-like or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See Also\n    --------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model.\n    orthogonal_mp_gram : Solve OMP problems using Gram matrix and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp(X, y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "raises": "",
        "see_also": "--------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model.\n    orthogonal_mp_gram : Solve OMP problems using Gram matrix and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp(X, y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "notes": "-----\n    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp(X, y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "examples": "--------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp(X, y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])"
      }
    },
    {
      "name": "orthogonal_mp_gram",
      "signature": "orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)",
      "documentation": {
        "description": "Gram Orthogonal Matching Pursuit (OMP).\n\n    Solves n_targets Orthogonal Matching Pursuit problems using only\n    the Gram matrix X.T * X and the product X.T * y.\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    Gram : array-like of shape (n_features, n_features)\n        Gram matrix of the input data: `X.T * X`.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets)\n        Input targets multiplied by `X`: `X.T * y`.\n\n    n_nonzero_coefs : int, default=None\n        Desired number of non-zero entries in the solution. If `None` (by\n        default) this value is set to 10% of n_features.\n\n    tol : float, default=None\n        Maximum squared norm of the residual. If not `None`,\n        overrides `n_nonzero_coefs`.\n\n    norms_squared : array-like of shape (n_targets,), default=None\n        Squared L2 norms of the lines of `y`. Required if `tol` is not None.\n\n    copy_Gram : bool, default=True\n        Whether the gram matrix must be copied by the algorithm. A `False`\n        value is only helpful if it is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    copy_Xy : bool, default=True\n        Whether the covariance vector `Xy` must be copied by the algorithm.\n        If `False`, it may be overwritten.\n\n    return_path : bool, default=False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        `(n_features, n_features)` or `(n_features, n_targets, n_features)` and\n        iterating over the last axis yields coefficients in increasing order\n        of active features.\n\n    n_iters : list or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See Also\n    --------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    lars_path : Compute Least Angle Regression or Lasso path using\n        LARS algorithm.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf",
        "parameters": {
          "Gram": {
            "type": "matrix of the input data: `X.T * X`.",
            "description": ""
          },
          "Xy": {
            "type": "array",
            "description": "like of shape (n_features,) or (n_features, n_targets)"
          },
          "Input": {
            "type": "targets multiplied by `X`: `X.T * y`.",
            "description": ""
          },
          "n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "Desired": {
            "type": "number of non-zero entries in the solution. If `None` (by",
            "description": "default) this value is set to 10% of n_features."
          },
          "tol": {
            "type": "float, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "squared norm of the residual. If not `None`,",
            "description": ""
          },
          "overrides": {
            "type": "`n_nonzero_coefs`.",
            "description": ""
          },
          "norms_squared": {
            "type": "array",
            "description": "like of shape (n_targets,), default=None"
          },
          "Squared": {
            "type": "L2 norms of the lines of `y`. Required if `tol` is not None.",
            "description": ""
          },
          "copy_Gram": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": "Returns\n-------"
          },
          "value": {
            "type": "is only helpful if it is already Fortran-ordered, otherwise a",
            "description": ""
          },
          "copy": {
            "type": "is made anyway.",
            "description": ""
          },
          "copy_Xy": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "`False`, it may be overwritten.",
            "description": ""
          },
          "return_path": {
            "type": "bool, default=False",
            "description": ""
          },
          "forward": {
            "type": "path. Useful for cross-validation.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "coef": {
            "type": "ndarray of shape (n_features,) or (n_features, n_targets)",
            "description": ""
          },
          "Coefficients": {
            "type": "of the OMP solution. If `return_path=True`, this contains",
            "description": ""
          },
          "the": {
            "type": "whole coefficient path. In this case its shape is",
            "description": "`(n_features, n_features)` or `(n_features, n_targets, n_features)` and"
          },
          "iterating": {
            "type": "over the last axis yields coefficients in increasing order",
            "description": ""
          },
          "of": {
            "type": "active features.",
            "description": ""
          },
          "n_iters": {
            "type": "list or int",
            "description": ""
          },
          "Number": {
            "type": "of active features across every target. Returned only if",
            "description": "`return_n_iter` is set to True."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "OrthogonalMatchingPursuit": {
            "type": "Orthogonal Matching Pursuit model (OMP).",
            "description": ""
          },
          "orthogonal_mp": {
            "type": "Solves n_targets Orthogonal Matching Pursuit problems.",
            "description": ""
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using",
            "description": ""
          },
          "LARS": {
            "type": "algorithm.",
            "description": "sklearn.decomposition.sparse_encode : Generic sparse coding."
          },
          "Each": {
            "type": "column of the result is the solution to a Lasso problem.",
            "description": "Notes\n-----"
          },
          "Orthogonal": {
            "type": "matching pursuit was introduced in G. Mallat, Z. Zhang,",
            "description": ""
          },
          "Matching": {
            "type": "Pursuit Technical Report - CS Technion, April 2008.",
            "description": ""
          },
          "Signal": {
            "type": "Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.",
            "description": "(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)"
          },
          "This": {
            "type": "implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,",
            "description": "M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal"
          },
          "https": {
            "type": "//www.cs.technion.ac.il/~ronrubin/Publications/KSVD",
            "description": "OMP-v2.pdf\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.linear_model import orthogonal_mp_gram\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n>>> coef.shape\n(100,)\n>>> X[:1,] @ coef"
          },
          "array": {
            "type": "[-78.68...]",
            "description": ""
          }
        },
        "returns": "-------\n    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        `(n_features, n_features)` or `(n_features, n_targets, n_features)` and\n        iterating over the last axis yields coefficients in increasing order\n        of active features.\n\n    n_iters : list or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See Also\n    --------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    lars_path : Compute Least Angle Regression or Lasso path using\n        LARS algorithm.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp_gram\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "raises": "",
        "see_also": "--------\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    lars_path : Compute Least Angle Regression or Lasso path using\n        LARS algorithm.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp_gram\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "notes": "-----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp_gram\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])",
        "examples": "--------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import orthogonal_mp_gram\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n    >>> coef.shape\n    (100,)\n    >>> X[:1,] @ coef\n    array([-78.68...])"
      }
    },
    {
      "name": "ridge_regression",
      "signature": "ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.0001, verbose=0, positive=False, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)",
      "documentation": {
        "description": "Solve the ridge equation by the method of normal equations.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n        Training data.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    alpha : float or array-like of shape (n_targets,)\n        Constant that multiplies the L2 term, controlling regularization\n        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n\n        When `alpha = 0`, the objective is equivalent to ordinary least\n        squares, solved by the :class:`LinearRegression` object. For numerical\n        reasons, using `alpha = 0` with the `Ridge` object is not advised.\n        Instead, you should use the :class:`LinearRegression` object.\n\n        If an array is passed, penalties are assumed to be specific to the\n        targets. Hence they must correspond in number.\n\n    sample_weight : float or array-like of shape (n_samples,), default=None\n        Individual weights for each sample. If given a float, every sample\n        will have the same weight. If sample_weight is not None and\n        solver='auto', the solver will be set to 'cholesky'.\n\n        .. versionadded:: 0.17\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. It is the most stable solver, in particular more stable\n          for singular matrices than 'cholesky' at the cost of being slower.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution via a Cholesky decomposition of\n          dot(X.T, X)\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its improved, unbiased version named SAGA. Both methods also use an\n          iterative procedure, and are often faster than other solvers when\n          both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n        - 'lbfgs' uses L-BFGS-B algorithm implemented in\n          `scipy.optimize.minimize`. It can be used only when `positive`\n          is True.\n\n        All solvers except 'svd' support both dense and sparse data. However, only\n        'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n        `fit_intercept` is True.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    max_iter : int, default=None\n        Maximum number of iterations for conjugate gradient solver.\n        For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n        by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n        1000. For 'lbfgs' solver, the default value is 15000.\n\n    tol : float, default=1e-4\n        Precision of the solution. Note that `tol` has no effect for solvers 'svd' and\n        'cholesky'.\n\n        .. versionchanged:: 1.2\n           Default value changed from 1e-3 to 1e-4 for consistency with other linear\n           models.\n\n    verbose : int, default=0\n        Verbosity level. Setting verbose > 0 will display additional\n        information depending on the solver used.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n        Only 'lbfgs' solver is supported in this case.\n\n    random_state : int, RandomState instance, default=None\n        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n        See :term:`Glossary <random_state>` for details.\n\n    return_n_iter : bool, default=False\n        If True, the method also returns `n_iter`, the actual number of\n        iteration performed by the solver.\n\n        .. versionadded:: 0.17\n\n    return_intercept : bool, default=False\n        If True and if X is sparse, the method also returns the intercept,\n        and the solver is automatically changed to 'sag'. This is only a\n        temporary fix for fitting the intercept with sparse data. For dense\n        data, use sklearn.linear_model._preprocess_data before your regression.\n\n        .. versionadded:: 0.17\n\n    check_input : bool, default=True\n        If False, the input arrays X and y will not be checked.\n\n        .. versionadded:: 0.21\n\n    Returns\n    -------\n    coef : ndarray of shape (n_features,) or (n_targets, n_features)\n        Weight vector(s).\n\n    n_iter : int, optional\n        The actual number of iteration performed by the solver.\n        Only returned if `return_n_iter` is True.\n\n    intercept : float or ndarray of shape (n_targets,)\n        The intercept of the model. Only returned if `return_intercept`\n        is True and if X is a scipy sparse array.\n\n    Notes\n    -----\n    This function won't compute the intercept.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.",
        "parameters": {
          "X": {
            "type": "{array",
            "description": "like, sparse matrix, LinearOperator} of shape         (n_samples, n_features)"
          },
          "Training": {
            "type": "data.",
            "description": ""
          },
          "y": {
            "type": "array",
            "description": "like of shape (n_samples,) or (n_samples, n_targets)"
          },
          "Target": {
            "type": "values.",
            "description": ""
          },
          "alpha": {
            "type": "float or array",
            "description": "like of shape (n_targets,)"
          },
          "Constant": {
            "type": "that multiplies the L2 term, controlling regularization",
            "description": "strength. `alpha` must be a non-negative float i.e. in `[0, inf)`."
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "If": {
            "type": "False, the input arrays X and y will not be checked.",
            "description": ".. versionadded:: 0.21\nReturns\n-------"
          },
          "sample_weight": {
            "type": "float or array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Individual": {
            "type": "weights for each sample. If given a float, every sample",
            "description": ""
          },
          "will": {
            "type": "have the same weight. If sample_weight is not None and",
            "description": "solver='auto', the solver will be set to 'cholesky'.\n.. versionadded:: 0.17"
          },
          "solver": {
            "type": "{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'",
            "description": ""
          },
          "Solver": {
            "type": "to use in the computational routines:",
            "description": "- 'auto' chooses the solver automatically based on the type of data.\n- 'svd' uses a Singular Value Decomposition of X to compute the Ridge\ncoefficients. It is the most stable solver, in particular more stable"
          },
          "for": {
            "type": "singular matrices than 'cholesky' at the cost of being slower.",
            "description": "- 'cholesky' uses the standard scipy.linalg.solve function to"
          },
          "obtain": {
            "type": "a closed-form solution via a Cholesky decomposition of",
            "description": ""
          },
          "dot": {
            "type": "X.T, X",
            "description": "- 'sparse_cg' uses the conjugate gradient solver as found in\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is"
          },
          "more": {
            "type": "appropriate than 'cholesky' for large-scale data",
            "description": "(possibility to set `tol` and `max_iter`).\n- 'lsqr' uses the dedicated regularized least-squares routine\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\nprocedure.\n- 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses"
          },
          "its": {
            "type": "improved, unbiased version named SAGA. Both methods also use an",
            "description": ""
          },
          "iterative": {
            "type": "procedure, and are often faster than other solvers when",
            "description": ""
          },
          "both": {
            "type": "n_samples and n_features are large. Note that 'sag' and",
            "description": "'saga' fast convergence is only guaranteed on features with"
          },
          "approximately": {
            "type": "the same scale. You can preprocess the data with a",
            "description": ""
          },
          "scaler": {
            "type": "from sklearn.preprocessing.",
            "description": "- 'lbfgs' uses L-BFGS-B algorithm implemented in\n`scipy.optimize.minimize`. It can be used only when `positive`"
          },
          "is": {
            "type": "True and if X is a scipy sparse array.",
            "description": "Notes\n-----"
          },
          "All": {
            "type": "solvers except 'svd' support both dense and sparse data. However, only",
            "description": "'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n`fit_intercept` is True.\n.. versionadded:: 0.17"
          },
          "Stochastic": {
            "type": "Average Gradient descent solver.",
            "description": ".. versionadded:: 0.19"
          },
          "SAGA": {
            "type": "solver.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations for conjugate gradient solver.",
            "description": ""
          },
          "For": {
            "type": "the 'sparse_cg' and 'lsqr' solvers, the default value is determined",
            "description": ""
          },
          "by": {
            "type": "scipy.sparse.linalg. For 'sag' and saga solver, the default value is",
            "description": "1000. For 'lbfgs' solver, the default value is 15000."
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Precision": {
            "type": "of the solution. Note that `tol` has no effect for solvers 'svd' and",
            "description": "'cholesky'.\n.. versionchanged:: 1.2"
          },
          "Default": {
            "type": "value changed from 1e-3 to 1e-4 for consistency with other linear",
            "description": "models."
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Verbosity": {
            "type": "level. Setting verbose > 0 will display additional",
            "description": ""
          },
          "information": {
            "type": "depending on the solver used.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Only": {
            "type": "returned if `return_n_iter` is True.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "when ``solver`` == 'sag' or 'saga' to shuffle the data.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>` for details.",
            "description": ""
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "iteration": {
            "type": "performed by the solver.",
            "description": ".. versionadded:: 0.17"
          },
          "return_intercept": {
            "type": "bool, default=False",
            "description": ""
          },
          "and": {
            "type": "the solver is automatically changed to 'sag'. This is only a",
            "description": ""
          },
          "temporary": {
            "type": "fix for fitting the intercept with sparse data. For dense",
            "description": "data, use sklearn.linear_model._preprocess_data before your regression.\n.. versionadded:: 0.17"
          },
          "check_input": {
            "type": "bool, default=True",
            "description": ""
          },
          "coef": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Weight": {
            "type": "vector(s).",
            "description": ""
          },
          "n_iter": {
            "type": "int, optional",
            "description": ""
          },
          "The": {
            "type": "intercept of the model. Only returned if `return_intercept`",
            "description": ""
          },
          "intercept": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "This": {
            "type": "function won't compute the intercept.",
            "description": ""
          },
          "Regularization": {
            "type": "improves the conditioning of the problem and",
            "description": ""
          },
          "reduces": {
            "type": "the variance of the estimates. Larger values specify stronger",
            "description": "regularization. Alpha corresponds to ``1 / (2C)`` in other linear"
          },
          "models": {
            "type": "such as :class:`~sklearn.linear_model.LogisticRegression` or",
            "description": ":class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are"
          },
          "assumed": {
            "type": "to be specific to the targets. Hence they must correspond in",
            "description": "number.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.linear_model import ridge_regression\n>>> rng = np.random.RandomState(0)\n>>> X = rng.randn(100, 4)\n>>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)\n>>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True)\n>>> list(coef)\n[np.float64(1.9...), np.float64(-1.0...), np.float64(-0.0...), np.float64(-0.0...)]\n>>> intercept\nnp.float64(-0.0...)"
          }
        },
        "returns": "-------\n    coef : ndarray of shape (n_features,) or (n_targets, n_features)\n        Weight vector(s).\n\n    n_iter : int, optional\n        The actual number of iteration performed by the solver.\n        Only returned if `return_n_iter` is True.\n\n    intercept : float or ndarray of shape (n_targets,)\n        The intercept of the model. Only returned if `return_intercept`\n        is True and if X is a scipy sparse array.\n\n    Notes\n    -----\n    This function won't compute the intercept.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import ridge_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.randn(100, 4)\n    >>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)\n    >>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True)\n    >>> list(coef)\n    [np.float64(1.9...), np.float64(-1.0...), np.float64(-0.0...), np.float64(-0.0...)]\n    >>> intercept\n    np.float64(-0.0...)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    This function won't compute the intercept.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import ridge_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.randn(100, 4)\n    >>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)\n    >>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True)\n    >>> list(coef)\n    [np.float64(1.9...), np.float64(-1.0...), np.float64(-0.0...), np.float64(-0.0...)]\n    >>> intercept\n    np.float64(-0.0...)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.linear_model import ridge_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.randn(100, 4)\n    >>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)\n    >>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True)\n    >>> list(coef)\n    [np.float64(1.9...), np.float64(-1.0...), np.float64(-0.0...), np.float64(-0.0...)]\n    >>> intercept\n    np.float64(-0.0...)"
      }
    }
  ],
  "classes": [
    {
      "name": "ARDRegression",
      "documentation": {
        "description": "Bayesian ARD regression.\n\n    Fit the weights of a regression model, using an ARD prior. The weights of\n    the regression model are assumed to be in Gaussian distributions.\n    Also estimate the parameters lambda (precisions of the distributions of the\n    weights) and alpha (precision of the distribution of the noise).\n    The estimation is done by an iterative procedures (Evidence Maximization)\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    max_iter : int, default=300\n        Maximum number of iterations.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    compute_score : bool, default=False\n        If True, compute the objective function at each step of the model.\n\n    threshold_lambda : float, default=10 000\n        Threshold for removing (pruning) weights with high precision from\n        the computation.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    alpha_ : float\n       estimated precision of the noise.\n\n    lambda_ : array-like of shape (n_features,)\n       estimated precisions of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        estimated variance-covariance matrix of the weights\n\n    scores_ : float\n        if computed, value of the objective function (to be maximized)\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n        .. versionadded:: 1.3\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    X_offset_ : float\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : float\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    BayesianRidge : Bayesian ridge regression.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.",
        "parameters": {
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations.",
            "description": ".. versionchanged:: 1.3"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Stop": {
            "type": "the algorithm if w has converged.",
            "description": ""
          },
          "alpha_1": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : shape parameter for the Gamma distribution prior"
          },
          "over": {
            "type": "the lambda parameter.",
            "description": ""
          },
          "alpha_2": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : inverse scale parameter (rate parameter) for the"
          },
          "Gamma": {
            "type": "distribution prior over the lambda parameter.",
            "description": ""
          },
          "lambda_1": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : shape parameter for the Gamma distribution prior"
          },
          "lambda_2": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : inverse scale parameter (rate parameter) for the"
          },
          "compute_score": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "`fit_intercept=True`, offset subtracted for centering data to a",
            "description": ""
          },
          "threshold_lambda": {
            "type": "float, default=10 000",
            "description": ""
          },
          "Threshold": {
            "type": "for removing (pruning) weights with high precision from",
            "description": ""
          },
          "the": {
            "type": "computation.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Verbose": {
            "type": "mode when fitting the model.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "Coefficients": {
            "type": "of the regression model (mean of distribution)",
            "description": ""
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "estimated": {
            "type": "variance-covariance matrix of the weights",
            "description": ""
          },
          "lambda_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "sigma_": {
            "type": "array",
            "description": "like of shape (n_features, n_features)"
          },
          "scores_": {
            "type": "float",
            "description": ""
          },
          "if": {
            "type": "computed, value of the objective function (to be maximized)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations to reach the stopping criterion.",
            "description": ".. versionadded:: 1.3"
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "``fit_intercept = False``."
          },
          "X_offset_": {
            "type": "float",
            "description": ""
          },
          "zero": {
            "type": "mean. Set to np.zeros(n_features) otherwise.",
            "description": ""
          },
          "X_scale_": {
            "type": "float",
            "description": ""
          },
          "Set": {
            "type": "to np.ones(n_features).",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "BayesianRidge": {
            "type": "Bayesian ridge regression.",
            "description": "Notes\n-----"
          },
          "For": {
            "type": "an example, see :ref:`examples/linear_model/plot_ard.py",
            "description": "<sphx_glr_auto_examples_linear_model_plot_ard.py>`.\nReferences\n----------\nD. J. C. MacKay, Bayesian nonlinear modeling for the prediction\ncompetition, ASHRAE Transactions, 1994.\nR. Salakhutdinov, Lecture notes on Statistical Machine Learning,"
          },
          "http": {
            "type": "//www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15",
            "description": ""
          },
          "Their": {
            "type": "alpha is our ``self.lambda_``",
            "description": ""
          },
          "ARD": {
            "type": "is a little different than the slide: only dimensions/features for",
            "description": ""
          },
          "which": {
            "type": "``self.lambda_ < self.threshold_lambda`` are kept and the rest are",
            "description": "discarded.\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.ARDRegression()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
          },
          "ARDRegression": {
            "type": "",
            "description": ">>> clf.predict([[1, 1]])"
          },
          "array": {
            "type": "[1.]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    BayesianRidge : Bayesian ridge regression.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])",
        "notes": "-----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the model according to the given training data and parameters.\n\n        Iterative procedure to maximize the evidence\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n        y : array-like of shape (n_samples,)\n            Target values (integers). Will be cast to X's dtype if necessary.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values (integers). Will be cast to X's dtype if necessary.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, return_std=False)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        In addition to the mean of the predictive distribution, also its\n        standard deviation can be returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        return_std : bool, default=False\n            Whether to return the standard deviation of posterior prediction.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples."
              },
              "return_std": {
                "type": "bool, default=False",
                "description": ""
              },
              "Whether": {
                "type": "to return the standard deviation of posterior prediction.",
                "description": "Returns\n-------"
              },
              "y_mean": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Mean": {
                "type": "of predictive distribution of query points.",
                "description": ""
              },
              "y_std": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Standard": {
                "type": "deviation of predictive distribution of query points.",
                "description": ""
              }
            },
            "returns": "-------\n        y_mean : array-like of shape (n_samples,)\n            Mean of predictive distribution of query points.\n\n        y_std : array-like of shape (n_samples,)\n            Standard deviation of predictive distribution of query points.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_predict_request",
          "signature": "set_predict_request(self: sklearn.linear_model._bayes.ARDRegression, *, return_std: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.ARDRegression",
          "documentation": {
            "description": "Request metadata passed to the ``predict`` method.",
            "parameters": {
              "return_std": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``return_std`` parameter in ``predict``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        return_std : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``return_std`` parameter in ``predict``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._bayes.ARDRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.ARDRegression",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BayesianRidge",
      "documentation": {
        "description": "Bayesian ridge regression.\n\n    Fit a Bayesian ridge model. See the Notes section for details on this\n    implementation and the optimization of the regularization parameters\n    lambda (precision of the weights) and alpha (precision of the noise).\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n    For an intuitive visualization of how the sinusoid is approximated by\n    a polynomial using different pairs of initial values, see\n    :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.\n\n    Parameters\n    ----------\n    max_iter : int, default=300\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    alpha_init : float, default=None\n        Initial value for alpha (precision of the noise).\n        If not set, alpha_init is 1/Var(y).\n\n        .. versionadded:: 0.22\n\n    lambda_init : float, default=None\n        Initial value for lambda (precision of the weights).\n        If not set, lambda_init is 1.\n\n        .. versionadded:: 0.22\n\n    compute_score : bool, default=False\n        If True, compute the log marginal likelihood at each iteration of the\n        optimization.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model.\n        The intercept is not treated as a probabilistic parameter\n        and thus has no associated variance. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        `fit_intercept = False`.\n\n    alpha_ : float\n       Estimated precision of the noise.\n\n    lambda_ : float\n       Estimated precision of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        Estimated variance-covariance matrix of the weights\n\n    scores_ : array-like of shape (n_iter_+1,)\n        If computed_score is True, value of the log marginal likelihood (to be\n        maximized) at each iteration of the optimization. The array starts\n        with the value of the log marginal likelihood obtained for the initial\n        values of alpha and lambda and ends with the value obtained for the\n        estimated alpha and lambda.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    X_offset_ : ndarray of shape (n_features,)\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : ndarray of shape (n_features,)\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ARDRegression : Bayesian ARD regression.\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.",
        "parameters": {
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations over the complete dataset before",
            "description": ""
          },
          "stopping": {
            "type": "independently of any early stopping criterion.",
            "description": ".. versionchanged:: 1.3"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Stop": {
            "type": "the algorithm if w has converged.",
            "description": ""
          },
          "alpha_1": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : shape parameter for the Gamma distribution prior"
          },
          "over": {
            "type": "the lambda parameter.",
            "description": ""
          },
          "alpha_2": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : inverse scale parameter (rate parameter) for the"
          },
          "Gamma": {
            "type": "distribution prior over the lambda parameter.",
            "description": ""
          },
          "lambda_1": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : shape parameter for the Gamma distribution prior"
          },
          "lambda_2": {
            "type": "float, default=1e",
            "description": "6\nHyper-parameter : inverse scale parameter (rate parameter) for the"
          },
          "alpha_init": {
            "type": "float, default=None",
            "description": ""
          },
          "Initial": {
            "type": "value for lambda (precision of the weights).",
            "description": ""
          },
          "If": {
            "type": "`fit_intercept=True`, offset subtracted for centering data to a",
            "description": ""
          },
          "lambda_init": {
            "type": "float, default=None",
            "description": ""
          },
          "compute_score": {
            "type": "bool, default=False",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model.",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations to reach the stopping criterion.",
            "description": ""
          },
          "and": {
            "type": "thus has no associated variance. If set",
            "description": ""
          },
          "to": {
            "type": "False, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Verbose": {
            "type": "mode when fitting the model.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "Coefficients": {
            "type": "of the regression model (mean of distribution)",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "`fit_intercept = False`."
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "Estimated": {
            "type": "variance-covariance matrix of the weights",
            "description": ""
          },
          "lambda_": {
            "type": "float",
            "description": ""
          },
          "sigma_": {
            "type": "array",
            "description": "like of shape (n_features, n_features)"
          },
          "scores_": {
            "type": "array",
            "description": "like of shape (n_iter_+1,)"
          },
          "with": {
            "type": "the value of the log marginal likelihood obtained for the initial",
            "description": ""
          },
          "values": {
            "type": "of alpha and lambda and ends with the value obtained for the",
            "description": ""
          },
          "estimated": {
            "type": "alpha and lambda.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "X_offset_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "zero": {
            "type": "mean. Set to np.zeros(n_features) otherwise.",
            "description": ""
          },
          "X_scale_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Set": {
            "type": "to np.ones(n_features).",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "ARDRegression": {
            "type": "Bayesian ARD regression.",
            "description": "Notes\n-----"
          },
          "There": {
            "type": "exist several strategies to perform Bayesian ridge regression. This",
            "description": ""
          },
          "implementation": {
            "type": "is based on the algorithm described in Appendix A of",
            "description": "(Tipping, 2001) where updates of the regularization parameters are done as"
          },
          "suggested": {
            "type": "in (MacKay, 1992). Note that according to A New",
            "description": ""
          },
          "View": {
            "type": "of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these",
            "description": ""
          },
          "update": {
            "type": "rules do not guarantee that the marginal likelihood is increasing",
            "description": ""
          },
          "between": {
            "type": "two consecutive iterations of the optimization.",
            "description": "References\n----------\nD. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\nVol. 4, No. 3, 1992.\nM. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,"
          },
          "Journal": {
            "type": "of Machine Learning Research, Vol. 1, 2001.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.BayesianRidge()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
          },
          "BayesianRidge": {
            "type": "",
            "description": ">>> clf.predict([[1, 1]])"
          },
          "array": {
            "type": "[1.]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    ARDRegression : Bayesian ARD regression.\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])",
        "notes": "-----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training data.\n        y : ndarray of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : ndarray of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.20\n               parameter *sample_weight* support to BayesianRidge.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": ""
              },
              "sample_weight": {
                "type": "ndarray of shape (n_samples,), default=None",
                "description": ""
              },
              "Individual": {
                "type": "weights for each sample.",
                "description": ".. versionadded:: 0.20"
              },
              "parameter": {
                "type": "*sample_weight* support to BayesianRidge.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, return_std=False)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        In addition to the mean of the predictive distribution, also its\n        standard deviation can be returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        return_std : bool, default=False\n            Whether to return the standard deviation of posterior prediction.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples."
              },
              "return_std": {
                "type": "bool, default=False",
                "description": ""
              },
              "Whether": {
                "type": "to return the standard deviation of posterior prediction.",
                "description": "Returns\n-------"
              },
              "y_mean": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Mean": {
                "type": "of predictive distribution of query points.",
                "description": ""
              },
              "y_std": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Standard": {
                "type": "deviation of predictive distribution of query points.",
                "description": ""
              }
            },
            "returns": "-------\n        y_mean : array-like of shape (n_samples,)\n            Mean of predictive distribution of query points.\n\n        y_std : array-like of shape (n_samples,)\n            Standard deviation of predictive distribution of query points.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._bayes.BayesianRidge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_predict_request",
          "signature": "set_predict_request(self: sklearn.linear_model._bayes.BayesianRidge, *, return_std: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge",
          "documentation": {
            "description": "Request metadata passed to the ``predict`` method.",
            "parameters": {
              "return_std": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``return_std`` parameter in ``predict``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        return_std : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``return_std`` parameter in ``predict``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._bayes.BayesianRidge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ElasticNet",
      "documentation": {
        "description": "Linear regression with combined L1 and L2 priors as regularizer.\n\n    Minimizes the objective function::\n\n            1 / (2 * n_samples) * ||y - Xw||^2_2\n            + alpha * l1_ratio * ||w||_1\n            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n            a * ||w||_1 + 0.5 * b * ||w||_2^2\n\n    where::\n\n            alpha = a + b and l1_ratio = a / (a + b)\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package while\n    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n    = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n    unless you supply your own sequence of alpha.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the penalty terms. Defaults to 1.0.\n        See the notes for the exact mathematical meaning of this\n        parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n        solved by the :class:`LinearRegression` object. For numerical\n        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n        Given this, you should use the :class:`LinearRegression` object.\n\n    l1_ratio : float, default=0.5\n        The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n        is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n        combination of L1 and L2.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If ``False``, the\n        data is assumed to be already centered.\n\n    precompute : bool or array-like of shape (n_features, n_features),                 default=False\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. The Gram matrix can also be passed as argument.\n        For sparse input this option is always ``False`` to preserve sparsity.\n        Check :ref:`an example on how to use a precomputed Gram Matrix in ElasticNet\n        <sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py>`\n        for details.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``, see Notes below.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the cost function formula).\n\n    sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n        Sparse representation of the `coef_`.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : list of int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    dual_gap_ : float or ndarray of shape (n_targets,)\n        Given param alpha, the dual gaps at the end of the optimization,\n        same shape as each observation of y.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    SGDRegressor : Implements elastic net regression with incremental training.\n    SGDClassifier : Implements logistic regression with elastic net penalty\n        (``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).\n\n    Notes\n    -----\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\text{samples}}`.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the penalty terms. Defaults to 1.0.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "solved": {
            "type": "by the :class:`LinearRegression` object. For numerical",
            "description": "reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised."
          },
          "Given": {
            "type": "param alpha, the dual gaps at the end of the optimization,",
            "description": ""
          },
          "l1_ratio": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "precise stopping criteria based on `tol` are the following: First, check that",
            "description": ""
          },
          "is": {
            "type": "smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.",
            "description": ""
          },
          "combination": {
            "type": "of L1 and L2.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. The Gram matrix can also be passed as argument."
          },
          "data": {
            "type": "is assumed to be already centered.",
            "description": ""
          },
          "precompute": {
            "type": "bool or array",
            "description": "like of shape (n_features, n_features),                 default=False"
          },
          "For": {
            "type": "sparse input this option is always ``False`` to preserve sparsity.",
            "description": ""
          },
          "Check": {
            "type": "ref:`an example on how to use a precomputed Gram Matrix in ElasticNet",
            "description": "<sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py>`"
          },
          "for": {
            "type": "details.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "so, then additionally check whether the dual gap is smaller than `tol` times",
            "description": ":math:`||y||_2^2 / n_{\text{samples}}`.\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNet\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNet(random_state=0)\n>>> regr.fit(X, y)"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``, see Notes below.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the cost function formula).",
            "description": ""
          },
          "sparse_coef_": {
            "type": "sparse matrix of shape (n_features,) or             (n_targets, n_features)",
            "description": ""
          },
          "Sparse": {
            "type": "representation of the `coef_`.",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "list of int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "the": {
            "type": "specified tolerance.",
            "description": ""
          },
          "dual_gap_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "same": {
            "type": "shape as each observation of y.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "ElasticNetCV": {
            "type": "Elastic net model with best model selection by",
            "description": "cross-validation."
          },
          "SGDRegressor": {
            "type": "Implements elastic net regression with incremental training.",
            "description": ""
          },
          "SGDClassifier": {
            "type": "Implements logistic regression with elastic net penalty",
            "description": "(``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).\nNotes\n-----"
          },
          "To": {
            "type": "avoid unnecessary memory duplication the X argument of the fit method",
            "description": ""
          },
          "should": {
            "type": "be directly passed as a Fortran-contiguous numpy array.",
            "description": ""
          },
          "that": {
            "type": "maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`",
            "description": ""
          },
          "ElasticNet": {
            "type": "random_state=0",
            "description": ">>> print(regr.coef_)\n[18.83816048 64.55968825]\n>>> print(regr.intercept_)\n1.451...\n>>> print(regr.predict([[0, 0]]))\n[1.451...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    SGDRegressor : Implements elastic net regression with incremental training.\n    SGDClassifier : Implements logistic regression with elastic net penalty\n        (``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).\n\n    Notes\n    -----\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\text{samples}}`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNet\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNet(random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNet(random_state=0)\n    >>> print(regr.coef_)\n    [18.83816048 64.55968825]\n    >>> print(regr.intercept_)\n    1.451...\n    >>> print(regr.predict([[0, 0]]))\n    [1.451...]",
        "notes": "-----\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\text{samples}}`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNet\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNet(random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNet(random_state=0)\n    >>> print(regr.coef_)\n    [18.83816048 64.55968825]\n    >>> print(regr.intercept_)\n    1.451...\n    >>> print(regr.predict([[0, 0]]))\n    [1.451...]",
        "examples": "--------\n    >>> from sklearn.linear_model import ElasticNet\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNet(random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNet(random_state=0)\n    >>> print(regr.coef_)\n    [18.83816048 64.55968825]\n    >>> print(regr.intercept_)\n    1.451...\n    >>> print(regr.predict([[0, 0]]))\n    [1.451...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, check_input=True)",
          "documentation": {
            "description": "Fit model with coordinate descent.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix, sparse array} of (n_samples, n_features)\n            Data.",
            "parameters": {
              "X": {
                "type": "{ndarray, sparse matrix, sparse array} of (n_samples, n_features)",
                "description": "Data."
              },
              "Note": {
                "type": "that large sparse matrices and arrays requiring `int64`",
                "description": ""
              },
              "indices": {
                "type": "are not accepted.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_targets)",
                "description": "Target. Will be cast to X's dtype if necessary."
              },
              "sample_weight": {
                "type": "float or array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights. Internally, the `sample_weight` vector will be",
                "description": ""
              },
              "rescaled": {
                "type": "to sum to `n_samples`.",
                "description": ".. versionadded:: 0.23"
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Allow": {
                "type": "to bypass several input checking.",
                "description": "Don't use this parameter unless you know what you do.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": "Notes\n-----"
              },
              "Coordinate": {
                "type": "descent is an algorithm that considers each column of",
                "description": ""
              },
              "data": {
                "type": "at a time hence it will automatically convert the X input",
                "description": ""
              },
              "as": {
                "type": "a Fortran-contiguous numpy array if necessary.",
                "description": ""
              },
              "To": {
                "type": "avoid memory re-allocation it is advised to allocate the",
                "description": ""
              },
              "initial": {
                "type": "data in memory directly using that format.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "raises": "",
            "see_also": "",
            "notes": "that large sparse matrices and arrays requiring `int64`\n            indices are not accepted.\n\n        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Target. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or array-like of shape (n_samples,), default=None\n            Sample weights. Internally, the `sample_weight` vector will be\n            rescaled to sum to `n_samples`.\n\n            .. versionadded:: 0.23\n\n        check_input : bool, default=True\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you do.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.ElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNet",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.ElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNet",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ElasticNetCV",
      "documentation": {
        "description": "Elastic Net model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    l1_ratio : float or list of float, default=0.5\n        Float between 0 and 1 passed to ElasticNet (scaling between\n        l1 and l2 penalties). For ``l1_ratio = 0``\n        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n        This parameter can be a list, in which case the different\n        values are tested by cross-validation and the one giving the best\n        prediction score is used. Note that a good choice of list of\n        values for l1_ratio is often to put more values close to 1\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n        .9, .95, .99, 1]``.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path, used for each l1_ratio.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - int, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    verbose : bool or int, default=0\n        Amount of verbosity.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    l1_ratio_ : float\n        The compromise between l1 and l2 penalization chosen by\n        cross validation.\n\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : float or ndarray of shape (n_targets, n_features)\n        Independent term in the decision function.\n\n    mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n        Mean square error for the test set on each fold, varying l1_ratio and\n        alpha.\n\n    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n        The grid of alphas used for fitting, for each l1_ratio.\n\n    dual_gap_ : float\n        The dual gaps at the end of the optimization for the optimal alpha.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    enet_path : Compute elastic net path with coordinate descent.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n\n    Notes\n    -----\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    The parameter `l1_ratio` corresponds to alpha in the glmnet R package\n    while alpha corresponds to the lambda parameter in glmnet.\n    More specifically, the optimization objective is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\n    for::\n\n        alpha = a + b and l1_ratio = a / (a + b).\n\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.",
        "parameters": {
          "l1_ratio": {
            "type": "float or list of float, default=0.5",
            "description": ""
          },
          "Float": {
            "type": "between 0 and 1 passed to ElasticNet (scaling between",
            "description": ""
          },
          "l1": {
            "type": "and l2 penalties). For ``l1_ratio = 0``",
            "description": ""
          },
          "the": {
            "type": "specified tolerance for the optimal alpha.",
            "description": ""
          },
          "For": {
            "type": "an example, see",
            "description": ":ref:`examples/linear_model/plot_lasso_model_selection.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNetCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNetCV(cv=5, random_state=0)\n>>> regr.fit(X, y)"
          },
          "This": {
            "type": "parameter can be a list, in which case the different",
            "description": ""
          },
          "values": {
            "type": "for l1_ratio is often to put more values close to 1",
            "description": "(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n.9, .95, .99, 1]``."
          },
          "prediction": {
            "type": "score is used. Note that a good choice of list of",
            "description": ""
          },
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "alphas": {
            "type": "array",
            "description": "like, default=None"
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "you are interested in controlling the L1 and L2 penalty",
            "description": "separately, keep in mind that this is equivalent to::"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram"
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "precompute": {
            "type": "'auto', bool or array",
            "description": "like of shape             (n_features, n_features), default='auto'"
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "parameter `l1_ratio` corresponds to alpha in the glmnet R package",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- int, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=0",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": ":",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "l1_ratio_": {
            "type": "float",
            "description": ""
          },
          "cross": {
            "type": "validation.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the cost function formula).",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets, n_features)",
            "description": ""
          },
          "Independent": {
            "type": "term in the decision function.",
            "description": ""
          },
          "mse_path_": {
            "type": "ndarray of shape (n_l1_ratio, n_alpha, n_folds)",
            "description": ""
          },
          "Mean": {
            "type": "square error for the test set on each fold, varying l1_ratio and",
            "description": "alpha."
          },
          "alphas_": {
            "type": "ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)",
            "description": ""
          },
          "dual_gap_": {
            "type": "float",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "enet_path": {
            "type": "Compute elastic net path with coordinate descent.",
            "description": ""
          },
          "ElasticNet": {
            "type": "Linear regression with combined L1 and L2 priors as regularizer.",
            "description": "Notes\n-----"
          },
          "In": {
            "type": "`fit`, once the best parameters `l1_ratio` and `alpha` are found through",
            "description": "cross-validation, the model is fit again using the entire training set."
          },
          "To": {
            "type": "avoid unnecessary memory duplication the `X` argument of the `fit`",
            "description": ""
          },
          "method": {
            "type": "should be directly passed as a Fortran-contiguous numpy array.",
            "description": ""
          },
          "while": {
            "type": "alpha corresponds to the lambda parameter in glmnet.",
            "description": ""
          },
          "More": {
            "type": "specifically, the optimization objective is::",
            "description": ""
          },
          "1": {
            "type": "/ (2 * n_samples) * ||y - Xw||^2_2",
            "description": "+ alpha * l1_ratio * ||w||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2"
          },
          "a": {
            "type": "* L1 + b * L2",
            "description": ""
          },
          "alpha": {
            "type": "= a + b and l1_ratio = a / (a + b).",
            "description": ""
          },
          "ElasticNetCV": {
            "type": "cv=5, random_state=0",
            "description": ">>> print(regr.alpha_)\n0.199...\n>>> print(regr.intercept_)\n0.398...\n>>> print(regr.predict([[0, 0]]))\n[0.398...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    enet_path : Compute elastic net path with coordinate descent.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n\n    Notes\n    -----\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    The parameter `l1_ratio` corresponds to alpha in the glmnet R package\n    while alpha corresponds to the lambda parameter in glmnet.\n    More specifically, the optimization objective is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\n    for::\n\n        alpha = a + b and l1_ratio = a / (a + b).\n\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNetCV\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNetCV(cv=5, random_state=0)\n    >>> print(regr.alpha_)\n    0.199...\n    >>> print(regr.intercept_)\n    0.398...\n    >>> print(regr.predict([[0, 0]]))\n    [0.398...]",
        "notes": "-----\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    The parameter `l1_ratio` corresponds to alpha in the glmnet R package\n    while alpha corresponds to the lambda parameter in glmnet.\n    More specifically, the optimization objective is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\n    for::\n\n        alpha = a + b and l1_ratio = a / (a + b).\n\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNetCV\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNetCV(cv=5, random_state=0)\n    >>> print(regr.alpha_)\n    0.199...\n    >>> print(regr.intercept_)\n    0.398...\n    >>> print(regr.predict([[0, 0]]))\n    [0.398...]",
        "examples": "--------\n    >>> from sklearn.linear_model import ElasticNetCV\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNetCV(cv=5, random_state=0)\n    >>> print(regr.alpha_)\n    0.199...\n    >>> print(regr.intercept_)\n    0.398...\n    >>> print(regr.predict([[0, 0]]))\n    [0.398...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **params)",
          "documentation": {
            "description": "Fit ElasticNet model with coordinate descent.\n\n        Fit is on grid of alphas and best alpha estimated by cross-validation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data. Pass directly as Fortran-contiguous data\n            to avoid unnecessary memory duplication. If y is mono-output,\n            X can be sparse. Note that large sparse matrices and arrays\n            requiring `int64` indices are not accepted.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : float or array-like of shape (n_samples,),                 default=None\n            Sample weights used for fitting and evaluation of the weighted\n            mean squared error of each cv-fold. Note that the cross validated\n            MSE that is finally used to find the best model is the unweighted\n            mean over the (weighted) MSEs of each test fold.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "can be sparse. Note that large sparse matrices and arrays",
                "description": ""
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data",
                "description": ""
              },
              "to": {
                "type": "avoid unnecessary memory duplication. If y is mono-output,",
                "description": ""
              },
              "requiring": {
                "type": "`int64` indices are not accepted.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or array",
                "description": "like of shape (n_samples,),                 default=None"
              },
              "Sample": {
                "type": "weights used for fitting and evaluation of the weighted",
                "description": ""
              },
              "mean": {
                "type": "over the (weighted) MSEs of each test fold.",
                "description": "**params : dict, default=None"
              },
              "MSE": {
                "type": "that is finally used to find the best model is the unweighted",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.ElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNetCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.ElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNetCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GammaRegressor",
      "documentation": {
        "description": "Generalized Linear Model with a Gamma distribution.\n\n    This regressor uses the 'log' link function.\n\n    Read more in the :ref:`User Guide <Generalized_linear_models>`.\n\n    .. versionadded:: 0.23\n\n    Parameters\n    ----------\n    alpha : float, default=1\n        Constant that multiplies the L2 penalty term and determines the\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\n        GLMs. In this case, the design matrix `X` must have full column rank\n        (no collinearities).\n        Values of `alpha` must be in the range `[0.0, inf)`.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the linear predictor `X @ coef_ + intercept_`.\n\n    solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n        Algorithm to use in the optimization problem:\n\n        'lbfgs'\n            Calls scipy's L-BFGS-B optimizer.\n\n        'newton-cholesky'\n            Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n            iterated reweighted least squares) with an inner Cholesky based solver.\n            This solver is a good choice for `n_samples` >> `n_features`, especially\n            with one-hot encoded categorical features with rare categories. Be aware\n            that the memory usage of this solver has a quadratic dependency on\n            `n_features` because it explicitly computes the Hessian matrix.\n\n            .. versionadded:: 1.2\n\n    max_iter : int, default=100\n        The maximal number of iterations for the solver.\n        Values must be in the range `[1, inf)`.\n\n    tol : float, default=1e-4\n        Stopping criterion. For the lbfgs solver,\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n        where ``g_j`` is the j-th component of the gradient (derivative) of\n        the objective function.\n        Values must be in the range `(0.0, inf)`.\n\n    warm_start : bool, default=False\n        If set to ``True``, reuse the solution of the previous call to ``fit``\n        as initialization for `coef_` and `intercept_`.\n\n    verbose : int, default=0\n        For the lbfgs solver set verbose to any positive number for verbosity.\n        Values must be in the range `[0, inf)`.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features,)\n        Estimated coefficients for the linear predictor (`X @ coef_ +\n        intercept_`) in the GLM.\n\n    intercept_ : float\n        Intercept (a.k.a. bias) added to linear predictor.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    n_iter_ : int\n        Actual number of iterations used in the solver.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n    TweedieRegressor : Generalized Linear Model with a Tweedie distribution.",
        "parameters": {
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L2 penalty term and determines the",
            "description": ""
          },
          "regularization": {
            "type": "strength. ``alpha = 0`` is equivalent to unpenalized",
            "description": "GLMs. In this case, the design matrix `X` must have full column rank\n(no collinearities)."
          },
          "Values": {
            "type": "must be in the range `[0, inf)`.",
            "description": "Attributes\n----------"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if a constant (a.k.a. bias or intercept) should be",
            "description": ""
          },
          "added": {
            "type": "to the linear predictor `X @ coef_ + intercept_`.",
            "description": ""
          },
          "solver": {
            "type": "{'lbfgs', 'newton",
            "description": "cholesky'}, default='lbfgs'"
          },
          "Algorithm": {
            "type": "to use in the optimization problem:",
            "description": "'lbfgs'"
          },
          "Calls": {
            "type": "scipy's L-BFGS-B optimizer.",
            "description": "'newton-cholesky'"
          },
          "Uses": {
            "type": "Newton-Raphson steps (in arbitrary precision arithmetic equivalent to",
            "description": ""
          },
          "iterated": {
            "type": "reweighted least squares) with an inner Cholesky based solver.",
            "description": ""
          },
          "This": {
            "type": "solver is a good choice for `n_samples` >> `n_features`, especially",
            "description": ""
          },
          "with": {
            "type": "one-hot encoded categorical features with rare categories. Be aware",
            "description": ""
          },
          "that": {
            "type": "the memory usage of this solver has a quadratic dependency on",
            "description": "`n_features` because it explicitly computes the Hessian matrix.\n.. versionadded:: 1.2"
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "The": {
            "type": "maximal number of iterations for the solver.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Stopping": {
            "type": "criterion. For the lbfgs solver,",
            "description": ""
          },
          "the": {
            "type": "objective function.",
            "description": ""
          },
          "where": {
            "type": "``g_j`` is the j-th component of the gradient (derivative) of",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "set to ``True``, reuse the solution of the previous call to ``fit``",
            "description": ""
          },
          "as": {
            "type": "initialization for `coef_` and `intercept_`.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "For": {
            "type": "the lbfgs solver set verbose to any positive number for verbosity.",
            "description": ""
          },
          "coef_": {
            "type": "array of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "coefficients for the linear predictor (`X @ coef_ +",
            "description": "intercept_`) in the GLM."
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Intercept": {
            "type": "a.k.a. bias",
            "description": "added to linear predictor."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations used in the solver.",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "PoissonRegressor": {
            "type": "Generalized Linear Model with a Poisson distribution.",
            "description": ""
          },
          "TweedieRegressor": {
            "type": "Generalized Linear Model with a Tweedie distribution.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.GammaRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [19, 26, 33, 30]\n>>> clf.fit(X, y)"
          },
          "GammaRegressor": {
            "type": "",
            "description": ">>> clf.score(X, y)\nnp.float64(0.773...)\n>>> clf.coef_"
          },
          "array": {
            "type": "[19.483..., 35.795...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n    TweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.GammaRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [19, 26, 33, 30]\n    >>> clf.fit(X, y)\n    GammaRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.773...)\n    >>> clf.coef_\n    array([0.072..., 0.066...])\n    >>> clf.intercept_\n    np.float64(2.896...)\n    >>> clf.predict([[1, 0], [2, 8]])\n    array([19.483..., 35.795...])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.GammaRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [19, 26, 33, 30]\n    >>> clf.fit(X, y)\n    GammaRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.773...)\n    >>> clf.coef_\n    array([0.072..., 0.066...])\n    >>> clf.intercept_\n    np.float64(2.896...)\n    >>> clf.predict([[1, 0], [2, 8]])\n    array([19.483..., 35.795...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit a Generalized Linear Model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "model.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using GLM with feature matrix X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "y_pred": {
                "type": "array of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : array of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Compute D^2, the percentage of deviance explained.\n\n        D^2 is a generalization of the coefficient of determination R^2.\n        R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n        :ref:`User Guide <regression_metrics>`.\n\n        D^2 is defined as\n        :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n        :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n        with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n        The mean :math:`\\bar{y}` is averaged by sample_weight.\n        Best possible score is 1.0 and it can be negative (because the model\n        can be arbitrarily worse).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True values of target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "True": {
                "type": "values of target.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": "D^2 of self.predict(X) w.r.t. y."
              }
            },
            "returns": "-------\n        score : float\n            D^2 of self.predict(X) w.r.t. y.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._glm.glm.GammaRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.GammaRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._glm.glm.GammaRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.GammaRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HuberRegressor",
      "documentation": {
        "description": "L2-regularized linear regression model that is robust to outliers.\n\n    The Huber Regressor optimizes the squared loss for the samples where\n    ``|(y - Xw - c) / sigma| < epsilon`` and the absolute loss for the samples\n    where ``|(y - Xw - c) / sigma| > epsilon``, where the model coefficients\n    ``w``, the intercept ``c`` and the scale ``sigma`` are parameters\n    to be optimized. The parameter `sigma` makes sure that if `y` is scaled up\n    or down by a certain factor, one does not need to rescale `epsilon` to\n    achieve the same robustness. Note that this does not take into account\n    the fact that the different features of `X` may be of different scales.\n\n    The Huber loss function has the advantage of not being heavily influenced\n    by the outliers while not completely ignoring their effect.\n\n    Read more in the :ref:`User Guide <huber_regression>`\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    epsilon : float, default=1.35\n        The parameter epsilon controls the number of samples that should be\n        classified as outliers. The smaller the epsilon, the more robust it is\n        to outliers. Epsilon must be in the range `[1, inf)`.\n\n    max_iter : int, default=100\n        Maximum number of iterations that\n        ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n\n    alpha : float, default=0.0001\n        Strength of the squared L2 regularization. Note that the penalty is\n        equal to ``alpha * ||w||^2``.\n        Must be in the range `[0, inf)`.\n\n    warm_start : bool, default=False\n        This is useful if the stored attributes of a previously used model\n        has to be reused. If set to False, then the coefficients will\n        be rewritten for every call to fit.\n        See :term:`the Glossary <warm_start>`.\n\n    fit_intercept : bool, default=True\n        Whether or not to fit the intercept. This can be set to False\n        if the data is already centered around the origin.\n\n    tol : float, default=1e-05\n        The iteration will stop when\n        ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n        where pg_i is the i-th component of the projected gradient.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Features got by optimizing the L2-regularized Huber loss.\n\n    intercept_ : float\n        Bias.\n\n    scale_ : float\n        The value by which ``|y - Xw - c|`` is scaled down.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations that\n        ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    outliers_ : array, shape (n_samples,)\n        A boolean mask which is set to True where the samples are identified\n        as outliers.\n\n    See Also\n    --------\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n           Concomitant scale estimates, p. 172\n    .. [2] Art B. Owen (2006), `A robust hybrid of lasso and ridge regression.\n           <https://artowen.su.domains/reports/hhu.pdf>`_",
        "parameters": {
          "epsilon": {
            "type": "float, default=1.35",
            "description": ""
          },
          "The": {
            "type": "value by which ``|y - Xw - c|`` is scaled down.",
            "description": ""
          },
          "classified": {
            "type": "as outliers. The smaller the epsilon, the more robust it is",
            "description": ""
          },
          "to": {
            "type": "outliers. Epsilon must be in the range `[1, inf)`.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations that",
            "description": "``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for."
          },
          "alpha": {
            "type": "float, default=0.0001",
            "description": ""
          },
          "Strength": {
            "type": "of the squared L2 regularization. Note that the penalty is",
            "description": ""
          },
          "equal": {
            "type": "to ``alpha * ||w||^2``.",
            "description": ""
          },
          "Must": {
            "type": "be in the range `[0, inf)`.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "This": {
            "type": "is useful if the stored attributes of a previously used model",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "be": {
            "type": "rewritten for every call to fit.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not to fit the intercept. This can be set to False",
            "description": ""
          },
          "if": {
            "type": "the data is already centered around the origin.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "05"
          },
          "where": {
            "type": "pg_i is the i-th component of the projected gradient.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array, shape (n_features,)",
            "description": ""
          },
          "Features": {
            "type": "got by optimizing the L2-regularized Huber loss.",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": "Bias."
          },
          "scale_": {
            "type": "float",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of iterations that",
            "description": "``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n.. versionchanged:: 0.20"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "In": {
            "type": "SciPy <= 1.0.0 the number of lbfgs iterations may exceed",
            "description": "``max_iter``. ``n_iter_`` will now report at most ``max_iter``."
          },
          "outliers_": {
            "type": "array, shape (n_samples,)",
            "description": ""
          },
          "A": {
            "type": "boolean mask which is set to True where the samples are identified",
            "description": ""
          },
          "as": {
            "type": "outliers.",
            "description": ""
          },
          "RANSACRegressor": {
            "type": "RANSAC (RANdom SAmple Consensus) algorithm.",
            "description": ""
          },
          "TheilSenRegressor": {
            "type": "Theil",
            "description": "Sen Estimator robust multivariate regression model."
          },
          "SGDRegressor": {
            "type": "Fitted by minimizing a regularized empirical loss with SGD.",
            "description": "References\n----------\n.. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics"
          },
          "Concomitant": {
            "type": "scale estimates, p. 172",
            "description": ".. [2] Art B. Owen (2006), `A robust hybrid of lasso and ridge regression.\n<https://artowen.su.domains/reports/hhu.pdf>`_\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import HuberRegressor, LinearRegression\n>>> from sklearn.datasets import make_regression\n>>> rng = np.random.RandomState(0)\n>>> X, y, coef = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n>>> X[:4] = rng.uniform(10, 20, (4, 2))\n>>> y[:4] = rng.uniform(10, 20, 4)\n>>> huber = HuberRegressor().fit(X, y)\n>>> huber.score(X, y)\n-7.284...\n>>> huber.predict(X[:1,])"
          },
          "array": {
            "type": "[806.7200...]",
            "description": ">>> linear = LinearRegression().fit(X, y)\n>>> print(\"True coefficients:\", coef)"
          },
          "True": {
            "type": "coefficients: [20.4923...  34.1698...]",
            "description": ">>> print(\"Huber coefficients:\", huber.coef_)"
          },
          "Huber": {
            "type": "coefficients: [17.7906... 31.0106...]",
            "description": ">>> print(\"Linear Regression coefficients:\", linear.coef_)"
          },
          "Linear": {
            "type": "Regression coefficients: [-1.9221...  7.0226...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n           Concomitant scale estimates, p. 172\n    .. [2] Art B. Owen (2006), `A robust hybrid of lasso and ridge regression.\n           <https://artowen.su.domains/reports/hhu.pdf>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n    >>> from sklearn.datasets import make_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X, y, coef = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n    >>> X[:4] = rng.uniform(10, 20, (4, 2))\n    >>> y[:4] = rng.uniform(10, 20, 4)\n    >>> huber = HuberRegressor().fit(X, y)\n    >>> huber.score(X, y)\n    -7.284...\n    >>> huber.predict(X[:1,])\n    array([806.7200...])\n    >>> linear = LinearRegression().fit(X, y)\n    >>> print(\"True coefficients:\", coef)\n    True coefficients: [20.4923...  34.1698...]\n    >>> print(\"Huber coefficients:\", huber.coef_)\n    Huber coefficients: [17.7906... 31.0106...]\n    >>> print(\"Linear Regression coefficients:\", linear.coef_)\n    Linear Regression coefficients: [-1.9221...  7.0226...]",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n    >>> from sklearn.datasets import make_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X, y, coef = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n    >>> X[:4] = rng.uniform(10, 20, (4, 2))\n    >>> y[:4] = rng.uniform(10, 20, 4)\n    >>> huber = HuberRegressor().fit(X, y)\n    >>> huber.score(X, y)\n    -7.284...\n    >>> huber.predict(X[:1,])\n    array([806.7200...])\n    >>> linear = LinearRegression().fit(X, y)\n    >>> print(\"True coefficients:\", coef)\n    True coefficients: [20.4923...  34.1698...]\n    >>> print(\"Huber coefficients:\", huber.coef_)\n    Huber coefficients: [17.7906... 31.0106...]\n    >>> print(\"Linear Regression coefficients:\", linear.coef_)\n    Linear Regression coefficients: [-1.9221...  7.0226...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,)\n            Weight given to each sample.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like, shape (n_samples,)"
              },
              "Target": {
                "type": "vector relative to X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,)"
              },
              "Weight": {
                "type": "given to each sample.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "`HuberRegressor` estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted `HuberRegressor` estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._huber.HuberRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._huber.HuberRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._huber.HuberRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._huber.HuberRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Lars",
      "documentation": {
        "description": "Least Angle Regression model a.k.a. LAR.\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    precompute : bool, 'auto' or array-like , default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    n_nonzero_coefs : int, default=500\n        Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    fit_path : bool, default=True\n        If True the full path is stored in the ``coef_path_`` attribute.\n        If you compute the solution for a large problem or many targets,\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\n        with a small alpha.\n\n    jitter : float, default=None\n        Upper bound on a uniform noise parameter to be added to the\n        `y` values, to satisfy the model's assumption of\n        one-at-a-time computations. Might help with stability.\n\n        .. versionadded:: 0.23\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for jittering. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller. If this is a list of array-like, the length of the outer\n        list is `n_targets`.\n\n    active_ : list of shape (n_alphas,) or list of such lists\n        Indices of active variables at the end of the path.\n        If this is a list of list, the length of the outer list is `n_targets`.\n\n    coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``. If this is a list\n        of array-like, the length of the outer list is `n_targets`.\n\n    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float or array-like of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path: Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.",
        "parameters": {
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram"
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": ""
          },
          "precompute": {
            "type": "bool, 'auto' or array",
            "description": "like , default='auto'"
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "n_nonzero_coefs": {
            "type": "int, default=500",
            "description": ""
          },
          "Target": {
            "type": "number of non-zero coefficients. Use ``np.inf`` for no limit.",
            "description": ""
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "The": {
            "type": "number of iterations taken by lars_path to find the",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the ``tol`` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "tolerance of the optimization.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "this is a list of list, the length of the outer list is `n_targets`.",
            "description": ""
          },
          "fit_path": {
            "type": "bool, default=True",
            "description": ""
          },
          "setting": {
            "type": "``fit_path`` to ``False`` will lead to a speedup, especially",
            "description": ""
          },
          "with": {
            "type": "a small alpha.",
            "description": ""
          },
          "jitter": {
            "type": "float, default=None",
            "description": ""
          },
          "Upper": {
            "type": "bound on a uniform noise parameter to be added to the",
            "description": "`y` values, to satisfy the model's assumption of\none-at-a-time computations. Might help with stability.\n.. versionadded:: 0.23"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for jittering. Pass an int",
            "description": ""
          },
          "for": {
            "type": "reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "alphas_": {
            "type": "array",
            "description": "like of shape (n_alphas + 1,) or list of such arrays"
          },
          "Maximum": {
            "type": "of covariances (in absolute value) at each iteration.",
            "description": "``n_alphas`` is either ``max_iter``, ``n_features`` or the"
          },
          "number": {
            "type": "of nodes in the path with ``alpha >= alpha_min``, whichever",
            "description": ""
          },
          "is": {
            "type": "smaller. If this is a list of array-like, the length of the outer",
            "description": ""
          },
          "list": {
            "type": "is `n_targets`.",
            "description": ""
          },
          "active_": {
            "type": "list of shape (n_alphas,) or list of such lists",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "coef_path_": {
            "type": "array",
            "description": "like of shape (n_features, n_alphas + 1) or list             of such arrays"
          },
          "present": {
            "type": "if the ``fit_path`` parameter is ``False``. If this is a list",
            "description": ""
          },
          "of": {
            "type": "array-like, the length of the outer list is `n_targets`.",
            "description": ""
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,) or (n_targets, n_features)"
          },
          "Parameter": {
            "type": "vector (w in the formulation formula).",
            "description": ""
          },
          "intercept_": {
            "type": "float or array",
            "description": "like of shape (n_targets,)"
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "array",
            "description": "like or int"
          },
          "grid": {
            "type": "of alphas for each target.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso",
            "description": ""
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": ""
          },
          "LarsCV": {
            "type": "Cross",
            "description": "validated Least Angle Regression model.\nsklearn.decomposition.sparse_encode : Sparse coding.\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.Lars(n_nonzero_coefs=1)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])"
          },
          "Lars": {
            "type": "n_nonzero_coefs=1",
            "description": ">>> print(reg.coef_)\n[ 0. -1.11...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path: Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    LarsCV : Cross-validated Least Angle Regression model.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n    Lars(n_nonzero_coefs=1)\n    >>> print(reg.coef_)\n    [ 0. -1.11...]",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n    Lars(n_nonzero_coefs=1)\n    >>> print(reg.coef_)\n    [ 0. -1.11...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, Xy=None)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Xy : array-like of shape (n_features,) or (n_features, n_targets),                 default=None\n            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n            only when the Gram matrix is precomputed.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._least_angle.Lars, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.Lars",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "Xy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``Xy`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``Xy`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._least_angle.Lars, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.Lars",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LarsCV",
      "documentation": {
        "description": "Cross-validated Least Angle Regression model.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    precompute : bool, 'auto' or array-like , default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    max_n_alphas : int, default=1000\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation.\n\n    n_jobs : int or None, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    Attributes\n    ----------\n    active_ : list of length n_alphas or list of such lists\n        Indices of active variables at the end of the path.\n        If this is a list of lists, the outer list length is `n_targets`.\n\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function\n\n    coef_path_ : array-like of shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array-like of shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array-like of shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.",
        "parameters": {
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram matrix"
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform.",
            "description": ""
          },
          "precompute": {
            "type": "bool, 'auto' or array",
            "description": "like , default='auto'"
          },
          "cannot": {
            "type": "be passed as argument since we will use only subsets of X.",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or an iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "max_n_alphas": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "machine-precision regularization in the computation of the",
            "description": ""
          },
          "residuals": {
            "type": "in the cross-validation.",
            "description": ""
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the ``tol`` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "number of iterations run by Lars with the optimal alpha.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "this is a list of lists, the outer list length is `n_targets`.",
            "description": ""
          },
          "active_": {
            "type": "list of length n_alphas or list of such lists",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "parameter": {
            "type": "vector (w in the formulation formula)",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "independent": {
            "type": "term in decision function",
            "description": ""
          },
          "coef_path_": {
            "type": "array",
            "description": "like of shape (n_features, n_alphas)"
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "alphas_": {
            "type": "array",
            "description": "like of shape (n_alphas,)"
          },
          "cv_alphas_": {
            "type": "array",
            "description": "like of shape (n_cv_alphas,)"
          },
          "all": {
            "type": "the values of alpha along the path for the different folds",
            "description": ""
          },
          "mse_path_": {
            "type": "array",
            "description": "like of shape (n_folds, n_cv_alphas)"
          },
          "n_iter_": {
            "type": "array",
            "description": "like or int"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso",
            "description": ""
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as",
            "description": ""
          },
          "regularizer": {
            "type": "aka the Lasso",
            "description": "."
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting",
            "description": ""
          },
          "along": {
            "type": "a regularization path.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "LassoLarsIC": {
            "type": "Lasso model fit with Lars using BIC",
            "description": ""
          },
          "or": {
            "type": "AIC for model selection.",
            "description": "sklearn.decomposition.sparse_encode : Sparse coding.\nNotes\n-----"
          },
          "In": {
            "type": "`fit`, once the best parameter `alpha` is found through",
            "description": "cross-validation, the model is fit again using the entire training set.\nExamples\n--------\n>>> from sklearn.linear_model import LarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n>>> reg = LarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9996...\n>>> reg.alpha_\nnp.float64(0.2961...)\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[154.3996...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n    >>> reg = LarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9996...\n    >>> reg.alpha_\n    np.float64(0.2961...)\n    >>> reg.predict(X[:1,])\n    array([154.3996...])",
        "notes": "-----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n    >>> reg = LarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9996...\n    >>> reg.alpha_\n    np.float64(0.2961...)\n    >>> reg.predict(X[:1,])\n    array([154.3996...])",
        "examples": "--------\n    >>> from sklearn.linear_model import LarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n    >>> reg = LarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9996...\n    >>> reg.alpha_\n    np.float64(0.2961...)\n    >>> reg.predict(X[:1,])\n    array([154.3996...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **params)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._least_angle.LarsCV, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LarsCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "Xy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``Xy`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``Xy`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._least_angle.LarsCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LarsCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Lasso",
      "documentation": {
        "description": "Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Technically the Lasso model is optimizing the same objective function as\n    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the L1 term, controlling regularization\n        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n\n        When `alpha = 0`, the objective is equivalent to ordinary least\n        squares, solved by the :class:`LinearRegression` object. For numerical\n        reasons, using `alpha = 0` with the `Lasso` object is not advised.\n        Instead, you should use the :class:`LinearRegression` object.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    precompute : bool or array-like of shape (n_features, n_features),                 default=False\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. The Gram matrix can also be passed as argument.\n        For sparse input this option is always ``False`` to preserve sparsity.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``, see Notes below.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the cost function formula).\n\n    dual_gap_ : float or ndarray of shape (n_targets,)\n        Given param alpha, the dual gaps at the end of the optimization,\n        same shape as each observation of y.\n\n    sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n        Readonly property derived from ``coef_``.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int or list of int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Regularization path using LARS.\n    lasso_path : Regularization path using Lasso.\n    LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n    LassoCV : Lasso alpha parameter by cross-validation.\n    LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n    sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to `1 / (2C)` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\\text{samples}}`.\n\n    The target can be a 2-dimensional array, resulting in the optimization of the\n    following objective::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\n    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n    sparsity in the coefficients.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L1 term, controlling regularization",
            "description": "strength. `alpha` must be a non-negative float i.e. in `[0, inf)`."
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. The Gram matrix can also be passed as argument."
          },
          "to": {
            "type": "False, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "precompute": {
            "type": "bool or array",
            "description": "like of shape (n_features, n_features),                 default=False"
          },
          "For": {
            "type": "sparse input this option is always ``False`` to preserve sparsity.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "so, then additionally check whether the dual gap is smaller than `tol` times",
            "description": ":math:`||y||_2^2 / n_{\\text{samples}}`."
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "target can be a 2-dimensional array, resulting in the optimization of the",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``, see Notes below.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the cost function formula).",
            "description": ""
          },
          "dual_gap_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Given": {
            "type": "param alpha, the dual gaps at the end of the optimization,",
            "description": ""
          },
          "same": {
            "type": "shape as each observation of y.",
            "description": ""
          },
          "sparse_coef_": {
            "type": "sparse matrix of shape (n_features, 1) or             (n_targets, n_features)",
            "description": ""
          },
          "Readonly": {
            "type": "property derived from ``coef_``.",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "int or list of int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "the": {
            "type": "specified tolerance.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "lars_path": {
            "type": "Regularization path using LARS.",
            "description": ""
          },
          "lasso_path": {
            "type": "Regularization path using Lasso.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso Path along the regularization parameter using LARS algorithm.",
            "description": ""
          },
          "LassoCV": {
            "type": "Lasso alpha parameter by cross",
            "description": "validation."
          },
          "LassoLarsCV": {
            "type": "Lasso least angle parameter algorithm by cross",
            "description": "validation.\nsklearn.decomposition.sparse_encode : Sparse coding array estimator.\nNotes\n-----"
          },
          "To": {
            "type": "avoid unnecessary memory duplication the X argument of the fit method",
            "description": ""
          },
          "should": {
            "type": "be directly passed as a Fortran-contiguous numpy array.",
            "description": ""
          },
          "Regularization": {
            "type": "improves the conditioning of the problem and",
            "description": ""
          },
          "reduces": {
            "type": "the variance of the estimates. Larger values specify stronger",
            "description": "regularization. Alpha corresponds to `1 / (2C)` in other linear"
          },
          "models": {
            "type": "such as :class:`~sklearn.linear_model.LogisticRegression` or",
            "description": ":class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are"
          },
          "assumed": {
            "type": "to be specific to the targets. Hence they must correspond in",
            "description": "number."
          },
          "that": {
            "type": "maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`",
            "description": ""
          },
          "is": {
            "type": "smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.",
            "description": ""
          },
          "following": {
            "type": "objective::",
            "description": "(1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11"
          },
          "where": {
            "type": "math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.",
            "description": ""
          },
          "It": {
            "type": "should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which",
            "description": ""
          },
          "instead": {
            "type": "penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise",
            "description": ""
          },
          "sparsity": {
            "type": "in the coefficients.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.Lasso(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
          },
          "Lasso": {
            "type": "alpha=0.1",
            "description": ">>> print(clf.coef_)\n[0.85 0.  ]\n>>> print(clf.intercept_)\n0.15..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Regularization path using LARS.\n    lasso_path : Regularization path using Lasso.\n    LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n    LassoCV : Lasso alpha parameter by cross-validation.\n    LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n    sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to `1 / (2C)` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\\text{samples}}`.\n\n    The target can be a 2-dimensional array, resulting in the optimization of the\n    following objective::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\n    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n    sparsity in the coefficients.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15...",
        "notes": "-----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to `1 / (2C)` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\\text{samples}}`.\n\n    The target can be a 2-dimensional array, resulting in the optimization of the\n    following objective::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\n    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n    sparsity in the coefficients.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15...",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, check_input=True)",
          "documentation": {
            "description": "Fit model with coordinate descent.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix, sparse array} of (n_samples, n_features)\n            Data.",
            "parameters": {
              "X": {
                "type": "{ndarray, sparse matrix, sparse array} of (n_samples, n_features)",
                "description": "Data."
              },
              "Note": {
                "type": "that large sparse matrices and arrays requiring `int64`",
                "description": ""
              },
              "indices": {
                "type": "are not accepted.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_targets)",
                "description": "Target. Will be cast to X's dtype if necessary."
              },
              "sample_weight": {
                "type": "float or array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights. Internally, the `sample_weight` vector will be",
                "description": ""
              },
              "rescaled": {
                "type": "to sum to `n_samples`.",
                "description": ".. versionadded:: 0.23"
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Allow": {
                "type": "to bypass several input checking.",
                "description": "Don't use this parameter unless you know what you do.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": "Notes\n-----"
              },
              "Coordinate": {
                "type": "descent is an algorithm that considers each column of",
                "description": ""
              },
              "data": {
                "type": "at a time hence it will automatically convert the X input",
                "description": ""
              },
              "as": {
                "type": "a Fortran-contiguous numpy array if necessary.",
                "description": ""
              },
              "To": {
                "type": "avoid memory re-allocation it is advised to allocate the",
                "description": ""
              },
              "initial": {
                "type": "data in memory directly using that format.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "raises": "",
            "see_also": "",
            "notes": "that large sparse matrices and arrays requiring `int64`\n            indices are not accepted.\n\n        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Target. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or array-like of shape (n_samples,), default=None\n            Sample weights. Internally, the `sample_weight` vector will be\n            rescaled to sum to `n_samples`.\n\n            .. versionadded:: 0.23\n\n        check_input : bool, default=True\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you do.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.Lasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.Lasso",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.Lasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.Lasso",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LassoCV",
      "documentation": {
        "description": "Lasso linear model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The best model is selected by cross-validation.\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - int, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, default=False\n        If positive, restrict regression coefficients to be positive.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the cost function formula).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    mse_path_ : ndarray of shape (n_alphas, n_folds)\n        Mean square error for the test set on each fold, varying alpha.\n\n    alphas_ : ndarray of shape (n_alphas,)\n        The grid of alphas used for fitting.\n\n    dual_gap_ : float or ndarray of shape (n_targets,)\n        The dual gap at the end of the optimization for the optimal alpha\n        (``alpha_``).\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n\n    Notes\n    -----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    For an example, see :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    :class:`LassoCV` leads to different results than a hyperparameter\n    search using :class:`~sklearn.model_selection.GridSearchCV` with a\n    :class:`Lasso` model. In :class:`LassoCV`, a model for a given\n    penalty `alpha` is warm started using the coefficients of the\n    closest model (trained at the previous iteration) on the\n    regularization path. It tends to speed up the hyperparameter\n    search.",
        "parameters": {
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "alphas": {
            "type": "array",
            "description": "like, default=None"
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "set to 'random', a random coefficient is updated every iteration",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram"
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "precompute": {
            "type": "'auto', bool or array",
            "description": "like of shape             (n_features, n_features), default='auto'"
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "dual gap at the end of the optimization for the optimal alpha",
            "description": "(``alpha_``)."
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- int, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "an example, see :ref:`examples/linear_model/plot_lasso_model_selection.py",
            "description": "<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n:class:`LassoCV` leads to different results than a hyperparameter"
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the cost function formula).",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "mse_path_": {
            "type": "ndarray of shape (n_alphas, n_folds)",
            "description": ""
          },
          "Mean": {
            "type": "square error for the test set on each fold, varying alpha.",
            "description": ""
          },
          "alphas_": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "dual_gap_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "the": {
            "type": "specified tolerance for the optimal alpha.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using LARS",
            "description": "algorithm."
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "Lasso": {
            "type": "The Lasso is a linear model that estimates sparse coefficients.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting along a regularization",
            "description": "path."
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso using the LARS algorithm.\nNotes\n-----"
          },
          "In": {
            "type": "`fit`, once the best parameter `alpha` is found through",
            "description": "cross-validation, the model is fit again using the entire training set."
          },
          "To": {
            "type": "avoid unnecessary memory duplication the `X` argument of the `fit`",
            "description": ""
          },
          "method": {
            "type": "should be directly passed as a Fortran-contiguous numpy array.",
            "description": ""
          },
          "search": {
            "type": "using :class:`~sklearn.model_selection.GridSearchCV` with a",
            "description": ":class:`Lasso` model. In :class:`LassoCV`, a model for a given"
          },
          "penalty": {
            "type": "`alpha` is warm started using the coefficients of the",
            "description": ""
          },
          "closest": {
            "type": "model (trained at the previous iteration) on the",
            "description": ""
          },
          "regularization": {
            "type": "path. It tends to speed up the hyperparameter",
            "description": "search.\nExamples\n--------\n>>> from sklearn.linear_model import LassoCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9993...\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[-78.4951...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n\n    Notes\n    -----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    For an example, see :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    :class:`LassoCV` leads to different results than a hyperparameter\n    search using :class:`~sklearn.model_selection.GridSearchCV` with a\n    :class:`Lasso` model. In :class:`LassoCV`, a model for a given\n    penalty `alpha` is warm started using the coefficients of the\n    closest model (trained at the previous iteration) on the\n    regularization path. It tends to speed up the hyperparameter\n    search.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.predict(X[:1,])\n    array([-78.4951...])",
        "notes": "-----\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` argument of the `fit`\n    method should be directly passed as a Fortran-contiguous numpy array.\n\n    For an example, see :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    :class:`LassoCV` leads to different results than a hyperparameter\n    search using :class:`~sklearn.model_selection.GridSearchCV` with a\n    :class:`Lasso` model. In :class:`LassoCV`, a model for a given\n    penalty `alpha` is warm started using the coefficients of the\n    closest model (trained at the previous iteration) on the\n    regularization path. It tends to speed up the hyperparameter\n    search.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.predict(X[:1,])\n    array([-78.4951...])",
        "examples": "--------\n    >>> from sklearn.linear_model import LassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.predict(X[:1,])\n    array([-78.4951...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **params)",
          "documentation": {
            "description": "Fit Lasso model with coordinate descent.\n\n        Fit is on grid of alphas and best alpha estimated by cross-validation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data. Pass directly as Fortran-contiguous data\n            to avoid unnecessary memory duplication. If y is mono-output,\n            X can be sparse. Note that large sparse matrices and arrays\n            requiring `int64` indices are not accepted.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : float or array-like of shape (n_samples,),                 default=None\n            Sample weights used for fitting and evaluation of the weighted\n            mean squared error of each cv-fold. Note that the cross validated\n            MSE that is finally used to find the best model is the unweighted\n            mean over the (weighted) MSEs of each test fold.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "can be sparse. Note that large sparse matrices and arrays",
                "description": ""
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data",
                "description": ""
              },
              "to": {
                "type": "avoid unnecessary memory duplication. If y is mono-output,",
                "description": ""
              },
              "requiring": {
                "type": "`int64` indices are not accepted.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or array",
                "description": "like of shape (n_samples,),                 default=None"
              },
              "Sample": {
                "type": "weights used for fitting and evaluation of the weighted",
                "description": ""
              },
              "mean": {
                "type": "over the (weighted) MSEs of each test fold.",
                "description": "**params : dict, default=None"
              },
              "MSE": {
                "type": "that is finally used to find the best model is the unweighted",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)",
          "documentation": {
            "description": "Compute Lasso path with coordinate descent.\n\n    The Lasso optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to True, forces coefficients to be positive.",
                "description": "(Only allowed when ``y.ndim == 1``).\n**params : kwargs"
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": ""
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "lars_path": {
                "type": "Compute Least Angle Regression or Lasso path using LARS",
                "description": "algorithm."
              },
              "Lasso": {
                "type": "The Lasso is a linear model that estimates sparse coefficients.",
                "description": ""
              },
              "LassoLars": {
                "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
                "description": ""
              },
              "LassoCV": {
                "type": "Lasso linear model with iterative fitting along a regularization",
                "description": "path."
              },
              "LassoLarsCV": {
                "type": "Cross",
                "description": "validated Lasso using the LARS algorithm.\nsklearn.decomposition.sparse_encode : Estimator that can be used to"
              },
              "transform": {
                "type": "signals into sparse linear combination of atoms from a fixed.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`."
              },
              "To": {
                "type": "avoid unnecessary memory duplication the X argument of the fit method",
                "description": ""
              },
              "should": {
                "type": "be directly passed as a Fortran-contiguous numpy array.",
                "description": ""
              },
              "Note": {
                "type": "that in certain cases, the Lars solver may be significantly",
                "description": ""
              },
              "faster": {
                "type": "to implement this functionality. In particular, linear",
                "description": ""
              },
              "interpolation": {
                "type": "can be used to retrieve model coefficients between the",
                "description": ""
              },
              "values": {
                "type": "output by lars_path",
                "description": "Examples\n--------"
              },
              "Comparing": {
                "type": "lasso_path and lars_path with interpolation:",
                "description": ">>> import numpy as np\n>>> from sklearn.linear_model import lasso_path\n>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n>>> y = np.array([1, 2, 3.1])\n>>> # Use lasso_path to compute a coefficient path\n>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n>>> print(coef_path)\n[[0.         0.         0.46874778]\n[0.2159048  0.4425765  0.23689075]]\n>>> # Now use lars_path and 1D linear interpolation to compute the\n>>> # same path\n>>> from sklearn.linear_model import lars_path\n>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n>>> from scipy import interpolate\n>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n...                                             coef_path_lars[:, ::-1])\n>>> print(coef_path_continuous([5., 1., .5]))\n[[0.         0.         0.46915237]\n[0.2159048  0.4425765  0.23668876]]"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "raises": "",
            "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "notes": "that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "examples": "--------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.LassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.LassoCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.LassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.LassoCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LassoLars",
      "documentation": {
        "description": "Lasso model fit with Least Angle Regression a.k.a. Lars.\n\n    It is a Linear Model trained with an L1 prior as regularizer.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the penalty term. Defaults to 1.0.\n        ``alpha = 0`` is equivalent to an ordinary least square, solved\n        by :class:`LinearRegression`. For numerical reasons, using\n        ``alpha = 0`` with the LassoLars object is not advised and you\n        should prefer the LinearRegression object.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    precompute : bool, 'auto' or array-like, default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    fit_path : bool, default=True\n        If ``True`` the full path is stored in the ``coef_path_`` attribute.\n        If you compute the solution for a large problem or many targets,\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\n        with a small alpha.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients will not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n\n    jitter : float, default=None\n        Upper bound on a uniform noise parameter to be added to the\n        `y` values, to satisfy the model's assumption of\n        one-at-a-time computations. Might help with stability.\n\n        .. versionadded:: 0.23\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for jittering. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller. If this is a list of array-like, the length of the outer\n        list is `n_targets`.\n\n    active_ : list of length n_alphas or list of such lists\n        Indices of active variables at the end of the path.\n        If this is a list of list, the length of the outer list is `n_targets`.\n\n    coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n        If a list is passed it's expected to be one of n_targets such arrays.\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``. If this is a list\n        of array-like, the length of the outer list is `n_targets`.\n\n    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float or array-like of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the penalty term. Defaults to 1.0.",
            "description": "``alpha = 0`` is equivalent to an ordinary least square, solved"
          },
          "by": {
            "type": "class:`LinearRegression`. For numerical reasons, using",
            "description": "``alpha = 0`` with the LassoLars object is not advised and you"
          },
          "should": {
            "type": "prefer the LinearRegression object.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram"
          },
          "to": {
            "type": "the ordinary-least-squares solution for small values of alpha.",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": ""
          },
          "precompute": {
            "type": "bool, 'auto' or array",
            "description": "like, default='auto'"
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "of covariances (in absolute value) at each iteration.",
            "description": "``n_alphas`` is either ``max_iter``, ``n_features`` or the"
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "The": {
            "type": "number of iterations taken by lars_path to find the",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the ``tol`` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "tolerance of the optimization.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "a list is passed it's expected to be one of n_targets such arrays.",
            "description": ""
          },
          "fit_path": {
            "type": "bool, default=True",
            "description": ""
          },
          "setting": {
            "type": "``fit_path`` to ``False`` will lead to a speedup, especially",
            "description": ""
          },
          "with": {
            "type": "a small alpha.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Restrict": {
            "type": "coefficients to be >= 0. Be aware that you might want to",
            "description": ""
          },
          "remove": {
            "type": "fit_intercept which is set True by default.",
            "description": ""
          },
          "Under": {
            "type": "the positive restriction the model coefficients will not converge",
            "description": ""
          },
          "Only": {
            "type": "coefficients up to the smallest alpha value (``alphas_[alphas_ >",
            "description": "0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso"
          },
          "algorithm": {
            "type": "are typically in congruence with the solution of the",
            "description": ""
          },
          "coordinate": {
            "type": "descent Lasso estimator.",
            "description": ""
          },
          "jitter": {
            "type": "float, default=None",
            "description": ""
          },
          "Upper": {
            "type": "bound on a uniform noise parameter to be added to the",
            "description": "`y` values, to satisfy the model's assumption of\none-at-a-time computations. Might help with stability.\n.. versionadded:: 0.23"
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "random number generation for jittering. Pass an int",
            "description": ""
          },
          "for": {
            "type": "reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "alphas_": {
            "type": "array",
            "description": "like of shape (n_alphas + 1,) or list of such arrays"
          },
          "number": {
            "type": "of nodes in the path with ``alpha >= alpha_min``, whichever",
            "description": ""
          },
          "is": {
            "type": "smaller. If this is a list of array-like, the length of the outer",
            "description": ""
          },
          "list": {
            "type": "is `n_targets`.",
            "description": ""
          },
          "active_": {
            "type": "list of length n_alphas or list of such lists",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "coef_path_": {
            "type": "array",
            "description": "like of shape (n_features, n_alphas + 1) or list             of such arrays"
          },
          "present": {
            "type": "if the ``fit_path`` parameter is ``False``. If this is a list",
            "description": ""
          },
          "of": {
            "type": "array-like, the length of the outer list is `n_targets`.",
            "description": ""
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,) or (n_targets, n_features)"
          },
          "Parameter": {
            "type": "vector (w in the formulation formula).",
            "description": ""
          },
          "intercept_": {
            "type": "float or array",
            "description": "like of shape (n_targets,)"
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "array",
            "description": "like or int"
          },
          "grid": {
            "type": "of alphas for each target.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso",
            "description": ""
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as",
            "description": ""
          },
          "regularizer": {
            "type": "aka the Lasso",
            "description": "."
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting",
            "description": ""
          },
          "along": {
            "type": "a regularization path.",
            "description": ""
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso, using the LARS algorithm."
          },
          "LassoLarsIC": {
            "type": "Lasso model fit with Lars using BIC",
            "description": ""
          },
          "or": {
            "type": "AIC for model selection.",
            "description": "sklearn.decomposition.sparse_encode : Sparse coding.\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLars(alpha=0.01)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])"
          },
          "LassoLars": {
            "type": "alpha=0.01",
            "description": ">>> print(reg.coef_)\n[ 0.         -0.955...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLars(alpha=0.01)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n    LassoLars(alpha=0.01)\n    >>> print(reg.coef_)\n    [ 0.         -0.955...]",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLars(alpha=0.01)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n    LassoLars(alpha=0.01)\n    >>> print(reg.coef_)\n    [ 0.         -0.955...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, Xy=None)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Xy : array-like of shape (n_features,) or (n_features, n_targets),                 default=None\n            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n            only when the Gram matrix is precomputed.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._least_angle.LassoLars, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLars",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "Xy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``Xy`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``Xy`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._least_angle.LassoLars, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLars",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LassoLarsCV",
      "documentation": {
        "description": "Cross-validated Lasso, using the LARS algorithm.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    precompute : bool or 'auto' , default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    max_n_alphas : int, default=1000\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation.\n\n    n_jobs : int or None, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsCV only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    coef_path_ : array-like of shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array-like of shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array-like of shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    active_ : list of int\n        Indices of active variables at the end of the path.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    The object solves the same problem as the\n    :class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n    :class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\n    by itself. In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\n    only a small number of features are selected compared to the total number,\n    for instance if there are very few samples compared to the number of\n    features.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.",
        "parameters": {
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram matrix"
          },
          "to": {
            "type": "the ordinary-least-squares solution for small values of alpha.",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform.",
            "description": ""
          },
          "precompute": {
            "type": "bool or 'auto' , default='auto'",
            "description": ""
          },
          "cannot": {
            "type": "be passed as argument since we will use only subsets of X.",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or an iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "max_n_alphas": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "object solves the same problem as the",
            "description": ":class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n:class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values"
          },
          "residuals": {
            "type": "in the cross-validation.",
            "description": ""
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "for": {
            "type": "instance if there are very few samples compared to the number of",
            "description": "features."
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the ``tol`` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "number of iterations run by Lars with the optimal alpha.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "True, X will be copied; else, it may be overwritten.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Restrict": {
            "type": "coefficients to be >= 0. Be aware that you might want to",
            "description": ""
          },
          "remove": {
            "type": "fit_intercept which is set True by default.",
            "description": ""
          },
          "Under": {
            "type": "the positive restriction the model coefficients do not converge",
            "description": ""
          },
          "Only": {
            "type": "coefficients up to the smallest alpha value (``alphas_[alphas_ >",
            "description": "0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso"
          },
          "algorithm": {
            "type": "are typically in congruence with the solution of the",
            "description": ""
          },
          "coordinate": {
            "type": "descent Lasso estimator.",
            "description": ""
          },
          "As": {
            "type": "a consequence using LassoLarsCV only makes sense for problems where",
            "description": ""
          },
          "a": {
            "type": "sparse solution is expected and/or reached.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "parameter": {
            "type": "vector (w in the formulation formula)",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "coef_path_": {
            "type": "array",
            "description": "like of shape (n_features, n_alphas)"
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "alphas_": {
            "type": "array",
            "description": "like of shape (n_alphas,)"
          },
          "cv_alphas_": {
            "type": "array",
            "description": "like of shape (n_cv_alphas,)"
          },
          "all": {
            "type": "the values of alpha along the path for the different folds",
            "description": ""
          },
          "mse_path_": {
            "type": "array",
            "description": "like of shape (n_folds, n_cv_alphas)"
          },
          "n_iter_": {
            "type": "array",
            "description": "like or int"
          },
          "active_": {
            "type": "list of int",
            "description": ""
          },
          "Indices": {
            "type": "of active variables at the end of the path.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso",
            "description": ""
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as",
            "description": ""
          },
          "regularizer": {
            "type": "aka the Lasso",
            "description": "."
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting",
            "description": ""
          },
          "along": {
            "type": "a regularization path.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "LassoLarsIC": {
            "type": "Lasso model fit with Lars using BIC",
            "description": ""
          },
          "or": {
            "type": "AIC for model selection.",
            "description": "sklearn.decomposition.sparse_encode : Sparse coding.\nNotes\n-----"
          },
          "by": {
            "type": "itself. In general, because of this property, it will be more stable.",
            "description": "However, it is more fragile to heavily multicollinear datasets."
          },
          "It": {
            "type": "is more efficient than the :class:`~sklearn.linear_model.LassoCV` if",
            "description": ""
          },
          "only": {
            "type": "a small number of features are selected compared to the total number,",
            "description": ""
          },
          "In": {
            "type": "`fit`, once the best parameter `alpha` is found through",
            "description": "cross-validation, the model is fit again using the entire training set.\nExamples\n--------\n>>> from sklearn.linear_model import LassoLarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4.0, random_state=0)\n>>> reg = LassoLarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9993...\n>>> reg.alpha_\nnp.float64(0.3972...)\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[-78.4831...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    The object solves the same problem as the\n    :class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n    :class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\n    by itself. In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\n    only a small number of features are selected compared to the total number,\n    for instance if there are very few samples compared to the number of\n    features.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.alpha_\n    np.float64(0.3972...)\n    >>> reg.predict(X[:1,])\n    array([-78.4831...])",
        "notes": "-----\n    The object solves the same problem as the\n    :class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n    :class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\n    by itself. In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\n    only a small number of features are selected compared to the total number,\n    for instance if there are very few samples compared to the number of\n    features.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.alpha_\n    np.float64(0.3972...)\n    >>> reg.predict(X[:1,])\n    array([-78.4831...])",
        "examples": "--------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.alpha_\n    np.float64(0.3972...)\n    >>> reg.predict(X[:1,])\n    array([-78.4831...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **params)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._least_angle.LassoLarsCV, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "Xy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``Xy`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``Xy`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._least_angle.LassoLarsCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LassoLarsIC",
      "documentation": {
        "description": "Lasso model fit with Lars using BIC or AIC for model selection.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    AIC is the Akaike information criterion [2]_ and BIC is the Bayes\n    Information criterion [3]_. Such criteria are useful to select the value\n    of the regularization parameter by making a trade-off between the\n    goodness of fit and the complexity of the model. A good model should\n    explain well the data while being simple.\n\n    Read more in the :ref:`User Guide <lasso_lars_ic>`.\n\n    Parameters\n    ----------\n    criterion : {'aic', 'bic'}, default='aic'\n        The type of criterion to use.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    precompute : bool, 'auto' or array-like, default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform. Can be used for\n        early stopping.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsIC only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    noise_variance : float, default=None\n        The estimated noise variance of the data. If `None`, an unbiased\n        estimate is computed by an OLS model. However, it is only possible\n        in the case where `n_samples > n_features + fit_intercept`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    alpha_ : float\n        the alpha parameter chosen by the information criterion\n\n    alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller. If a list, it will be of length `n_targets`.\n\n    n_iter_ : int\n        number of iterations run by lars_path to find the grid of\n        alphas.\n\n    criterion_ : array-like of shape (n_alphas,)\n        The value of the information criteria ('aic', 'bic') across all\n        alphas. The alpha which has the smallest information criterion is\n        chosen, as specified in [1]_.\n\n    noise_variance_ : float\n        The estimated noise variance from the data used to compute the\n        criterion.\n\n        .. versionadded:: 1.1\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    The number of degrees of freedom is computed as in [1]_.\n\n    To have more details regarding the mathematical formulation of the\n    AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n\n    References\n    ----------\n    .. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n            \"On the degrees of freedom of the lasso.\"\n            The Annals of Statistics 35.5 (2007): 2173-2192.\n            <0712.0881>`\n\n    .. [2] `Wikipedia entry on the Akaike information criterion\n            <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n\n    .. [3] `Wikipedia entry on the Bayesian information criterion\n            <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_",
        "parameters": {
          "criterion": {
            "type": "{'aic', 'bic'}, default='aic'",
            "description": ""
          },
          "The": {
            "type": "Annals of Statistics 35.5 (2007): 2173-2192.",
            "description": "<0712.0881>`\n.. [2] `Wikipedia entry on the Akaike information criterion\n<https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n.. [3] `Wikipedia entry on the Bayesian information criterion\n<https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLarsIC(criterion='bic')\n>>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n>>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n>>> reg.fit(X, y)"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram matrix to speed up",
            "description": "calculations. If set to ``'auto'`` let us decide. The Gram"
          },
          "to": {
            "type": "the ordinary-least-squares solution for small values of alpha.",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": ""
          },
          "precompute": {
            "type": "bool, 'auto' or array",
            "description": "like, default='auto'"
          },
          "matrix": {
            "type": "can also be passed as argument.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=500",
            "description": ""
          },
          "Maximum": {
            "type": "of covariances (in absolute value) at each iteration.",
            "description": "``n_alphas`` is either ``max_iter``, ``n_features`` or the"
          },
          "early": {
            "type": "stopping.",
            "description": ""
          },
          "eps": {
            "type": "float, default=np.finfo(float).eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Unlike the ``tol`` parameter in some iterative\noptimization-based algorithms, this parameter does not control"
          },
          "the": {
            "type": "alpha parameter chosen by the information criterion",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "True, X will be copied; else, it may be overwritten.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Restrict": {
            "type": "coefficients to be >= 0. Be aware that you might want to",
            "description": ""
          },
          "remove": {
            "type": "fit_intercept which is set True by default.",
            "description": ""
          },
          "Under": {
            "type": "the positive restriction the model coefficients do not converge",
            "description": ""
          },
          "Only": {
            "type": "coefficients up to the smallest alpha value (``alphas_[alphas_ >",
            "description": "0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso"
          },
          "algorithm": {
            "type": "are typically in congruence with the solution of the",
            "description": ""
          },
          "coordinate": {
            "type": "descent Lasso estimator.",
            "description": ""
          },
          "As": {
            "type": "a consequence using LassoLarsIC only makes sense for problems where",
            "description": ""
          },
          "a": {
            "type": "sparse solution is expected and/or reached.",
            "description": ""
          },
          "noise_variance": {
            "type": "float, default=None",
            "description": ""
          },
          "estimate": {
            "type": "is computed by an OLS model. However, it is only possible",
            "description": ""
          },
          "in": {
            "type": "the case where `n_samples > n_features + fit_intercept`.",
            "description": ".. versionadded:: 1.1\nAttributes\n----------"
          },
          "coef_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "parameter": {
            "type": "vector (w in the formulation formula)",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "alphas_": {
            "type": "array",
            "description": "like of shape (n_alphas + 1,) or list of such arrays"
          },
          "number": {
            "type": "of iterations run by lars_path to find the grid of",
            "description": "alphas."
          },
          "is": {
            "type": "smaller. If a list, it will be of length `n_targets`.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "criterion_": {
            "type": "array",
            "description": "like of shape (n_alphas,)"
          },
          "noise_variance_": {
            "type": "float",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso",
            "description": ""
          },
          "path": {
            "type": "using LARS algorithm.",
            "description": ""
          },
          "lasso_path": {
            "type": "Compute Lasso path with coordinate descent.",
            "description": ""
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as",
            "description": ""
          },
          "regularizer": {
            "type": "aka the Lasso",
            "description": "."
          },
          "LassoCV": {
            "type": "Lasso linear model with iterative fitting",
            "description": ""
          },
          "along": {
            "type": "a regularization path.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso, using the LARS algorithm.\nsklearn.decomposition.sparse_encode : Sparse coding.\nNotes\n-----"
          },
          "To": {
            "type": "have more details regarding the mathematical formulation of the",
            "description": ""
          },
          "AIC": {
            "type": "and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.",
            "description": "References\n----------\n.. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n\"On the degrees of freedom of the lasso.\""
          },
          "LassoLarsIC": {
            "type": "criterion='bic'",
            "description": ">>> print(reg.coef_)\n[ 0.  -1.11...]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    The number of degrees of freedom is computed as in [1]_.\n\n    To have more details regarding the mathematical formulation of the\n    AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n\n    References\n    ----------\n    .. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n            \"On the degrees of freedom of the lasso.\"\n            The Annals of Statistics 35.5 (2007): 2173-2192.\n            <0712.0881>`\n\n    .. [2] `Wikipedia entry on the Akaike information criterion\n            <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n\n    .. [3] `Wikipedia entry on the Bayesian information criterion\n            <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLarsIC(criterion='bic')\n    >>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n    >>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n    >>> reg.fit(X, y)\n    LassoLarsIC(criterion='bic')\n    >>> print(reg.coef_)\n    [ 0.  -1.11...]",
        "notes": "-----\n    The number of degrees of freedom is computed as in [1]_.\n\n    To have more details regarding the mathematical formulation of the\n    AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n\n    References\n    ----------\n    .. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n            \"On the degrees of freedom of the lasso.\"\n            The Annals of Statistics 35.5 (2007): 2173-2192.\n            <0712.0881>`\n\n    .. [2] `Wikipedia entry on the Akaike information criterion\n            <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n\n    .. [3] `Wikipedia entry on the Bayesian information criterion\n            <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLarsIC(criterion='bic')\n    >>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n    >>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n    >>> reg.fit(X, y)\n    LassoLarsIC(criterion='bic')\n    >>> print(reg.coef_)\n    [ 0.  -1.11...]",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLarsIC(criterion='bic')\n    >>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n    >>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n    >>> reg.fit(X, y)\n    LassoLarsIC(criterion='bic')\n    >>> print(reg.coef_)\n    [ 0.  -1.11...]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, copy_X=None)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        copy_X : bool, default=None\n            If provided, this parameter will override the choice\n            of copy_X made at instance creation.\n            If ``True``, X will be copied; else, it may be overwritten.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=None",
                "description": ""
              },
              "If": {
                "type": "``True``, X will be copied; else, it may be overwritten.",
                "description": "Returns\n-------"
              },
              "of": {
                "type": "copy_X made at instance creation.",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._least_angle.LassoLarsIC, *, copy_X: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsIC",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "copy_X": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``copy_X`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        copy_X : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``copy_X`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._least_angle.LassoLarsIC, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsIC",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LinearRegression",
      "documentation": {
        "description": "Ordinary least squares Linear Regression.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n    to minimize the residual sum of squares between the observed targets in\n    the dataset, and the targets predicted by the linear approximation.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This will only provide\n        speedup in case of sufficiently large problems, that is if firstly\n        `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n        to `True`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive. This\n        option is only supported for dense arrays.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.\n        If multiple targets are passed during the fit (y 2D), this\n        is a 2D array of shape (n_targets, n_features), while if only\n        one target is passed, this is a 1D array of length n_features.\n\n    rank_ : int\n        Rank of matrix `X`. Only available when `X` is dense.\n\n    singular_ : array of shape (min(X, y),)\n        Singular values of `X`. Only available when `X` is dense.\n\n    intercept_ : float or array of shape (n_targets,)\n        Independent term in the linear model. Set to 0.0 if\n        `fit_intercept = False`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression addresses some of the\n        problems of Ordinary Least Squares by imposing a penalty on the\n        size of the coefficients with l2 regularization.\n    Lasso : The Lasso is a linear model that estimates\n        sparse coefficients with l1 regularization.\n    ElasticNet : Elastic-Net is a linear regression\n        model trained with both l1 and l2 -norm regularization of the\n        coefficients.\n\n    Notes\n    -----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n    (scipy.optimize.nnls) wrapped as a predictor object.",
        "parameters": {
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "`True`. ``None`` means 1 unless in a",
            "description": ":obj:`joblib.parallel_backend` context. ``-1`` means using all\nprocessors. See :term:`Glossary <n_jobs>` for more details."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "multiple targets are passed during the fit (y 2D), this",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "The": {
            "type": "number of jobs to use for the computation. This will only provide",
            "description": ""
          },
          "speedup": {
            "type": "in case of sufficiently large problems, that is if firstly",
            "description": "`n_targets > 1` and secondly `X` is sparse or if `positive` is set"
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive. This",
            "description": ""
          },
          "option": {
            "type": "is only supported for dense arrays.",
            "description": ".. versionadded:: 0.24\nAttributes\n----------"
          },
          "coef_": {
            "type": "array of shape (n_features, ) or (n_targets, n_features)",
            "description": ""
          },
          "Estimated": {
            "type": "coefficients for the linear regression problem.",
            "description": ""
          },
          "is": {
            "type": "a 2D array of shape (n_targets, n_features), while if only",
            "description": ""
          },
          "one": {
            "type": "target is passed, this is a 1D array of length n_features.",
            "description": ""
          },
          "rank_": {
            "type": "int",
            "description": ""
          },
          "Rank": {
            "type": "of matrix `X`. Only available when `X` is dense.",
            "description": ""
          },
          "singular_": {
            "type": "array of shape (min(X, y),)",
            "description": ""
          },
          "Singular": {
            "type": "values of `X`. Only available when `X` is dense.",
            "description": ""
          },
          "intercept_": {
            "type": "float or array of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in the linear model. Set to 0.0 if",
            "description": "`fit_intercept = False`."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "Ridge": {
            "type": "Ridge regression addresses some of the",
            "description": ""
          },
          "problems": {
            "type": "of Ordinary Least Squares by imposing a penalty on the",
            "description": ""
          },
          "size": {
            "type": "of the coefficients with l2 regularization.",
            "description": ""
          },
          "Lasso": {
            "type": "The Lasso is a linear model that estimates",
            "description": ""
          },
          "sparse": {
            "type": "coefficients with l1 regularization.",
            "description": ""
          },
          "ElasticNet": {
            "type": "Elastic",
            "description": "Net is a linear regression"
          },
          "model": {
            "type": "trained with both l1 and l2 -norm regularization of the",
            "description": "coefficients.\nNotes\n-----"
          },
          "From": {
            "type": "the implementation point of view, this is just plain Ordinary",
            "description": ""
          },
          "Least": {
            "type": "Squares (scipy.linalg.lstsq) or Non Negative Least Squares",
            "description": "(scipy.optimize.nnls) wrapped as a predictor object.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_"
          },
          "array": {
            "type": "[16.]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Ridge : Ridge regression addresses some of the\n        problems of Ordinary Least Squares by imposing a penalty on the\n        size of the coefficients with l2 regularization.\n    Lasso : The Lasso is a linear model that estimates\n        sparse coefficients with l1 regularization.\n    ElasticNet : Elastic-Net is a linear regression\n        model trained with both l1 and l2 -norm regularization of the\n        coefficients.\n\n    Notes\n    -----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n    (scipy.optimize.nnls) wrapped as a predictor object.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_\n    np.float64(3.0...)\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])",
        "notes": "-----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n    (scipy.optimize.nnls) wrapped as a predictor object.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_\n    np.float64(3.0...)\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_\n    np.float64(3.0...)\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.17\n               parameter *sample_weight* support to LinearRegression.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Individual": {
                "type": "weights for each sample.",
                "description": ".. versionadded:: 0.17"
              },
              "parameter": {
                "type": "*sample_weight* support to LinearRegression.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "Estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted Estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LogisticRegression",
      "documentation": {
        "description": "Logistic Regression (aka logit, MaxEnt) classifier.\n\n    This class implements regularized logistic regression using the\n    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n    that regularization is applied by default**. It can handle both dense\n    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n    floats for optimal performance; any other input format will be converted\n    (and copied).\n\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n    with primal formulation, or no regularization. The 'liblinear' solver\n    supports both L1 and L2 regularization, with a dual formulation only for\n    the L2 penalty. The Elastic-Net regularization is only supported by the\n    'saga' solver.\n\n    For :term:`multiclass` problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary\n    classification but can be extended to handle multiclass by using\n    :class:`~sklearn.multiclass.OneVsRestClassifier`.\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n        Specify the norm of the penalty:\n\n        - `None`: no penalty is added;\n        - `'l2'`: add a L2 penalty term and it is the default choice;\n        - `'l1'`: add a L1 penalty term;\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n        .. warning::\n           Some penalties may not work with some solvers. See the parameter\n           `solver` below, to know the compatibility between the penalty and\n           solver.\n\n        .. versionadded:: 0.19\n           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n    dual : bool, default=False\n        Dual (constrained) or primal (regularized, see also\n        :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    tol : float, default=1e-4\n        Tolerance for stopping criteria.\n\n    C : float, default=1.0\n        Inverse of regularization strength; must be a positive float.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    intercept_scaling : float, default=1\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           *class_weight='balanced'*\n\n    random_state : int, RandomState instance, default=None\n        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n        data. See :term:`Glossary <random_state>` for details.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n          and 'saga' are faster for large ones;\n        - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the\n          full multinomial loss;\n        - 'liblinear' can only handle binary classification by default. To apply a\n          one-versus-rest scheme for the multiclass setting one can wrap it with the\n          :class:`~sklearn.multiclass.OneVsRestClassifier`.\n        - 'newton-cholesky' is a good choice for\n          `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n          categorical features with rare categories. Be aware that the memory usage\n          of this solver has a quadratic dependency on `n_features * n_classes`\n          because it explicitly computes the full Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen and on\n           (multinomial) multiclass support:\n\n           ================= ============================== ======================\n           solver            penalty                        multinomial multiclass\n           ================= ============================== ======================\n           'lbfgs'           'l2', None                     yes\n           'liblinear'       'l1', 'l2'                     no\n           'newton-cg'       'l2', None                     yes\n           'newton-cholesky' 'l2', None                     no\n           'sag'             'l2', None                     yes\n           'saga'            'elasticnet', 'l1', 'l2', None yes\n           ================= ============================== ======================\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. seealso::\n           Refer to the :ref:`User Guide <Logistic_regression>` for more\n           information regarding :class:`LogisticRegression` and more specifically the\n           :ref:`Table <logistic_regression_solvers>`\n           summarizing solver/penalty supports.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.22\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n        .. deprecated:: 1.5\n           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n           From then on, the recommended 'multinomial' will always be used for\n           `n_classes >= 3`.\n           Solvers that do not support 'multinomial' will raise an error.\n           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n           still want to use OvR.\n\n    verbose : int, default=0\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int, default=None\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    l1_ratio : float, default=None\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n        combination of L1 and L2.\n\n    Attributes\n    ----------\n\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log_loss\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf",
        "parameters": {
          "penalty": {
            "type": "{'l1', 'l2', 'elasticnet', None}, default='l2'",
            "description": ""
          },
          "Specify": {
            "type": "the norm of the penalty:",
            "description": "- `None`: no penalty is added;\n- `'l2'`: add a L2 penalty term and it is the default choice;\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n.. warning::"
          },
          "Some": {
            "type": "penalties may not work with some solvers. See the parameter",
            "description": "`solver` below, to know the compatibility between the penalty and\nsolver.\n.. versionadded:: 0.19"
          },
          "l1": {
            "type": "penalty with SAGA solver (allowing 'multinomial' + L1)",
            "description": ""
          },
          "dual": {
            "type": "bool, default=False",
            "description": ""
          },
          "Dual": {
            "type": "constrained",
            "description": "or primal (regularized, see also\n:ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation"
          },
          "is": {
            "type": "only implemented for l2 penalty with liblinear solver. Prefer dual=False when",
            "description": ""
          },
          "n_samples": {
            "type": "> n_features.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Tolerance": {
            "type": "for stopping criteria.",
            "description": ""
          },
          "C": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Inverse": {
            "type": "of regularization strength; must be a positive float.",
            "description": ""
          },
          "Like": {
            "type": "in support vector machines, smaller values specify stronger",
            "description": "regularization."
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if a constant (a.k.a. bias or intercept) should be",
            "description": ""
          },
          "added": {
            "type": "to the decision function.",
            "description": ""
          },
          "intercept_scaling": {
            "type": "is appended to the instance vector.",
            "description": ""
          },
          "Useful": {
            "type": "only when the solver 'liblinear' is used",
            "description": ""
          },
          "and": {
            "type": "otherwise selects 'multinomial'.",
            "description": ".. versionadded:: 0.18"
          },
          "The": {
            "type": "underlying C implementation uses a random number generator to",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "To": {
            "type": "choose a solver, you might want to consider the following aspects:",
            "description": "- For small datasets, 'liblinear' is a good choice, whereas 'sag'"
          },
          "class_weight": {
            "type": "dict or 'balanced', default=None",
            "description": ""
          },
          "Weights": {
            "type": "associated with classes in the form ``{class_label: weight}``.",
            "description": ""
          },
          "If": {
            "type": "`fit_intercept` is set to False, the intercept is set to zero.",
            "description": "`intercept_` is of shape (1,) when the given problem is binary."
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "Note": {
            "type": "that these weights will be multiplied with sample_weight (passed",
            "description": ""
          },
          "through": {
            "type": "the fit method) if sample_weight is specified.",
            "description": ".. versionadded:: 0.17\n*class_weight='balanced'*"
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the",
            "description": "data. See :term:`Glossary <random_state>` for details."
          },
          "solver": {
            "type": "penalty                        multinomial multiclass",
            "description": "================= ============================== ======================\n'lbfgs'           'l2', None                     yes\n'liblinear'       'l1', 'l2'                     no\n'newton-cg'       'l2', None                     yes\n'newton-cholesky' 'l2', None                     no\n'sag'             'l2', None                     yes\n'saga'            'elasticnet', 'l1', 'l2', None yes\n================= ============================== ======================\n.. note::\n'sag' and 'saga' fast convergence is only guaranteed on features"
          },
          "Algorithm": {
            "type": "to use in the optimization problem. Default is 'lbfgs'.",
            "description": ""
          },
          "full": {
            "type": "multinomial loss;",
            "description": "- 'liblinear' can only handle binary classification by default. To apply a\none-versus-rest scheme for the multiclass setting one can wrap it with the\n:class:`~sklearn.multiclass.OneVsRestClassifier`.\n- 'newton-cholesky' is a good choice for\n`n_samples` >> `n_features * n_classes`, especially with one-hot encoded"
          },
          "categorical": {
            "type": "features with rare categories. Be aware that the memory usage",
            "description": ""
          },
          "of": {
            "type": "this solver has a quadratic dependency on `n_features * n_classes`",
            "description": ""
          },
          "because": {
            "type": "it explicitly computes the full Hessian matrix.",
            "description": ".. warning::"
          },
          "with": {
            "type": "approximately the same scale. You can preprocess the data with",
            "description": ""
          },
          "a": {
            "type": "scaler from :mod:`sklearn.preprocessing`.",
            "description": ".. seealso::"
          },
          "Refer": {
            "type": "to the :ref:`User Guide <Logistic_regression>` for more",
            "description": ""
          },
          "information": {
            "type": "regarding :class:`LogisticRegression` and more specifically the",
            "description": ":ref:`Table <logistic_regression_solvers>`"
          },
          "summarizing": {
            "type": "solver/penalty supports.",
            "description": ".. versionadded:: 0.17"
          },
          "Stochastic": {
            "type": "Average Gradient descent solver for 'multinomial' case.",
            "description": ".. versionchanged:: 0.22"
          },
          "SAGA": {
            "type": "",
            "description": "Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n:arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support"
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations taken for the solvers to converge.",
            "description": ""
          },
          "multi_class": {
            "type": "{'auto', 'ovr', 'multinomial'}, default='auto'",
            "description": ""
          },
          "across": {
            "type": "the entire probability distribution, *even when the data is",
            "description": "binary*. 'multinomial' is unavailable when solver='liblinear'.\n'auto' selects 'ovr' if the data is binary, or if solver='liblinear',"
          },
          "Default": {
            "type": "changed from 'ovr' to 'auto' in 0.22.",
            "description": ".. deprecated:: 1.5\n``multi_class`` was deprecated in version 1.5 and will be removed in 1.7."
          },
          "From": {
            "type": "then on, the recommended 'multinomial' will always be used for",
            "description": "`n_classes >= 3`."
          },
          "Solvers": {
            "type": "that do not support 'multinomial' will raise an error.",
            "description": ""
          },
          "Use": {
            "type": "`sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you",
            "description": ""
          },
          "still": {
            "type": "want to use OvR.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "For": {
            "type": "a comaprison of the LogisticRegression with other classifiers see:",
            "description": ":ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`."
          },
          "number": {
            "type": "of iteration across all classes is given.",
            "description": ".. versionchanged:: 0.20"
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, reuse the solution of the previous call to fit as",
            "description": "initialization, otherwise, just erase the previous solution."
          },
          "Useless": {
            "type": "for liblinear solver. See :term:`the Glossary <warm_start>`.",
            "description": ".. versionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "set": {
            "type": "to 'liblinear' regardless of whether 'multi_class' is specified or",
            "description": "not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "l1_ratio": {
            "type": "float, default=None",
            "description": ""
          },
          "used": {
            "type": "if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent",
            "description": ""
          },
          "to": {
            "type": "have slightly different results for the same input data. If",
            "description": ""
          },
          "combination": {
            "type": "of L1 and L2.",
            "description": "Attributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes, )",
            "description": ""
          },
          "A": {
            "type": "list of class labels known to the classifier.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
            "description": ""
          },
          "Coefficient": {
            "type": "of the features in the decision function.",
            "description": "`coef_` is of shape (1, n_features) when the given problem is binary."
          },
          "In": {
            "type": "SciPy <= 1.0.0 the number of lbfgs iterations may exceed",
            "description": "``max_iter``. ``n_iter_`` will now report at most ``max_iter``."
          },
          "intercept_": {
            "type": "ndarray of shape (1,) or (n_classes,)",
            "description": ""
          },
          "Intercept": {
            "type": "a.k.a. bias",
            "description": "added to the decision function."
          },
          "corresponds": {
            "type": "to outcome 1 (True) and `-intercept_` corresponds to",
            "description": ""
          },
          "outcome": {
            "type": "0 (False).",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "ndarray of shape (n_classes,) or (1, )",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations for all classes. If binary or multinomial,",
            "description": ""
          },
          "it": {
            "type": "returns only 1 element. For liblinear solver, only the maximum",
            "description": ""
          },
          "SGDClassifier": {
            "type": "Incrementally trained logistic regression (when given",
            "description": ""
          },
          "the": {
            "type": "parameter ``loss=\"log_loss\"``).",
            "description": ""
          },
          "LogisticRegressionCV": {
            "type": "Logistic regression with built",
            "description": "in cross validation.\nNotes\n-----"
          },
          "select": {
            "type": "features when fitting the model. It is thus not uncommon,",
            "description": ""
          },
          "that": {
            "type": "happens, try with a smaller tol parameter.",
            "description": ""
          },
          "Predict": {
            "type": "output may not match that of standalone liblinear in certain",
            "description": "cases. See :ref:`differences from liblinear <liblinear_differences>`"
          },
          "in": {
            "type": "the narrative documentation.",
            "description": "References\n----------\nL-BFGS-B -- Software for Large-scale Bound-constrained Optimization"
          },
          "Ciyou": {
            "type": "Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.",
            "description": ""
          },
          "http": {
            "type": "//users.iems.northwestern.edu/~nocedal/lbfgsb.html",
            "description": ""
          },
          "LIBLINEAR": {
            "type": "",
            "description": "A Library for Large Linear Classification"
          },
          "https": {
            "type": "//www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])"
          },
          "SAG": {
            "type": "",
            "description": "Mark Schmidt, Nicolas Le Roux, and Francis Bach"
          },
          "Minimizing": {
            "type": "Finite Sums with the Stochastic Average Gradient",
            "description": ""
          },
          "for": {
            "type": "Non-Strongly Convex Composite Objectives\" <1407.0202>`",
            "description": "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent"
          },
          "methods": {
            "type": "for logistic regression and maximum entropy models.",
            "description": ""
          },
          "Machine": {
            "type": "Learning 85(1-2):41-75.",
            "description": ""
          },
          "array": {
            "type": "[0, 0]",
            "description": ">>> clf.predict_proba(X[:2, :])\narray([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n[9.7...e-01, 2.8...e-02, ...e-08]])\n>>> clf.score(X, y)\n0.97..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log_loss\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n\n    For a comaprison of the LogisticRegression with other classifiers see:\n    :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.",
        "notes": "that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           *class_weight='balanced'*\n\n    random_state : int, RandomState instance, default=None\n        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n        data. See :term:`Glossary <random_state>` for details.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n          and 'saga' are faster for large ones;\n        - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the\n          full multinomial loss;\n        - 'liblinear' can only handle binary classification by default. To apply a\n          one-versus-rest scheme for the multiclass setting one can wrap it with the\n          :class:`~sklearn.multiclass.OneVsRestClassifier`.\n        - 'newton-cholesky' is a good choice for\n          `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n          categorical features with rare categories. Be aware that the memory usage\n          of this solver has a quadratic dependency on `n_features * n_classes`\n          because it explicitly computes the full Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen and on\n           (multinomial) multiclass support:\n\n           ================= ============================== ======================\n           solver            penalty                        multinomial multiclass\n           ================= ============================== ======================\n           'lbfgs'           'l2', None                     yes\n           'liblinear'       'l1', 'l2'                     no\n           'newton-cg'       'l2', None                     yes\n           'newton-cholesky' 'l2', None                     no\n           'sag'             'l2', None                     yes\n           'saga'            'elasticnet', 'l1', 'l2', None yes\n           ================= ============================== ======================\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. seealso::\n           Refer to the :ref:`User Guide <Logistic_regression>` for more\n           information regarding :class:`LogisticRegression` and more specifically the\n           :ref:`Table <logistic_regression_solvers>`\n           summarizing solver/penalty supports.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.22\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n        .. deprecated:: 1.5\n           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n           From then on, the recommended 'multinomial' will always be used for\n           `n_classes >= 3`.\n           Solvers that do not support 'multinomial' will raise an error.\n           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n           still want to use OvR.\n\n    verbose : int, default=0\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int, default=None\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    l1_ratio : float, default=None\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n        combination of L1 and L2.\n\n    Attributes\n    ----------\n\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log_loss\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n\n    For a comaprison of the LogisticRegression with other classifiers see:\n    :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.",
        "examples": "--------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n\n    For a comaprison of the LogisticRegression with other classifiers see:\n    :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`."
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like of shape (n_samples,) default=None\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n            .. versionadded:: 0.17\n               *sample_weight* support to LogisticRegression.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "vector relative to X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,) default=None"
              },
              "Array": {
                "type": "of weights that are assigned to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, then each sample is given unit weight.",
                "description": ".. versionadded:: 0.17\n*sample_weight* support to LogisticRegression.\nReturns\n-------\nself"
              },
              "Fitted": {
                "type": "estimator.",
                "description": "Notes\n-----"
              },
              "The": {
                "type": "SAGA solver supports both float64 and float32 bit arrays.",
                "description": ""
              }
            },
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        The SAGA solver supports both float64 and float32 bit arrays.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The SAGA solver supports both float64 and float32 bit arrays.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Predict logarithm of probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Vector": {
                "type": "to be scored, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "T": {
                "type": "array",
                "description": "like of shape (n_samples, n_classes)"
              },
              "Returns": {
                "type": "the log-probability of the sample for each class in the",
                "description": "model, where classes are ordered as they are in ``self.classes_``."
              }
            },
            "returns": "-------\n        T : array-like of shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        For a multi_class problem, if multi_class is set to be \"multinomial\"\n        the softmax function is used to find the predicted probability of\n        each class.\n        Else use a one-vs-rest approach, i.e. calculate the probability\n        of each class assuming it to be positive using the logistic function\n        and normalize these values across all the classes.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Vector": {
                "type": "to be scored, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "T": {
                "type": "array",
                "description": "like of shape (n_samples, n_classes)"
              },
              "Returns": {
                "type": "the probability of the sample for each class in the model,",
                "description": ""
              },
              "where": {
                "type": "classes are ordered as they are in ``self.classes_``.",
                "description": ""
              }
            },
            "returns": "-------\n        T : array-like of shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LogisticRegressionCV",
      "documentation": {
        "description": "Logistic Regression CV (aka logit, MaxEnt) classifier.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    This class implements logistic regression using liblinear, newton-cg, sag\n    or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n    regularization with primal formulation. The liblinear solver supports both\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n    Elastic-Net penalty is only supported by the saga solver.\n\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n    is selected by the cross-validator\n    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    Cs : int or list of floats, default=10\n        Each of the values in Cs describes the inverse of regularization\n        strength. If Cs is as an int, then a grid of Cs values are chosen\n        in a logarithmic scale between 1e-4 and 1e4.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    cv : int or cross-validation generator, default=None\n        The default cross-validation generator used is Stratified K-Folds.\n        If an integer is provided, then it is the number of folds used.\n        See the module :mod:`sklearn.model_selection` module for the\n        list of possible cross-validation objects.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    dual : bool, default=False\n        Dual (constrained) or primal (regularized, see also\n        :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n        Specify the norm of the penalty:\n\n        - `'l2'`: add a L2 penalty term (used by default);\n        - `'l1'`: add a L1 penalty term;\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n        .. warning::\n           Some penalties may not work with some solvers. See the parameter\n           `solver` below, to know the compatibility between the penalty and\n           solver.\n\n    scoring : str or callable, default=None\n        A string (see :ref:`scoring_parameter`) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. For a list of scoring functions\n        that can be used, look at :mod:`sklearn.metrics`. The\n        default scoring option used is 'accuracy'.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n          and 'saga' are faster for large ones;\n        - For multiclass problems, all solvers except 'liblinear' minimize the full\n          multinomial loss;\n        - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n          because it does not handle warm-starting.\n        - 'liblinear' can only handle binary classification by default. To apply a\n          one-versus-rest scheme for the multiclass setting one can wrap it with the\n          :class:`~sklearn.multiclass.OneVsRestClassifier`.\n        - 'newton-cholesky' is a good choice for\n          `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n          categorical features with rare categories. Be aware that the memory usage\n          of this solver has a quadratic dependency on `n_features * n_classes`\n          because it explicitly computes the full Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen and on\n           (multinomial) multiclass support:\n\n           ================= ============================== ======================\n           solver            penalty                        multinomial multiclass\n           ================= ============================== ======================\n           'lbfgs'           'l2'                           yes\n           'liblinear'       'l1', 'l2'                     no\n           'newton-cg'       'l2'                           yes\n           'newton-cholesky' 'l2',                          no\n           'sag'             'l2',                          yes\n           'saga'            'elasticnet', 'l1', 'l2'       yes\n           ================= ============================== ======================\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    tol : float, default=1e-4\n        Tolerance for stopping criteria.\n\n    max_iter : int, default=100\n        Maximum number of iterations of the optimization algorithm.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           class_weight == 'balanced'\n\n    n_jobs : int, default=None\n        Number of CPU cores used during the cross-validation loop.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n        positive number for verbosity.\n\n    refit : bool, default=True\n        If set to True, the scores are averaged across all folds, and the\n        coefs and the C that corresponds to the best score is taken, and a\n        final refit is done using these parameters.\n        Otherwise the coefs, intercepts and C that correspond to the\n        best scores across folds are averaged.\n\n    intercept_scaling : float, default=1\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n        .. deprecated:: 1.5\n           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n           From then on, the recommended 'multinomial' will always be used for\n           `n_classes >= 3`.\n           Solvers that do not support 'multinomial' will raise an error.\n           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you\n           still want to use OvR.\n\n    random_state : int, RandomState instance, default=None\n        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n        Note that this only applies to the solver and not the cross-validation\n        generator. See :term:`Glossary <random_state>` for details.\n\n    l1_ratios : list of float, default=None\n        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n        using ``penalty='l2'``, while 1 is equivalent to using\n        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n        of L1 and L2.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem\n        is binary.\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape(1,) when the problem is binary.\n\n    Cs_ : ndarray of shape (n_cs)\n        Array of C i.e. inverse of regularization parameter values used\n        for cross-validation.\n\n    l1_ratios_ : ndarray of shape (n_l1_ratios)\n        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\n    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n        dict with classes as the keys, and the path of coefficients obtained\n        during cross-validating across each fold and then across each Cs\n        after doing an OvR for the corresponding class as values.\n        If the 'multi_class' option is set to 'multinomial', then\n        the coefs_paths are the coefficients corresponding to each class.\n        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n\n    scores_ : dict\n        dict with classes as the keys, and the values as the\n        grid of scores obtained during cross-validating each fold, after doing\n        an OvR for the corresponding class. If the 'multi_class' option\n        given is 'multinomial' then the same scores are repeated across\n        all classes, since this is the multinomial class. Each dict value\n        has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n        ``penalty='elasticnet'``.\n\n    C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n        Array of C that maps to the best scores across every class. If refit is\n        set to False, then for each class, the best C is the average of the\n        C's that correspond to the best scores for each fold.\n        `C_` is of shape(n_classes,) when the problem is binary.\n\n    l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n        Array of l1_ratio that maps to the best scores across every class. If\n        refit is set to False, then for each class, the best l1_ratio is the\n        average of the l1_ratio's that correspond to the best scores for each\n        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n\n    n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n        Actual number of iterations for all classes, folds and Cs.\n        In the binary or multinomial cases, the first dimension is equal to 1.\n        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    LogisticRegression : Logistic regression without tuning the\n        hyperparameter `C`.",
        "parameters": {
          "Cs": {
            "type": "int or list of floats, default=10",
            "description": ""
          },
          "Each": {
            "type": "dict value has shape ``(n_folds, n_cs, n_features)`` or",
            "description": "``(n_folds, n_cs, n_features + 1)`` depending on whether the"
          },
          "in": {
            "type": "a logarithmic scale between 1e-4 and 1e4.",
            "description": ""
          },
          "Like": {
            "type": "in support vector machines, smaller values specify stronger",
            "description": "regularization."
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if a constant (a.k.a. bias or intercept) should be",
            "description": ""
          },
          "added": {
            "type": "to the decision function.",
            "description": ""
          },
          "cv": {
            "type": "int or cross",
            "description": "validation generator, default=None"
          },
          "The": {
            "type": "list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.",
            "description": ""
          },
          "If": {
            "type": "``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,",
            "description": "n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "list": {
            "type": "of possible cross-validation objects.",
            "description": ".. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "dual": {
            "type": "bool, default=False",
            "description": ""
          },
          "Dual": {
            "type": "constrained",
            "description": "or primal (regularized, see also\n:ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation"
          },
          "is": {
            "type": "binary.",
            "description": ""
          },
          "n_samples": {
            "type": "> n_features.",
            "description": ""
          },
          "penalty": {
            "type": "{'l1', 'l2', 'elasticnet'}, default='l2'",
            "description": ""
          },
          "Specify": {
            "type": "the norm of the penalty:",
            "description": "- `'l2'`: add a L2 penalty term (used by default);\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n.. warning::"
          },
          "Some": {
            "type": "penalties may not work with some solvers. See the parameter",
            "description": "`solver` below, to know the compatibility between the penalty and\nsolver."
          },
          "scoring": {
            "type": "str or callable, default=None",
            "description": ""
          },
          "A": {
            "type": "list of class labels known to the classifier.",
            "description": ""
          },
          "a": {
            "type": "scaler from :mod:`sklearn.preprocessing`.",
            "description": ".. versionadded:: 0.17"
          },
          "that": {
            "type": "can be used, look at :mod:`sklearn.metrics`. The",
            "description": ""
          },
          "default": {
            "type": "scoring option used is 'accuracy'.",
            "description": ""
          },
          "solver": {
            "type": "penalty                        multinomial multiclass",
            "description": "================= ============================== ======================\n'lbfgs'           'l2'                           yes\n'liblinear'       'l1', 'l2'                     no\n'newton-cg'       'l2'                           yes\n'newton-cholesky' 'l2',                          no\n'sag'             'l2',                          yes\n'saga'            'elasticnet', 'l1', 'l2'       yes\n================= ============================== ======================\n.. note::\n'sag' and 'saga' fast convergence is only guaranteed on features"
          },
          "Algorithm": {
            "type": "to use in the optimization problem. Default is 'lbfgs'.",
            "description": ""
          },
          "To": {
            "type": "lessen the effect of regularization on synthetic feature weight",
            "description": "(and therefore on the intercept) intercept_scaling has to be increased."
          },
          "and": {
            "type": "otherwise selects 'multinomial'.",
            "description": ".. versionadded:: 0.18"
          },
          "multinomial": {
            "type": "loss;",
            "description": "- 'liblinear' might be slower in :class:`LogisticRegressionCV`"
          },
          "because": {
            "type": "it explicitly computes the full Hessian matrix.",
            "description": ".. warning::"
          },
          "categorical": {
            "type": "features with rare categories. Be aware that the memory usage",
            "description": ""
          },
          "of": {
            "type": "L1 and L2.",
            "description": "Attributes\n----------"
          },
          "with": {
            "type": "approximately the same scale. You can preprocess the data with",
            "description": ""
          },
          "Stochastic": {
            "type": "Average Gradient descent solver for 'multinomial' case.",
            "description": ".. versionchanged:: 0.22"
          },
          "SAGA": {
            "type": "solver.",
            "description": ".. versionadded:: 1.2\nnewton-cholesky solver."
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Tolerance": {
            "type": "for stopping criteria.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations of the optimization algorithm.",
            "description": ""
          },
          "class_weight": {
            "type": "== 'balanced'",
            "description": ""
          },
          "Weights": {
            "type": "associated with classes in the form ``{class_label: weight}``.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "all other features.",
            "description": ""
          },
          "Note": {
            "type": "that this only applies to the solver and not the cross-validation",
            "description": "generator. See :term:`Glossary <random_state>` for details."
          },
          "through": {
            "type": "the fit method) if sample_weight is specified.",
            "description": ".. versionadded:: 0.17"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "for": {
            "type": "cross-validation.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "For": {
            "type": "the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any",
            "description": ""
          },
          "positive": {
            "type": "number for verbosity.",
            "description": ""
          },
          "refit": {
            "type": "is set to False, then for each class, the best l1_ratio is the",
            "description": ""
          },
          "coefs": {
            "type": "and the C that corresponds to the best score is taken, and a",
            "description": ""
          },
          "final": {
            "type": "refit is done using these parameters.",
            "description": ""
          },
          "Otherwise": {
            "type": "the coefs, intercepts and C that correspond to the",
            "description": ""
          },
          "best": {
            "type": "scores across folds are averaged.",
            "description": ""
          },
          "intercept_scaling": {
            "type": "is appended to the instance vector.",
            "description": ""
          },
          "Useful": {
            "type": "only when the solver 'liblinear' is used",
            "description": ""
          },
          "multi_class": {
            "type": "{'auto, 'ovr', 'multinomial'}, default='auto'",
            "description": ""
          },
          "across": {
            "type": "the entire probability distribution, *even when the data is",
            "description": "binary*. 'multinomial' is unavailable when solver='liblinear'.\n'auto' selects 'ovr' if the data is binary, or if solver='liblinear',"
          },
          "Default": {
            "type": "changed from 'ovr' to 'auto' in 0.22.",
            "description": ".. deprecated:: 1.5\n``multi_class`` was deprecated in version 1.5 and will be removed in 1.7."
          },
          "From": {
            "type": "then on, the recommended 'multinomial' will always be used for",
            "description": "`n_classes >= 3`."
          },
          "Solvers": {
            "type": "that do not support 'multinomial' will raise an error.",
            "description": ""
          },
          "Use": {
            "type": "`sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you",
            "description": ""
          },
          "still": {
            "type": "want to use OvR.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.",
            "description": ""
          },
          "l1_ratios": {
            "type": "list of float, default=None",
            "description": ""
          },
          "Only": {
            "type": "used if ``penalty='elasticnet'``. A value of 0 is equivalent to",
            "description": ""
          },
          "using": {
            "type": "``penalty='l2'``, while 1 is equivalent to using",
            "description": "``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes, )",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
            "description": ""
          },
          "Coefficient": {
            "type": "of the features in the decision function.",
            "description": "`coef_` is of shape (1, n_features) when the given problem"
          },
          "intercept_": {
            "type": "ndarray of shape (1,) or (n_classes,)",
            "description": ""
          },
          "Intercept": {
            "type": "a.k.a. bias",
            "description": "added to the decision function."
          },
          "Cs_": {
            "type": "ndarray of shape (n_cs)",
            "description": ""
          },
          "Array": {
            "type": "of l1_ratio that maps to the best scores across every class. If",
            "description": ""
          },
          "l1_ratios_": {
            "type": "ndarray of shape (n_l1_ratios)",
            "description": ""
          },
          "coefs_paths_": {
            "type": "ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)",
            "description": ""
          },
          "dict": {
            "type": "with classes as the keys, and the values as the",
            "description": ""
          },
          "during": {
            "type": "cross-validating across each fold and then across each Cs",
            "description": ""
          },
          "after": {
            "type": "doing an OvR for the corresponding class as values.",
            "description": ""
          },
          "the": {
            "type": "coefs_paths are the coefficients corresponding to each class.",
            "description": ""
          },
          "intercept": {
            "type": "is fit or not. If ``penalty='elasticnet'``, the shape is",
            "description": "``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``."
          },
          "scores_": {
            "type": "dict",
            "description": ""
          },
          "grid": {
            "type": "of scores obtained during cross-validating each fold, after doing",
            "description": ""
          },
          "an": {
            "type": "OvR for the corresponding class. If the 'multi_class' option",
            "description": ""
          },
          "given": {
            "type": "is 'multinomial' then the same scores are repeated across",
            "description": ""
          },
          "all": {
            "type": "classes, since this is the multinomial class. Each dict value",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "C_": {
            "type": "ndarray of shape (n_classes,) or (n_classes",
            "description": "1,)"
          },
          "set": {
            "type": "to False, then for each class, the best C is the average of the",
            "description": "C's that correspond to the best scores for each fold.\n`C_` is of shape(n_classes,) when the problem is binary."
          },
          "l1_ratio_": {
            "type": "ndarray of shape (n_classes,) or (n_classes",
            "description": "1,)"
          },
          "average": {
            "type": "of the l1_ratio's that correspond to the best scores for each",
            "description": "fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary."
          },
          "n_iter_": {
            "type": "ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations for all classes, folds and Cs.",
            "description": ""
          },
          "In": {
            "type": "the binary or multinomial cases, the first dimension is equal to 1.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "LogisticRegression": {
            "type": "Logistic regression without tuning the",
            "description": ""
          },
          "hyperparameter": {
            "type": "`C`.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegressionCV\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])"
          },
          "array": {
            "type": "[0, 0]",
            "description": ">>> clf.predict_proba(X[:2, :]).shape\n(2, 3)\n>>> clf.score(X, y)\n0.98..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    LogisticRegression : Logistic regression without tuning the\n        hyperparameter `C`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegressionCV\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :]).shape\n    (2, 3)\n    >>> clf.score(X, y)\n    0.98...",
        "notes": "that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           class_weight == 'balanced'\n\n    n_jobs : int, default=None\n        Number of CPU cores used during the cross-validation loop.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n        positive number for verbosity.\n\n    refit : bool, default=True\n        If set to True, the scores are averaged across all folds, and the\n        coefs and the C that corresponds to the best score is taken, and a\n        final refit is done using these parameters.\n        Otherwise the coefs, intercepts and C that correspond to the\n        best scores across folds are averaged.\n\n    intercept_scaling : float, default=1\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n        .. deprecated:: 1.5\n           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n           From then on, the recommended 'multinomial' will always be used for\n           `n_classes >= 3`.\n           Solvers that do not support 'multinomial' will raise an error.\n           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you\n           still want to use OvR.\n\n    random_state : int, RandomState instance, default=None\n        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.",
        "examples": "--------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegressionCV\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :]).shape\n    (2, 3)\n    >>> clf.score(X, y)\n    0.98..."
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **params)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like of shape (n_samples,) default=None\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n        **params : dict\n            Parameters to pass to the underlying splitter and scorer.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "vector relative to X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,) default=None"
              },
              "Array": {
                "type": "of weights that are assigned to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, then each sample is given unit weight.",
                "description": "**params : dict"
              }
            },
            "returns": "-------\n        self : object\n            Fitted LogisticRegressionCV estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Predict logarithm of probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Vector": {
                "type": "to be scored, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "T": {
                "type": "array",
                "description": "like of shape (n_samples, n_classes)"
              },
              "Returns": {
                "type": "the log-probability of the sample for each class in the",
                "description": "model, where classes are ordered as they are in ``self.classes_``."
              }
            },
            "returns": "-------\n        T : array-like of shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        For a multi_class problem, if multi_class is set to be \"multinomial\"\n        the softmax function is used to find the predicted probability of\n        each class.\n        Else use a one-vs-rest approach, i.e. calculate the probability\n        of each class assuming it to be positive using the logistic function\n        and normalize these values across all the classes.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Vector": {
                "type": "to be scored, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "T": {
                "type": "array",
                "description": "like of shape (n_samples, n_classes)"
              },
              "Returns": {
                "type": "the probability of the sample for each class in the model,",
                "description": ""
              },
              "where": {
                "type": "classes are ordered as they are in ``self.classes_``.",
                "description": ""
              }
            },
            "returns": "-------\n        T : array-like of shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None, **score_params)",
          "documentation": {
            "description": "Score using the `scoring` option on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        **score_params : dict\n            Parameters to pass to the `score` method of the underlying scorer.\n\n            .. versionadded:: 1.4",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "True": {
                "type": "labels for X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "**score_params : dict"
              }
            },
            "returns": "-------\n        score : float\n            Score of self.predict(X) w.r.t. y.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MultiTaskElasticNet",
      "documentation": {
        "description": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n\n    The optimization objective for MultiTaskElasticNet is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n\n    i.e. the sum of norms of each row.\n\n    Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\n    l1_ratio : float, default=0.5\n        The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n        is an L2 penalty.\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    intercept_ : ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    coef_ : ndarray of shape (n_targets, n_features)\n        Parameter vector (W in the cost function formula). If a 1D y is\n        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    dual_gap_ : float\n        The dual gaps at the end of the optimization.\n\n    eps_ : float\n        The tolerance scaled scaled by the variance of the target `y`.\n\n    sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n        Sparse representation of the `coef_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    MultiTaskLasso : Multi-task Lasso model trained with L1/L2\n        mixed-norm as regularizer.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L1/L2 term. Defaults to 1.0.",
            "description": ""
          },
          "l1_ratio": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "algorithm used to fit the model is coordinate descent.",
            "description": ""
          },
          "For": {
            "type": "``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.",
            "description": ""
          },
          "is": {
            "type": "an L2 penalty.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "set to 'random', a random coefficient is updated every iteration",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, reuse the solution of the previous call to fit as",
            "description": "initialization, otherwise, just erase the previous solution."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "intercept_": {
            "type": "ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (W in the cost function formula). If a 1D y is",
            "description": ""
          },
          "passed": {
            "type": "in at fit (non multi-task usage), ``coef_`` is then a 1D array.",
            "description": ""
          },
          "Note": {
            "type": "that ``coef_`` stores the transpose of ``W``, ``W.T``.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "the": {
            "type": "specified tolerance.",
            "description": ""
          },
          "dual_gap_": {
            "type": "float",
            "description": ""
          },
          "eps_": {
            "type": "float",
            "description": ""
          },
          "sparse_coef_": {
            "type": "sparse matrix of shape (n_features,) or             (n_targets, n_features)",
            "description": ""
          },
          "Sparse": {
            "type": "representation of the `coef_`.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "MultiTaskElasticNetCV": {
            "type": "Multi",
            "description": "task L1/L2 ElasticNet with built-in\ncross-validation."
          },
          "ElasticNet": {
            "type": "Linear regression with combined L1 and L2 priors as regularizer.",
            "description": ""
          },
          "MultiTaskLasso": {
            "type": "Multi",
            "description": "task Lasso model trained with L1/L2\nmixed-norm as regularizer.\nNotes\n-----"
          },
          "To": {
            "type": "avoid unnecessary memory duplication the X and y arguments of the fit",
            "description": ""
          },
          "method": {
            "type": "should be directly passed as Fortran-contiguous numpy arrays.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])"
          },
          "MultiTaskElasticNet": {
            "type": "alpha=0.1",
            "description": ">>> print(clf.coef_)\n[[0.45663524 0.45612256]\n[0.45663524 0.45612256]]\n>>> print(clf.intercept_)\n[0.0872422 0.0872422]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    MultiTaskLasso : Multi-task Lasso model trained with L1/L2\n        mixed-norm as regularizer.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNet(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.45663524 0.45612256]\n     [0.45663524 0.45612256]]\n    >>> print(clf.intercept_)\n    [0.0872422 0.0872422]",
        "notes": "that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    dual_gap_ : float\n        The dual gaps at the end of the optimization.\n\n    eps_ : float\n        The tolerance scaled scaled by the variance of the target `y`.\n\n    sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n        Sparse representation of the `coef_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    MultiTaskLasso : Multi-task Lasso model trained with L1/L2\n        mixed-norm as regularizer.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNet(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.45663524 0.45612256]\n     [0.45663524 0.45612256]]\n    >>> print(clf.intercept_)\n    [0.0872422 0.0872422]",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNet(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.45663524 0.45612256]\n     [0.45663524 0.45612256]]\n    >>> print(clf.intercept_)\n    [0.0872422 0.0872422]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit MultiTaskElasticNet model with coordinate descent.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Data.\n        y : ndarray of shape (n_samples, n_targets)\n            Target. Will be cast to X's dtype if necessary.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": "Data."
              },
              "y": {
                "type": "ndarray of shape (n_samples, n_targets)",
                "description": "Target. Will be cast to X's dtype if necessary.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": "Notes\n-----"
              },
              "Coordinate": {
                "type": "descent is an algorithm that considers each column of",
                "description": ""
              },
              "data": {
                "type": "at a time hence it will automatically convert the X input",
                "description": ""
              },
              "as": {
                "type": "a Fortran-contiguous numpy array if necessary.",
                "description": ""
              },
              "To": {
                "type": "avoid memory re-allocation it is advised to allocate the",
                "description": ""
              },
              "initial": {
                "type": "data in memory directly using that format.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNet",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNet",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MultiTaskElasticNetCV",
      "documentation": {
        "description": "Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for MultiTaskElasticNet is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    l1_ratio : float or list of float, default=0.5\n        The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n        is an L2 penalty.\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n        This parameter can be a list, in which case the different\n        values are tested by cross-validation and the one giving the best\n        prediction score is used. Note that a good choice of list of\n        values for l1_ratio is often to put more values close to 1\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n        .9, .95, .99, 1]``.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If not provided, set automatically.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - int, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    verbose : bool or int, default=0\n        Amount of verbosity.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation. Note that this is\n        used only if multiple values for l1_ratio are given.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    intercept_ : ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    coef_ : ndarray of shape (n_targets, n_features)\n        Parameter vector (W in the cost function formula).\n        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n        Mean square error for the test set on each fold, varying alpha.\n\n    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n        The grid of alphas used for fitting, for each l1_ratio.\n\n    l1_ratio_ : float\n        Best l1_ratio obtained by cross-validation.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    dual_gap_ : float\n        The dual gap at the end of the optimization for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm\n        as regularizer and built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.",
        "parameters": {
          "l1_ratio": {
            "type": "float or list of float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "algorithm used to fit the model is coordinate descent.",
            "description": ""
          },
          "For": {
            "type": "int/None inputs, :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "is": {
            "type": "an L2 penalty.",
            "description": ""
          },
          "This": {
            "type": "parameter can be a list, in which case the different",
            "description": ""
          },
          "values": {
            "type": "for l1_ratio is often to put more values close to 1",
            "description": "(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n.9, .95, .99, 1]``."
          },
          "prediction": {
            "type": "score is used. Note that a good choice of list of",
            "description": ""
          },
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "alphas": {
            "type": "array",
            "description": "like, default=None"
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "set to 'random', a random coefficient is updated every iteration",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- int, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=0",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "used": {
            "type": "only if multiple values for l1_ratio are given.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "intercept_": {
            "type": "ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (W in the cost function formula).",
            "description": ""
          },
          "Note": {
            "type": "that ``coef_`` stores the transpose of ``W``, ``W.T``.",
            "description": ""
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "mse_path_": {
            "type": "ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)",
            "description": ""
          },
          "Mean": {
            "type": "square error for the test set on each fold, varying alpha.",
            "description": ""
          },
          "alphas_": {
            "type": "ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)",
            "description": ""
          },
          "l1_ratio_": {
            "type": "float",
            "description": ""
          },
          "Best": {
            "type": "l1_ratio obtained by cross-validation.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "the": {
            "type": "specified tolerance for the optimal alpha.",
            "description": ""
          },
          "dual_gap_": {
            "type": "float",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "MultiTaskElasticNet": {
            "type": "Multi",
            "description": "task L1/L2 ElasticNet with built-in cross-validation."
          },
          "ElasticNetCV": {
            "type": "Elastic net model with best model selection by",
            "description": "cross-validation."
          },
          "MultiTaskLassoCV": {
            "type": "Multi",
            "description": "task Lasso model trained with L1 norm"
          },
          "as": {
            "type": "regularizer and built-in cross-validation.",
            "description": "Notes\n-----"
          },
          "In": {
            "type": "`fit`, once the best parameters `l1_ratio` and `alpha` are found through",
            "description": "cross-validation, the model is fit again using the entire training set."
          },
          "To": {
            "type": "avoid unnecessary memory duplication the `X` and `y` arguments of the",
            "description": "`fit` method should be directly passed as Fortran-contiguous numpy arrays.\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n>>> clf.fit([[0,0], [1, 1], [2, 2]],\n...         [[0, 0], [1, 1], [2, 2]])"
          },
          "MultiTaskElasticNetCV": {
            "type": "cv=3",
            "description": ">>> print(clf.coef_)\n[[0.52875032 0.46958558]\n[0.52875032 0.46958558]]\n>>> print(clf.intercept_)\n[0.00166409 0.00166409]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm\n        as regularizer and built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]],\n    ...         [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNetCV(cv=3)\n    >>> print(clf.coef_)\n    [[0.52875032 0.46958558]\n     [0.52875032 0.46958558]]\n    >>> print(clf.intercept_)\n    [0.00166409 0.00166409]",
        "notes": "that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n        Mean square error for the test set on each fold, varying alpha.\n\n    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n        The grid of alphas used for fitting, for each l1_ratio.\n\n    l1_ratio_ : float\n        Best l1_ratio obtained by cross-validation.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    dual_gap_ : float\n        The dual gap at the end of the optimization for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm\n        as regularizer and built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]],\n    ...         [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNetCV(cv=3)\n    >>> print(clf.coef_)\n    [[0.52875032 0.46958558]\n     [0.52875032 0.46958558]]\n    >>> print(clf.intercept_)\n    [0.00166409 0.00166409]",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]],\n    ...         [[0, 0], [1, 1], [2, 2]])\n    MultiTaskElasticNetCV(cv=3)\n    >>> print(clf.coef_)\n    [[0.52875032 0.46958558]\n     [0.52875032 0.46958558]]\n    >>> print(clf.intercept_)\n    [0.00166409 0.00166409]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **params)",
          "documentation": {
            "description": "Fit MultiTaskElasticNet model with coordinate descent.\n\n        Fit is on grid of alphas and best alpha estimated by cross-validation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training data.\n        y : ndarray of shape (n_samples, n_targets)\n            Training target variable. Will be cast to X's dtype if necessary.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "target variable. Will be cast to X's dtype if necessary.",
                "description": "**params : dict, default=None"
              },
              "y": {
                "type": "ndarray of shape (n_samples, n_targets)",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MultiTaskLasso",
      "documentation": {
        "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <multi_task_lasso>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_targets, n_features)\n        Parameter vector (W in the cost function formula).\n        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    intercept_ : ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    dual_gap_ : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    eps_ : float\n        The tolerance scaled scaled by the variance of the target `y`.\n\n    sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n        Sparse representation of the `coef_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\n    MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in\n        cross-validation.\n    MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L1/L2 term. Defaults to 1.0.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "set to 'random', a random coefficient is updated every iteration",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "algorithm used to fit the model is coordinate descent.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, reuse the solution of the previous call to fit as",
            "description": "initialization, otherwise, just erase the previous solution."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (W in the cost function formula).",
            "description": ""
          },
          "Note": {
            "type": "that ``coef_`` stores the transpose of ``W``, ``W.T``.",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "the": {
            "type": "specified tolerance.",
            "description": ""
          },
          "dual_gap_": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "eps_": {
            "type": "float",
            "description": ""
          },
          "sparse_coef_": {
            "type": "sparse matrix of shape (n_features,) or             (n_targets, n_features)",
            "description": ""
          },
          "Sparse": {
            "type": "representation of the `coef_`.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as regularizer (aka the Lasso).",
            "description": ""
          },
          "MultiTaskLassoCV": {
            "type": "Multi",
            "description": "task L1 regularized linear model with built-in\ncross-validation."
          },
          "MultiTaskElasticNetCV": {
            "type": "Multi",
            "description": "task L1/L2 ElasticNet with built-in cross-validation.\nNotes\n-----"
          },
          "To": {
            "type": "avoid unnecessary memory duplication the X and y arguments of the fit",
            "description": ""
          },
          "method": {
            "type": "should be directly passed as Fortran-contiguous numpy arrays.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n>>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])"
          },
          "MultiTaskLasso": {
            "type": "alpha=0.1",
            "description": ">>> print(clf.coef_)\n[[0.         0.60809415]\n[0.         0.94592424]]\n>>> print(clf.intercept_)\n[-0.41888636 -0.87382323]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\n    MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in\n        cross-validation.\n    MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n    >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n    MultiTaskLasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.         0.60809415]\n    [0.         0.94592424]]\n    >>> print(clf.intercept_)\n    [-0.41888636 -0.87382323]",
        "notes": "that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    intercept_ : ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    dual_gap_ : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    eps_ : float\n        The tolerance scaled scaled by the variance of the target `y`.\n\n    sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n        Sparse representation of the `coef_`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\n    MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in\n        cross-validation.\n    MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X and y arguments of the fit\n    method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n    >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n    MultiTaskLasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.         0.60809415]\n    [0.         0.94592424]]\n    >>> print(clf.intercept_)\n    [-0.41888636 -0.87382323]",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n    >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n    MultiTaskLasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.         0.60809415]\n    [0.         0.94592424]]\n    >>> print(clf.intercept_)\n    [-0.41888636 -0.87382323]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit MultiTaskElasticNet model with coordinate descent.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Data.\n        y : ndarray of shape (n_samples, n_targets)\n            Target. Will be cast to X's dtype if necessary.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": "Data."
              },
              "y": {
                "type": "ndarray of shape (n_samples, n_targets)",
                "description": "Target. Will be cast to X's dtype if necessary.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": "Notes\n-----"
              },
              "Coordinate": {
                "type": "descent is an algorithm that considers each column of",
                "description": ""
              },
              "data": {
                "type": "at a time hence it will automatically convert the X input",
                "description": ""
              },
              "as": {
                "type": "a Fortran-contiguous numpy array if necessary.",
                "description": ""
              },
              "To": {
                "type": "avoid memory re-allocation it is advised to allocate the",
                "description": ""
              },
              "initial": {
                "type": "data in memory directly using that format.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        Coordinate descent is an algorithm that considers each column of\n        data at a time hence it will automatically convert the X input\n        as a Fortran-contiguous numpy array if necessary.\n\n        To avoid memory re-allocation it is advised to allocate the\n        initial data in memory directly using that format.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)",
          "documentation": {
            "description": "Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    l1_ratio : float, default=0.5\n        Number between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If None alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default=True\n        If set to False, the input validation checks are skipped (including the\n        Gram matrix when provided). It is assumed that they are handled\n        by the caller.\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "l1_ratio": {
                "type": "float, default=0.5",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "l1": {
                "type": "and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to False, the input validation checks are skipped (including the",
                "description": ""
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "check_input": {
                "type": "bool, default=True",
                "description": ""
              },
              "Gram": {
                "type": "matrix when provided). It is assumed that they are handled",
                "description": ""
              },
              "by": {
                "type": "the caller.",
                "description": "**params : kwargs"
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": "(Is returned when ``return_n_iter`` is set to True)."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "MultiTaskElasticNet": {
                "type": "Multi",
                "description": "task ElasticNet model trained with L1/L2 mixed-norm     as regularizer."
              },
              "MultiTaskElasticNetCV": {
                "type": "Multi",
                "description": "task L1/L2 ElasticNet with built-in cross-validation."
              },
              "ElasticNet": {
                "type": "Linear regression with combined L1 and L2 priors as regularizer.",
                "description": ""
              },
              "ElasticNetCV": {
                "type": "Elastic Net model with iterative fitting along a regularization path.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\nExamples\n--------\n>>> from sklearn.linear_model import enet_path\n>>> from sklearn.datasets import make_regression\n>>> X, y, true_coef = make_regression(\n...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n... )\n>>> true_coef"
              },
              "array": {
                "type": "[ 0.        ,  0.        ,  0.        , 97.9..., 45.7...]",
                "description": ">>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n>>> alphas.shape\n(3,)\n>>> estimated_coef\narray([[ 0.        ,  0.78...,  0.56...],\n[ 0.        ,  1.12...,  0.61...],\n[-0.        , -2.12..., -1.12...],\n[ 0.        , 23.04..., 88.93...],\n[ 0.        , 10.63..., 41.56...]])"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "raises": "",
            "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n    ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n    ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "notes": "-----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])",
            "examples": "--------\n    >>> from sklearn.linear_model import enet_path\n    >>> from sklearn.datasets import make_regression\n    >>> X, y, true_coef = make_regression(\n    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n    ... )\n    >>> true_coef\n    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])\n    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n    >>> alphas.shape\n    (3,)\n    >>> estimated_coef\n     array([[ 0.        ,  0.78...,  0.56...],\n            [ 0.        ,  1.12...,  0.61...],\n            [-0.        , -2.12..., -1.12...],\n            [ 0.        , 23.04..., 88.93...],\n            [ 0.        , 10.63..., 41.56...]])"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLasso",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLasso",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MultiTaskLassoCV",
      "documentation": {
        "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for MultiTaskLasso is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <multi_task_lasso>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If not provided, set automatically.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    max_iter : int, default=1000\n        The maximum number of iterations.\n\n    tol : float, default=1e-4\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - int, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation. Note that this is\n        used only if multiple values for l1_ratio are given.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator that selects a random\n        feature to update. Used when ``selection`` == 'random'.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    selection : {'cyclic', 'random'}, default='cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    intercept_ : ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    coef_ : ndarray of shape (n_targets, n_features)\n        Parameter vector (W in the cost function formula).\n        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    mse_path_ : ndarray of shape (n_alphas, n_folds)\n        Mean square error for the test set on each fold, varying alpha.\n\n    alphas_ : ndarray of shape (n_alphas,)\n        The grid of alphas used for fitting.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    dual_gap_ : float\n        The dual gap at the end of the optimization for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n        mixed-norm as regularizer.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.",
        "parameters": {
          "eps": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Length": {
            "type": "of the path. ``eps=1e-3`` means that",
            "description": "``alpha_min / alpha_max = 1e-3``."
          },
          "n_alphas": {
            "type": "int, default=100",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "alphas": {
            "type": "array",
            "description": "like, default=None"
          },
          "List": {
            "type": "of alphas where to compute the models.",
            "description": ""
          },
          "If": {
            "type": "set to 'random', a random coefficient is updated every iteration",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "algorithm used to fit the model is coordinate descent.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "smaller": {
            "type": "than ``tol``, the optimization code checks the",
            "description": ""
          },
          "dual": {
            "type": "gap for optimality and continues until it is smaller",
            "description": ""
          },
          "than": {
            "type": "``tol``.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- int, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "int/None inputs, :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Amount": {
            "type": "of verbosity.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "used": {
            "type": "only if multiple values for l1_ratio are given.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "feature": {
            "type": "to update. Used when ``selection`` == 'random'.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "selection": {
            "type": "{'cyclic', 'random'}, default='cyclic'",
            "description": ""
          },
          "rather": {
            "type": "than looping over features sequentially by default. This",
            "description": "(setting to 'random') often leads to significantly faster convergence"
          },
          "especially": {
            "type": "when tol is higher than 1e-4.",
            "description": "Attributes\n----------"
          },
          "intercept_": {
            "type": "ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (W in the cost function formula).",
            "description": ""
          },
          "Note": {
            "type": "that ``coef_`` stores the transpose of ``W``, ``W.T``.",
            "description": ""
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "mse_path_": {
            "type": "ndarray of shape (n_alphas, n_folds)",
            "description": ""
          },
          "Mean": {
            "type": "square error for the test set on each fold, varying alpha.",
            "description": ""
          },
          "alphas_": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "the": {
            "type": "specified tolerance for the optimal alpha.",
            "description": ""
          },
          "dual_gap_": {
            "type": "float",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "MultiTaskElasticNet": {
            "type": "Multi",
            "description": "task ElasticNet model trained with L1/L2\nmixed-norm as regularizer."
          },
          "ElasticNetCV": {
            "type": "Elastic net model with best model selection by",
            "description": "cross-validation."
          },
          "MultiTaskElasticNetCV": {
            "type": "Multi",
            "description": "task L1/L2 ElasticNet with built-in\ncross-validation.\nNotes\n-----"
          },
          "In": {
            "type": "`fit`, once the best parameter `alpha` is found through",
            "description": "cross-validation, the model is fit again using the entire training set."
          },
          "To": {
            "type": "avoid unnecessary memory duplication the `X` and `y` arguments of the",
            "description": "`fit` method should be directly passed as Fortran-contiguous numpy arrays.\nExamples\n--------\n>>> from sklearn.linear_model import MultiTaskLassoCV\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.metrics import r2_score\n>>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n>>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n>>> r2_score(y, reg.predict(X))\n0.9994...\n>>> reg.alpha_\nnp.float64(0.5713...)\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[[153.7971...,  94.9015...]]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n        mixed-norm as regularizer.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import MultiTaskLassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.metrics import r2_score\n    >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n    >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n    >>> r2_score(y, reg.predict(X))\n    0.9994...\n    >>> reg.alpha_\n    np.float64(0.5713...)\n    >>> reg.predict(X[:1,])\n    array([[153.7971...,  94.9015...]])",
        "notes": "that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    alpha_ : float\n        The amount of penalization chosen by cross validation.\n\n    mse_path_ : ndarray of shape (n_alphas, n_folds)\n        Mean square error for the test set on each fold, varying alpha.\n\n    alphas_ : ndarray of shape (n_alphas,)\n        The grid of alphas used for fitting.\n\n    n_iter_ : int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    dual_gap_ : float\n        The dual gap at the end of the optimization for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n        mixed-norm as regularizer.\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n        cross-validation.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    To avoid unnecessary memory duplication the `X` and `y` arguments of the\n    `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import MultiTaskLassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.metrics import r2_score\n    >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n    >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n    >>> r2_score(y, reg.predict(X))\n    0.9994...\n    >>> reg.alpha_\n    np.float64(0.5713...)\n    >>> reg.predict(X[:1,])\n    array([[153.7971...,  94.9015...]])",
        "examples": "--------\n    >>> from sklearn.linear_model import MultiTaskLassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.metrics import r2_score\n    >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n    >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n    >>> r2_score(y, reg.predict(X))\n    0.9994...\n    >>> reg.alpha_\n    np.float64(0.5713...)\n    >>> reg.predict(X[:1,])\n    array([[153.7971...,  94.9015...]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **params)",
          "documentation": {
            "description": "Fit MultiTaskLasso model with coordinate descent.\n\n        Fit is on grid of alphas and best alpha estimated by cross-validation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Data.\n        y : ndarray of shape (n_samples, n_targets)\n            Target. Will be cast to X's dtype if necessary.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": "Data."
              },
              "y": {
                "type": "ndarray of shape (n_samples, n_targets)",
                "description": "Target. Will be cast to X's dtype if necessary.\n**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "path",
          "signature": "lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)",
          "documentation": {
            "description": "Compute Lasso path with coordinate descent.\n\n    The Lasso optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n        Target values.\n\n    eps : float, default=1e-3\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, default=100\n        Number of alphas along the regularization path.\n\n    alphas : array-like, default=None\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically.\n\n    precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : bool, default=True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array-like of shape (n_features, ), default=None\n        The initial values of the coefficients.\n\n    verbose : bool or int, default=False\n        Amount of verbosity.\n\n    return_n_iter : bool, default=False\n        Whether to return the number of iterations or not.\n\n    positive : bool, default=False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    **params : kwargs\n        Keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data. Pass directly as Fortran-contiguous data to avoid",
                "description": ""
              },
              "unnecessary": {
                "type": "memory duplication. If ``y`` is mono-output then ``X``",
                "description": ""
              },
              "can": {
                "type": "be sparse.",
                "description": ""
              },
              "y": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "eps": {
                "type": "float, default=1e",
                "description": "3"
              },
              "Length": {
                "type": "of the path. ``eps=1e-3`` means that",
                "description": "``alpha_min / alpha_max = 1e-3``."
              },
              "n_alphas": {
                "type": "int, default=100",
                "description": ""
              },
              "Number": {
                "type": "of alphas along the regularization path.",
                "description": ""
              },
              "alphas": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "List": {
                "type": "of alphas where to compute the models.",
                "description": ""
              },
              "If": {
                "type": "set to True, forces coefficients to be positive.",
                "description": "(Only allowed when ``y.ndim == 1``).\n**params : kwargs"
              },
              "precompute": {
                "type": "'auto', bool or array",
                "description": "like of shape             (n_features, n_features), default='auto'"
              },
              "Whether": {
                "type": "to return the number of iterations or not.",
                "description": ""
              },
              "matrix": {
                "type": "can also be passed as argument.",
                "description": ""
              },
              "Xy": {
                "type": "= np.dot(X.T, y) that can be precomputed. It is useful",
                "description": ""
              },
              "only": {
                "type": "when the Gram matrix is precomputed.",
                "description": ""
              },
              "copy_X": {
                "type": "bool, default=True",
                "description": ""
              },
              "coef_init": {
                "type": "array",
                "description": "like of shape (n_features, ), default=None"
              },
              "The": {
                "type": "number of iterations taken by the coordinate descent optimizer to",
                "description": ""
              },
              "verbose": {
                "type": "bool or int, default=False",
                "description": ""
              },
              "Amount": {
                "type": "of verbosity.",
                "description": ""
              },
              "return_n_iter": {
                "type": "bool, default=False",
                "description": ""
              },
              "positive": {
                "type": "bool, default=False",
                "description": ""
              },
              "Keyword": {
                "type": "arguments passed to the coordinate descent solver.",
                "description": "Returns\n-------"
              },
              "coefs": {
                "type": "ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)",
                "description": ""
              },
              "Coefficients": {
                "type": "along the path.",
                "description": ""
              },
              "dual_gaps": {
                "type": "ndarray of shape (n_alphas,)",
                "description": ""
              },
              "n_iters": {
                "type": "list of int",
                "description": ""
              },
              "reach": {
                "type": "the specified tolerance for each alpha.",
                "description": ""
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "lars_path": {
                "type": "Compute Least Angle Regression or Lasso path using LARS",
                "description": "algorithm."
              },
              "Lasso": {
                "type": "The Lasso is a linear model that estimates sparse coefficients.",
                "description": ""
              },
              "LassoLars": {
                "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
                "description": ""
              },
              "LassoCV": {
                "type": "Lasso linear model with iterative fitting along a regularization",
                "description": "path."
              },
              "LassoLarsCV": {
                "type": "Cross",
                "description": "validated Lasso using the LARS algorithm.\nsklearn.decomposition.sparse_encode : Estimator that can be used to"
              },
              "transform": {
                "type": "signals into sparse linear combination of atoms from a fixed.",
                "description": "Notes\n-----"
              },
              "For": {
                "type": "an example, see",
                "description": ":ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`."
              },
              "To": {
                "type": "avoid unnecessary memory duplication the X argument of the fit method",
                "description": ""
              },
              "should": {
                "type": "be directly passed as a Fortran-contiguous numpy array.",
                "description": ""
              },
              "Note": {
                "type": "that in certain cases, the Lars solver may be significantly",
                "description": ""
              },
              "faster": {
                "type": "to implement this functionality. In particular, linear",
                "description": ""
              },
              "interpolation": {
                "type": "can be used to retrieve model coefficients between the",
                "description": ""
              },
              "values": {
                "type": "output by lars_path",
                "description": "Examples\n--------"
              },
              "Comparing": {
                "type": "lasso_path and lars_path with interpolation:",
                "description": ">>> import numpy as np\n>>> from sklearn.linear_model import lasso_path\n>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n>>> y = np.array([1, 2, 3.1])\n>>> # Use lasso_path to compute a coefficient path\n>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n>>> print(coef_path)\n[[0.         0.         0.46874778]\n[0.2159048  0.4425765  0.23689075]]\n>>> # Now use lars_path and 1D linear interpolation to compute the\n>>> # same path\n>>> from sklearn.linear_model import lars_path\n>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n>>> from scipy import interpolate\n>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n...                                             coef_path_lars[:, ::-1])\n>>> print(coef_path_continuous([5., 1., .5]))\n[[0.         0.         0.46915237]\n[0.2159048  0.4425765  0.23668876]]"
              }
            },
            "returns": "-------\n    alphas : ndarray of shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : ndarray of shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : list of int\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "raises": "",
            "see_also": "--------\n    lars_path : Compute Least Angle Regression or Lasso path using LARS\n        algorithm.\n    Lasso : The Lasso is a linear model that estimates sparse coefficients.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoCV : Lasso linear model with iterative fitting along a regularization\n        path.\n    LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n    sklearn.decomposition.sparse_encode : Estimator that can be used to\n        transform signals into sparse linear combination of atoms from a fixed.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "notes": "that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    --------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]",
            "examples": "--------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> import numpy as np\n    >>> from sklearn.linear_model import lasso_path\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]"
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLassoCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLassoCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OrthogonalMatchingPursuit",
      "documentation": {
        "description": "Orthogonal Matching Pursuit model (OMP).\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    n_nonzero_coefs : int, default=None\n        Desired number of non-zero entries in the solution. Ignored if `tol` is set.\n        When `None` and `tol` is also `None`, this value is either set to 10% of\n        `n_features` or 1, whichever is greater.\n\n    tol : float, default=None\n        Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    precompute : 'auto' or bool, default='auto'\n        Whether to use a precomputed Gram and Xy matrix to speed up\n        calculations. Improves performance when :term:`n_targets` or\n        :term:`n_samples` is very large. Note that if you already have such\n        matrices, you can pass them directly to the fit method.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formula).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int or array-like\n        Number of active features across every target.\n\n    n_nonzero_coefs_ : int or None\n        The number of non-zero coefficients in the solution or `None` when `tol` is\n        set. If `n_nonzero_coefs` is None and `tol` is None this value is either set\n        to 10% of `n_features` or 1, whichever is greater.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit\n        problems using only the Gram matrix X.T * X and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n    OrthogonalMatchingPursuitCV : Cross-validated\n        Orthogonal Matching Pursuit model (OMP).\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf",
        "parameters": {
          "n_nonzero_coefs": {
            "type": "int, default=None",
            "description": ""
          },
          "Desired": {
            "type": "number of non-zero entries in the solution. Ignored if `tol` is set.",
            "description": ""
          },
          "When": {
            "type": "`None` and `tol` is also `None`, this value is either set to 10% of",
            "description": "`n_features` or 1, whichever is greater."
          },
          "tol": {
            "type": "float, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "squared norm of the residual. If not None, overrides n_nonzero_coefs.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use a precomputed Gram and Xy matrix to speed up",
            "description": "calculations. Improves performance when :term:`n_targets` or\n:term:`n_samples` is very large. Note that if you already have such\nmatrices, you can pass them directly to the fit method.\nAttributes\n----------"
          },
          "to": {
            "type": "10% of `n_features` or 1, whichever is greater.",
            "description": ""
          },
          "precompute": {
            "type": "'auto' or bool, default='auto'",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the formula).",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "int or array",
            "description": "like"
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "n_nonzero_coefs_": {
            "type": "int or None",
            "description": ""
          },
          "The": {
            "type": "number of non-zero coefficients in the solution or `None` when `tol` is",
            "description": "set. If `n_nonzero_coefs` is None and `tol` is None this value is either set"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "orthogonal_mp": {
            "type": "Solves n_targets Orthogonal Matching Pursuit problems.",
            "description": ""
          },
          "orthogonal_mp_gram": {
            "type": "Solves n_targets Orthogonal Matching Pursuit",
            "description": ""
          },
          "problems": {
            "type": "using only the Gram matrix X.T * X and the product X.T * y.",
            "description": ""
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using LARS algorithm.",
            "description": ""
          },
          "Lars": {
            "type": "Least Angle Regression model a.k.a. LAR.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": "sklearn.decomposition.sparse_encode : Generic sparse coding."
          },
          "Each": {
            "type": "column of the result is the solution to a Lasso problem.",
            "description": ""
          },
          "OrthogonalMatchingPursuitCV": {
            "type": "Cross",
            "description": "validated"
          },
          "Orthogonal": {
            "type": "matching pursuit was introduced in G. Mallat, Z. Zhang,",
            "description": ""
          },
          "Matching": {
            "type": "Pursuit Technical Report - CS Technion, April 2008.",
            "description": ""
          },
          "Signal": {
            "type": "Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.",
            "description": "(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)"
          },
          "This": {
            "type": "implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,",
            "description": "M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal"
          },
          "https": {
            "type": "//www.cs.technion.ac.il/~ronrubin/Publications/KSVD",
            "description": "OMP-v2.pdf\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuit\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuit().fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[-78.3854...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit\n        problems using only the Gram matrix X.T * X and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n    OrthogonalMatchingPursuitCV : Cross-validated\n        Orthogonal Matching Pursuit model (OMP).\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])",
        "notes": "-----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])",
        "examples": "--------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._omp.OrthogonalMatchingPursuit, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._omp.OrthogonalMatchingPursuit",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OrthogonalMatchingPursuitCV",
      "documentation": {
        "description": "Cross-validated Orthogonal Matching Pursuit model (OMP).\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        Whether the design matrix X must be copied by the algorithm. A false\n        value is only helpful if X is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    max_iter : int, default=None\n        Maximum numbers of iterations to perform, therefore maximum features\n        to include. 10% of ``n_features`` but at least 5 if available.\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    Attributes\n    ----------\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the problem formulation).\n\n    n_nonzero_coefs_ : int\n        Estimated number of non-zero coefficients giving the best mean squared\n        error over the cross-validation folds.\n\n    n_iter_ : int or array-like\n        Number of active features across every target for the model refit with\n        the best hyperparameters got by cross-validating across all folds.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit\n        problems using only the Gram matrix X.T * X and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n    LarsCV : Cross-validated Least Angle Regression model.\n    LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n\n    Notes\n    -----\n    In `fit`, once the optimal number of non-zero coefficients is found through\n    cross-validation, the model is fit again using the entire training set.",
        "parameters": {
          "copy": {
            "type": "is made anyway.",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "value": {
            "type": "is only helpful if X is already Fortran-ordered, otherwise a",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "to": {
            "type": "include. 10% of ``n_features`` but at least 5 if available.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "numbers of iterations to perform, therefore maximum features",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "verbose": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "Sets": {
            "type": "the verbosity amount.",
            "description": "Attributes\n----------"
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Parameter": {
            "type": "vector (w in the problem formulation).",
            "description": ""
          },
          "n_nonzero_coefs_": {
            "type": "int",
            "description": ""
          },
          "Estimated": {
            "type": "number of non-zero coefficients giving the best mean squared",
            "description": ""
          },
          "error": {
            "type": "over the cross-validation folds.",
            "description": ""
          },
          "n_iter_": {
            "type": "int or array",
            "description": "like"
          },
          "the": {
            "type": "best hyperparameters got by cross-validating across all folds.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "orthogonal_mp": {
            "type": "Solves n_targets Orthogonal Matching Pursuit problems.",
            "description": ""
          },
          "orthogonal_mp_gram": {
            "type": "Solves n_targets Orthogonal Matching Pursuit",
            "description": ""
          },
          "problems": {
            "type": "using only the Gram matrix X.T * X and the product X.T * y.",
            "description": ""
          },
          "lars_path": {
            "type": "Compute Least Angle Regression or Lasso path using LARS algorithm.",
            "description": ""
          },
          "Lars": {
            "type": "Least Angle Regression model a.k.a. LAR.",
            "description": ""
          },
          "LassoLars": {
            "type": "Lasso model fit with Least Angle Regression a.k.a. Lars.",
            "description": ""
          },
          "OrthogonalMatchingPursuit": {
            "type": "Orthogonal Matching Pursuit model (OMP).",
            "description": ""
          },
          "LarsCV": {
            "type": "Cross",
            "description": "validated Least Angle Regression model."
          },
          "LassoLarsCV": {
            "type": "Cross",
            "description": "validated Lasso model fit with Least Angle Regression.\nsklearn.decomposition.sparse_encode : Generic sparse coding."
          },
          "Each": {
            "type": "column of the result is the solution to a Lasso problem.",
            "description": "Notes\n-----"
          },
          "In": {
            "type": "`fit`, once the optimal number of non-zero coefficients is found through",
            "description": "cross-validation, the model is fit again using the entire training set.\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=100, n_informative=10,\n...                        noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.n_nonzero_coefs_\nnp.int64(10)\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[-78.3854...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n    orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit\n        problems using only the Gram matrix X.T * X and the product X.T * y.\n    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n    Lars : Least Angle Regression model a.k.a. LAR.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n    LarsCV : Cross-validated Least Angle Regression model.\n    LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.\n    sklearn.decomposition.sparse_encode : Generic sparse coding.\n        Each column of the result is the solution to a Lasso problem.\n\n    Notes\n    -----\n    In `fit`, once the optimal number of non-zero coefficients is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=100, n_informative=10,\n    ...                        noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.n_nonzero_coefs_\n    np.int64(10)\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])",
        "notes": "-----\n    In `fit`, once the optimal number of non-zero coefficients is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=100, n_informative=10,\n    ...                        noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.n_nonzero_coefs_\n    np.int64(10)\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])",
        "examples": "--------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=100, n_informative=10,\n    ...                        noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.n_nonzero_coefs_\n    np.int64(10)\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, **fit_params)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        **fit_params : dict\n            Parameters to pass to the underlying splitter.\n\n            .. versionadded:: 1.4\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": "**fit_params : dict"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.4",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._omp.OrthogonalMatchingPursuitCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._omp.OrthogonalMatchingPursuitCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PassiveAggressiveClassifier",
      "documentation": {
        "description": "Passive Aggressive Classifier.\n\n    Read more in the :ref:`User Guide <passive_aggressive>`.\n\n    Parameters\n    ----------\n    C : float, default=1.0\n        Maximum step size (regularization). Defaults to 1.0.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least `tol` for\n        `n_iter_no_change` consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    loss : str, default=\"hinge\"\n        The loss function to be used:\n        hinge: equivalent to PA-I in the reference paper.\n        squared_hinge: equivalent to PA-II in the reference paper.\n\n    n_jobs : int or None, default=None\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Used to shuffle the training data, when ``shuffle`` is set to\n        ``True``. Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n\n    class_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        .. versionadded:: 0.17\n           parameter *class_weight* to automatically weight samples.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So average=10 will begin averaging after seeing 10 samples.\n\n        .. versionadded:: 0.19\n           parameter *average* to use weights averaging in SGD.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n        Weights assigned to the features.\n\n    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    classes_ : ndarray of shape (n_classes,)\n        The unique classes labels.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression.\n    Perceptron : Linear perceptron classifier.\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)",
        "parameters": {
          "C": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Maximum": {
            "type": "step size (regularization). Defaults to 1.0.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not the training data should be shuffled after each epoch.",
            "description": ""
          },
          "data": {
            "type": "is assumed to be already centered.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "unique classes labels.",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": ":meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.\n.. versionadded:: 0.19"
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > previous_loss - tol",
            "description": ".\n.. versionadded:: 0.19"
          },
          "early_stopping": {
            "type": "bool, default=False",
            "description": ""
          },
          "score": {
            "type": "is not improving. If set to True, it will automatically set aside",
            "description": ""
          },
          "a": {
            "type": "stratified fraction of training data as validation and terminate",
            "description": ""
          },
          "training": {
            "type": "when validation score is not improving by at least `tol` for",
            "description": "`n_iter_no_change` consecutive epochs.\n.. versionadded:: 0.20"
          },
          "validation_fraction": {
            "type": "float, default=0.1",
            "description": ""
          },
          "early": {
            "type": "stopping. Must be between 0 and 1.",
            "description": ""
          },
          "Only": {
            "type": "used if early_stopping is True.",
            "description": ".. versionadded:: 0.20"
          },
          "n_iter_no_change": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of weight updates performed during training.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "loss": {
            "type": "str, default=\"hinge\"",
            "description": ""
          },
          "hinge": {
            "type": "equivalent to PA",
            "description": "I in the reference paper."
          },
          "squared_hinge": {
            "type": "equivalent to PA",
            "description": "II in the reference paper."
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "to shuffle the training data, when ``shuffle`` is set to",
            "description": "``True``. Pass an int for reproducible output across multiple"
          },
          "function": {
            "type": "calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, computes the averaged SGD weights and stores the",
            "description": ""
          },
          "Repeatedly": {
            "type": "calling fit or partial_fit when warm_start is True can",
            "description": ""
          },
          "result": {
            "type": "in the ``coef_`` attribute. If set to an int greater than 1,",
            "description": ""
          },
          "because": {
            "type": "of the way the data is shuffled.",
            "description": ""
          },
          "class_weight": {
            "type": "dict, {class_label: weight} or \"balanced\" or None,             default=None",
            "description": ""
          },
          "Preset": {
            "type": "for the class_weight fit parameter.",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "are": {
            "type": "supposed to have weight one.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ".. versionadded:: 0.17"
          },
          "parameter": {
            "type": "*average* to use weights averaging in SGD.",
            "description": "Attributes\n----------"
          },
          "average": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "averaging": {
            "type": "will begin once the total number of samples seen reaches",
            "description": "average. So average=10 will begin averaging after seeing 10 samples.\n.. versionadded:: 0.19"
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
            "description": ""
          },
          "Constants": {
            "type": "in decision function.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "For": {
            "type": "multiclass fits, it is the maximum over every binary fit.",
            "description": ""
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "SGDClassifier": {
            "type": "Incrementally trained logistic regression.",
            "description": ""
          },
          "Perceptron": {
            "type": "Linear perceptron classifier.",
            "description": "References\n----------"
          },
          "Online": {
            "type": "Passive-Aggressive Algorithms",
            "description": "<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n... tol=1e-3)\n>>> clf.fit(X, y)"
          },
          "PassiveAggressiveClassifier": {
            "type": "random_state=0",
            "description": ">>> print(clf.coef_)\n[[0.26642044 0.45070924 0.67251877 0.64185414]]\n>>> print(clf.intercept_)\n[1.84127814]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SGDClassifier : Incrementally trained logistic regression.\n    Perceptron : Linear perceptron classifier.\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import PassiveAggressiveClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n    ... tol=1e-3)\n    >>> clf.fit(X, y)\n    PassiveAggressiveClassifier(random_state=0)\n    >>> print(clf.coef_)\n    [[0.26642044 0.45070924 0.67251877 0.64185414]]\n    >>> print(clf.intercept_)\n    [1.84127814]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import PassiveAggressiveClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n    ... tol=1e-3)\n    >>> clf.fit(X, y)\n    PassiveAggressiveClassifier(random_state=0)\n    >>> print(clf.coef_)\n    [[0.26642044 0.45070924 0.67251877 0.64185414]]\n    >>> print(clf.intercept_)\n    [1.84127814]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, coef_init=None, intercept_init=None)",
          "documentation": {
            "description": "Fit linear model with Passive Aggressive algorithm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        coef_init : ndarray of shape (n_classes, n_features)\n            The initial coefficients to warm-start the optimization.\n\n        intercept_init : ndarray of shape (n_classes,)\n            The initial intercept to warm-start the optimization.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "coef_init": {
                "type": "ndarray of shape (n_classes, n_features)",
                "description": ""
              },
              "The": {
                "type": "initial intercept to warm-start the optimization.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "ndarray of shape (n_classes,)",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None)",
          "documentation": {
            "description": "Fit linear model with Passive Aggressive algorithm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Subset of the training data.\n\n        y : array-like of shape (n_samples,)\n            Subset of the target values.\n\n        classes : ndarray of shape (n_classes,)\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all)`, where y_all is the\n            target vector of the entire dataset.\n            This argument is required for the first call to partial_fit\n            and can be omitted in the subsequent calls.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of the target values.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "classes": {
                "type": "ndarray of shape (n_classes,)",
                "description": ""
              },
              "Classes": {
                "type": "across all calls to partial_fit.",
                "description": ""
              },
              "Can": {
                "type": "be obtained by via `np.unique(y_all)`, where y_all is the",
                "description": ""
              },
              "target": {
                "type": "vector of the entire dataset.",
                "description": ""
              },
              "This": {
                "type": "argument is required for the first call to partial_fit",
                "description": ""
              },
              "and": {
                "type": "can be omitted in the subsequent calls.",
                "description": ""
              },
              "Note": {
                "type": "that y doesn't need to contain all labels in `classes`.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "that y doesn't need to contain all labels in `classes`.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``intercept_init`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``intercept_init`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``classes`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``classes`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PassiveAggressiveRegressor",
      "documentation": {
        "description": "Passive Aggressive Regressor.\n\n    Read more in the :ref:`User Guide <passive_aggressive>`.\n\n    Parameters\n    ----------\n\n    C : float, default=1.0\n        Maximum step size (regularization). Defaults to 1.0.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation.\n        score is not improving. If set to True, it will automatically set aside\n        a fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    loss : str, default=\"epsilon_insensitive\"\n        The loss function to be used:\n        epsilon_insensitive: equivalent to PA-I in the reference paper.\n        squared_epsilon_insensitive: equivalent to PA-II in the reference\n        paper.\n\n    epsilon : float, default=0.1\n        If the difference between the current prediction and the correct label\n        is below this threshold, the model is not updated.\n\n    random_state : int, RandomState instance, default=None\n        Used to shuffle the training data, when ``shuffle`` is set to\n        ``True``. Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So average=10 will begin averaging after seeing 10 samples.\n\n        .. versionadded:: 0.19\n           parameter *average* to use weights averaging in SGD.\n\n    Attributes\n    ----------\n    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n        Weights assigned to the features.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    See Also\n    --------\n    SGDRegressor : Linear model fitted by minimizing a regularized\n        empirical loss with SGD.\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).",
        "parameters": {
          "C": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Maximum": {
            "type": "step size (regularization). Defaults to 1.0.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not the training data should be shuffled after each epoch.",
            "description": ""
          },
          "data": {
            "type": "is assumed to be already centered. Defaults to True.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations to reach the stopping criterion.",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": ":meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.\n.. versionadded:: 0.19"
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > previous_loss - tol",
            "description": ".\n.. versionadded:: 0.19"
          },
          "early_stopping": {
            "type": "bool, default=False",
            "description": ""
          },
          "score": {
            "type": "is not improving. If set to True, it will automatically set aside",
            "description": ""
          },
          "a": {
            "type": "fraction of training data as validation and terminate",
            "description": ""
          },
          "training": {
            "type": "when validation score is not improving by at least tol for",
            "description": ""
          },
          "n_iter_no_change": {
            "type": "int, default=5",
            "description": ""
          },
          "validation_fraction": {
            "type": "float, default=0.1",
            "description": ""
          },
          "early": {
            "type": "stopping. Must be between 0 and 1.",
            "description": ""
          },
          "Only": {
            "type": "used if early_stopping is True.",
            "description": ".. versionadded:: 0.20"
          },
          "Number": {
            "type": "of weight updates performed during training.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "loss": {
            "type": "str, default=\"epsilon_insensitive\"",
            "description": ""
          },
          "epsilon_insensitive": {
            "type": "equivalent to PA",
            "description": "I in the reference paper."
          },
          "squared_epsilon_insensitive": {
            "type": "equivalent to PA",
            "description": "II in the reference\npaper."
          },
          "epsilon": {
            "type": "float, default=0.1",
            "description": ""
          },
          "If": {
            "type": "the difference between the current prediction and the correct label",
            "description": ""
          },
          "is": {
            "type": "below this threshold, the model is not updated.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "to shuffle the training data, when ``shuffle`` is set to",
            "description": "``True``. Pass an int for reproducible output across multiple"
          },
          "function": {
            "type": "calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, computes the averaged SGD weights and stores the",
            "description": ""
          },
          "Repeatedly": {
            "type": "calling fit or partial_fit when warm_start is True can",
            "description": ""
          },
          "result": {
            "type": "in the ``coef_`` attribute. If set to an int greater than 1,",
            "description": ""
          },
          "because": {
            "type": "of the way the data is shuffled.",
            "description": ""
          },
          "average": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "averaging": {
            "type": "will begin once the total number of samples seen reaches",
            "description": "average. So average=10 will begin averaging after seeing 10 samples.\n.. versionadded:: 0.19"
          },
          "parameter": {
            "type": "*average* to use weights averaging in SGD.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "intercept_": {
            "type": "array, shape = [1] if n_classes == 2 else [n_classes]",
            "description": ""
          },
          "Constants": {
            "type": "in decision function.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "SGDRegressor": {
            "type": "Linear model fitted by minimizing a regularized",
            "description": ""
          },
          "empirical": {
            "type": "loss with SGD.",
            "description": "References\n----------"
          },
          "Online": {
            "type": "Passive-Aggressive Algorithms",
            "description": "<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, random_state=0)\n>>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n... tol=1e-3)\n>>> regr.fit(X, y)"
          },
          "PassiveAggressiveRegressor": {
            "type": "max_iter=100, random_state=0",
            "description": ">>> print(regr.coef_)\n[20.48736655 34.18818427 67.59122734 87.94731329]\n>>> print(regr.intercept_)\n[-0.02306214]\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-0.02306214]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SGDRegressor : Linear model fitted by minimizing a regularized\n        empirical loss with SGD.\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import PassiveAggressiveRegressor\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=4, random_state=0)\n    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n    ... tol=1e-3)\n    >>> regr.fit(X, y)\n    PassiveAggressiveRegressor(max_iter=100, random_state=0)\n    >>> print(regr.coef_)\n    [20.48736655 34.18818427 67.59122734 87.94731329]\n    >>> print(regr.intercept_)\n    [-0.02306214]\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-0.02306214]",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import PassiveAggressiveRegressor\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=4, random_state=0)\n    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n    ... tol=1e-3)\n    >>> regr.fit(X, y)\n    PassiveAggressiveRegressor(max_iter=100, random_state=0)\n    >>> print(regr.coef_)\n    [20.48736655 34.18818427 67.59122734 87.94731329]\n    >>> print(regr.intercept_)\n    [-0.02306214]\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-0.02306214]"
      },
      "methods": [
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, coef_init=None, intercept_init=None)",
          "documentation": {
            "description": "Fit linear model with Passive Aggressive algorithm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        coef_init : array, shape = [n_features]\n            The initial coefficients to warm-start the optimization.\n\n        intercept_init : array, shape = [1]\n            The initial intercept to warm-start the optimization.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "numpy array of shape [n_samples]",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "coef_init": {
                "type": "array, shape = [n_features]",
                "description": ""
              },
              "The": {
                "type": "initial intercept to warm-start the optimization.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "array, shape = [1]",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y)",
          "documentation": {
            "description": "Fit linear model with Passive Aggressive algorithm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Subset of training data.\n\n        y : numpy array of shape [n_samples]\n            Subset of target values.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of target values.",
                "description": "Returns\n-------"
              },
              "y": {
                "type": "numpy array of shape [n_samples]",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "ndarray": {
                "type": "of shape (n_samples,)",
                "description": ""
              },
              "Predicted": {
                "type": "target values per element in X.",
                "description": ""
              }
            },
            "returns": "-------\n        ndarray of shape (n_samples,)\n           Predicted target values per element in X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``intercept_init`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``intercept_init`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Perceptron",
      "documentation": {
        "description": "Linear perceptron classifier.\n\n    The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier`\n    by fixing the `loss` and `learning_rate` parameters as::\n\n        SGDClassifier(loss=\"perceptron\", learning_rate=\"constant\")\n\n    Other available parameters are described below and are forwarded to\n    :class:`~sklearn.linear_model.SGDClassifier`.\n\n    Read more in the :ref:`User Guide <perceptron>`.\n\n    Parameters\n    ----------\n\n    penalty : {'l2','l1','elasticnet'}, default=None\n        The penalty (aka regularization term) to be used.\n\n    alpha : float, default=0.0001\n        Constant that multiplies the regularization term if regularization is\n        used.\n\n    l1_ratio : float, default=0.15\n        The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n        `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n        Only used if `penalty='elasticnet'`.\n\n        .. versionadded:: 0.24\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    eta0 : float, default=1\n        Constant by which the updates are multiplied.\n\n    n_jobs : int, default=None\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, default=0\n        Used to shuffle the training data, when ``shuffle`` is set to\n        ``True``. Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least `tol` for\n        `n_iter_no_change` consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution. See\n        :term:`the Glossary <warm_start>`.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The unique classes labels.\n\n    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n        Weights assigned to the features.\n\n    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    See Also\n    --------\n    sklearn.linear_model.SGDClassifier : Linear classifiers\n        (SVM, logistic regression, etc.) with SGD training.\n\n    Notes\n    -----\n    ``Perceptron`` is a classification algorithm which shares the same\n    underlying implementation with ``SGDClassifier``. In fact,\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n    eta0=1, learning_rate=\"constant\", penalty=None)`.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Perceptron and references therein.",
        "parameters": {
          "penalty": {
            "type": "{'l2','l1','elasticnet'}, default=None",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations to reach the stopping criterion.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=0.0001",
            "description": ""
          },
          "Constant": {
            "type": "by which the updates are multiplied.",
            "description": ""
          },
          "l1_ratio": {
            "type": "float, default=0.15",
            "description": ""
          },
          "Only": {
            "type": "used if early_stopping is True.",
            "description": ".. versionadded:: 0.20"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use early stopping to terminate training when validation",
            "description": ""
          },
          "data": {
            "type": "is assumed to be already centered.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": ":meth:`partial_fit` method.\n.. versionadded:: 0.19"
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > previous_loss - tol",
            "description": ".\n.. versionadded:: 0.19"
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "eta0": {
            "type": "float, default=1",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=0",
            "description": ""
          },
          "Used": {
            "type": "to shuffle the training data, when ``shuffle`` is set to",
            "description": "``True``. Pass an int for reproducible output across multiple"
          },
          "function": {
            "type": "calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.linear_model.SGDClassifier : Linear classifiers\n(SVM, logistic regression, etc.) with SGD training.\nNotes\n-----\n``Perceptron`` is a classification algorithm which shares the same"
          },
          "early_stopping": {
            "type": "bool, default=False",
            "description": ""
          },
          "score": {
            "type": "is not improving. If set to True, it will automatically set aside",
            "description": ""
          },
          "a": {
            "type": "stratified fraction of training data as validation and terminate",
            "description": ""
          },
          "training": {
            "type": "when validation score is not improving by at least `tol` for",
            "description": "`n_iter_no_change` consecutive epochs.\n.. versionadded:: 0.20"
          },
          "validation_fraction": {
            "type": "float, default=0.1",
            "description": ""
          },
          "early": {
            "type": "stopping. Must be between 0 and 1.",
            "description": ""
          },
          "n_iter_no_change": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of weight updates performed during training.",
            "description": ""
          },
          "class_weight": {
            "type": "dict, {class_label: weight} or \"balanced\", default=None",
            "description": ""
          },
          "Preset": {
            "type": "for the class_weight fit parameter.",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "are": {
            "type": "supposed to have weight one.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, reuse the solution of the previous call to fit as",
            "description": "initialization, otherwise, just erase the previous solution. See\n:term:`the Glossary <warm_start>`.\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
            "description": ""
          },
          "Constants": {
            "type": "in decision function.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "For": {
            "type": "multiclass fits, it is the maximum over every binary fit.",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "underlying": {
            "type": "implementation with ``SGDClassifier``. In fact,",
            "description": "``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\neta0=1, learning_rate=\"constant\", penalty=None)`.\nReferences\n----------"
          },
          "https": {
            "type": "//en.wikipedia.org/wiki/Perceptron and references therein.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.linear_model import Perceptron\n>>> X, y = load_digits(return_X_y=True)\n>>> clf = Perceptron(tol=1e-3, random_state=0)\n>>> clf.fit(X, y)"
          },
          "Perceptron": {
            "type": "",
            "description": ">>> clf.score(X, y)\n0.939..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.linear_model.SGDClassifier : Linear classifiers\n        (SVM, logistic regression, etc.) with SGD training.\n\n    Notes\n    -----\n    ``Perceptron`` is a classification algorithm which shares the same\n    underlying implementation with ``SGDClassifier``. In fact,\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n    eta0=1, learning_rate=\"constant\", penalty=None)`.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Perceptron and references therein.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import Perceptron\n    >>> X, y = load_digits(return_X_y=True)\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\n    >>> clf.fit(X, y)\n    Perceptron()\n    >>> clf.score(X, y)\n    0.939...",
        "notes": "-----\n    ``Perceptron`` is a classification algorithm which shares the same\n    underlying implementation with ``SGDClassifier``. In fact,\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n    eta0=1, learning_rate=\"constant\", penalty=None)`.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Perceptron and references therein.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import Perceptron\n    >>> X, y = load_digits(return_X_y=True)\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\n    >>> clf.fit(X, y)\n    Perceptron()\n    >>> clf.score(X, y)\n    0.939...",
        "examples": "--------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import Perceptron\n    >>> X, y = load_digits(return_X_y=True)\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\n    >>> clf.fit(X, y)\n    Perceptron()\n    >>> clf.score(X, y)\n    0.939..."
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)",
          "documentation": {
            "description": "Fit linear model with Stochastic Gradient Descent.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : ndarray of shape (n_samples,)\n            Target values.\n\n        coef_init : ndarray of shape (n_classes, n_features), default=None\n            The initial coefficients to warm-start the optimization.\n\n        intercept_init : ndarray of shape (n_classes,), default=None\n            The initial intercept to warm-start the optimization.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed. These weights will\n            be multiplied with class_weight (passed through the\n            constructor) if class_weight is specified.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "coef_init": {
                "type": "ndarray of shape (n_classes, n_features), default=None",
                "description": ""
              },
              "The": {
                "type": "initial intercept to warm-start the optimization.",
                "description": ""
              },
              "intercept_init": {
                "type": "ndarray of shape (n_classes,), default=None",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed. These weights will",
                "description": ""
              },
              "be": {
                "type": "multiplied with class_weight (passed through the",
                "description": "constructor) if class_weight is specified.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "documentation": {
            "description": "Perform one epoch of stochastic gradient descent on given samples.\n\n        Internally, this method uses ``max_iter = 1``. Therefore, it is not\n        guaranteed that a minimum of the cost function is reached after calling\n        it once. Matters such as objective convergence, early stopping, and\n        learning rate adjustments should be handled by the user.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Subset of the training data.\n\n        y : ndarray of shape (n_samples,)\n            Subset of the target values.\n\n        classes : ndarray of shape (n_classes,), default=None\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all)`, where y_all is the\n            target vector of the entire dataset.\n            This argument is required for the first call to partial_fit\n            and can be omitted in the subsequent calls.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of the target values.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "classes": {
                "type": "ndarray of shape (n_classes,), default=None",
                "description": ""
              },
              "Classes": {
                "type": "across all calls to partial_fit.",
                "description": ""
              },
              "Can": {
                "type": "be obtained by via `np.unique(y_all)`, where y_all is the",
                "description": ""
              },
              "target": {
                "type": "vector of the entire dataset.",
                "description": ""
              },
              "This": {
                "type": "argument is required for the first call to partial_fit",
                "description": ""
              },
              "and": {
                "type": "can be omitted in the subsequent calls.",
                "description": ""
              },
              "Note": {
                "type": "that y doesn't need to contain all labels in `classes`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "that y doesn't need to contain all labels in `classes`.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._perceptron.Perceptron, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``intercept_init`` parameter in ``fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._perceptron.Perceptron, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``classes`` parameter in ``partial_fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._perceptron.Perceptron, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PoissonRegressor",
      "documentation": {
        "description": "Generalized Linear Model with a Poisson distribution.\n\n    This regressor uses the 'log' link function.\n\n    Read more in the :ref:`User Guide <Generalized_linear_models>`.\n\n    .. versionadded:: 0.23\n\n    Parameters\n    ----------\n    alpha : float, default=1\n        Constant that multiplies the L2 penalty term and determines the\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\n        GLMs. In this case, the design matrix `X` must have full column rank\n        (no collinearities).\n        Values of `alpha` must be in the range `[0.0, inf)`.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the linear predictor (`X @ coef + intercept`).\n\n    solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n        Algorithm to use in the optimization problem:\n\n        'lbfgs'\n            Calls scipy's L-BFGS-B optimizer.\n\n        'newton-cholesky'\n            Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n            iterated reweighted least squares) with an inner Cholesky based solver.\n            This solver is a good choice for `n_samples` >> `n_features`, especially\n            with one-hot encoded categorical features with rare categories. Be aware\n            that the memory usage of this solver has a quadratic dependency on\n            `n_features` because it explicitly computes the Hessian matrix.\n\n            .. versionadded:: 1.2\n\n    max_iter : int, default=100\n        The maximal number of iterations for the solver.\n        Values must be in the range `[1, inf)`.\n\n    tol : float, default=1e-4\n        Stopping criterion. For the lbfgs solver,\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n        where ``g_j`` is the j-th component of the gradient (derivative) of\n        the objective function.\n        Values must be in the range `(0.0, inf)`.\n\n    warm_start : bool, default=False\n        If set to ``True``, reuse the solution of the previous call to ``fit``\n        as initialization for ``coef_`` and ``intercept_`` .\n\n    verbose : int, default=0\n        For the lbfgs solver set verbose to any positive number for verbosity.\n        Values must be in the range `[0, inf)`.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features,)\n        Estimated coefficients for the linear predictor (`X @ coef_ +\n        intercept_`) in the GLM.\n\n    intercept_ : float\n        Intercept (a.k.a. bias) added to linear predictor.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Actual number of iterations used in the solver.\n\n    See Also\n    --------\n    TweedieRegressor : Generalized Linear Model with a Tweedie distribution.",
        "parameters": {
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L2 penalty term and determines the",
            "description": ""
          },
          "regularization": {
            "type": "strength. ``alpha = 0`` is equivalent to unpenalized",
            "description": "GLMs. In this case, the design matrix `X` must have full column rank\n(no collinearities)."
          },
          "Values": {
            "type": "must be in the range `[0, inf)`.",
            "description": "Attributes\n----------"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if a constant (a.k.a. bias or intercept) should be",
            "description": ""
          },
          "added": {
            "type": "to the linear predictor (`X @ coef + intercept`).",
            "description": ""
          },
          "solver": {
            "type": "{'lbfgs', 'newton",
            "description": "cholesky'}, default='lbfgs'"
          },
          "Algorithm": {
            "type": "to use in the optimization problem:",
            "description": "'lbfgs'"
          },
          "Calls": {
            "type": "scipy's L-BFGS-B optimizer.",
            "description": "'newton-cholesky'"
          },
          "Uses": {
            "type": "Newton-Raphson steps (in arbitrary precision arithmetic equivalent to",
            "description": ""
          },
          "iterated": {
            "type": "reweighted least squares) with an inner Cholesky based solver.",
            "description": ""
          },
          "This": {
            "type": "solver is a good choice for `n_samples` >> `n_features`, especially",
            "description": ""
          },
          "with": {
            "type": "one-hot encoded categorical features with rare categories. Be aware",
            "description": ""
          },
          "that": {
            "type": "the memory usage of this solver has a quadratic dependency on",
            "description": "`n_features` because it explicitly computes the Hessian matrix.\n.. versionadded:: 1.2"
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "The": {
            "type": "maximal number of iterations for the solver.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Stopping": {
            "type": "criterion. For the lbfgs solver,",
            "description": ""
          },
          "the": {
            "type": "objective function.",
            "description": ""
          },
          "where": {
            "type": "``g_j`` is the j-th component of the gradient (derivative) of",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "set to ``True``, reuse the solution of the previous call to ``fit``",
            "description": ""
          },
          "as": {
            "type": "initialization for ``coef_`` and ``intercept_`` .",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "For": {
            "type": "the lbfgs solver set verbose to any positive number for verbosity.",
            "description": ""
          },
          "coef_": {
            "type": "array of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "coefficients for the linear predictor (`X @ coef_ +",
            "description": "intercept_`) in the GLM."
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Intercept": {
            "type": "a.k.a. bias",
            "description": "added to linear predictor."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations used in the solver.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "TweedieRegressor": {
            "type": "Generalized Linear Model with a Tweedie distribution.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.PoissonRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [12, 17, 22, 21]\n>>> clf.fit(X, y)"
          },
          "PoissonRegressor": {
            "type": "",
            "description": ">>> clf.score(X, y)\nnp.float64(0.990...)\n>>> clf.coef_"
          },
          "array": {
            "type": "[10.676..., 21.875...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    TweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.PoissonRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [12, 17, 22, 21]\n    >>> clf.fit(X, y)\n    PoissonRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.990...)\n    >>> clf.coef_\n    array([0.121..., 0.158...])\n    >>> clf.intercept_\n    np.float64(2.088...)\n    >>> clf.predict([[1, 1], [3, 4]])\n    array([10.676..., 21.875...])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.PoissonRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [12, 17, 22, 21]\n    >>> clf.fit(X, y)\n    PoissonRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.990...)\n    >>> clf.coef_\n    array([0.121..., 0.158...])\n    >>> clf.intercept_\n    np.float64(2.088...)\n    >>> clf.predict([[1, 1], [3, 4]])\n    array([10.676..., 21.875...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit a Generalized Linear Model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "model.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using GLM with feature matrix X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "y_pred": {
                "type": "array of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : array of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Compute D^2, the percentage of deviance explained.\n\n        D^2 is a generalization of the coefficient of determination R^2.\n        R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n        :ref:`User Guide <regression_metrics>`.\n\n        D^2 is defined as\n        :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n        :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n        with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n        The mean :math:`\\bar{y}` is averaged by sample_weight.\n        Best possible score is 1.0 and it can be negative (because the model\n        can be arbitrarily worse).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True values of target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "True": {
                "type": "values of target.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": "D^2 of self.predict(X) w.r.t. y."
              }
            },
            "returns": "-------\n        score : float\n            D^2 of self.predict(X) w.r.t. y.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._glm.glm.PoissonRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.PoissonRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._glm.glm.PoissonRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.PoissonRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QuantileRegressor",
      "documentation": {
        "description": "Linear regression model that predicts conditional quantiles.\n\n    The linear :class:`QuantileRegressor` optimizes the pinball loss for a\n    desired `quantile` and is robust to outliers.\n\n    This model uses an L1 regularization like\n    :class:`~sklearn.linear_model.Lasso`.\n\n    Read more in the :ref:`User Guide <quantile_regression>`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    quantile : float, default=0.5\n        The quantile that the model tries to predict. It must be strictly\n        between 0 and 1. If 0.5 (default), the model predicts the 50%\n        quantile, i.e. the median.\n\n    alpha : float, default=1.0\n        Regularization constant that multiplies the L1 penalty term.\n\n    fit_intercept : bool, default=True\n        Whether or not to fit the intercept.\n\n    solver : {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='highs'\n        Method used by :func:`scipy.optimize.linprog` to solve the linear\n        programming formulation.\n\n        It is recommended to use the highs methods because\n        they are the fastest ones. Solvers \"highs-ds\", \"highs-ipm\" and \"highs\"\n        support sparse input data and, in fact, always convert to sparse csc.\n\n        From `scipy>=1.11.0`, \"interior-point\" is not available anymore.\n\n        .. versionchanged:: 1.4\n           The default of `solver` changed to `\"highs\"` in version 1.4.\n\n    solver_options : dict, default=None\n        Additional parameters passed to :func:`scipy.optimize.linprog` as\n        options. If `None` and if `solver='interior-point'`, then\n        `{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the\n        sake of stability.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features,)\n        Estimated coefficients for the features.\n\n    intercept_ : float\n        The intercept of the model, aka bias term.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        The actual number of iterations performed by the solver.\n\n    See Also\n    --------\n    Lasso : The Lasso is a linear model that estimates sparse coefficients\n        with l1 regularization.\n    HuberRegressor : Linear regression model that is robust to outliers.",
        "parameters": {
          "quantile": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations performed by the solver.",
            "description": ""
          },
          "between": {
            "type": "0 and 1. If 0.5 (default), the model predicts the 50%",
            "description": "quantile, i.e. the median."
          },
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Regularization": {
            "type": "constant that multiplies the L1 penalty term.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not to fit the intercept.",
            "description": ""
          },
          "solver": {
            "type": "{'highs",
            "description": "ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='highs'"
          },
          "Method": {
            "type": "used by :func:`scipy.optimize.linprog` to solve the linear",
            "description": ""
          },
          "programming": {
            "type": "formulation.",
            "description": ""
          },
          "It": {
            "type": "is recommended to use the highs methods because",
            "description": ""
          },
          "they": {
            "type": "are the fastest ones. Solvers \"highs-ds\", \"highs-ipm\" and \"highs\"",
            "description": ""
          },
          "support": {
            "type": "sparse input data and, in fact, always convert to sparse csc.",
            "description": ""
          },
          "From": {
            "type": "`scipy>=1.11.0`, \"interior-point\" is not available anymore.",
            "description": ".. versionchanged:: 1.4"
          },
          "solver_options": {
            "type": "dict, default=None",
            "description": ""
          },
          "Additional": {
            "type": "parameters passed to :func:`scipy.optimize.linprog` as",
            "description": "options. If `None` and if `solver='interior-point'`, then\n`{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the"
          },
          "sake": {
            "type": "of stability.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "array of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "coefficients for the features.",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "Lasso": {
            "type": "The Lasso is a linear model that estimates sparse coefficients",
            "description": ""
          },
          "with": {
            "type": "l1 regularization.",
            "description": ""
          },
          "HuberRegressor": {
            "type": "Linear regression model that is robust to outliers.",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import QuantileRegressor\n>>> import numpy as np\n>>> n_samples, n_features = 10, 2\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> # the two following lines are optional in practice\n>>> from sklearn.utils.fixes import sp_version, parse_version\n>>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\n>>> np.mean(y <= reg.predict(X))\nnp.float64(0.8)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Lasso : The Lasso is a linear model that estimates sparse coefficients\n        with l1 regularization.\n    HuberRegressor : Linear regression model that is robust to outliers.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import QuantileRegressor\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 2\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> # the two following lines are optional in practice\n    >>> from sklearn.utils.fixes import sp_version, parse_version\n    >>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\n    >>> np.mean(y <= reg.predict(X))\n    np.float64(0.8)",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import QuantileRegressor\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 2\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> # the two following lines are optional in practice\n    >>> from sklearn.utils.fixes import sp_version, parse_version\n    >>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\n    >>> np.mean(y <= reg.predict(X))\n    np.float64(0.8)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._quantile.QuantileRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._quantile.QuantileRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._quantile.QuantileRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._quantile.QuantileRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RANSACRegressor",
      "documentation": {
        "description": "RANSAC (RANdom SAmple Consensus) algorithm.\n\n    RANSAC is an iterative algorithm for the robust estimation of parameters\n    from a subset of inliers from the complete data set.\n\n    Read more in the :ref:`User Guide <ransac_regression>`.\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        Base estimator object which implements the following methods:\n\n        * `fit(X, y)`: Fit model to given training data and target values.\n        * `score(X, y)`: Returns the mean accuracy on the given test data,\n          which is used for the stop criterion defined by `stop_score`.\n          Additionally, the score is used to decide which of two equally\n          large consensus sets is chosen as the better one.\n        * `predict(X)`: Returns predicted values using the linear model,\n          which is used to compute residual error using loss function.\n\n        If `estimator` is None, then\n        :class:`~sklearn.linear_model.LinearRegression` is used for\n        target values of dtype float.\n\n        Note that the current implementation only supports regression\n        estimators.\n\n    min_samples : int (>= 1) or float ([0, 1]), default=None\n        Minimum number of samples chosen randomly from original data. Treated\n        as an absolute number of samples for `min_samples >= 1`, treated as a\n        relative number `ceil(min_samples * X.shape[0])` for\n        `min_samples < 1`. This is typically chosen as the minimal number of\n        samples necessary to estimate the given `estimator`. By default a\n        :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and\n        `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\n        dependent upon the model, so if a `estimator` other than\n        :class:`~sklearn.linear_model.LinearRegression` is used, the user must\n        provide a value.\n\n    residual_threshold : float, default=None\n        Maximum residual for a data sample to be classified as an inlier.\n        By default the threshold is chosen as the MAD (median absolute\n        deviation) of the target values `y`. Points whose residuals are\n        strictly equal to the threshold are considered as inliers.\n\n    is_data_valid : callable, default=None\n        This function is called with the randomly selected data before the\n        model is fitted to it: `is_data_valid(X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n\n    is_model_valid : callable, default=None\n        This function is called with the estimated model and the randomly\n        selected data: `is_model_valid(model, X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n        Rejecting samples with this function is computationally costlier than\n        with `is_data_valid`. `is_model_valid` should therefore only be used if\n        the estimated model is needed for making the rejection decision.\n\n    max_trials : int, default=100\n        Maximum number of iterations for random sample selection.\n\n    max_skips : int, default=np.inf\n        Maximum number of iterations that can be skipped due to finding zero\n        inliers or invalid data defined by ``is_data_valid`` or invalid models\n        defined by ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    stop_n_inliers : int, default=np.inf\n        Stop iteration if at least this number of inliers are found.\n\n    stop_score : float, default=np.inf\n        Stop iteration if score is greater equal than this threshold.\n\n    stop_probability : float in range [0, 1], default=0.99\n        RANSAC iteration stops if at least one outlier-free set of the training\n        data is sampled in RANSAC. This requires to generate at least N\n        samples (iterations)::\n\n            N >= log(1 - probability) / log(1 - e**m)\n\n        where the probability (confidence) is typically set to high value such\n        as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n        the total number of samples.\n\n    loss : str, callable, default='absolute_error'\n        String inputs, 'absolute_error' and 'squared_error' are supported which\n        find the absolute error and squared error per sample respectively.\n\n        If ``loss`` is a callable, then it should be a function that takes\n        two arrays as inputs, the true and predicted value and returns a 1-D\n        array with the i-th value of the array corresponding to the loss\n        on ``X[i]``.\n\n        If the loss on a sample is greater than the ``residual_threshold``,\n        then this sample is classified as an outlier.\n\n        .. versionadded:: 0.18\n\n    random_state : int, RandomState instance, default=None\n        The generator used to initialize the centers.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    estimator_ : object\n        Final model fitted on the inliers predicted by the \"best\" model found\n        during RANSAC sampling (copy of the `estimator` object).\n\n    n_trials_ : int\n        Number of random selection trials until one of the stop criteria is\n        met. It is always ``<= max_trials``.\n\n    inlier_mask_ : bool array of shape [n_samples]\n        Boolean mask of inliers classified as ``True``.\n\n    n_skips_no_inliers_ : int\n        Number of iterations skipped due to finding zero inliers.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_data_ : int\n        Number of iterations skipped due to invalid data defined by\n        ``is_data_valid``.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_model_ : int\n        Number of iterations skipped due to an invalid model defined by\n        ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/RANSAC\n    .. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n    .. [3] https://bmva-archive.org.uk/bmvc/2009/Papers/Paper355/Paper355.pdf",
        "parameters": {
          "estimator": {
            "type": "object, default=None",
            "description": ""
          },
          "Base": {
            "type": "estimator object which implements the following methods:",
            "description": "* `fit(X, y)`: Fit model to given training data and target values.\n* `score(X, y)`: Returns the mean accuracy on the given test data,"
          },
          "which": {
            "type": "is used to compute residual error using loss function.",
            "description": ""
          },
          "large": {
            "type": "consensus sets is chosen as the better one.",
            "description": "* `predict(X)`: Returns predicted values using the linear model,"
          },
          "If": {
            "type": "the loss on a sample is greater than the ``residual_threshold``,",
            "description": ""
          },
          "target": {
            "type": "values of dtype float.",
            "description": ""
          },
          "Note": {
            "type": "that the current implementation only supports regression",
            "description": "estimators."
          },
          "min_samples": {
            "type": "int (>= 1) or float ([0, 1]), default=None",
            "description": ""
          },
          "Minimum": {
            "type": "number of samples chosen randomly from original data. Treated",
            "description": ""
          },
          "as": {
            "type": "0.99 (the default) and e is the current fraction of inliers w.r.t.",
            "description": ""
          },
          "relative": {
            "type": "number `ceil(min_samples * X.shape[0])` for",
            "description": "`min_samples < 1`. This is typically chosen as the minimal number of"
          },
          "samples": {
            "type": "iterations",
            "description": ":"
          },
          "dependent": {
            "type": "upon the model, so if a `estimator` other than",
            "description": ":class:`~sklearn.linear_model.LinearRegression` is used, the user must"
          },
          "provide": {
            "type": "a value.",
            "description": ""
          },
          "residual_threshold": {
            "type": "float, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations that can be skipped due to finding zero",
            "description": ""
          },
          "By": {
            "type": "default the threshold is chosen as the MAD (median absolute",
            "description": "deviation) of the target values `y`. Points whose residuals are"
          },
          "strictly": {
            "type": "equal to the threshold are considered as inliers.",
            "description": ""
          },
          "is_data_valid": {
            "type": "callable, default=None",
            "description": ""
          },
          "This": {
            "type": "function is called with the estimated model and the randomly",
            "description": ""
          },
          "model": {
            "type": "is fitted to it: `is_data_valid(X, y)`. If its return value is",
            "description": ""
          },
          "False": {
            "type": "the current randomly chosen sub-sample is skipped.",
            "description": ""
          },
          "is_model_valid": {
            "type": "callable, default=None",
            "description": ""
          },
          "selected": {
            "type": "data: `is_model_valid(model, X, y)`. If its return value is",
            "description": ""
          },
          "Rejecting": {
            "type": "samples with this function is computationally costlier than",
            "description": ""
          },
          "with": {
            "type": "`is_data_valid`. `is_model_valid` should therefore only be used if",
            "description": ""
          },
          "the": {
            "type": "total number of samples.",
            "description": ""
          },
          "max_trials": {
            "type": "int, default=100",
            "description": ""
          },
          "max_skips": {
            "type": "int, default=np.inf",
            "description": ""
          },
          "inliers": {
            "type": "or invalid data defined by ``is_data_valid`` or invalid models",
            "description": ""
          },
          "defined": {
            "type": "by ``is_model_valid``.",
            "description": ".. versionadded:: 0.19"
          },
          "stop_n_inliers": {
            "type": "int, default=np.inf",
            "description": ""
          },
          "Stop": {
            "type": "iteration if score is greater equal than this threshold.",
            "description": ""
          },
          "stop_score": {
            "type": "float, default=np.inf",
            "description": ""
          },
          "stop_probability": {
            "type": "float in range [0, 1], default=0.99",
            "description": ""
          },
          "RANSAC": {
            "type": "iteration stops if at least one outlier-free set of the training",
            "description": ""
          },
          "data": {
            "type": "is sampled in RANSAC. This requires to generate at least N",
            "description": ""
          },
          "N": {
            "type": ">= log(1 - probability) / log(1 - e**m)",
            "description": ""
          },
          "where": {
            "type": "the probability (confidence) is typically set to high value such",
            "description": ""
          },
          "loss": {
            "type": "str, callable, default='absolute_error'",
            "description": ""
          },
          "String": {
            "type": "inputs, 'absolute_error' and 'squared_error' are supported which",
            "description": ""
          },
          "find": {
            "type": "the absolute error and squared error per sample respectively.",
            "description": ""
          },
          "two": {
            "type": "arrays as inputs, the true and predicted value and returns a 1-D",
            "description": ""
          },
          "array": {
            "type": "[-31.9417...]",
            "description": ""
          },
          "on": {
            "type": "``X[i]``.",
            "description": ""
          },
          "then": {
            "type": "this sample is classified as an outlier.",
            "description": ".. versionadded:: 0.18"
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "The": {
            "type": "generator used to initialize the centers.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "estimator_": {
            "type": "object",
            "description": ""
          },
          "Final": {
            "type": "model fitted on the inliers predicted by the \"best\" model found",
            "description": ""
          },
          "during": {
            "type": "RANSAC sampling (copy of the `estimator` object).",
            "description": ""
          },
          "n_trials_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "inlier_mask_": {
            "type": "bool array of shape [n_samples]",
            "description": ""
          },
          "Boolean": {
            "type": "mask of inliers classified as ``True``.",
            "description": ""
          },
          "n_skips_no_inliers_": {
            "type": "int",
            "description": ""
          },
          "n_skips_invalid_data_": {
            "type": "int",
            "description": ""
          },
          "n_skips_invalid_model_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "HuberRegressor": {
            "type": "Linear regression model that is robust to outliers.",
            "description": ""
          },
          "TheilSenRegressor": {
            "type": "Theil",
            "description": "Sen Estimator robust multivariate regression model."
          },
          "SGDRegressor": {
            "type": "Fitted by minimizing a regularized empirical loss with SGD.",
            "description": "References\n----------\n.. [1] https://en.wikipedia.org/wiki/RANSAC\n.. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n.. [3] https://bmva-archive.org.uk/bmvc/2009/Papers/Paper355/Paper355.pdf\nExamples\n--------\n>>> from sklearn.linear_model import RANSACRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = RANSACRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9885...\n>>> reg.predict(X[:1,])"
          },
          "For": {
            "type": "a more detailed example, see",
            "description": ":ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/RANSAC\n    .. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n    .. [3] https://bmva-archive.org.uk/bmvc/2009/Papers/Paper355/Paper355.pdf\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import RANSACRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9885...\n    >>> reg.predict(X[:1,])\n    array([-31.9417...])\n\n    For a more detailed example, see\n    :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`",
        "notes": "that the current implementation only supports regression\n        estimators.\n\n    min_samples : int (>= 1) or float ([0, 1]), default=None\n        Minimum number of samples chosen randomly from original data. Treated\n        as an absolute number of samples for `min_samples >= 1`, treated as a\n        relative number `ceil(min_samples * X.shape[0])` for\n        `min_samples < 1`. This is typically chosen as the minimal number of\n        samples necessary to estimate the given `estimator`. By default a\n        :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and\n        `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\n        dependent upon the model, so if a `estimator` other than\n        :class:`~sklearn.linear_model.LinearRegression` is used, the user must\n        provide a value.\n\n    residual_threshold : float, default=None\n        Maximum residual for a data sample to be classified as an inlier.\n        By default the threshold is chosen as the MAD (median absolute\n        deviation) of the target values `y`. Points whose residuals are\n        strictly equal to the threshold are considered as inliers.\n\n    is_data_valid : callable, default=None\n        This function is called with the randomly selected data before the\n        model is fitted to it: `is_data_valid(X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n\n    is_model_valid : callable, default=None\n        This function is called with the estimated model and the randomly\n        selected data: `is_model_valid(model, X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n        Rejecting samples with this function is computationally costlier than\n        with `is_data_valid`. `is_model_valid` should therefore only be used if\n        the estimated model is needed for making the rejection decision.\n\n    max_trials : int, default=100\n        Maximum number of iterations for random sample selection.\n\n    max_skips : int, default=np.inf\n        Maximum number of iterations that can be skipped due to finding zero\n        inliers or invalid data defined by ``is_data_valid`` or invalid models\n        defined by ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    stop_n_inliers : int, default=np.inf\n        Stop iteration if at least this number of inliers are found.\n\n    stop_score : float, default=np.inf\n        Stop iteration if score is greater equal than this threshold.\n\n    stop_probability : float in range [0, 1], default=0.99\n        RANSAC iteration stops if at least one outlier-free set of the training\n        data is sampled in RANSAC. This requires to generate at least N\n        samples (iterations)::\n\n            N >= log(1 - probability) / log(1 - e**m)\n\n        where the probability (confidence) is typically set to high value such\n        as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n        the total number of samples.\n\n    loss : str, callable, default='absolute_error'\n        String inputs, 'absolute_error' and 'squared_error' are supported which\n        find the absolute error and squared error per sample respectively.\n\n        If ``loss`` is a callable, then it should be a function that takes\n        two arrays as inputs, the true and predicted value and returns a 1-D\n        array with the i-th value of the array corresponding to the loss\n        on ``X[i]``.\n\n        If the loss on a sample is greater than the ``residual_threshold``,\n        then this sample is classified as an outlier.\n\n        .. versionadded:: 0.18\n\n    random_state : int, RandomState instance, default=None\n        The generator used to initialize the centers.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    estimator_ : object\n        Final model fitted on the inliers predicted by the \"best\" model found\n        during RANSAC sampling (copy of the `estimator` object).\n\n    n_trials_ : int\n        Number of random selection trials until one of the stop criteria is\n        met. It is always ``<= max_trials``.\n\n    inlier_mask_ : bool array of shape [n_samples]\n        Boolean mask of inliers classified as ``True``.\n\n    n_skips_no_inliers_ : int\n        Number of iterations skipped due to finding zero inliers.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_data_ : int\n        Number of iterations skipped due to invalid data defined by\n        ``is_data_valid``.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_model_ : int\n        Number of iterations skipped due to an invalid model defined by\n        ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/RANSAC\n    .. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n    .. [3] https://bmva-archive.org.uk/bmvc/2009/Papers/Paper355/Paper355.pdf\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import RANSACRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9885...\n    >>> reg.predict(X[:1,])\n    array([-31.9417...])\n\n    For a more detailed example, see\n    :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`",
        "examples": "--------\n    >>> from sklearn.linear_model import RANSACRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9885...\n    >>> reg.predict(X[:1,])\n    array([-31.9417...])\n\n    For a more detailed example, see\n    :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, *, sample_weight=None, **fit_params)",
          "documentation": {
            "description": "Fit estimator using RANSAC algorithm.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Individual": {
                "type": "weights for each sample",
                "description": ""
              },
              "raises": {
                "type": "error if sample_weight is passed and estimator",
                "description": ""
              },
              "fit": {
                "type": "method does not support it.",
                "description": ".. versionadded:: 0.18\n**fit_params : dict"
              }
            },
            "returns": "-------\n        self : object\n            Fitted `RANSACRegressor` estimator.\n\n        Raises\n        ------\n        ValueError\n            If no valid consensus set could be found. This occurs if\n            `is_data_valid` and `is_model_valid` return False for all\n            `max_trials` randomly chosen sub-samples.",
            "raises": "error if sample_weight is passed and estimator\n            fit method does not support it.\n\n            .. versionadded:: 0.18\n\n        **fit_params : dict\n            Parameters routed to the `fit` method of the sub-estimator via the\n            metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.\n\n        Returns\n        -------\n        self : object\n            Fitted `RANSACRegressor` estimator.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X, **params)",
          "documentation": {
            "description": "Predict using the estimated model.\n\n        This is a wrapper for `estimator_.predict(X)`.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        **params : dict\n            Parameters routed to the `predict` method of the sub-estimator via\n            the metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like or sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "**params : dict"
              }
            },
            "returns": "-------\n        y : array, shape = [n_samples] or [n_samples, n_targets]",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, **params)",
          "documentation": {
            "description": "Return the score of the prediction.\n\n        This is a wrapper for `estimator_.score(X, y)`.\n\n        Parameters\n        ----------\n        X : (array-like or sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        **params : dict\n            Parameters routed to the `score` method of the sub-estimator via\n            the metadata routing API.\n\n            .. versionadded:: 1.5\n\n                Only available if\n                `sklearn.set_config(enable_metadata_routing=True)` is set. See\n                :ref:`Metadata Routing User Guide <metadata_routing>` for more\n                details.",
            "parameters": {
              "X": {
                "type": "(array",
                "description": "like or sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_targets)"
              },
              "Target": {
                "type": "values.",
                "description": "**params : dict"
              }
            },
            "returns": "-------\n        z : float\n            Score of the prediction.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._ransac.RANSACRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ransac.RANSACRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Ridge",
      "documentation": {
        "description": "Linear least squares with l2 regularization.\n\n    Minimizes the objective function::\n\n    ||y - Xw||^2_2 + alpha * ||w||^2_2\n\n    This model solves a regression model where the loss function is\n    the linear least squares function and regularization is given by\n    the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n    This estimator has built-in support for multi-variate regression\n    (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n        Constant that multiplies the L2 term, controlling regularization\n        strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n\n        When `alpha = 0`, the objective is equivalent to ordinary least\n        squares, solved by the :class:`LinearRegression` object. For numerical\n        reasons, using `alpha = 0` with the `Ridge` object is not advised.\n        Instead, you should use the :class:`LinearRegression` object.\n\n        If an array is passed, penalties are assumed to be specific to the\n        targets. Hence they must correspond in number.\n\n    fit_intercept : bool, default=True\n        Whether to fit the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. ``X`` and ``y`` are expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=None\n        Maximum number of iterations for conjugate gradient solver.\n        For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n        by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n        For 'lbfgs' solver, the default value is 15000.\n\n    tol : float, default=1e-4\n        The precision of the solution (`coef_`) is determined by `tol` which\n        specifies a different convergence criterion for each solver:\n\n        - 'svd': `tol` has no impact.\n\n        - 'cholesky': `tol` has no impact.\n\n        - 'sparse_cg': norm of residuals smaller than `tol`.\n\n        - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n          which control the norm of the residual vector in terms of the norms of\n          matrix and coefficients.\n\n        - 'sag' and 'saga': relative change of coef smaller than `tol`.\n\n        - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n          smaller than `tol`.\n\n        .. versionchanged:: 1.2\n           Default value changed from 1e-3 to 1e-4 for consistency with other linear\n           models.\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. It is the most stable solver, in particular more stable\n          for singular matrices than 'cholesky' at the cost of being slower.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its improved, unbiased version named SAGA. Both methods also use an\n          iterative procedure, and are often faster than other solvers when\n          both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n        - 'lbfgs' uses L-BFGS-B algorithm implemented in\n          `scipy.optimize.minimize`. It can be used only when `positive`\n          is True.\n\n        All solvers except 'svd' support both dense and sparse data. However, only\n        'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n        `fit_intercept` is True.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n        Only 'lbfgs' solver is supported in this case.\n\n    random_state : int, RandomState instance, default=None\n        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n        See :term:`Glossary <random_state>` for details.\n\n        .. versionadded:: 0.17\n           `random_state` to support Stochastic Average Gradient.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : None or ndarray of shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n        .. versionadded:: 0.17\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    solver_ : str\n        The solver that was used at fit time by the computational\n        routines.\n\n        .. versionadded:: 1.5\n\n    See Also\n    --------\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n    :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n        combines ridge regression with the kernel trick.\n\n    Notes\n    -----\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.",
        "parameters": {
          "alpha": {
            "type": "{float, ndarray of shape (n_targets,)}, default=1.0",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L2 term, controlling regularization",
            "description": "strength. `alpha` must be a non-negative float i.e. in `[0, inf)`."
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "If": {
            "type": "True, X will be copied; else, it may be overwritten.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to fit the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. ``X`` and ``y`` are expected to be centered)."
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations for conjugate gradient solver.",
            "description": ""
          },
          "For": {
            "type": "'lbfgs' solver, the default value is 15000.",
            "description": ""
          },
          "by": {
            "type": "scipy.sparse.linalg. For 'sag' solver, the default value is 1000.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "The": {
            "type": "solver that was used at fit time by the computational",
            "description": "routines.\n.. versionadded:: 1.5"
          },
          "specifies": {
            "type": "a different convergence criterion for each solver:",
            "description": "- 'svd': `tol` has no impact.\n- 'cholesky': `tol` has no impact.\n- 'sparse_cg': norm of residuals smaller than `tol`.\n- 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,"
          },
          "which": {
            "type": "control the norm of the residual vector in terms of the norms of",
            "description": ""
          },
          "matrix": {
            "type": "and coefficients.",
            "description": "- 'sag' and 'saga': relative change of coef smaller than `tol`.\n- 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|"
          },
          "smaller": {
            "type": "than `tol`.",
            "description": ".. versionchanged:: 1.2"
          },
          "Default": {
            "type": "value changed from 1e-3 to 1e-4 for consistency with other linear",
            "description": "models."
          },
          "solver": {
            "type": "{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'",
            "description": ""
          },
          "Solver": {
            "type": "to use in the computational routines:",
            "description": "- 'auto' chooses the solver automatically based on the type of data.\n- 'svd' uses a Singular Value Decomposition of X to compute the Ridge\ncoefficients. It is the most stable solver, in particular more stable"
          },
          "for": {
            "type": "singular matrices than 'cholesky' at the cost of being slower.",
            "description": "- 'cholesky' uses the standard scipy.linalg.solve function to"
          },
          "obtain": {
            "type": "a closed-form solution.",
            "description": "- 'sparse_cg' uses the conjugate gradient solver as found in\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is"
          },
          "more": {
            "type": "appropriate than 'cholesky' for large-scale data",
            "description": "(possibility to set `tol` and `max_iter`).\n- 'lsqr' uses the dedicated regularized least-squares routine\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\nprocedure.\n- 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses"
          },
          "its": {
            "type": "improved, unbiased version named SAGA. Both methods also use an",
            "description": ""
          },
          "iterative": {
            "type": "procedure, and are often faster than other solvers when",
            "description": ""
          },
          "both": {
            "type": "n_samples and n_features are large. Note that 'sag' and",
            "description": "'saga' fast convergence is only guaranteed on features with"
          },
          "approximately": {
            "type": "the same scale. You can preprocess the data with a",
            "description": ""
          },
          "scaler": {
            "type": "from sklearn.preprocessing.",
            "description": "- 'lbfgs' uses L-BFGS-B algorithm implemented in\n`scipy.optimize.minimize`. It can be used only when `positive`"
          },
          "is": {
            "type": "True.",
            "description": ""
          },
          "All": {
            "type": "solvers except 'svd' support both dense and sparse data. However, only",
            "description": "'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n`fit_intercept` is True.\n.. versionadded:: 0.17"
          },
          "Stochastic": {
            "type": "Average Gradient descent solver.",
            "description": ".. versionadded:: 0.19"
          },
          "SAGA": {
            "type": "solver.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "Only": {
            "type": "'lbfgs' solver is supported in this case.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "when ``solver`` == 'sag' or 'saga' to shuffle the data.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
            "description": ""
          },
          "Weight": {
            "type": "vector(s).",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "``fit_intercept = False``."
          },
          "n_iter_": {
            "type": "None or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations for each target. Available only for",
            "description": ""
          },
          "sag": {
            "type": "and lsqr solvers. Other solvers will return None.",
            "description": ".. versionadded:: 0.17"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "solver_": {
            "type": "str",
            "description": ""
          },
          "RidgeClassifier": {
            "type": "Ridge classifier.",
            "description": ""
          },
          "RidgeCV": {
            "type": "Ridge regression with built",
            "description": "in cross validation.\n:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression"
          },
          "combines": {
            "type": "ridge regression with the kernel trick.",
            "description": "Notes\n-----"
          },
          "Regularization": {
            "type": "improves the conditioning of the problem and",
            "description": ""
          },
          "reduces": {
            "type": "the variance of the estimates. Larger values specify stronger",
            "description": "regularization. Alpha corresponds to ``1 / (2C)`` in other linear"
          },
          "models": {
            "type": "such as :class:`~sklearn.linear_model.LogisticRegression` or",
            "description": ":class:`~sklearn.svm.LinearSVC`.\nExamples\n--------\n>>> from sklearn.linear_model import Ridge\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> clf = Ridge(alpha=1.0)\n>>> clf.fit(X, y)"
          },
          "Ridge": {
            "type": "",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n    :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n        combines ridge regression with the kernel trick.\n\n    Notes\n    -----\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import Ridge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = Ridge(alpha=1.0)\n    >>> clf.fit(X, y)\n    Ridge()",
        "notes": "-----\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import Ridge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = Ridge(alpha=1.0)\n    >>> clf.fit(X, y)\n    Ridge()",
        "examples": "--------\n    >>> from sklearn.linear_model import Ridge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = Ridge(alpha=1.0)\n    >>> clf.fit(X, y)\n    Ridge()"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit Ridge regression model.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.",
            "parameters": {
              "X": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_targets)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or ndarray of shape (n_samples,), default=None",
                "description": ""
              },
              "Individual": {
                "type": "weights for each sample. If given a float, every sample",
                "description": ""
              },
              "will": {
                "type": "have the same weight.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._ridge.Ridge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.Ridge",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._ridge.Ridge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.Ridge",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RidgeCV",
      "documentation": {
        "description": "Ridge regression with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs efficient Leave-One-Out Cross-Validation.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n        If using Leave-One-Out cross-validation, alphas must be strictly positive.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    scoring : str, callable, default=None\n        A string (see :ref:`scoring_parameter`) or a scorer callable object /\n        function with signature ``scorer(estimator, X, y)``. If None, the\n        negative mean squared error if cv is 'auto' or None (i.e. when using\n        leave-one-out cross-validation), and r2 score otherwise.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n        Flag indicating which strategy to use when performing\n        Leave-One-Out Cross-Validation. Options are::\n\n            'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n            'svd' : force use of singular value decomposition of X when X is\n                dense, eigenvalue decomposition of X^T.X when X is sparse.\n            'eigen' : force computation via eigendecomposition of X.X^T\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending on the shape of the training data.\n\n    store_cv_results : bool, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_results_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n        .. versionchanged:: 1.5\n            Parameter name changed from `store_cv_values` to `store_cv_results`.\n\n    alpha_per_target : bool, default=False\n        Flag indicating whether to optimize the alpha value (picked from the\n        `alphas` parameter list) for each target separately (for multi-output\n        settings: multiple prediction targets). When set to `True`, after\n        fitting, the `alpha_` attribute will contain a value for each target.\n        When set to `False`, a single alpha is used for all targets.\n\n        .. versionadded:: 0.24\n\n    store_cv_values : bool\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n        .. deprecated:: 1.5\n            `store_cv_values` is deprecated in version 1.5 in favor of\n            `store_cv_results` and will be removed in version 1.7.\n\n    Attributes\n    ----------\n    cv_results_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation values for each alpha (only available if\n        ``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been\n        called, this attribute will contain the mean squared errors if\n        `scoring is None` otherwise it will contain standardized per point\n        prediction values.\n\n        .. versionchanged:: 1.5\n            `cv_values_` changed to `cv_results_`.\n\n    coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float or ndarray of shape (n_targets,)\n        Estimated regularization parameter, or, if ``alpha_per_target=True``,\n        the estimated regularization parameter for each target.\n\n    best_score_ : float or ndarray of shape (n_targets,)\n        Score of base estimator with best alpha, or, if\n        ``alpha_per_target=True``, a score for each target.\n\n        .. versionadded:: 0.23\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n    RidgeClassifierCV : Ridge classifier with built-in cross validation.",
        "parameters": {
          "alphas": {
            "type": "array",
            "description": "like of shape (n_alphas,), default=(0.1, 1.0, 10.0)"
          },
          "Array": {
            "type": "of alpha values to try.",
            "description": ""
          },
          "Regularization": {
            "type": "strength; must be a positive float. Regularization",
            "description": ""
          },
          "improves": {
            "type": "the conditioning of the problem and reduces the variance of",
            "description": ""
          },
          "the": {
            "type": "estimated regularization parameter for each target.",
            "description": ""
          },
          "Alpha": {
            "type": "corresponds to ``1 / (2C)`` in other linear models such as",
            "description": ":class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`."
          },
          "If": {
            "type": "using Leave-One-Out cross-validation, alphas must be strictly positive.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "scoring": {
            "type": "str, callable, default=None",
            "description": ""
          },
          "A": {
            "type": "string (see :ref:`scoring_parameter`) or a scorer callable object /",
            "description": ""
          },
          "function": {
            "type": "with signature ``scorer(estimator, X, y)``. If None, the",
            "description": ""
          },
          "negative": {
            "type": "mean squared error if cv is 'auto' or None (i.e. when using",
            "description": "leave-one-out cross-validation), and r2 score otherwise."
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or an iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the efficient Leave-One-Out cross-validation\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, if ``y`` is binary or multiclass,",
            "description": ":class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n:class:`~sklearn.model_selection.KFold` is used."
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here."
          },
          "gcv_mode": {
            "type": "{'auto', 'svd', 'eigen'}, default='auto'",
            "description": ""
          },
          "Flag": {
            "type": "indicating if the cross-validation values corresponding to",
            "description": ""
          },
          "The": {
            "type": "'auto' mode is the default and is intended to pick the cheaper",
            "description": ""
          },
          "option": {
            "type": "of the two depending on the shape of the training data.",
            "description": ""
          },
          "store_cv_results": {
            "type": "bool, default=False",
            "description": ""
          },
          "each": {
            "type": "alpha should be stored in the ``cv_values_`` attribute (see",
            "description": "below). This flag is only compatible with ``cv=None`` (i.e. using\nLeave-One-Out Cross-Validation).\n.. deprecated:: 1.5\n`store_cv_values` is deprecated in version 1.5 in favor of\n`store_cv_results` and will be removed in version 1.7.\nAttributes\n----------"
          },
          "Parameter": {
            "type": "name changed from `store_cv_values` to `store_cv_results`.",
            "description": ""
          },
          "alpha_per_target": {
            "type": "bool, default=False",
            "description": ""
          },
          "settings": {
            "type": "multiple prediction targets). When set to `True`, after",
            "description": "fitting, the `alpha_` attribute will contain a value for each target."
          },
          "When": {
            "type": "set to `False`, a single alpha is used for all targets.",
            "description": ".. versionadded:: 0.24"
          },
          "store_cv_values": {
            "type": "bool",
            "description": ""
          },
          "cv_results_": {
            "type": "ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional",
            "description": "Cross-validation values for each alpha (only available if\n``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been\ncalled, this attribute will contain the mean squared errors if\n`scoring is None` otherwise it will contain standardized per point"
          },
          "prediction": {
            "type": "values.",
            "description": ".. versionchanged:: 1.5\n`cv_values_` changed to `cv_results_`."
          },
          "coef_": {
            "type": "ndarray of shape (n_features) or (n_targets, n_features)",
            "description": ""
          },
          "Weight": {
            "type": "vector(s).",
            "description": ""
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "``fit_intercept = False``."
          },
          "alpha_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Estimated": {
            "type": "regularization parameter, or, if ``alpha_per_target=True``,",
            "description": ""
          },
          "best_score_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Score": {
            "type": "of base estimator with best alpha, or, if",
            "description": "``alpha_per_target=True``, a score for each target.\n.. versionadded:: 0.23"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "Ridge": {
            "type": "Ridge regression.",
            "description": ""
          },
          "RidgeClassifier": {
            "type": "Classifier based on ridge regression on {",
            "description": "1, 1} labels."
          },
          "RidgeClassifierCV": {
            "type": "Ridge classifier with built",
            "description": "in cross validation.\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> X, y = load_diabetes(return_X_y=True)\n>>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.5166..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Ridge : Ridge regression.\n    RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n    RidgeClassifierCV : Ridge classifier with built-in cross validation.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.5166...",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.5166..."
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **params)",
          "documentation": {
            "description": "Fit Ridge regression model with cv.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training data. If using GCV, will be cast to float64\n            if necessary.\n\n        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.\n\n        **params : dict, default=None\n            Parameters to be passed to the underlying scorer.\n\n            .. versionadded:: 1.5\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "data. If using GCV, will be cast to float64",
                "description": ""
              },
              "if": {
                "type": "necessary.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_targets)",
                "description": ""
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or ndarray of shape (n_samples,), default=None",
                "description": ""
              },
              "Individual": {
                "type": "weights for each sample. If given a float, every sample",
                "description": ""
              },
              "will": {
                "type": "have the same weight.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        When sample_weight is provided, the selected hyperparameter may depend\n        on whether we use leave-one-out cross-validation (cv=None or cv='auto')\n        or another form of cross-validation, because only leave-one-out\n        cross-validation takes the sample weights into account when computing\n        the validation score.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        When sample_weight is provided, the selected hyperparameter may depend\n        on whether we use leave-one-out cross-validation (cv=None or cv='auto')\n        or another form of cross-validation, because only leave-one-out\n        cross-validation takes the sample weights into account when computing\n        the validation score.",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._ridge.RidgeCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._ridge.RidgeCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RidgeClassifier",
      "documentation": {
        "description": "Classifier using Ridge regression.\n\n    This classifier first converts the target values into ``{-1, 1}`` and\n    then treats the problem as a regression task (multi-output regression in\n    the multiclass case).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to false, no\n        intercept will be used in calculations (e.g. data is expected to be\n        already centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, default=None\n        Maximum number of iterations for conjugate gradient solver.\n        The default value is determined by scipy.sparse.linalg.\n\n    tol : float, default=1e-4\n        The precision of the solution (`coef_`) is determined by `tol` which\n        specifies a different convergence criterion for each solver:\n\n        - 'svd': `tol` has no impact.\n\n        - 'cholesky': `tol` has no impact.\n\n        - 'sparse_cg': norm of residuals smaller than `tol`.\n\n        - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n          which control the norm of the residual vector in terms of the norms of\n          matrix and coefficients.\n\n        - 'sag' and 'saga': relative change of coef smaller than `tol`.\n\n        - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n          smaller than `tol`.\n\n        .. versionchanged:: 1.2\n           Default value changed from 1e-3 to 1e-4 for consistency with other linear\n           models.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. It is the most stable solver, in particular more stable\n          for singular matrices than 'cholesky' at the cost of being slower.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its unbiased and more flexible version named SAGA. Both methods\n          use an iterative procedure, and are often faster than other solvers\n          when both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n          .. versionadded:: 0.17\n             Stochastic Average Gradient descent solver.\n          .. versionadded:: 0.19\n             SAGA solver.\n\n        - 'lbfgs' uses L-BFGS-B algorithm implemented in\n          `scipy.optimize.minimize`. It can be used only when `positive`\n          is True.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive.\n        Only 'lbfgs' solver is supported in this case.\n\n    random_state : int, RandomState instance, default=None\n        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n        See :term:`Glossary <random_state>` for details.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : None or ndarray of shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    solver_ : str\n        The solver that was used at fit time by the computational\n        routines.\n\n        .. versionadded:: 1.5\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Regularization": {
            "type": "strength; must be a positive float. Regularization",
            "description": ""
          },
          "improves": {
            "type": "the conditioning of the problem and reduces the variance of",
            "description": ""
          },
          "the": {
            "type": "estimates. Larger values specify stronger regularization.",
            "description": ""
          },
          "Alpha": {
            "type": "corresponds to ``1 / (2C)`` in other linear models such as",
            "description": ":class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`."
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set to false, no",
            "description": ""
          },
          "intercept": {
            "type": "will be used in calculations (e.g. data is expected to be",
            "description": ""
          },
          "already": {
            "type": "centered).",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "not given, all classes are supposed to have weight one.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=None",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations for conjugate gradient solver.",
            "description": ""
          },
          "The": {
            "type": "solver that was used at fit time by the computational",
            "description": "routines.\n.. versionadded:: 1.5"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "specifies": {
            "type": "a different convergence criterion for each solver:",
            "description": "- 'svd': `tol` has no impact.\n- 'cholesky': `tol` has no impact.\n- 'sparse_cg': norm of residuals smaller than `tol`.\n- 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,"
          },
          "which": {
            "type": "control the norm of the residual vector in terms of the norms of",
            "description": ""
          },
          "matrix": {
            "type": "and coefficients.",
            "description": "- 'sag' and 'saga': relative change of coef smaller than `tol`.\n- 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|"
          },
          "smaller": {
            "type": "than `tol`.",
            "description": ".. versionchanged:: 1.2"
          },
          "Default": {
            "type": "value changed from 1e-3 to 1e-4 for consistency with other linear",
            "description": "models."
          },
          "class_weight": {
            "type": "dict or 'balanced', default=None",
            "description": ""
          },
          "Weights": {
            "type": "associated with classes in the form ``{class_label: weight}``.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "solver": {
            "type": "{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'",
            "description": ""
          },
          "Solver": {
            "type": "to use in the computational routines:",
            "description": "- 'auto' chooses the solver automatically based on the type of data.\n- 'svd' uses a Singular Value Decomposition of X to compute the Ridge\ncoefficients. It is the most stable solver, in particular more stable"
          },
          "for": {
            "type": "singular matrices than 'cholesky' at the cost of being slower.",
            "description": "- 'cholesky' uses the standard scipy.linalg.solve function to"
          },
          "obtain": {
            "type": "a closed-form solution.",
            "description": "- 'sparse_cg' uses the conjugate gradient solver as found in\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is"
          },
          "more": {
            "type": "appropriate than 'cholesky' for large-scale data",
            "description": "(possibility to set `tol` and `max_iter`).\n- 'lsqr' uses the dedicated regularized least-squares routine\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\nprocedure.\n- 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses"
          },
          "its": {
            "type": "unbiased and more flexible version named SAGA. Both methods",
            "description": ""
          },
          "use": {
            "type": "an iterative procedure, and are often faster than other solvers",
            "description": ""
          },
          "when": {
            "type": "both n_samples and n_features are large. Note that 'sag' and",
            "description": "'saga' fast convergence is only guaranteed on features with"
          },
          "approximately": {
            "type": "the same scale. You can preprocess the data with a",
            "description": ""
          },
          "scaler": {
            "type": "from sklearn.preprocessing.",
            "description": ".. versionadded:: 0.17"
          },
          "Stochastic": {
            "type": "Average Gradient descent solver.",
            "description": ".. versionadded:: 0.19"
          },
          "SAGA": {
            "type": "solver.",
            "description": "- 'lbfgs' uses L-BFGS-B algorithm implemented in\n`scipy.optimize.minimize`. It can be used only when `positive`"
          },
          "is": {
            "type": "True.",
            "description": ""
          },
          "positive": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to ``True``, forces the coefficients to be positive.",
            "description": ""
          },
          "Only": {
            "type": "'lbfgs' solver is supported in this case.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "when ``solver`` == 'sag' or 'saga' to shuffle the data.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
            "description": ""
          },
          "Coefficient": {
            "type": "of the features in the decision function.",
            "description": "``coef_`` is of shape (1, n_features) when the given problem is binary."
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "``fit_intercept = False``."
          },
          "n_iter_": {
            "type": "None or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations for each target. Available only for",
            "description": ""
          },
          "sag": {
            "type": "and lsqr solvers. Other solvers will return None.",
            "description": ""
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "solver_": {
            "type": "str",
            "description": ""
          },
          "Ridge": {
            "type": "Ridge regression.",
            "description": ""
          },
          "RidgeClassifierCV": {
            "type": "Ridge classifier with built",
            "description": "in cross validation.\nNotes\n-----"
          },
          "For": {
            "type": "multi-class classification, n_class classifiers are trained in",
            "description": ""
          },
          "a": {
            "type": "one-versus-all approach. Concretely, this is implemented by taking",
            "description": ""
          },
          "advantage": {
            "type": "of the multi-variate response support in Ridge.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifier\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifier().fit(X, y)\n>>> clf.score(X, y)\n0.9595..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Ridge : Ridge regression.\n    RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifier\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    0.9595...",
        "notes": "-----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifier\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    0.9595...",
        "examples": "--------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifier\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    0.9595..."
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit Ridge classifier model.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : ndarray of shape (n_samples,)\n            Target values.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.\n\n            .. versionadded:: 0.17\n               *sample_weight* support to RidgeClassifier.",
            "parameters": {
              "X": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or ndarray of shape (n_samples,), default=None",
                "description": ""
              },
              "Individual": {
                "type": "weights for each sample. If given a float, every sample",
                "description": ""
              },
              "will": {
                "type": "have the same weight.",
                "description": ".. versionadded:: 0.17\n*sample_weight* support to RidgeClassifier.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Instance": {
                "type": "of the estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Instance of the estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in `X`.\n\n        Parameters\n        ----------\n        X : {array-like, spare matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to predict the targets.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, spare matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to predict the targets.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_outputs)",
                "description": ""
              },
              "Vector": {
                "type": "or matrix containing the predictions. In binary and",
                "description": ""
              },
              "multiclass": {
                "type": "problems, this is a vector containing `n_samples`. In",
                "description": ""
              },
              "a": {
                "type": "multilabel problem, it returns a matrix of shape",
                "description": "`(n_samples, n_outputs)`."
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            Vector or matrix containing the predictions. In binary and\n            multiclass problems, this is a vector containing `n_samples`. In\n            a multilabel problem, it returns a matrix of shape\n            `(n_samples, n_outputs)`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._ridge.RidgeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._ridge.RidgeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RidgeClassifierCV",
      "documentation": {
        "description": "Ridge classifier with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs Leave-One-Out Cross-Validation. Currently,\n    only the n_features > n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n        If using Leave-One-Out cross-validation, alphas must be strictly positive.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    scoring : str, callable, default=None\n        A string (see :ref:`scoring_parameter`) or a scorer callable object /\n        function with signature ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    store_cv_results : bool, default=False\n        Flag indicating if the cross-validation results corresponding to\n        each alpha should be stored in the ``cv_results_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n        .. versionchanged:: 1.5\n            Parameter name changed from `store_cv_values` to `store_cv_results`.\n\n    store_cv_values : bool\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n        .. deprecated:: 1.5\n            `store_cv_values` is deprecated in version 1.5 in favor of\n            `store_cv_results` and will be removed in version 1.7.\n\n    Attributes\n    ----------\n    cv_results_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation results for each alpha (only if ``store_cv_results=True`` and\n        ``cv=None``). After ``fit()`` has been called, this attribute will\n        contain the mean squared errors if `scoring is None` otherwise it\n        will contain standardized per point prediction values.\n\n        .. versionchanged:: 1.5\n            `cv_values_` changed to `cv_results_`.\n\n    coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    best_score_ : float\n        Score of base estimator with best alpha.\n\n        .. versionadded:: 0.23\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.",
        "parameters": {
          "alphas": {
            "type": "array",
            "description": "like of shape (n_alphas,), default=(0.1, 1.0, 10.0)"
          },
          "Array": {
            "type": "of alpha values to try.",
            "description": ""
          },
          "Regularization": {
            "type": "strength; must be a positive float. Regularization",
            "description": ""
          },
          "improves": {
            "type": "the conditioning of the problem and reduces the variance of",
            "description": ""
          },
          "the": {
            "type": "estimates. Larger values specify stronger regularization.",
            "description": ""
          },
          "Alpha": {
            "type": "corresponds to ``1 / (2C)`` in other linear models such as",
            "description": ":class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`."
          },
          "If": {
            "type": "not given, all classes are supposed to have weight one.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations",
            "description": "(i.e. data is expected to be centered)."
          },
          "scoring": {
            "type": "str, callable, default=None",
            "description": ""
          },
          "A": {
            "type": "string (see :ref:`scoring_parameter`) or a scorer callable object /",
            "description": ""
          },
          "function": {
            "type": "with signature ``scorer(estimator, X, y)``.",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or an iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the efficient Leave-One-Out cross-validation\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here."
          },
          "class_weight": {
            "type": "dict or 'balanced', default=None",
            "description": ""
          },
          "Weights": {
            "type": "associated with classes in the form ``{class_label: weight}``.",
            "description": ""
          },
          "The": {
            "type": "classes labels.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "store_cv_results": {
            "type": "bool, default=False",
            "description": ""
          },
          "Flag": {
            "type": "indicating if the cross-validation values corresponding to",
            "description": ""
          },
          "each": {
            "type": "alpha should be stored in the ``cv_values_`` attribute (see",
            "description": "below). This flag is only compatible with ``cv=None`` (i.e. using\nLeave-One-Out Cross-Validation).\n.. deprecated:: 1.5\n`store_cv_values` is deprecated in version 1.5 in favor of\n`store_cv_results` and will be removed in version 1.7.\nAttributes\n----------"
          },
          "Parameter": {
            "type": "name changed from `store_cv_values` to `store_cv_results`.",
            "description": ""
          },
          "store_cv_values": {
            "type": "bool",
            "description": ""
          },
          "cv_results_": {
            "type": "ndarray of shape (n_samples, n_targets, n_alphas), optional",
            "description": "Cross-validation results for each alpha (only if ``store_cv_results=True`` and\n``cv=None``). After ``fit()`` has been called, this attribute will"
          },
          "contain": {
            "type": "the mean squared errors if `scoring is None` otherwise it",
            "description": ""
          },
          "will": {
            "type": "contain standardized per point prediction values.",
            "description": ".. versionchanged:: 1.5\n`cv_values_` changed to `cv_results_`."
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) or (n_targets, n_features)",
            "description": ""
          },
          "Coefficient": {
            "type": "of the features in the decision function.",
            "description": "``coef_`` is of shape (1, n_features) when the given problem is binary."
          },
          "intercept_": {
            "type": "float or ndarray of shape (n_targets,)",
            "description": ""
          },
          "Independent": {
            "type": "term in decision function. Set to 0.0 if",
            "description": "``fit_intercept = False``."
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "Estimated": {
            "type": "regularization parameter.",
            "description": ""
          },
          "best_score_": {
            "type": "float",
            "description": ""
          },
          "Score": {
            "type": "of base estimator with best alpha.",
            "description": ".. versionadded:: 0.23"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "Ridge": {
            "type": "Ridge regression.",
            "description": ""
          },
          "RidgeClassifier": {
            "type": "Ridge classifier.",
            "description": ""
          },
          "RidgeCV": {
            "type": "Ridge regression with built",
            "description": "in cross validation.\nNotes\n-----"
          },
          "For": {
            "type": "multi-class classification, n_class classifiers are trained in",
            "description": ""
          },
          "a": {
            "type": "one-versus-all approach. Concretely, this is implemented by taking",
            "description": ""
          },
          "advantage": {
            "type": "of the multi-variate response support in Ridge.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.9630..."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    Ridge : Ridge regression.\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.9630...",
        "notes": "-----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.9630...",
        "examples": "--------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.9630..."
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **params)",
          "documentation": {
            "description": "Fit Ridge classifier with cv.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features. When using GCV,\n            will be cast to float64 if necessary.\n\n        y : ndarray of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.\n\n        **params : dict, default=None\n            Parameters to be passed to the underlying scorer.\n\n            .. versionadded:: 1.5\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "vectors, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features. When using GCV,",
                "description": ""
              },
              "will": {
                "type": "have the same weight.",
                "description": "**params : dict, default=None"
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values. Will be cast to X's dtype if necessary.",
                "description": ""
              },
              "sample_weight": {
                "type": "float or ndarray of shape (n_samples,), default=None",
                "description": ""
              },
              "Individual": {
                "type": "weights for each sample. If given a float, every sample",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in `X`.\n\n        Parameters\n        ----------\n        X : {array-like, spare matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to predict the targets.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, spare matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to predict the targets.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_outputs)",
                "description": ""
              },
              "Vector": {
                "type": "or matrix containing the predictions. In binary and",
                "description": ""
              },
              "multiclass": {
                "type": "problems, this is a vector containing `n_samples`. In",
                "description": ""
              },
              "a": {
                "type": "multilabel problem, it returns a matrix of shape",
                "description": "`(n_samples, n_outputs)`."
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            Vector or matrix containing the predictions. In binary and\n            multiclass problems, this is a vector containing `n_samples`. In\n            a multilabel problem, it returns a matrix of shape\n            `(n_samples, n_outputs)`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._ridge.RidgeClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifierCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._ridge.RidgeClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifierCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SGDClassifier",
      "documentation": {
        "description": "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n\n    This estimator implements regularized linear models with stochastic\n    gradient descent (SGD) learning: the gradient of the loss is estimated\n    each sample at a time and the model is updated along the way with a\n    decreasing strength schedule (aka learning rate). SGD allows minibatch\n    (online/out-of-core) learning via the `partial_fit` method.\n    For best results using the default learning rate schedule, the data should\n    have zero mean and unit variance.\n\n    This implementation works with data represented as dense or sparse arrays\n    of floating point values for the features. The model it fits can be\n    controlled with the loss parameter; by default, it fits a linear support\n    vector machine (SVM).\n\n    The regularizer is a penalty added to the loss function that shrinks model\n    parameters towards the zero vector using either the squared euclidean norm\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n    parameter update crosses the 0.0 value because of the regularizer, the\n    update is truncated to 0.0 to allow for learning sparse models and achieve\n    online feature selection.\n\n    Read more in the :ref:`User Guide <sgd>`.\n\n    Parameters\n    ----------\n    loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\n        The loss function to be used.\n\n        - 'hinge' gives a linear SVM.\n        - 'log_loss' gives logistic regression, a probabilistic classifier.\n        - 'modified_huber' is another smooth loss that brings tolerance to\n          outliers as well as probability estimates.\n        - 'squared_hinge' is like hinge but is quadratically penalized.\n        - 'perceptron' is the linear loss used by the perceptron algorithm.\n        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n          'squared_epsilon_insensitive' are designed for regression but can be useful\n          in classification as well; see\n          :class:`~sklearn.linear_model.SGDRegressor` for a description.\n\n        More details about the losses formulas can be found in the :ref:`User Guide\n        <sgd_mathematical_formulation>` and you can find a visualisation of the loss\n        functions in\n        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.\n\n    penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\n        which is the standard regularizer for linear SVM models. 'l1' and\n        'elasticnet' might bring sparsity to the model (feature selection)\n        not achievable with 'l2'. No penalty is added when set to `None`.\n\n        You can see a visualisation of the penalties in\n        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\n\n    alpha : float, default=0.0001\n        Constant that multiplies the regularization term. The higher the\n        value, the stronger the regularization. Also used to compute the\n        learning rate when `learning_rate` is set to 'optimal'.\n        Values must be in the range `[0.0, inf)`.\n\n    l1_ratio : float, default=0.15\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n        Only used if `penalty` is 'elasticnet'.\n        Values must be in the range `[0.0, 1.0]`.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, training will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n        Convergence is checked against the training loss or the\n        validation loss depending on the `early_stopping` parameter.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n        Values must be in the range `[0, inf)`.\n\n    epsilon : float, default=0.1\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n        Values must be in the range `[0.0, inf)`.\n\n    n_jobs : int, default=None\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n        Integer values must be in the range `[0, 2**32 - 1]`.\n\n    learning_rate : str, default='optimal'\n        The learning rate schedule:\n\n        - 'constant': `eta = eta0`\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n          where `t0` is chosen by a heuristic proposed by Leon Bottou.\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\n        - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n          Each time n_iter_no_change consecutive epochs fail to decrease the\n          training loss by tol or fail to increase validation score by tol if\n          `early_stopping` is `True`, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option.\n\n    eta0 : float, default=0.0\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n        the default schedule 'optimal'.\n        Values must be in the range `[0.0, inf)`.\n\n    power_t : float, default=0.5\n        The exponent for inverse scaling learning rate.\n        Values must be in the range `(-inf, inf)`.\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to `True`, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score returned by the `score` method is not\n        improving by at least tol for n_iter_no_change consecutive epochs.\n\n        See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an",
        "parameters": {
          "loss": {
            "type": "{'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations before reaching the stopping criterion.",
            "description": ""
          },
          "outliers": {
            "type": "as well as probability estimates.",
            "description": "- 'squared_hinge' is like hinge but is quadratically penalized.\n- 'perceptron' is the linear loss used by the perceptron algorithm.\n- The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n'squared_epsilon_insensitive' are designed for regression but can be useful"
          },
          "in": {
            "type": "classification as well; see",
            "description": ":class:`~sklearn.linear_model.SGDRegressor` for a description."
          },
          "More": {
            "type": "details about the losses formulas can be found in the :ref:`User Guide",
            "description": "<sgd_mathematical_formulation>` and you can find a visualisation of the loss"
          },
          "functions": {
            "type": "in",
            "description": ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`."
          },
          "penalty": {
            "type": "{'l2', 'l1', 'elasticnet', None}, default='l2'",
            "description": ""
          },
          "which": {
            "type": "is the standard regularizer for linear SVM models. 'l1' and",
            "description": "'elasticnet' might bring sparsity to the model (feature selection)"
          },
          "not": {
            "type": "achievable with 'l2'. No penalty is added when set to `None`.",
            "description": ""
          },
          "You": {
            "type": "can see a visualisation of the penalties in",
            "description": ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`."
          },
          "alpha": {
            "type": "float, default=0.0001",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the regularization term. The higher the",
            "description": "value, the stronger the regularization. Also used to compute the"
          },
          "learning": {
            "type": "rate when `learning_rate` is set to 'optimal'.",
            "description": ""
          },
          "Values": {
            "type": "must be in the range `(0.0, 1.0)`.",
            "description": ".. versionadded:: 0.20"
          },
          "l1_ratio": {
            "type": "float, default=0.15",
            "description": ""
          },
          "Only": {
            "type": "used if `early_stopping` is True.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use early stopping to terminate training when validation",
            "description": ""
          },
          "data": {
            "type": "is assumed to be already centered.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": ":meth:`partial_fit` method."
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > best_loss - tol",
            "description": "for ``n_iter_no_change`` consecutive\nepochs."
          },
          "Convergence": {
            "type": "is checked against the training loss or the",
            "description": ""
          },
          "validation": {
            "type": "loss depending on the `early_stopping` parameter.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "epsilon": {
            "type": "float, default=0.1",
            "description": ""
          },
          "Epsilon": {
            "type": "in the epsilon-insensitive loss functions; only if `loss` is",
            "description": "'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'."
          },
          "For": {
            "type": "multiclass fits, it is the maximum over every binary fit.",
            "description": ""
          },
          "important": {
            "type": "to get the prediction exactly right.",
            "description": ""
          },
          "and": {
            "type": "the correct label are ignored if they are less than this threshold.",
            "description": ""
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "for shuffling the data, when ``shuffle`` is set to ``True``.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.svm.LinearSVC : Linear support vector classification."
          },
          "Integer": {
            "type": "values must be in the range `[1, n_samples]`.",
            "description": "Attributes\n----------"
          },
          "learning_rate": {
            "type": "str, default='optimal'",
            "description": ""
          },
          "where": {
            "type": "`t0` is chosen by a heuristic proposed by Leon Bottou.",
            "description": "- 'invscaling': `eta = eta0 / pow(t, power_t)`\n- 'adaptive': `eta = eta0`, as long as the training keeps decreasing."
          },
          "Each": {
            "type": "time n_iter_no_change consecutive epochs fail to decrease the",
            "description": ""
          },
          "training": {
            "type": "when validation score returned by the `score` method is not",
            "description": ""
          },
          "Added": {
            "type": "'n_iter_no_change' option",
            "description": ""
          },
          "eta0": {
            "type": "float, default=0.0",
            "description": ""
          },
          "the": {
            "type": "default schedule 'optimal'.",
            "description": ""
          },
          "power_t": {
            "type": "float, default=0.5",
            "description": ""
          },
          "early_stopping": {
            "type": "bool, default=False",
            "description": ""
          },
          "score": {
            "type": "is not improving. If set to `True`, it will automatically set aside",
            "description": ""
          },
          "a": {
            "type": "stratified fraction of training data as validation and terminate",
            "description": ""
          },
          "improving": {
            "type": "by at least tol for n_iter_no_change consecutive epochs.",
            "description": ""
          },
          "example": {
            "type": "of the effects of early stopping.",
            "description": ".. versionadded:: 0.20"
          },
          "validation_fraction": {
            "type": "float, default=0.1",
            "description": ""
          },
          "early": {
            "type": "stopping. Must be between 0 and 1.",
            "description": ""
          },
          "n_iter_no_change": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "class_weight": {
            "type": "dict, {class_label: weight} or \"balanced\", default=None",
            "description": ""
          },
          "Preset": {
            "type": "for the class_weight fit parameter.",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "are": {
            "type": "supposed to have weight one.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to `True`, computes the averaged SGD weights across all",
            "description": ""
          },
          "Repeatedly": {
            "type": "calling fit or partial_fit when warm_start is True can",
            "description": ""
          },
          "result": {
            "type": "in a different solution than when calling fit a single time",
            "description": ""
          },
          "because": {
            "type": "of the way the data is shuffled.",
            "description": ""
          },
          "If": {
            "type": "a dynamic learning rate is used, the learning rate is adapted",
            "description": ""
          },
          "depending": {
            "type": "on the number of samples already seen. Calling ``fit`` resets",
            "description": ""
          },
          "this": {
            "type": "counter, while ``partial_fit`` will result in increasing the",
            "description": ""
          },
          "existing": {
            "type": "counter.",
            "description": ""
          },
          "average": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "updates": {
            "type": "and stores the result in the ``coef_`` attribute. If set to",
            "description": ""
          },
          "an": {
            "type": "int greater than 1, averaging will begin once the total number of",
            "description": ""
          },
          "samples": {
            "type": "seen reaches `average`. So ``average=10`` will begin",
            "description": ""
          },
          "averaging": {
            "type": "after seeing 10 samples.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
            "description": ""
          },
          "Constants": {
            "type": "in decision function.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "classes_": {
            "type": "array of shape (n_classes,)",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "LogisticRegression": {
            "type": "Logistic regression.",
            "description": ""
          },
          "Perceptron": {
            "type": "Inherits from SGDClassifier. ``Perceptron()`` is equivalent to",
            "description": "``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\npenalty=None)``.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> Y = np.array([1, 1, 2, 2])\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> clf = make_pipeline(StandardScaler(),\n...                     SGDClassifier(max_iter=1000, tol=1e-3))\n>>> clf.fit(X, Y)"
          },
          "Pipeline": {
            "type": "steps=[('standardscaler', StandardScaler(",
            "description": "),\n('sgdclassifier', SGDClassifier())])\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
          },
          "towards": {
            "type": "the zero vector using either the squared euclidean norm",
            "description": ""
          },
          "L2": {
            "type": "or the absolute norm L1 or a combination of both (Elastic Net). If the",
            "description": ""
          },
          "parameter": {
            "type": "update crosses the 0.0 value because of the regularizer, the",
            "description": ""
          },
          "update": {
            "type": "is truncated to 0.0 to allow for learning sparse models and achieve",
            "description": ""
          },
          "online": {
            "type": "feature selection.",
            "description": ""
          },
          "Read": {
            "type": "more in the :ref:`User Guide <sgd>`.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.svm.LinearSVC : Linear support vector classification.\n    LogisticRegression : Logistic regression.\n    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n        penalty=None)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> Y = np.array([1, 1, 2, 2])\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\n    >>> clf = make_pipeline(StandardScaler(),\n    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n    >>> clf.fit(X, Y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('sgdclassifier', SGDClassifier())])\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]",
        "notes": "",
        "examples": "of the effects of early stopping.\n\n        .. versionadded:: 0.20\n            Added 'early_stopping' option\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if `early_stopping` is True.\n        Values must be in the range `(0.0, 1.0)`.\n\n        .. versionadded:: 0.20\n            Added 'validation_fraction' option\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before stopping\n        fitting.\n        Convergence is checked against the training loss or the\n        validation loss depending on the `early_stopping` parameter.\n        Integer values must be in the range `[1, max_iter)`.\n\n        .. versionadded:: 0.20\n            Added 'n_iter_no_change' option\n\n    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit`` will result in increasing the\n        existing counter.\n\n    average : bool or int, default=False\n        When set to `True`, computes the averaged SGD weights across all\n        updates and stores the result in the ``coef_`` attribute. If set to\n        an int greater than 1, averaging will begin once the total number of\n        samples seen reaches `average`. So ``average=10`` will begin\n        averaging after seeing 10 samples.\n        Integer values must be in the range `[1, n_samples]`.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n        Weights assigned to the features.\n\n    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations before reaching the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    classes_ : array of shape (n_classes,)\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.svm.LinearSVC : Linear support vector classification.\n    LogisticRegression : Logistic regression.\n    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n        penalty=None)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> Y = np.array([1, 1, 2, 2])\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\n    >>> clf = make_pipeline(StandardScaler(),\n    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n    >>> clf.fit(X, Y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('sgdclassifier', SGDClassifier())])\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)",
          "documentation": {
            "description": "Fit linear model with Stochastic Gradient Descent.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : ndarray of shape (n_samples,)\n            Target values.\n\n        coef_init : ndarray of shape (n_classes, n_features), default=None\n            The initial coefficients to warm-start the optimization.\n\n        intercept_init : ndarray of shape (n_classes,), default=None\n            The initial intercept to warm-start the optimization.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed. These weights will\n            be multiplied with class_weight (passed through the\n            constructor) if class_weight is specified.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "coef_init": {
                "type": "ndarray of shape (n_classes, n_features), default=None",
                "description": ""
              },
              "The": {
                "type": "initial intercept to warm-start the optimization.",
                "description": ""
              },
              "intercept_init": {
                "type": "ndarray of shape (n_classes,), default=None",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed. These weights will",
                "description": ""
              },
              "be": {
                "type": "multiplied with class_weight (passed through the",
                "description": "constructor) if class_weight is specified.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "documentation": {
            "description": "Perform one epoch of stochastic gradient descent on given samples.\n\n        Internally, this method uses ``max_iter = 1``. Therefore, it is not\n        guaranteed that a minimum of the cost function is reached after calling\n        it once. Matters such as objective convergence, early stopping, and\n        learning rate adjustments should be handled by the user.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Subset of the training data.\n\n        y : ndarray of shape (n_samples,)\n            Subset of the target values.\n\n        classes : ndarray of shape (n_classes,), default=None\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all)`, where y_all is the\n            target vector of the entire dataset.\n            This argument is required for the first call to partial_fit\n            and can be omitted in the subsequent calls.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of the target values.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "classes": {
                "type": "ndarray of shape (n_classes,), default=None",
                "description": ""
              },
              "Classes": {
                "type": "across all calls to partial_fit.",
                "description": ""
              },
              "Can": {
                "type": "be obtained by via `np.unique(y_all)`, where y_all is the",
                "description": ""
              },
              "target": {
                "type": "vector of the entire dataset.",
                "description": ""
              },
              "This": {
                "type": "argument is required for the first call to partial_fit",
                "description": ""
              },
              "and": {
                "type": "can be omitted in the subsequent calls.",
                "description": ""
              },
              "Note": {
                "type": "that y doesn't need to contain all labels in `classes`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "that y doesn't need to contain all labels in `classes`.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Log of probability estimates.\n\n        This method is only available for log loss and modified Huber loss.\n\n        When loss=\"modified_huber\", probability estimates may be hard zeros\n        and ones, so taking the logarithm is not possible.\n\n        See ``predict_proba`` for details.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data for prediction.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data for prediction.",
                "description": "Returns\n-------"
              },
              "T": {
                "type": "array",
                "description": "like, shape (n_samples, n_classes)"
              },
              "Returns": {
                "type": "the log-probability of the sample for each class in the",
                "description": "model, where classes are ordered as they are in\n`self.classes_`."
              }
            },
            "returns": "-------\n        T : array-like, shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Probability estimates.\n\n        This method is only available for log loss and modified Huber loss.\n\n        Multiclass probability estimates are derived from binary (one-vs.-rest)\n        estimates by simple normalization, as recommended by Zadrozny and\n        Elkan.\n\n        Binary probability estimates for loss=\"modified_huber\" are given by\n        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n        it is necessary to perform proper probability calibration by wrapping\n        the classifier with\n        :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data for prediction.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data for prediction.",
                "description": "Returns\n-------"
              },
              "ndarray": {
                "type": "of shape (n_samples, n_classes)",
                "description": ""
              },
              "Returns": {
                "type": "the probability of the sample for each class in the model,",
                "description": ""
              },
              "where": {
                "type": "classes are ordered as they are in `self.classes_`.",
                "description": "References\n----------"
              },
              "Zadrozny": {
                "type": "and Elkan, \"Transforming classifier scores into multiclass",
                "description": ""
              },
              "probability": {
                "type": "estimates\", SIGKDD'02,",
                "description": ""
              },
              "https": {
                "type": "//dl.acm.org/doi/pdf/10.1145/775047.775151",
                "description": ""
              },
              "The": {
                "type": "justification for the formula in the loss=\"modified_huber\"",
                "description": ""
              },
              "case": {
                "type": "is in the appendix B in:",
                "description": ""
              },
              "http": {
                "type": "//jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf",
                "description": ""
              }
            },
            "returns": "-------\n        ndarray of shape (n_samples, n_classes)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``intercept_init`` parameter in ``fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``classes`` parameter in ``partial_fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SGDOneClassSVM",
      "documentation": {
        "description": "Solves linear One-Class SVM using Stochastic Gradient Descent.\n\n    This implementation is meant to be used with a kernel approximation\n    technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results\n    similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by\n    default.\n\n    Read more in the :ref:`User Guide <sgd_online_one_class_svm>`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    nu : float, default=0.5\n        The nu parameter of the One Class SVM: an upper bound on the\n        fraction of training errors and a lower bound of the fraction of\n        support vectors. Should be in the interval (0, 1]. By default 0.5\n        will be taken.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. Defaults to True.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        `partial_fit`. Defaults to 1000.\n        Values must be in the range `[1, inf)`.\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol). Defaults to 1e-3.\n        Values must be in the range `[0.0, inf)`.\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n        Defaults to True.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    learning_rate : {'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'\n        The learning rate schedule to use with `fit`. (If using `partial_fit`,\n        learning rate must be controlled directly).\n\n        - 'constant': `eta = eta0`\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n          where t0 is chosen by a heuristic proposed by Leon Bottou.\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\n        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n          Each time n_iter_no_change consecutive epochs fail to decrease the\n          training loss by tol or fail to increase validation score by tol if\n          early_stopping is True, the current learning rate is divided by 5.\n\n    eta0 : float, default=0.0\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n        the default schedule 'optimal'.\n        Values must be in the range `[0.0, inf)`.\n\n    power_t : float, default=0.5\n        The exponent for inverse scaling learning rate.\n        Values must be in the range `(-inf, inf)`.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit``  will result in increasing the\n        existing counter.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So ``average=10`` will begin averaging after seeing 10\n        samples.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (1, n_features)\n        Weights assigned to the features.\n\n    offset_ : ndarray of shape (1,)\n        Offset used to define the decision function from the raw scores.\n        We have the relation: decision_function = score_samples - offset.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n\n    Notes\n    -----\n    This estimator has a linear complexity in the number of training samples\n    and is thus better suited than the `sklearn.svm.OneClassSVM`\n    implementation for datasets with a large number of training samples (say\n    > 10,000).",
        "parameters": {
          "nu": {
            "type": "float, default=0.5",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations to reach the stopping criterion.",
            "description": ""
          },
          "fraction": {
            "type": "of training errors and a lower bound of the fraction of",
            "description": ""
          },
          "support": {
            "type": "vectors. Should be in the interval (0, 1]. By default 0.5",
            "description": ""
          },
          "will": {
            "type": "be taken.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not the training data should be shuffled after each epoch.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": "`partial_fit`. Defaults to 1000."
          },
          "Values": {
            "type": "must be in the range `(-inf, inf)`.",
            "description": ""
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > previous_loss - tol",
            "description": ". Defaults to 1e-3."
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "Defaults": {
            "type": "to True.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "the": {
            "type": "default schedule 'optimal'.",
            "description": ""
          },
          "instance": {
            "type": "used by `np.random`.",
            "description": ""
          },
          "learning_rate": {
            "type": "{'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'",
            "description": ""
          },
          "learning": {
            "type": "rate must be controlled directly).",
            "description": "- 'constant': `eta = eta0`\n- 'optimal': `eta = 1.0 / (alpha * (t + t0))`"
          },
          "where": {
            "type": "t0 is chosen by a heuristic proposed by Leon Bottou.",
            "description": "- 'invscaling': `eta = eta0 / pow(t, power_t)`\n- 'adaptive': eta = eta0, as long as the training keeps decreasing."
          },
          "Each": {
            "type": "time n_iter_no_change consecutive epochs fail to decrease the",
            "description": ""
          },
          "training": {
            "type": "loss by tol or fail to increase validation score by tol if",
            "description": ""
          },
          "early_stopping": {
            "type": "is True, the current learning rate is divided by 5.",
            "description": ""
          },
          "eta0": {
            "type": "float, default=0.0",
            "description": ""
          },
          "power_t": {
            "type": "float, default=0.5",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, computes the averaged SGD weights and stores the",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\nNotes\n-----"
          },
          "Repeatedly": {
            "type": "calling fit or partial_fit when warm_start is True can",
            "description": ""
          },
          "result": {
            "type": "in the ``coef_`` attribute. If set to an int greater than 1,",
            "description": ""
          },
          "because": {
            "type": "of the way the data is shuffled.",
            "description": ""
          },
          "If": {
            "type": "a dynamic learning rate is used, the learning rate is adapted",
            "description": ""
          },
          "depending": {
            "type": "on the number of samples already seen. Calling ``fit`` resets",
            "description": ""
          },
          "this": {
            "type": "counter, while ``partial_fit``  will result in increasing the",
            "description": ""
          },
          "existing": {
            "type": "counter.",
            "description": ""
          },
          "average": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "averaging": {
            "type": "will begin once the total number of samples seen reaches",
            "description": "average. So ``average=10`` will begin averaging after seeing 10\nsamples.\nAttributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features)",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "offset_": {
            "type": "ndarray of shape (1,)",
            "description": ""
          },
          "Offset": {
            "type": "used to define the decision function from the raw scores.",
            "description": ""
          },
          "We": {
            "type": "have the relation: decision_function = score_samples - offset.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "This": {
            "type": "estimator has a linear complexity in the number of training samples",
            "description": ""
          },
          "and": {
            "type": "is thus better suited than the `sklearn.svm.OneClassSVM`",
            "description": ""
          },
          "implementation": {
            "type": "for datasets with a large number of training samples (say",
            "description": "> 10,000).\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import linear_model\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> clf = linear_model.SGDOneClassSVM(random_state=42)\n>>> clf.fit(X)"
          },
          "SGDOneClassSVM": {
            "type": "random_state=42",
            "description": ">>> print(clf.predict([[4, 4]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n\n    Notes\n    -----\n    This estimator has a linear complexity in the number of training samples\n    and is thus better suited than the `sklearn.svm.OneClassSVM`\n    implementation for datasets with a large number of training samples (say\n    > 10,000).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> clf = linear_model.SGDOneClassSVM(random_state=42)\n    >>> clf.fit(X)\n    SGDOneClassSVM(random_state=42)\n\n    >>> print(clf.predict([[4, 4]]))\n    [1]",
        "notes": "-----\n    This estimator has a linear complexity in the number of training samples\n    and is thus better suited than the `sklearn.svm.OneClassSVM`\n    implementation for datasets with a large number of training samples (say\n    > 10,000).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> clf = linear_model.SGDOneClassSVM(random_state=42)\n    >>> clf.fit(X)\n    SGDOneClassSVM(random_state=42)\n\n    >>> print(clf.predict([[4, 4]]))\n    [1]",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> clf = linear_model.SGDOneClassSVM(random_state=42)\n    >>> clf.fit(X)\n    SGDOneClassSVM(random_state=42)\n\n    >>> print(clf.predict([[4, 4]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Signed distance to the separating hyperplane.\n\n        Signed distance is positive for an inlier and negative for an\n        outlier.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Testing data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Testing": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "dec": {
                "type": "array",
                "description": "like, shape (n_samples,)"
              },
              "Decision": {
                "type": "function values of the samples.",
                "description": ""
              }
            },
            "returns": "-------\n        dec : array-like, shape (n_samples,)\n            Decision function values of the samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, coef_init=None, offset_init=None, sample_weight=None)",
          "documentation": {
            "description": "Fit linear One-Class SVM with Stochastic Gradient Descent.\n\n        This solves an equivalent optimization problem of the\n        One-Class SVM primal optimization problem and returns a weight vector\n        w and an offset rho such that the decision function is given by\n        <w, x> - rho.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        coef_init : array, shape (n_classes, n_features)\n            The initial coefficients to warm-start the optimization.\n\n        offset_init : array, shape (n_classes,)\n            The initial offset to warm-start the optimization.\n\n        sample_weight : array-like, shape (n_samples,), optional\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed. These weights will\n            be multiplied with class_weight (passed through the\n            constructor) if class_weight is specified.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": ""
              },
              "coef_init": {
                "type": "array, shape (n_classes, n_features)",
                "description": ""
              },
              "The": {
                "type": "initial offset to warm-start the optimization.",
                "description": ""
              },
              "offset_init": {
                "type": "array, shape (n_classes,)",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), optional"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed. These weights will",
                "description": ""
              },
              "be": {
                "type": "multiplied with class_weight (passed through the",
                "description": "constructor) if class_weight is specified.\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "a fitted instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **kwargs)",
          "documentation": {
            "description": "Perform fit on X and returns labels for X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "input samples.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**kwargs : dict"
              },
              "Arguments": {
                "type": "to be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              },
              "1": {
                "type": "for inliers, -1 for outliers.",
                "description": ""
              },
              "to": {
                "type": "be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              }
            },
            "returns": "-1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Fit linear One-Class SVM with Stochastic Gradient Descent.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Subset of the training data.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like, shape (n_samples,), optional\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of the training data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), optional"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "a fitted instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Return labels (1 inlier, -1 outlier) of the samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Testing data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Testing": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Labels": {
                "type": "of the samples.",
                "description": ""
              }
            },
            "returns": "-------\n        y : array, shape (n_samples,)\n            Labels of the samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Raw scoring function of the samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Testing data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Testing": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "score_samples": {
                "type": "array",
                "description": "like, shape (n_samples,)"
              },
              "Unshiffted": {
                "type": "scoring function values of the samples.",
                "description": ""
              }
            },
            "returns": "-------\n        score_samples : array-like, shape (n_samples,)\n            Unshiffted scoring function values of the samples.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDOneClassSVM, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', offset_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDOneClassSVM",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "offset_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        offset_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``offset_init`` parameter in ``fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDOneClassSVM, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDOneClassSVM",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SGDRegressor",
      "documentation": {
        "description": "Linear model fitted by minimizing a regularized empirical loss with SGD.\n\n    SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n    estimated each sample at a time and the model is updated along the way with\n    a decreasing strength schedule (aka learning rate).\n\n    The regularizer is a penalty added to the loss function that shrinks model\n    parameters towards the zero vector using either the squared euclidean norm\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n    parameter update crosses the 0.0 value because of the regularizer, the\n    update is truncated to 0.0 to allow for learning sparse models and achieve\n    online feature selection.\n\n    This implementation works with data represented as dense numpy arrays of\n    floating point values for the features.\n\n    Read more in the :ref:`User Guide <sgd>`.\n\n    Parameters\n    ----------\n    loss : str, default='squared_error'\n        The loss function to be used. The possible values are 'squared_error',\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n\n        The 'squared_error' refers to the ordinary least squares fit.\n        'huber' modifies 'squared_error' to focus less on getting outliers\n        correct by switching from squared to linear loss past a distance of\n        epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n        linear past that; this is the loss function used in SVR.\n        'squared_epsilon_insensitive' is the same but becomes squared loss past\n        a tolerance of epsilon.\n\n        More details about the losses formulas can be found in the\n        :ref:`User Guide <sgd_mathematical_formulation>`.\n\n    penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\n        which is the standard regularizer for linear SVM models. 'l1' and\n        'elasticnet' might bring sparsity to the model (feature selection)\n        not achievable with 'l2'. No penalty is added when set to `None`.\n\n        You can see a visualisation of the penalties in\n        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\n\n    alpha : float, default=0.0001\n        Constant that multiplies the regularization term. The higher the\n        value, the stronger the regularization. Also used to compute the\n        learning rate when `learning_rate` is set to 'optimal'.\n        Values must be in the range `[0.0, inf)`.\n\n    l1_ratio : float, default=0.15\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n        Only used if `penalty` is 'elasticnet'.\n        Values must be in the range `[0.0, 1.0]`.\n\n    fit_intercept : bool, default=True\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered.\n\n    max_iter : int, default=1000\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, default=1e-3\n        The stopping criterion. If it is not None, training will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n        Convergence is checked against the training loss or the\n        validation loss depending on the `early_stopping` parameter.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : int, default=0\n        The verbosity level.\n        Values must be in the range `[0, inf)`.\n\n    epsilon : float, default=0.1\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n        Values must be in the range `[0.0, inf)`.\n\n    random_state : int, RandomState instance, default=None\n        Used for shuffling the data, when ``shuffle`` is set to ``True``.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    learning_rate : str, default='invscaling'\n        The learning rate schedule:\n\n        - 'constant': `eta = eta0`\n        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n          where t0 is chosen by a heuristic proposed by Leon Bottou.\n        - 'invscaling': `eta = eta0 / pow(t, power_t)`\n        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n          Each time n_iter_no_change consecutive epochs fail to decrease the\n          training loss by tol or fail to increase validation score by tol if\n          early_stopping is True, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option.\n\n    eta0 : float, default=0.01\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.01.\n        Values must be in the range `[0.0, inf)`.\n\n    power_t : float, default=0.25\n        The exponent for inverse scaling learning rate.\n        Values must be in the range `(-inf, inf)`.\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a fraction of training data as validation and terminate\n        training when validation score returned by the `score` method is not\n        improving by at least `tol` for `n_iter_no_change` consecutive\n        epochs.\n\n        See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an",
        "parameters": {
          "loss": {
            "type": "str, default='squared_error'",
            "description": ""
          },
          "The": {
            "type": "actual number of iterations before reaching the stopping criterion.",
            "description": ""
          },
          "correct": {
            "type": "by switching from squared to linear loss past a distance of",
            "description": "epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is"
          },
          "linear": {
            "type": "past that; this is the loss function used in SVR.",
            "description": "'squared_epsilon_insensitive' is the same but becomes squared loss past"
          },
          "a": {
            "type": "fraction of training data as validation and terminate",
            "description": ""
          },
          "More": {
            "type": "details about the losses formulas can be found in the",
            "description": ":ref:`User Guide <sgd_mathematical_formulation>`."
          },
          "penalty": {
            "type": "{'l2', 'l1', 'elasticnet', None}, default='l2'",
            "description": ""
          },
          "which": {
            "type": "is the standard regularizer for linear SVM models. 'l1' and",
            "description": "'elasticnet' might bring sparsity to the model (feature selection)"
          },
          "not": {
            "type": "achievable with 'l2'. No penalty is added when set to `None`.",
            "description": ""
          },
          "You": {
            "type": "can see a visualisation of the penalties in",
            "description": ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`."
          },
          "alpha": {
            "type": "float, default=0.0001",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the regularization term. The higher the",
            "description": "value, the stronger the regularization. Also used to compute the"
          },
          "learning": {
            "type": "rate when `learning_rate` is set to 'optimal'.",
            "description": ""
          },
          "Values": {
            "type": "must be in the range `(0.0, 1.0)`.",
            "description": ".. versionadded:: 0.20"
          },
          "l1_ratio": {
            "type": "float, default=0.15",
            "description": ""
          },
          "Only": {
            "type": "used if `early_stopping` is True.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to use early stopping to terminate training when validation",
            "description": ""
          },
          "data": {
            "type": "is assumed to be already centered.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "It": {
            "type": "only impacts the behavior in the ``fit`` method, and not the",
            "description": ":meth:`partial_fit` method."
          },
          "tol": {
            "type": "float or None, default=1e",
            "description": "3"
          },
          "when": {
            "type": "loss > best_loss - tol",
            "description": "for ``n_iter_no_change`` consecutive\nepochs."
          },
          "Convergence": {
            "type": "is checked against the training loss or the",
            "description": ""
          },
          "validation": {
            "type": "loss depending on the `early_stopping` parameter.",
            "description": ""
          },
          "shuffle": {
            "type": "bool, default=True",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "epsilon": {
            "type": "float, default=0.1",
            "description": ""
          },
          "Epsilon": {
            "type": "in the epsilon-insensitive loss functions; only if `loss` is",
            "description": "'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'."
          },
          "For": {
            "type": "epsilon-insensitive, any differences between the current prediction",
            "description": ""
          },
          "important": {
            "type": "to get the prediction exactly right.",
            "description": ""
          },
          "and": {
            "type": "the correct label are ignored if they are less than this threshold.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": ""
          },
          "Used": {
            "type": "for shuffling the data, when ``shuffle`` is set to ``True``.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "learning_rate": {
            "type": "str, default='invscaling'",
            "description": ""
          },
          "where": {
            "type": "t0 is chosen by a heuristic proposed by Leon Bottou.",
            "description": "- 'invscaling': `eta = eta0 / pow(t, power_t)`\n- 'adaptive': eta = eta0, as long as the training keeps decreasing."
          },
          "Each": {
            "type": "time n_iter_no_change consecutive epochs fail to decrease the",
            "description": ""
          },
          "training": {
            "type": "when validation score returned by the `score` method is not",
            "description": ""
          },
          "early_stopping": {
            "type": "bool, default=False",
            "description": ""
          },
          "Added": {
            "type": "'n_iter_no_change' option",
            "description": ""
          },
          "eta0": {
            "type": "float, default=0.01",
            "description": ""
          },
          "power_t": {
            "type": "float, default=0.25",
            "description": ""
          },
          "score": {
            "type": "is not improving. If set to True, it will automatically set aside",
            "description": ""
          },
          "improving": {
            "type": "by at least `tol` for `n_iter_no_change` consecutive",
            "description": "epochs."
          },
          "example": {
            "type": "of the effects of early stopping.",
            "description": ".. versionadded:: 0.20"
          },
          "validation_fraction": {
            "type": "float, default=0.1",
            "description": ""
          },
          "early": {
            "type": "stopping. Must be between 0 and 1.",
            "description": ""
          },
          "n_iter_no_change": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "Integer": {
            "type": "values must be in the range `[1, max_iter)`.",
            "description": ".. versionadded:: 0.20"
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "When": {
            "type": "set to True, computes the averaged SGD weights across all",
            "description": ""
          },
          "Repeatedly": {
            "type": "calling fit or partial_fit when warm_start is True can",
            "description": ""
          },
          "result": {
            "type": "in a different solution than when calling fit a single time",
            "description": ""
          },
          "because": {
            "type": "of the way the data is shuffled.",
            "description": ""
          },
          "If": {
            "type": "a dynamic learning rate is used, the learning rate is adapted",
            "description": ""
          },
          "depending": {
            "type": "on the number of samples already seen. Calling ``fit`` resets",
            "description": ""
          },
          "this": {
            "type": "counter, while ``partial_fit``  will result in increasing the",
            "description": ""
          },
          "existing": {
            "type": "counter.",
            "description": ""
          },
          "average": {
            "type": "bool or int, default=False",
            "description": ""
          },
          "updates": {
            "type": "and stores the result in the ``coef_`` attribute. If set to",
            "description": ""
          },
          "an": {
            "type": "int greater than 1, averaging will begin once the total number of",
            "description": ""
          },
          "samples": {
            "type": "seen reaches `average`. So ``average=10`` will begin",
            "description": ""
          },
          "averaging": {
            "type": "after seeing 10 samples.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features.",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (1,)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "t_": {
            "type": "int",
            "description": ""
          },
          "Same": {
            "type": "as ``(n_iter_ * n_samples + 1)``.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "HuberRegressor": {
            "type": "Linear regression model that is robust to outliers.",
            "description": ""
          },
          "Lars": {
            "type": "Least Angle Regression model.",
            "description": ""
          },
          "Lasso": {
            "type": "Linear Model trained with L1 prior as regularizer.",
            "description": ""
          },
          "RANSACRegressor": {
            "type": "RANSAC (RANdom SAmple Consensus) algorithm.",
            "description": ""
          },
          "Ridge": {
            "type": "Linear least squares with l2 regularization.",
            "description": "sklearn.svm.SVR : Epsilon-Support Vector Regression."
          },
          "TheilSenRegressor": {
            "type": "Theil",
            "description": "Sen Estimator robust multivariate regression model.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDRegressor\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> reg = make_pipeline(StandardScaler(),\n...                     SGDRegressor(max_iter=1000, tol=1e-3))\n>>> reg.fit(X, y)"
          },
          "Pipeline": {
            "type": "steps=[('standardscaler', StandardScaler(",
            "description": "),\n('sgdregressor', SGDRegressor())])"
          },
          "towards": {
            "type": "the zero vector using either the squared euclidean norm",
            "description": ""
          },
          "L2": {
            "type": "or the absolute norm L1 or a combination of both (Elastic Net). If the",
            "description": ""
          },
          "parameter": {
            "type": "update crosses the 0.0 value because of the regularizer, the",
            "description": ""
          },
          "update": {
            "type": "is truncated to 0.0 to allow for learning sparse models and achieve",
            "description": ""
          },
          "online": {
            "type": "feature selection.",
            "description": ""
          },
          "This": {
            "type": "implementation works with data represented as dense numpy arrays of",
            "description": ""
          },
          "floating": {
            "type": "point values for the features.",
            "description": ""
          },
          "Read": {
            "type": "more in the :ref:`User Guide <sgd>`.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    Lars : Least Angle Regression model.\n    Lasso : Linear Model trained with L1 prior as regularizer.\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    Ridge : Linear least squares with l2 regularization.\n    sklearn.svm.SVR : Epsilon-Support Vector Regression.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import SGDRegressor\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\n    >>> reg = make_pipeline(StandardScaler(),\n    ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n    >>> reg.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('sgdregressor', SGDRegressor())])",
        "notes": "",
        "examples": "of the effects of early stopping.\n\n        .. versionadded:: 0.20\n            Added 'early_stopping' option\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if `early_stopping` is True.\n        Values must be in the range `(0.0, 1.0)`.\n\n        .. versionadded:: 0.20\n            Added 'validation_fraction' option\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before stopping\n        fitting.\n        Convergence is checked against the training loss or the\n        validation loss depending on the `early_stopping` parameter.\n        Integer values must be in the range `[1, max_iter)`.\n\n        .. versionadded:: 0.20\n            Added 'n_iter_no_change' option\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit``  will result in increasing the\n        existing counter.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights across all\n        updates and stores the result in the ``coef_`` attribute. If set to\n        an int greater than 1, averaging will begin once the total number of\n        samples seen reaches `average`. So ``average=10`` will begin\n        averaging after seeing 10 samples.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,)\n        Weights assigned to the features.\n\n    intercept_ : ndarray of shape (1,)\n        The intercept term.\n\n    n_iter_ : int\n        The actual number of iterations before reaching the stopping criterion.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples + 1)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    Lars : Least Angle Regression model.\n    Lasso : Linear Model trained with L1 prior as regularizer.\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    Ridge : Linear least squares with l2 regularization.\n    sklearn.svm.SVR : Epsilon-Support Vector Regression.\n    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import SGDRegressor\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> # Always scale the input. The most convenient way is to use a pipeline.\n    >>> reg = make_pipeline(StandardScaler(),\n    ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n    >>> reg.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('sgdregressor', SGDRegressor())])"
      },
      "methods": [
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)",
          "documentation": {
            "description": "Fit linear model with Stochastic Gradient Descent.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : ndarray of shape (n_samples,)\n            Target values.\n\n        coef_init : ndarray of shape (n_features,), default=None\n            The initial coefficients to warm-start the optimization.\n\n        intercept_init : ndarray of shape (1,), default=None\n            The initial intercept to warm-start the optimization.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples (1. for unweighted).",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "coef_init": {
                "type": "ndarray of shape (n_features,), default=None",
                "description": ""
              },
              "The": {
                "type": "initial intercept to warm-start the optimization.",
                "description": ""
              },
              "intercept_init": {
                "type": "ndarray of shape (1,), default=None",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples (1. for unweighted).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "`SGDRegressor` estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted `SGDRegressor` estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Perform one epoch of stochastic gradient descent on given samples.\n\n        Internally, this method uses ``max_iter = 1``. Therefore, it is not\n        guaranteed that a minimum of the cost function is reached after calling\n        it once. Matters such as objective convergence and early stopping\n        should be handled by the user.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Subset of training data.\n\n        y : numpy array of shape (n_samples,)\n            Subset of target values.\n\n        sample_weight : array-like, shape (n_samples,), default=None\n            Weights applied to individual samples.\n            If not provided, uniform weights are assumed.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Subset": {
                "type": "of target values.",
                "description": ""
              },
              "y": {
                "type": "numpy array of shape (n_samples,)",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like, shape (n_samples,), default=None"
              },
              "Weights": {
                "type": "applied to individual samples.",
                "description": ""
              },
              "If": {
                "type": "not provided, uniform weights are assumed.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix}, shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "ndarray": {
                "type": "of shape (n_samples,)",
                "description": ""
              },
              "Predicted": {
                "type": "target values per element in X.",
                "description": ""
              }
            },
            "returns": "-------\n        ndarray of shape (n_samples,)\n           Predicted target values per element in X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "coef_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "intercept_init": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``coef_init`` parameter in ``fit``.\n\n        intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``intercept_init`` parameter in ``fit``.\n\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TheilSenRegressor",
      "documentation": {
        "description": "Theil-Sen Estimator: robust multivariate regression model.\n\n    The algorithm calculates least square solutions on subsets with size\n    n_subsamples of the samples in X. Any value of n_subsamples between the\n    number of features and samples leads to an estimator with a compromise\n    between robustness and efficiency. Since the number of least square\n    solutions is \"n_samples choose n_subsamples\", it can be extremely large\n    and can therefore be limited with max_subpopulation. If this limit is\n    reached, the subsets are chosen randomly. In a final step, the spatial\n    median (or L1 median) is calculated of all least square solutions.\n\n    Read more in the :ref:`User Guide <theil_sen_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n        .. deprecated:: 1.6\n            `copy_X` was deprecated in 1.6 and will be removed in 1.8.\n            It has no effect as a copy is always made.\n\n    max_subpopulation : int, default=1e4\n        Instead of computing with a set of cardinality 'n choose k', where n is\n        the number of samples and k is the number of subsamples (at least\n        number of features), consider only a stochastic subpopulation of a\n        given maximal size if 'n choose k' is larger than max_subpopulation.\n        For other than small problem sizes this parameter will determine\n        memory usage and runtime if n_subsamples is not changed. Note that the\n        data type should be int but floats such as 1e4 can be accepted too.\n\n    n_subsamples : int, default=None\n        Number of samples to calculate the parameters. This is at least the\n        number of features (plus 1 if fit_intercept=True) and the number of\n        samples as a maximum. A lower number leads to a higher breakdown\n        point and a low efficiency while a high number leads to a low\n        breakdown point and a high efficiency. If None, take the\n        minimum number of subsamples leading to maximal robustness.\n        If n_subsamples is set to n_samples, Theil-Sen is identical to least\n        squares.\n\n    max_iter : int, default=300\n        Maximum number of iterations for the calculation of spatial median.\n\n    tol : float, default=1e-3\n        Tolerance when calculating spatial median.\n\n    random_state : int, RandomState instance or None, default=None\n        A random number generator instance to define the state of the random\n        permutations generator. Pass an int for reproducible output across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,)\n        Coefficients of the regression model (median of distribution).\n\n    intercept_ : float\n        Estimated intercept of regression model.\n\n    breakdown_ : float\n        Approximated breakdown point.\n\n    n_iter_ : int\n        Number of iterations needed for the spatial median.\n\n    n_subpopulation_ : int\n        Number of combinations taken into account from 'n choose k', where n is\n        the number of samples and k is the number of subsamples.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n      http://home.olemiss.edu/~xdang/papers/MTSE.pdf",
        "parameters": {
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to calculate the intercept for this model. If set",
            "description": ""
          },
          "to": {
            "type": "false, no intercept will be used in calculations.",
            "description": ""
          },
          "copy_X": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "n_subsamples is set to n_samples, Theil-Sen is identical to least",
            "description": "squares."
          },
          "It": {
            "type": "has no effect as a copy is always made.",
            "description": ""
          },
          "max_subpopulation": {
            "type": "int, default=1e4",
            "description": ""
          },
          "Instead": {
            "type": "of computing with a set of cardinality 'n choose k', where n is",
            "description": ""
          },
          "the": {
            "type": "number of samples and k is the number of subsamples.",
            "description": ""
          },
          "number": {
            "type": "of features (plus 1 if fit_intercept=True) and the number of",
            "description": ""
          },
          "given": {
            "type": "maximal size if 'n choose k' is larger than max_subpopulation.",
            "description": ""
          },
          "For": {
            "type": "other than small problem sizes this parameter will determine",
            "description": ""
          },
          "memory": {
            "type": "usage and runtime if n_subsamples is not changed. Note that the",
            "description": ""
          },
          "data": {
            "type": "type should be int but floats such as 1e4 can be accepted too.",
            "description": ""
          },
          "n_subsamples": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "samples": {
            "type": "as a maximum. A lower number leads to a higher breakdown",
            "description": ""
          },
          "point": {
            "type": "and a low efficiency while a high number leads to a low",
            "description": ""
          },
          "breakdown": {
            "type": "point and a high efficiency. If None, take the",
            "description": ""
          },
          "minimum": {
            "type": "number of subsamples leading to maximal robustness.",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=300",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations for the calculation of spatial median.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "3"
          },
          "Tolerance": {
            "type": "when calculating spatial median.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "A": {
            "type": "random number generator instance to define the state of the random",
            "description": ""
          },
          "permutations": {
            "type": "generator. Pass an int for reproducible output across",
            "description": ""
          },
          "multiple": {
            "type": "function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "Verbose": {
            "type": "mode when fitting the model.",
            "description": "Attributes\n----------"
          },
          "coef_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Coefficients": {
            "type": "of the regression model (median of distribution).",
            "description": ""
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Estimated": {
            "type": "intercept of regression model.",
            "description": ""
          },
          "breakdown_": {
            "type": "float",
            "description": ""
          },
          "Approximated": {
            "type": "breakdown point.",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_subpopulation_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "HuberRegressor": {
            "type": "Linear regression model that is robust to outliers.",
            "description": ""
          },
          "RANSACRegressor": {
            "type": "RANSAC (RANdom SAmple Consensus) algorithm.",
            "description": ""
          },
          "SGDRegressor": {
            "type": "Fitted by minimizing a regularized empirical loss with SGD.",
            "description": "References\n----------\n- Theil-Sen Estimators in a Multiple Linear Regression Model, 2009"
          },
          "Xin": {
            "type": "Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang",
            "description": ""
          },
          "http": {
            "type": "//home.olemiss.edu/~xdang/papers/MTSE.pdf",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])"
          },
          "array": {
            "type": "[-31.5871...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    HuberRegressor : Linear regression model that is robust to outliers.\n    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\n    References\n    ----------\n    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n      http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import TheilSenRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9884...\n    >>> reg.predict(X[:1,])\n    array([-31.5871...])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import TheilSenRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9884...\n    >>> reg.predict(X[:1,])\n    array([-31.5871...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit linear model.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training data.\n        y : ndarray of shape (n_samples,)\n            Target values.",
            "parameters": {
              "X": {
                "type": "ndarray of shape (n_samples, n_features)",
                "description": ""
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "returns an instance of self.",
                "description": ""
              },
              "Fitted": {
                "type": "`TheilSenRegressor` estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : returns an instance of self.\n            Fitted `TheilSenRegressor` estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like or sparse matrix, shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "C": {
                "type": "array, shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        C : array, shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._theil_sen.TheilSenRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._theil_sen.TheilSenRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TweedieRegressor",
      "documentation": {
        "description": "Generalized Linear Model with a Tweedie distribution.\n\n    This estimator can be used to model different GLMs depending on the\n    ``power`` parameter, which determines the underlying distribution.\n\n    Read more in the :ref:`User Guide <Generalized_linear_models>`.\n\n    .. versionadded:: 0.23\n\n    Parameters\n    ----------\n    power : float, default=0\n            The power determines the underlying target distribution according\n            to the following table:\n\n            +-------+------------------------+\n            | Power | Distribution           |\n            +=======+========================+\n            | 0     | Normal                 |\n            +-------+------------------------+\n            | 1     | Poisson                |\n            +-------+------------------------+\n            | (1,2) | Compound Poisson Gamma |\n            +-------+------------------------+\n            | 2     | Gamma                  |\n            +-------+------------------------+\n            | 3     | Inverse Gaussian       |\n            +-------+------------------------+\n\n            For ``0 < power < 1``, no distribution exists.\n\n    alpha : float, default=1\n        Constant that multiplies the L2 penalty term and determines the\n        regularization strength. ``alpha = 0`` is equivalent to unpenalized\n        GLMs. In this case, the design matrix `X` must have full column rank\n        (no collinearities).\n        Values of `alpha` must be in the range `[0.0, inf)`.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the linear predictor (`X @ coef + intercept`).\n\n    link : {'auto', 'identity', 'log'}, default='auto'\n        The link function of the GLM, i.e. mapping from linear predictor\n        `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n        the link depending on the chosen `power` parameter as follows:\n\n        - 'identity' for ``power <= 0``, e.g. for the Normal distribution\n        - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian\n          distributions\n\n    solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n        Algorithm to use in the optimization problem:\n\n        'lbfgs'\n            Calls scipy's L-BFGS-B optimizer.\n\n        'newton-cholesky'\n            Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n            iterated reweighted least squares) with an inner Cholesky based solver.\n            This solver is a good choice for `n_samples` >> `n_features`, especially\n            with one-hot encoded categorical features with rare categories. Be aware\n            that the memory usage of this solver has a quadratic dependency on\n            `n_features` because it explicitly computes the Hessian matrix.\n\n            .. versionadded:: 1.2\n\n    max_iter : int, default=100\n        The maximal number of iterations for the solver.\n        Values must be in the range `[1, inf)`.\n\n    tol : float, default=1e-4\n        Stopping criterion. For the lbfgs solver,\n        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n        where ``g_j`` is the j-th component of the gradient (derivative) of\n        the objective function.\n        Values must be in the range `(0.0, inf)`.\n\n    warm_start : bool, default=False\n        If set to ``True``, reuse the solution of the previous call to ``fit``\n        as initialization for ``coef_`` and ``intercept_`` .\n\n    verbose : int, default=0\n        For the lbfgs solver set verbose to any positive number for verbosity.\n        Values must be in the range `[0, inf)`.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features,)\n        Estimated coefficients for the linear predictor (`X @ coef_ +\n        intercept_`) in the GLM.\n\n    intercept_ : float\n        Intercept (a.k.a. bias) added to linear predictor.\n\n    n_iter_ : int\n        Actual number of iterations used in the solver.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n    GammaRegressor : Generalized Linear Model with a Gamma distribution.",
        "parameters": {
          "power": {
            "type": "float, default=0",
            "description": ""
          },
          "The": {
            "type": "maximal number of iterations for the solver.",
            "description": ""
          },
          "to": {
            "type": "the following table:",
            "description": "+-------+------------------------+\n| Power | Distribution           |\n+=======+========================+\n| 0     | Normal                 |\n+-------+------------------------+\n| 1     | Poisson                |\n+-------+------------------------+\n| (1,2) | Compound Poisson Gamma |\n+-------+------------------------+\n| 2     | Gamma                  |\n+-------+------------------------+\n| 3     | Inverse Gaussian       |\n+-------+------------------------+"
          },
          "For": {
            "type": "the lbfgs solver set verbose to any positive number for verbosity.",
            "description": ""
          },
          "alpha": {
            "type": "float, default=1",
            "description": ""
          },
          "Constant": {
            "type": "that multiplies the L2 penalty term and determines the",
            "description": ""
          },
          "regularization": {
            "type": "strength. ``alpha = 0`` is equivalent to unpenalized",
            "description": "GLMs. In this case, the design matrix `X` must have full column rank\n(no collinearities)."
          },
          "Values": {
            "type": "must be in the range `[0, inf)`.",
            "description": "Attributes\n----------"
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if a constant (a.k.a. bias or intercept) should be",
            "description": ""
          },
          "added": {
            "type": "to the linear predictor (`X @ coef + intercept`).",
            "description": ""
          },
          "link": {
            "type": "{'auto', 'identity', 'log'}, default='auto'",
            "description": ""
          },
          "the": {
            "type": "objective function.",
            "description": ""
          },
          "solver": {
            "type": "{'lbfgs', 'newton",
            "description": "cholesky'}, default='lbfgs'"
          },
          "Algorithm": {
            "type": "to use in the optimization problem:",
            "description": "'lbfgs'"
          },
          "Calls": {
            "type": "scipy's L-BFGS-B optimizer.",
            "description": "'newton-cholesky'"
          },
          "Uses": {
            "type": "Newton-Raphson steps (in arbitrary precision arithmetic equivalent to",
            "description": ""
          },
          "iterated": {
            "type": "reweighted least squares) with an inner Cholesky based solver.",
            "description": ""
          },
          "This": {
            "type": "solver is a good choice for `n_samples` >> `n_features`, especially",
            "description": ""
          },
          "with": {
            "type": "one-hot encoded categorical features with rare categories. Be aware",
            "description": ""
          },
          "that": {
            "type": "the memory usage of this solver has a quadratic dependency on",
            "description": "`n_features` because it explicitly computes the Hessian matrix.\n.. versionadded:: 1.2"
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Stopping": {
            "type": "criterion. For the lbfgs solver,",
            "description": ""
          },
          "where": {
            "type": "``g_j`` is the j-th component of the gradient (derivative) of",
            "description": ""
          },
          "warm_start": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "set to ``True``, reuse the solution of the previous call to ``fit``",
            "description": ""
          },
          "as": {
            "type": "initialization for ``coef_`` and ``intercept_`` .",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "coef_": {
            "type": "array of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "coefficients for the linear predictor (`X @ coef_ +",
            "description": "intercept_`) in the GLM."
          },
          "intercept_": {
            "type": "float",
            "description": ""
          },
          "Intercept": {
            "type": "a.k.a. bias",
            "description": "added to linear predictor."
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Actual": {
            "type": "number of iterations used in the solver.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "PoissonRegressor": {
            "type": "Generalized Linear Model with a Poisson distribution.",
            "description": ""
          },
          "GammaRegressor": {
            "type": "Generalized Linear Model with a Gamma distribution.",
            "description": "Examples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.TweedieRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [2, 3.5, 5, 5.5]\n>>> clf.fit(X, y)"
          },
          "TweedieRegressor": {
            "type": "",
            "description": ">>> clf.score(X, y)\nnp.float64(0.839...)\n>>> clf.coef_"
          },
          "array": {
            "type": "[2.500..., 4.599...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n    GammaRegressor : Generalized Linear Model with a Gamma distribution.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.TweedieRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [2, 3.5, 5, 5.5]\n    >>> clf.fit(X, y)\n    TweedieRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.839...)\n    >>> clf.coef_\n    array([0.599..., 0.299...])\n    >>> clf.intercept_\n    np.float64(1.600...)\n    >>> clf.predict([[1, 1], [3, 4]])\n    array([2.500..., 4.599...])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.TweedieRegressor()\n    >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n    >>> y = [2, 3.5, 5, 5.5]\n    >>> clf.fit(X, y)\n    TweedieRegressor()\n    >>> clf.score(X, y)\n    np.float64(0.839...)\n    >>> clf.coef_\n    array([0.599..., 0.299...])\n    >>> clf.intercept_\n    np.float64(1.600...)\n    >>> clf.predict([[1, 1], [3, 4]])\n    array([2.500..., 4.599...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit a Generalized Linear Model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "model.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict using GLM with feature matrix X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)\nSamples.\nReturns\n-------"
              },
              "y_pred": {
                "type": "array of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "predicted values.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : array of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Compute D^2, the percentage of deviance explained.\n\n        D^2 is a generalization of the coefficient of determination R^2.\n        R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n        :ref:`User Guide <regression_metrics>`.\n\n        D^2 is defined as\n        :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n        :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n        with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n        The mean :math:`\\bar{y}` is averaged by sample_weight.\n        Best possible score is 1.0 and it can be negative (because the model\n        can be arbitrarily worse).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,)\n            True values of target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "True": {
                "type": "values of target.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": "D^2 of self.predict(X) w.r.t. y."
              }
            },
            "returns": "-------\n        score : float\n            D^2 of self.predict(X) w.r.t. y.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.linear_model._glm.glm.TweedieRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.TweedieRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.linear_model._glm.glm.TweedieRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.TweedieRegressor",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    }
  ]
}