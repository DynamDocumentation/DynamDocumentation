{
  "description": "Tools for model inspection.",
  "functions": [
    {
      "name": "partial_dependence",
      "signature": "partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average')",
      "documentation": {
        "description": "Partial dependence of ``features``.\n\n    Partial dependence of a feature (or a set of features) corresponds to\n    the average response of an estimator for each possible value of the\n    feature.\n\n    Read more in the :ref:`User Guide <partial_dependence>`.\n\n    .. warning::\n\n        For :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n        :class:`~sklearn.ensemble.GradientBoostingRegressor`, the\n        `'recursion'` method (used by default) will not account for the `init`\n        predictor of the boosting process. In practice, this will produce\n        the same values as `'brute'` up to a constant offset in the target\n        response, provided that `init` is a constant estimator (which is the\n        default). However, if `init` is not a constant estimator, the\n        partial dependence values are incorrect for `'recursion'` because the\n        offset will be sample-dependent. It is preferable to use the `'brute'`\n        method. Note that this only applies to\n        :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n        :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to\n        :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n        :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\n    Parameters\n    ----------\n    estimator : BaseEstimator\n        A fitted estimator object implementing :term:`predict`,\n        :term:`predict_proba`, or :term:`decision_function`.\n        Multioutput-multiclass classifiers are not supported.\n\n    X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)\n        ``X`` is used to generate a grid of values for the target\n        ``features`` (where the partial dependence will be evaluated), and\n        also to generate values for the complement features when the\n        `method` is 'brute'.\n\n    features : array-like of {int, str, bool} or int or str\n        The feature (e.g. `[0]`) or pair of interacting features\n        (e.g. `[(0, 1)]`) for which the partial dependency should be computed.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights are used to calculate weighted means when averaging the\n        model output. If `None`, then samples are equally weighted. If\n        `sample_weight` is not `None`, then `method` will be set to `'brute'`.\n        Note that `sample_weight` is ignored for `kind='individual'`.\n\n        .. versionadded:: 1.3\n\n    categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None\n        Indicates the categorical features.\n\n        - `None`: no feature will be considered categorical;\n        - boolean array-like: boolean mask of shape `(n_features,)`\n            indicating which features are categorical. Thus, this array has\n            the same shape has `X.shape[1]`;\n        - integer or string array-like: integer indices or strings\n            indicating categorical features.\n\n        .. versionadded:: 1.2\n\n    feature_names : array-like of shape (n_features,), dtype=str, default=None\n        Name of each feature; `feature_names[i]` holds the name of the feature\n        with index `i`.\n        By default, the name of the feature corresponds to their numerical\n        index for NumPy array and their column name for pandas dataframe.\n\n        .. versionadded:: 1.2\n\n    response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'\n        Specifies whether to use :term:`predict_proba` or\n        :term:`decision_function` as the target response. For regressors\n        this parameter is ignored and the response is always the output of\n        :term:`predict`. By default, :term:`predict_proba` is tried first\n        and we revert to :term:`decision_function` if it doesn't exist. If\n        ``method`` is 'recursion', the response is always the output of\n        :term:`decision_function`.\n\n    percentiles : tuple of float, default=(0.05, 0.95)\n        The lower and upper percentile used to create the extreme values\n        for the grid. Must be in [0, 1].\n\n    grid_resolution : int, default=100\n        The number of equally spaced points on the grid, for each target\n        feature.\n\n    method : {'auto', 'recursion', 'brute'}, default='auto'\n        The method used to calculate the averaged predictions:\n\n        - `'recursion'` is only supported for some tree-based estimators\n          (namely\n          :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n          :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n          :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n          :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n          :class:`~sklearn.tree.DecisionTreeRegressor`,\n          :class:`~sklearn.ensemble.RandomForestRegressor`,\n          ) when `kind='average'`.\n          This is more efficient in terms of speed.\n          With this method, the target response of a\n          classifier is always the decision function, not the predicted\n          probabilities. Since the `'recursion'` method implicitly computes\n          the average of the Individual Conditional Expectation (ICE) by\n          design, it is not compatible with ICE and thus `kind` must be\n          `'average'`.\n\n        - `'brute'` is supported for any estimator, but is more\n          computationally intensive.\n\n        - `'auto'`: the `'recursion'` is used for estimators that support it,\n          and `'brute'` is used otherwise. If `sample_weight` is not `None`,\n          then `'brute'` is used regardless of the estimator.\n\n        Please see :ref:`this note <pdp_method_differences>` for\n        differences between the `'brute'` and `'recursion'` method.\n\n    kind : {'average', 'individual', 'both'}, default='average'\n        Whether to return the partial dependence averaged across all the\n        samples in the dataset or one value per sample or both.\n        See Returns below.\n\n        Note that the fast `method='recursion'` option is only available for\n        `kind='average'` and `sample_weights=None`. Computing individual\n        dependencies and doing weighted averages requires using the slower\n        `method='brute'`.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    predictions : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)\n            The predictions for all the points in the grid for all\n            samples in X. This is also known as Individual\n            Conditional Expectation (ICE).\n            Only available when `kind='individual'` or `kind='both'`.\n\n        average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)\n            The predictions for all the points in the grid, averaged\n            over all samples in X (or over the training data if\n            `method` is 'recursion').\n            Only available when `kind='average'` or `kind='both'`.\n\n        grid_values : seq of 1d ndarrays\n            The values with which the grid has been created. The generated\n            grid is a cartesian product of the arrays in `grid_values` where\n            `len(grid_values) == len(features)`. The size of each array\n            `grid_values[j]` is either `grid_resolution`, or the number of\n            unique values in `X[:, j]`, whichever is smaller.\n\n            .. versionadded:: 1.3\n\n        `n_outputs` corresponds to the number of classes in a multi-class\n        setting, or to the number of tasks for multi-output regression.\n        For classical regression and binary classification `n_outputs==1`.\n        `n_values_feature_j` corresponds to the size `grid_values[j]`.\n\n    See Also\n    --------\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\n    PartialDependenceDisplay : Partial Dependence visualization.",
        "parameters": {
          "estimator": {
            "type": "BaseEstimator",
            "description": ""
          },
          "A": {
            "type": "fitted estimator object implementing :term:`predict`,",
            "description": ":term:`predict_proba`, or :term:`decision_function`.\nMultioutput-multiclass classifiers are not supported."
          },
          "X": {
            "type": "{array",
            "description": "like, sparse matrix or dataframe} of shape (n_samples, n_features)\n``X`` is used to generate a grid of values for the target\n``features`` (where the partial dependence will be evaluated), and"
          },
          "also": {
            "type": "to generate values for the complement features when the",
            "description": "`method` is 'brute'."
          },
          "features": {
            "type": "array",
            "description": "like of {int, str, bool} or int or str"
          },
          "The": {
            "type": "values with which the grid has been created. The generated",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Sample": {
            "type": "weights are used to calculate weighted means when averaging the",
            "description": ""
          },
          "model": {
            "type": "output. If `None`, then samples are equally weighted. If",
            "description": "`sample_weight` is not `None`, then `method` will be set to `'brute'`."
          },
          "Note": {
            "type": "that the fast `method='recursion'` option is only available for",
            "description": "`kind='average'` and `sample_weights=None`. Computing individual"
          },
          "categorical_features": {
            "type": "array",
            "description": "like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None"
          },
          "Indicates": {
            "type": "the categorical features.",
            "description": "- `None`: no feature will be considered categorical;\n- boolean array-like: boolean mask of shape `(n_features,)`"
          },
          "indicating": {
            "type": "categorical features.",
            "description": ".. versionadded:: 1.2"
          },
          "the": {
            "type": "average of the Individual Conditional Expectation (ICE) by",
            "description": "design, it is not compatible with ICE and thus `kind` must be\n`'average'`.\n- `'brute'` is supported for any estimator, but is more"
          },
          "feature_names": {
            "type": "array",
            "description": "like of shape (n_features,), dtype=str, default=None"
          },
          "Name": {
            "type": "of each feature; `feature_names[i]` holds the name of the feature",
            "description": ""
          },
          "with": {
            "type": "index `i`.",
            "description": ""
          },
          "By": {
            "type": "default, the name of the feature corresponds to their numerical",
            "description": ""
          },
          "index": {
            "type": "for NumPy array and their column name for pandas dataframe.",
            "description": ".. versionadded:: 1.2"
          },
          "response_method": {
            "type": "{'auto', 'predict_proba', 'decision_function'},             default='auto'",
            "description": ""
          },
          "Specifies": {
            "type": "whether to use :term:`predict_proba` or",
            "description": ":term:`decision_function` as the target response. For regressors"
          },
          "this": {
            "type": "parameter is ignored and the response is always the output of",
            "description": ":term:`predict`. By default, :term:`predict_proba` is tried first"
          },
          "and": {
            "type": "`'brute'` is used otherwise. If `sample_weight` is not `None`,",
            "description": ""
          },
          "percentiles": {
            "type": "tuple of float, default=(0.05, 0.95)",
            "description": ""
          },
          "for": {
            "type": "the grid. Must be in [0, 1].",
            "description": ""
          },
          "grid_resolution": {
            "type": "int, default=100",
            "description": ""
          },
          "method": {
            "type": "{'auto', 'recursion', 'brute'}, default='auto'",
            "description": ""
          },
          "This": {
            "type": "is more efficient in terms of speed.",
            "description": ""
          },
          "With": {
            "type": "this method, the target response of a",
            "description": ""
          },
          "classifier": {
            "type": "is always the decision function, not the predicted",
            "description": "probabilities. Since the `'recursion'` method implicitly computes"
          },
          "computationally": {
            "type": "intensive.",
            "description": "- `'auto'`: the `'recursion'` is used for estimators that support it,"
          },
          "then": {
            "type": "`'brute'` is used regardless of the estimator.",
            "description": ""
          },
          "Please": {
            "type": "see :ref:`this note <pdp_method_differences>` for",
            "description": ""
          },
          "differences": {
            "type": "between the `'brute'` and `'recursion'` method.",
            "description": ""
          },
          "kind": {
            "type": "{'average', 'individual', 'both'}, default='average'",
            "description": ""
          },
          "Whether": {
            "type": "to return the partial dependence averaged across all the",
            "description": ""
          },
          "samples": {
            "type": "in X. This is also known as Individual",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nPartialDependenceDisplay.from_estimator : Plot Partial Dependence."
          },
          "dependencies": {
            "type": "and doing weighted averages requires using the slower",
            "description": "`method='brute'`.\n.. versionadded:: 0.24\nReturns\n-------"
          },
          "predictions": {
            "type": ":class:`~sklearn.utils.Bunch`",
            "description": "Dictionary-like object, with the following attributes."
          },
          "individual": {
            "type": "ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)",
            "description": ""
          },
          "Conditional": {
            "type": "Expectation (ICE).",
            "description": ""
          },
          "Only": {
            "type": "available when `kind='average'` or `kind='both'`.",
            "description": ""
          },
          "average": {
            "type": "ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)",
            "description": ""
          },
          "over": {
            "type": "all samples in X (or over the training data if",
            "description": "`method` is 'recursion')."
          },
          "grid_values": {
            "type": "seq of 1d ndarrays",
            "description": ""
          },
          "grid": {
            "type": "is a cartesian product of the arrays in `grid_values` where",
            "description": "`len(grid_values) == len(features)`. The size of each array\n`grid_values[j]` is either `grid_resolution`, or the number of"
          },
          "unique": {
            "type": "values in `X[:, j]`, whichever is smaller.",
            "description": ".. versionadded:: 1.3\n`n_outputs` corresponds to the number of classes in a multi-class\nsetting, or to the number of tasks for multi-output regression."
          },
          "For": {
            "type": "classical regression and binary classification `n_outputs==1`.",
            "description": "`n_values_feature_j` corresponds to the size `grid_values[j]`."
          },
          "PartialDependenceDisplay": {
            "type": "Partial Dependence visualization.",
            "description": "Examples\n--------\n>>> X = [[0, 0, 2], [1, 0, 0]]\n>>> y = [0, 1]\n>>> from sklearn.ensemble import GradientBoostingClassifier\n>>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n>>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),\n...                    grid_resolution=2) # doctest: +SKIP\n(array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])"
          }
        },
        "returns": "-------\n    predictions : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)\n            The predictions for all the points in the grid for all\n            samples in X. This is also known as Individual\n            Conditional Expectation (ICE).\n            Only available when `kind='individual'` or `kind='both'`.\n\n        average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)\n            The predictions for all the points in the grid, averaged\n            over all samples in X (or over the training data if\n            `method` is 'recursion').\n            Only available when `kind='average'` or `kind='both'`.\n\n        grid_values : seq of 1d ndarrays\n            The values with which the grid has been created. The generated\n            grid is a cartesian product of the arrays in `grid_values` where\n            `len(grid_values) == len(features)`. The size of each array\n            `grid_values[j]` is either `grid_resolution`, or the number of\n            unique values in `X[:, j]`, whichever is smaller.\n\n            .. versionadded:: 1.3\n\n        `n_outputs` corresponds to the number of classes in a multi-class\n        setting, or to the number of tasks for multi-output regression.\n        For classical regression and binary classification `n_outputs==1`.\n        `n_values_feature_j` corresponds to the size `grid_values[j]`.\n\n    See Also\n    --------\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\n    PartialDependenceDisplay : Partial Dependence visualization.\n\n    Examples\n    --------\n    >>> X = [[0, 0, 2], [1, 0, 0]]\n    >>> y = [0, 1]\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n    >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n    >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),\n    ...                    grid_resolution=2) # doctest: +SKIP\n    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])",
        "raises": "",
        "see_also": "--------\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\n    PartialDependenceDisplay : Partial Dependence visualization.\n\n    Examples\n    --------\n    >>> X = [[0, 0, 2], [1, 0, 0]]\n    >>> y = [0, 1]\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n    >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n    >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),\n    ...                    grid_resolution=2) # doctest: +SKIP\n    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])",
        "notes": "that `sample_weight` is ignored for `kind='individual'`.\n\n        .. versionadded:: 1.3\n\n    categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None\n        Indicates the categorical features.\n\n        - `None`: no feature will be considered categorical;\n        - boolean array-like: boolean mask of shape `(n_features,)`\n            indicating which features are categorical. Thus, this array has\n            the same shape has `X.shape[1]`;\n        - integer or string array-like: integer indices or strings\n            indicating categorical features.\n\n        .. versionadded:: 1.2\n\n    feature_names : array-like of shape (n_features,), dtype=str, default=None\n        Name of each feature; `feature_names[i]` holds the name of the feature\n        with index `i`.\n        By default, the name of the feature corresponds to their numerical\n        index for NumPy array and their column name for pandas dataframe.\n\n        .. versionadded:: 1.2\n\n    response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'\n        Specifies whether to use :term:`predict_proba` or\n        :term:`decision_function` as the target response. For regressors\n        this parameter is ignored and the response is always the output of\n        :term:`predict`. By default, :term:`predict_proba` is tried first\n        and we revert to :term:`decision_function` if it doesn't exist. If\n        ``method`` is 'recursion', the response is always the output of\n        :term:`decision_function`.\n\n    percentiles : tuple of float, default=(0.05, 0.95)\n        The lower and upper percentile used to create the extreme values\n        for the grid. Must be in [0, 1].\n\n    grid_resolution : int, default=100\n        The number of equally spaced points on the grid, for each target\n        feature.\n\n    method : {'auto', 'recursion', 'brute'}, default='auto'\n        The method used to calculate the averaged predictions:\n\n        - `'recursion'` is only supported for some tree-based estimators\n          (namely\n          :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n          :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n          :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n          :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n          :class:`~sklearn.tree.DecisionTreeRegressor`,\n          :class:`~sklearn.ensemble.RandomForestRegressor`,\n          ) when `kind='average'`.\n          This is more efficient in terms of speed.\n          With this method, the target response of a\n          classifier is always the decision function, not the predicted\n          probabilities. Since the `'recursion'` method implicitly computes\n          the average of the Individual Conditional Expectation (ICE) by\n          design, it is not compatible with ICE and thus `kind` must be\n          `'average'`.\n\n        - `'brute'` is supported for any estimator, but is more\n          computationally intensive.\n\n        - `'auto'`: the `'recursion'` is used for estimators that support it,\n          and `'brute'` is used otherwise. If `sample_weight` is not `None`,\n          then `'brute'` is used regardless of the estimator.\n\n        Please see :ref:`this note <pdp_method_differences>` for\n        differences between the `'brute'` and `'recursion'` method.\n\n    kind : {'average', 'individual', 'both'}, default='average'\n        Whether to return the partial dependence averaged across all the\n        samples in the dataset or one value per sample or both.\n        See Returns below.",
        "examples": "--------\n    >>> X = [[0, 0, 2], [1, 0, 0]]\n    >>> y = [0, 1]\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n    >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n    >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),\n    ...                    grid_resolution=2) # doctest: +SKIP\n    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])"
      }
    },
    {
      "name": "permutation_importance",
      "signature": "permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0)",
      "documentation": {
        "description": "Permutation importance for feature evaluation [BRE]_.\n\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\n    data set used to train the estimator or a hold-out set. The permutation\n    importance of a feature is calculated as follows. First, a baseline metric,\n    defined by :term:`scoring`, is evaluated on a (potentially different)\n    dataset defined by the `X`. Next, a feature column from the validation set\n    is permuted and the metric is evaluated again. The permutation importance\n    is defined to be the difference between the baseline metric and metric from\n    permutating the feature column.\n\n    Read more in the :ref:`User Guide <permutation_importance>`.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator that has already been :term:`fitted` and is compatible\n        with :term:`scorer`.\n\n    X : ndarray or DataFrame, shape (n_samples, n_features)\n        Data on which permutation importance will be computed.\n\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n        Targets for supervised or `None` for unsupervised.\n\n    scoring : str, callable, list, tuple, or dict, default=None\n        Scorer to use.\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring_callable`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        Passing multiple scores to `scoring` is more efficient than calling\n        `permutation_importance` for each of the scores as it reuses\n        predictions to avoid redundant computation.\n\n        If None, the estimator's default scorer is used.\n\n    n_repeats : int, default=5\n        Number of times to permute a feature.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel. The computation is done by computing\n        permutation score for each columns and parallelized over the columns.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Pseudo-random number generator to control the permutations of each\n        feature.\n        Pass an int to get reproducible results across function calls.\n        See :term:`Glossary <random_state>`.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights used in scoring.\n\n        .. versionadded:: 0.24\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to compute feature importance\n        in each repeat (without replacement).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\n          will be used.\n\n        While using this option may provide less accurate importance estimates,\n        it keeps the method tractable when evaluating feature importance on\n        large datasets. In combination with `n_repeats`, this allows to control\n        the computational speed vs statistical accuracy trade-off of this method.\n\n        .. versionadded:: 1.0\n\n    Returns\n    -------\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\n        Dictionary-like object, with the following attributes.\n\n        importances_mean : ndarray of shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray of shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray of shape (n_features, n_repeats)\n            Raw permutation importance scores.\n\n        If there are multiple scoring metrics in the scoring parameter\n        `result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n        `Bunch` objects like above as values.\n\n    References\n    ----------\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n             2001. <10.1023/A:1010933404324>`",
        "parameters": {
          "estimator": {
            "type": "object",
            "description": ""
          },
          "An": {
            "type": "estimator that has already been :term:`fitted` and is compatible",
            "description": ""
          },
          "with": {
            "type": "term:`scorer`.",
            "description": ""
          },
          "X": {
            "type": "ndarray or DataFrame, shape (n_samples, n_features)",
            "description": ""
          },
          "Data": {
            "type": "on which permutation importance will be computed.",
            "description": ""
          },
          "y": {
            "type": "array",
            "description": "like or None, shape (n_samples, ) or (n_samples, n_classes)"
          },
          "Targets": {
            "type": "for supervised or `None` for unsupervised.",
            "description": ""
          },
          "scoring": {
            "type": "str, callable, list, tuple, or dict, default=None",
            "description": ""
          },
          "Scorer": {
            "type": "to use.",
            "description": ""
          },
          "If": {
            "type": "there are multiple scoring metrics in the scoring parameter",
            "description": "`result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n`Bunch` objects like above as values.\nReferences\n----------\n.. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n2001. <10.1023/A:1010933404324>`\nExamples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.inspection import permutation_importance\n>>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n>>> y = [1, 1, 1, 0, 0, 0]\n>>> clf = LogisticRegression().fit(X, y)\n>>> result = permutation_importance(clf, X, y, n_repeats=10,\n...                                 random_state=0)\n>>> result.importances_mean"
          },
          "names": {
            "type": "and the values are the metric scores;",
            "description": "- a dictionary with metric names as keys and callables a values."
          },
          "Passing": {
            "type": "multiple scores to `scoring` is more efficient than calling",
            "description": "`permutation_importance` for each of the scores as it reuses"
          },
          "predictions": {
            "type": "to avoid redundant computation.",
            "description": ""
          },
          "n_repeats": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of jobs to run in parallel. The computation is done by computing",
            "description": ""
          },
          "n_jobs": {
            "type": "int or None, default=None",
            "description": ""
          },
          "permutation": {
            "type": "score for each columns and parallelized over the columns.",
            "description": "`None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n`-1` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance, default=None",
            "description": "Pseudo-random number generator to control the permutations of each\nfeature."
          },
          "Pass": {
            "type": "an int to get reproducible results across function calls.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": ""
          },
          "sample_weight": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Sample": {
            "type": "weights used in scoring.",
            "description": ".. versionadded:: 0.24"
          },
          "max_samples": {
            "type": "int or float, default=1.0",
            "description": ""
          },
          "The": {
            "type": "number of samples to draw from X to compute feature importance",
            "description": ""
          },
          "in": {
            "type": "each repeat (without replacement).",
            "description": "- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples.\n- If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples"
          },
          "will": {
            "type": "be used.",
            "description": ""
          },
          "While": {
            "type": "using this option may provide less accurate importance estimates,",
            "description": ""
          },
          "it": {
            "type": "keeps the method tractable when evaluating feature importance on",
            "description": ""
          },
          "large": {
            "type": "datasets. In combination with `n_repeats`, this allows to control",
            "description": ""
          },
          "the": {
            "type": "computational speed vs statistical accuracy trade-off of this method.",
            "description": ".. versionadded:: 1.0\nReturns\n-------"
          },
          "result": {
            "type": ":class:`~sklearn.utils.Bunch` or dict of such instances",
            "description": "Dictionary-like object, with the following attributes."
          },
          "importances_mean": {
            "type": "ndarray of shape (n_features, )",
            "description": ""
          },
          "Mean": {
            "type": "of feature importance over `n_repeats`.",
            "description": ""
          },
          "importances_std": {
            "type": "ndarray of shape (n_features, )",
            "description": ""
          },
          "Standard": {
            "type": "deviation over `n_repeats`.",
            "description": ""
          },
          "importances": {
            "type": "ndarray of shape (n_features, n_repeats)",
            "description": ""
          },
          "Raw": {
            "type": "permutation importance scores.",
            "description": ""
          },
          "array": {
            "type": "[0.2211..., 0.       , 0.       ]",
            "description": ""
          }
        },
        "returns": "-------\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\n        Dictionary-like object, with the following attributes.\n\n        importances_mean : ndarray of shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray of shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray of shape (n_features, n_repeats)\n            Raw permutation importance scores.\n\n        If there are multiple scoring metrics in the scoring parameter\n        `result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n        `Bunch` objects like above as values.\n\n    References\n    ----------\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n             2001. <10.1023/A:1010933404324>`\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.inspection import permutation_importance\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n    >>> y = [1, 1, 1, 0, 0, 0]\n    >>> clf = LogisticRegression().fit(X, y)\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n    ...                                 random_state=0)\n    >>> result.importances_mean\n    array([0.4666..., 0.       , 0.       ])\n    >>> result.importances_std\n    array([0.2211..., 0.       , 0.       ])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.inspection import permutation_importance\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n    >>> y = [1, 1, 1, 0, 0, 0]\n    >>> clf = LogisticRegression().fit(X, y)\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n    ...                                 random_state=0)\n    >>> result.importances_mean\n    array([0.4666..., 0.       , 0.       ])\n    >>> result.importances_std\n    array([0.2211..., 0.       , 0.       ])"
      }
    }
  ],
  "classes": [
    {
      "name": "DecisionBoundaryDisplay",
      "documentation": {
        "description": "Decisions boundary visualization.\n\n    It is recommended to use\n    :func:`~sklearn.inspection.DecisionBoundaryDisplay.from_estimator`\n    to create a :class:`DecisionBoundaryDisplay`. All parameters are stored as\n    attributes.\n\n    Read more in the :ref:`User Guide <visualizations>`.\n\n    .. versionadded:: 1.1\n\n    Parameters\n    ----------\n    xx0 : ndarray of shape (grid_resolution, grid_resolution)\n        First output of :func:`meshgrid <numpy.meshgrid>`.\n\n    xx1 : ndarray of shape (grid_resolution, grid_resolution)\n        Second output of :func:`meshgrid <numpy.meshgrid>`.\n\n    response : ndarray of shape (grid_resolution, grid_resolution)\n        Values of the response function.\n\n    xlabel : str, default=None\n        Default label to place on x axis.\n\n    ylabel : str, default=None\n        Default label to place on y axis.\n\n    Attributes\n    ----------\n    surface_ : matplotlib `QuadContourSet` or `QuadMesh`\n        If `plot_method` is 'contour' or 'contourf', `surface_` is a\n        :class:`QuadContourSet <matplotlib.contour.QuadContourSet>`. If\n        `plot_method` is 'pcolormesh', `surface_` is a\n        :class:`QuadMesh <matplotlib.collections.QuadMesh>`.\n\n    ax_ : matplotlib Axes\n        Axes with decision boundary.\n\n    figure_ : matplotlib Figure\n        Figure containing the decision boundary.\n\n    See Also\n    --------\n    DecisionBoundaryDisplay.from_estimator : Plot decision boundary given an estimator.",
        "parameters": {
          "xx0": {
            "type": "ndarray of shape (grid_resolution, grid_resolution)",
            "description": ""
          },
          "First": {
            "type": "output of :func:`meshgrid <numpy.meshgrid>`.",
            "description": ""
          },
          "xx1": {
            "type": "ndarray of shape (grid_resolution, grid_resolution)",
            "description": ""
          },
          "Second": {
            "type": "output of :func:`meshgrid <numpy.meshgrid>`.",
            "description": ""
          },
          "response": {
            "type": "ndarray of shape (grid_resolution, grid_resolution)",
            "description": ""
          },
          "Values": {
            "type": "of the response function.",
            "description": ""
          },
          "xlabel": {
            "type": "str, default=None",
            "description": ""
          },
          "Default": {
            "type": "label to place on y axis.",
            "description": "Attributes\n----------"
          },
          "ylabel": {
            "type": "str, default=None",
            "description": ""
          },
          "surface_": {
            "type": "matplotlib `QuadContourSet` or `QuadMesh`",
            "description": ""
          },
          "If": {
            "type": "`plot_method` is 'contour' or 'contourf', `surface_` is a",
            "description": ":class:`QuadContourSet <matplotlib.contour.QuadContourSet>`. If\n`plot_method` is 'pcolormesh', `surface_` is a\n:class:`QuadMesh <matplotlib.collections.QuadMesh>`."
          },
          "ax_": {
            "type": "matplotlib Axes",
            "description": ""
          },
          "Axes": {
            "type": "with decision boundary.",
            "description": ""
          },
          "figure_": {
            "type": "matplotlib Figure",
            "description": ""
          },
          "Figure": {
            "type": "containing the decision boundary.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nDecisionBoundaryDisplay.from_estimator : Plot decision boundary given an estimator.\nExamples\n--------\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.inspection import DecisionBoundaryDisplay\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> iris = load_iris()\n>>> feature_1, feature_2 = np.meshgrid(\n...     np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n...     np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n... )\n>>> grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n>>> tree = DecisionTreeClassifier().fit(iris.data[:, :2], iris.target)\n>>> y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n>>> display = DecisionBoundaryDisplay(\n...     xx0=feature_1, xx1=feature_2, response=y_pred\n... )\n>>> display.plot()\n<...>\n>>> display.ax_.scatter(\n...     iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\"\n... )\n<...>\n>>> plt.show()"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    DecisionBoundaryDisplay.from_estimator : Plot decision boundary given an estimator.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> import numpy as np\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.inspection import DecisionBoundaryDisplay\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> iris = load_iris()\n    >>> feature_1, feature_2 = np.meshgrid(\n    ...     np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n    ...     np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n    ... )\n    >>> grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    >>> tree = DecisionTreeClassifier().fit(iris.data[:, :2], iris.target)\n    >>> y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n    >>> display = DecisionBoundaryDisplay(\n    ...     xx0=feature_1, xx1=feature_2, response=y_pred\n    ... )\n    >>> display.plot()\n    <...>\n    >>> display.ax_.scatter(\n    ...     iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\"\n    ... )\n    <...>\n    >>> plt.show()",
        "notes": "",
        "examples": "--------\n    >>> import matplotlib.pyplot as plt\n    >>> import numpy as np\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.inspection import DecisionBoundaryDisplay\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> iris = load_iris()\n    >>> feature_1, feature_2 = np.meshgrid(\n    ...     np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n    ...     np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n    ... )\n    >>> grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    >>> tree = DecisionTreeClassifier().fit(iris.data[:, :2], iris.target)\n    >>> y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n    >>> display = DecisionBoundaryDisplay(\n    ...     xx0=feature_1, xx1=feature_2, response=y_pred\n    ... )\n    >>> display.plot()\n    <...>\n    >>> display.ax_.scatter(\n    ...     iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\"\n    ... )\n    <...>\n    >>> plt.show()"
      },
      "methods": [
        {
          "name": "from_estimator",
          "signature": "from_estimator(estimator, X, *, grid_resolution=100, eps=1.0, plot_method='contourf', response_method='auto', class_of_interest=None, xlabel=None, ylabel=None, ax=None, **kwargs)",
          "documentation": {
            "description": "Plot decision boundary given an estimator.\n\n        Read more in the :ref:`User Guide <visualizations>`.\n\n        Parameters\n        ----------\n        estimator : object\n            Trained estimator used to plot the decision boundary.\n\n        X : {array-like, sparse matrix, dataframe} of shape (n_samples, 2)\n            Input data that should be only 2-dimensional.\n\n        grid_resolution : int, default=100\n            Number of grid points to use for plotting decision boundary.\n            Higher values will make the plot look nicer but be slower to\n            render.\n\n        eps : float, default=1.0\n            Extends the minimum and maximum values of X for evaluating the\n            response function.\n\n        plot_method : {'contourf', 'contour', 'pcolormesh'}, default='contourf'\n            Plotting method to call when plotting the response. Please refer\n            to the following matplotlib documentation for details:\n            :func:`contourf <matplotlib.pyplot.contourf>`,\n            :func:`contour <matplotlib.pyplot.contour>`,\n            :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`.\n\n        response_method : {'auto', 'predict_proba', 'decision_function',                 'predict'}, default='auto'\n            Specifies whether to use :term:`predict_proba`,\n            :term:`decision_function`, :term:`predict` as the target response.\n            If set to 'auto', the response method is tried in the following order:\n            :term:`decision_function`, :term:`predict_proba`, :term:`predict`.\n            For multiclass problems, :term:`predict` is selected when\n            `response_method=\"auto\"`.\n\n        class_of_interest : int, float, bool or str, default=None\n            The class considered when plotting the decision. If None,\n            `estimator.classes_[1]` is considered as the positive class\n            for binary classifiers. Must have an explicit value for\n            multiclass classifiers when `response_method` is 'predict_proba'\n            or 'decision_function'.\n\n            .. versionadded:: 1.4\n\n        xlabel : str, default=None\n            The label used for the x-axis. If `None`, an attempt is made to\n            extract a label from `X` if it is a dataframe, otherwise an empty\n            string is used.\n\n        ylabel : str, default=None\n            The label used for the y-axis. If `None`, an attempt is made to\n            extract a label from `X` if it is a dataframe, otherwise an empty\n            string is used.\n\n        ax : Matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Additional keyword arguments to be passed to the\n            `plot_method`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.inspection.DecisionBoundaryDisplay`\n            Object that stores the result.\n\n        See Also\n        --------\n        DecisionBoundaryDisplay : Decision boundary visualization.\n        sklearn.metrics.ConfusionMatrixDisplay.from_estimator : Plot the\n            confusion matrix given an estimator, the data, and the label.\n        sklearn.metrics.ConfusionMatrixDisplay.from_predictions : Plot the\n            confusion matrix given the true and predicted labels.",
            "parameters": {
              "estimator": {
                "type": "object",
                "description": ""
              },
              "Trained": {
                "type": "estimator used to plot the decision boundary.",
                "description": ""
              },
              "X": {
                "type": "{array",
                "description": "like, sparse matrix, dataframe} of shape (n_samples, 2)"
              },
              "Input": {
                "type": "data that should be only 2-dimensional.",
                "description": ""
              },
              "grid_resolution": {
                "type": "int, default=100",
                "description": ""
              },
              "Number": {
                "type": "of grid points to use for plotting decision boundary.",
                "description": ""
              },
              "Higher": {
                "type": "values will make the plot look nicer but be slower to",
                "description": "render."
              },
              "eps": {
                "type": "float, default=1.0",
                "description": ""
              },
              "Extends": {
                "type": "the minimum and maximum values of X for evaluating the",
                "description": ""
              },
              "response": {
                "type": "function.",
                "description": ""
              },
              "plot_method": {
                "type": "{'contourf', 'contour', 'pcolormesh'}, default='contourf'",
                "description": ""
              },
              "Plotting": {
                "type": "method to call when plotting the response. Please refer",
                "description": ""
              },
              "to": {
                "type": "the following matplotlib documentation for details:",
                "description": ":func:`contourf <matplotlib.pyplot.contourf>`,\n:func:`contour <matplotlib.pyplot.contour>`,\n:func:`pcolormesh <matplotlib.pyplot.pcolormesh>`."
              },
              "response_method": {
                "type": "{'auto', 'predict_proba', 'decision_function',                 'predict'}, default='auto'",
                "description": ""
              },
              "Specifies": {
                "type": "whether to use :term:`predict_proba`,",
                "description": ":term:`decision_function`, :term:`predict` as the target response."
              },
              "If": {
                "type": "set to 'auto', the response method is tried in the following order:",
                "description": ":term:`decision_function`, :term:`predict_proba`, :term:`predict`."
              },
              "For": {
                "type": "multiclass problems, :term:`predict` is selected when",
                "description": "`response_method=\"auto\"`."
              },
              "class_of_interest": {
                "type": "int, float, bool or str, default=None",
                "description": ""
              },
              "The": {
                "type": "label used for the y-axis. If `None`, an attempt is made to",
                "description": ""
              },
              "for": {
                "type": "binary classifiers. Must have an explicit value for",
                "description": ""
              },
              "multiclass": {
                "type": "classifiers when `response_method` is 'predict_proba'",
                "description": ""
              },
              "or": {
                "type": "'decision_function'.",
                "description": ".. versionadded:: 1.4"
              },
              "xlabel": {
                "type": "str, default=None",
                "description": ""
              },
              "extract": {
                "type": "a label from `X` if it is a dataframe, otherwise an empty",
                "description": ""
              },
              "string": {
                "type": "is used.",
                "description": ""
              },
              "ylabel": {
                "type": "str, default=None",
                "description": ""
              },
              "ax": {
                "type": "Matplotlib axes, default=None",
                "description": ""
              },
              "Axes": {
                "type": "object to plot on. If `None`, a new figure and axes is",
                "description": "created.\n**kwargs : dict"
              },
              "Additional": {
                "type": "keyword arguments to be passed to the",
                "description": "`plot_method`.\nReturns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.inspection.DecisionBoundaryDisplay`",
                "description": ""
              },
              "Object": {
                "type": "that stores the result.",
                "description": ""
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "DecisionBoundaryDisplay": {
                "type": "Decision boundary visualization.",
                "description": "sklearn.metrics.ConfusionMatrixDisplay.from_estimator : Plot the"
              },
              "confusion": {
                "type": "matrix given the true and predicted labels.",
                "description": "Examples\n--------\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.inspection import DecisionBoundaryDisplay\n>>> iris = load_iris()\n>>> X = iris.data[:, :2]\n>>> classifier = LogisticRegression().fit(X, iris.target)\n>>> disp = DecisionBoundaryDisplay.from_estimator(\n...     classifier, X, response_method=\"predict\",\n...     xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n...     alpha=0.5,\n... )\n>>> disp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\n<...>\n>>> plt.show()"
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.inspection.DecisionBoundaryDisplay`\n            Object that stores the result.\n\n        See Also\n        --------\n        DecisionBoundaryDisplay : Decision boundary visualization.\n        sklearn.metrics.ConfusionMatrixDisplay.from_estimator : Plot the\n            confusion matrix given an estimator, the data, and the label.\n        sklearn.metrics.ConfusionMatrixDisplay.from_predictions : Plot the\n            confusion matrix given the true and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import load_iris\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.inspection import DecisionBoundaryDisplay\n        >>> iris = load_iris()\n        >>> X = iris.data[:, :2]\n        >>> classifier = LogisticRegression().fit(X, iris.target)\n        >>> disp = DecisionBoundaryDisplay.from_estimator(\n        ...     classifier, X, response_method=\"predict\",\n        ...     xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n        ...     alpha=0.5,\n        ... )\n        >>> disp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\n        <...>\n        >>> plt.show()",
            "raises": "",
            "see_also": "--------\n        DecisionBoundaryDisplay : Decision boundary visualization.\n        sklearn.metrics.ConfusionMatrixDisplay.from_estimator : Plot the\n            confusion matrix given an estimator, the data, and the label.\n        sklearn.metrics.ConfusionMatrixDisplay.from_predictions : Plot the\n            confusion matrix given the true and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import load_iris\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.inspection import DecisionBoundaryDisplay\n        >>> iris = load_iris()\n        >>> X = iris.data[:, :2]\n        >>> classifier = LogisticRegression().fit(X, iris.target)\n        >>> disp = DecisionBoundaryDisplay.from_estimator(\n        ...     classifier, X, response_method=\"predict\",\n        ...     xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n        ...     alpha=0.5,\n        ... )\n        >>> disp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\n        <...>\n        >>> plt.show()",
            "notes": "",
            "examples": "--------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import load_iris\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.inspection import DecisionBoundaryDisplay\n        >>> iris = load_iris()\n        >>> X = iris.data[:, :2]\n        >>> classifier = LogisticRegression().fit(X, iris.target)\n        >>> disp = DecisionBoundaryDisplay.from_estimator(\n        ...     classifier, X, response_method=\"predict\",\n        ...     xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n        ...     alpha=0.5,\n        ... )\n        >>> disp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\n        <...>\n        >>> plt.show()"
          }
        },
        {
          "name": "plot",
          "signature": "plot(self, plot_method='contourf', ax=None, xlabel=None, ylabel=None, **kwargs)",
          "documentation": {
            "description": "Plot visualization.\n\n        Parameters\n        ----------\n        plot_method : {'contourf', 'contour', 'pcolormesh'}, default='contourf'\n            Plotting method to call when plotting the response. Please refer\n            to the following matplotlib documentation for details:\n            :func:`contourf <matplotlib.pyplot.contourf>`,\n            :func:`contour <matplotlib.pyplot.contour>`,\n            :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`.\n\n        ax : Matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        xlabel : str, default=None\n            Overwrite the x-axis label.\n\n        ylabel : str, default=None\n            Overwrite the y-axis label.\n\n        **kwargs : dict\n            Additional keyword arguments to be passed to the `plot_method`.",
            "parameters": {
              "plot_method": {
                "type": "{'contourf', 'contour', 'pcolormesh'}, default='contourf'",
                "description": ""
              },
              "Plotting": {
                "type": "method to call when plotting the response. Please refer",
                "description": ""
              },
              "to": {
                "type": "the following matplotlib documentation for details:",
                "description": ":func:`contourf <matplotlib.pyplot.contourf>`,\n:func:`contour <matplotlib.pyplot.contour>`,\n:func:`pcolormesh <matplotlib.pyplot.pcolormesh>`."
              },
              "ax": {
                "type": "Matplotlib axes, default=None",
                "description": ""
              },
              "Axes": {
                "type": "object to plot on. If `None`, a new figure and axes is",
                "description": "created."
              },
              "xlabel": {
                "type": "str, default=None",
                "description": ""
              },
              "Overwrite": {
                "type": "the y-axis label.",
                "description": "**kwargs : dict"
              },
              "ylabel": {
                "type": "str, default=None",
                "description": ""
              },
              "Additional": {
                "type": "keyword arguments to be passed to the `plot_method`.",
                "description": "Returns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.inspection.DecisionBoundaryDisplay`",
                "description": ""
              },
              "Object": {
                "type": "that stores computed values.",
                "description": ""
              }
            },
            "returns": "-------\n        display: :class:`~sklearn.inspection.DecisionBoundaryDisplay`\n            Object that stores computed values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PartialDependenceDisplay",
      "documentation": {
        "description": "Partial Dependence Plot (PDP).\n\n    This can also display individual partial dependencies which are often\n    referred to as: Individual Condition Expectation (ICE).\n\n    It is recommended to use\n    :func:`~sklearn.inspection.PartialDependenceDisplay.from_estimator` to create a\n    :class:`~sklearn.inspection.PartialDependenceDisplay`. All parameters are\n    stored as attributes.\n\n    Read more in\n    :ref:`sphx_glr_auto_examples_miscellaneous_plot_partial_dependence_visualization_api.py`\n    and the :ref:`User Guide <partial_dependence>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    pd_results : list of Bunch\n        Results of :func:`~sklearn.inspection.partial_dependence` for\n        ``features``.\n\n    features : list of (int,) or list of (int, int)\n        Indices of features for a given plot. A tuple of one integer will plot\n        a partial dependence curve of one feature. A tuple of two integers will\n        plot a two-way partial dependence curve as a contour plot.\n\n    feature_names : list of str\n        Feature names corresponding to the indices in ``features``.\n\n    target_idx : int\n\n        - In a multiclass setting, specifies the class for which the PDPs\n          should be computed. Note that for binary classification, the\n          positive class (index 1) is always used.\n        - In a multioutput setting, specifies the task for which the PDPs\n          should be computed.\n\n        Ignored in binary classification or classical regression settings.\n\n    deciles : dict\n        Deciles for feature indices in ``features``.\n\n    kind : {'average', 'individual', 'both'} or list of such str,             default='average'\n        Whether to plot the partial dependence averaged across all the samples\n        in the dataset or one line per sample or both.\n\n        - ``kind='average'`` results in the traditional PD plot;\n        - ``kind='individual'`` results in the ICE plot;\n        - ``kind='both'`` results in plotting both the ICE and PD on the same\n          plot.\n\n        A list of such strings can be provided to specify `kind` on a per-plot\n        basis. The length of the list should be the same as the number of\n        interaction requested in `features`.\n\n        .. note::\n           ICE ('individual' or 'both') is not a valid option for 2-ways\n           interactions plot. As a result, an error will be raised.\n           2-ways interaction plots should always be configured to\n           use the 'average' kind instead.\n\n        .. note::\n           The fast ``method='recursion'`` option is only available for\n           `kind='average'` and `sample_weights=None`. Computing individual\n           dependencies and doing weighted averages requires using the slower\n           `method='brute'`.\n\n        .. versionadded:: 0.24\n           Add `kind` parameter with `'average'`, `'individual'`, and `'both'`\n           options.\n\n        .. versionadded:: 1.1\n           Add the possibility to pass a list of string specifying `kind`\n           for each plot.\n\n    subsample : float, int or None, default=1000\n        Sampling for ICE curves when `kind` is 'individual' or 'both'.\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to be used to plot ICE curves. If int, represents the\n        maximum absolute number of samples to use.\n\n        Note that the full dataset is still used to calculate partial\n        dependence when `kind='both'`.\n\n        .. versionadded:: 0.24\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness of the selected samples when subsamples is not\n        `None`. See :term:`Glossary <random_state>` for details.\n\n        .. versionadded:: 0.24\n\n    is_categorical : list of (bool,) or list of (bool, bool), default=None\n        Whether each target feature in `features` is categorical or not.\n        The list should be same size as `features`. If `None`, all features\n        are assumed to be continuous.\n\n        .. versionadded:: 1.2\n\n    Attributes\n    ----------\n    bounding_ax_ : matplotlib Axes or None\n        If `ax` is an axes or None, the `bounding_ax_` is the axes where the\n        grid of partial dependence plots are drawn. If `ax` is a list of axes\n        or a numpy array of axes, `bounding_ax_` is None.\n\n    axes_ : ndarray of matplotlib Axes\n        If `ax` is an axes or None, `axes_[i, j]` is the axes on the i-th row\n        and j-th column. If `ax` is a list of axes, `axes_[i]` is the i-th item\n        in `ax`. Elements that are None correspond to a nonexisting axes in\n        that position.\n\n    lines_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `lines_[i, j]` is the partial dependence\n        curve on the i-th row and j-th column. If `ax` is a list of axes,\n        `lines_[i]` is the partial dependence curve corresponding to the i-th\n        item in `ax`. Elements that are None correspond to a nonexisting axes\n        or an axes that does not include a line plot.\n\n    deciles_vlines_ : ndarray of matplotlib LineCollection\n        If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n        representing the x axis deciles of the i-th row and j-th column. If\n        `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n        `ax`. Elements that are None correspond to a nonexisting axes or an\n        axes that does not include a PDP plot.\n\n        .. versionadded:: 0.23\n\n    deciles_hlines_ : ndarray of matplotlib LineCollection\n        If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n        representing the y axis deciles of the i-th row and j-th column. If\n        `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n        `ax`. Elements that are None correspond to a nonexisting axes or an\n        axes that does not include a 2-way plot.\n\n        .. versionadded:: 0.23\n\n    contours_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `contours_[i, j]` is the partial dependence\n        plot on the i-th row and j-th column. If `ax` is a list of axes,\n        `contours_[i]` is the partial dependence plot corresponding to the i-th\n        item in `ax`. Elements that are None correspond to a nonexisting axes\n        or an axes that does not include a contour plot.\n\n    bars_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `bars_[i, j]` is the partial dependence bar\n        plot on the i-th row and j-th column (for a categorical feature).\n        If `ax` is a list of axes, `bars_[i]` is the partial dependence bar\n        plot corresponding to the i-th item in `ax`. Elements that are None\n        correspond to a nonexisting axes or an axes that does not include a\n        bar plot.\n\n        .. versionadded:: 1.2\n\n    heatmaps_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `heatmaps_[i, j]` is the partial dependence\n        heatmap on the i-th row and j-th column (for a pair of categorical\n        features) . If `ax` is a list of axes, `heatmaps_[i]` is the partial\n        dependence heatmap corresponding to the i-th item in `ax`. Elements\n        that are None correspond to a nonexisting axes or an axes that does not\n        include a heatmap.\n\n        .. versionadded:: 1.2\n\n    figure_ : matplotlib Figure\n        Figure containing partial dependence plots.\n\n    See Also\n    --------\n    partial_dependence : Compute Partial Dependence values.\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.",
        "parameters": {
          "pd_results": {
            "type": "list of Bunch",
            "description": ""
          },
          "Results": {
            "type": "of :func:`~sklearn.inspection.partial_dependence` for",
            "description": "``features``."
          },
          "features": {
            "type": "list of (int,) or list of (int, int)",
            "description": ""
          },
          "Indices": {
            "type": "of features for a given plot. A tuple of one integer will plot",
            "description": ""
          },
          "a": {
            "type": "partial dependence curve of one feature. A tuple of two integers will",
            "description": ""
          },
          "plot": {
            "type": "corresponding to the i-th item in `ax`. Elements that are None",
            "description": ""
          },
          "feature_names": {
            "type": "list of str",
            "description": ""
          },
          "Feature": {
            "type": "names corresponding to the indices in ``features``.",
            "description": ""
          },
          "target_idx": {
            "type": "int",
            "description": "- In a multiclass setting, specifies the class for which the PDPs"
          },
          "should": {
            "type": "be computed.",
            "description": ""
          },
          "positive": {
            "type": "class (index 1) is always used.",
            "description": "- In a multioutput setting, specifies the task for which the PDPs"
          },
          "Ignored": {
            "type": "in binary classification or classical regression settings.",
            "description": ""
          },
          "deciles": {
            "type": "dict",
            "description": ""
          },
          "Deciles": {
            "type": "for feature indices in ``features``.",
            "description": ""
          },
          "kind": {
            "type": "{'average', 'individual', 'both'} or list of such str,             default='average'",
            "description": ""
          },
          "Whether": {
            "type": "each target feature in `features` is categorical or not.",
            "description": ""
          },
          "in": {
            "type": "`ax`. Elements that are None correspond to a nonexisting axes in",
            "description": ""
          },
          "A": {
            "type": "list of such strings can be provided to specify `kind` on a per-plot",
            "description": "basis. The length of the list should be the same as the number of"
          },
          "interaction": {
            "type": "requested in `features`.",
            "description": ".. note::"
          },
          "ICE": {
            "type": "'individual' or 'both'",
            "description": "is not a valid option for 2-ways"
          },
          "interactions": {
            "type": "plot. As a result, an error will be raised.",
            "description": "2-ways interaction plots should always be configured to"
          },
          "use": {
            "type": "the 'average' kind instead.",
            "description": ".. note::"
          },
          "The": {
            "type": "list should be same size as `features`. If `None`, all features",
            "description": ""
          },
          "dependencies": {
            "type": "and doing weighted averages requires using the slower",
            "description": "`method='brute'`.\n.. versionadded:: 0.24"
          },
          "Add": {
            "type": "the possibility to pass a list of string specifying `kind`",
            "description": ""
          },
          "for": {
            "type": "each plot.",
            "description": ""
          },
          "subsample": {
            "type": "float, int or None, default=1000",
            "description": ""
          },
          "Sampling": {
            "type": "for ICE curves when `kind` is 'individual' or 'both'.",
            "description": ""
          },
          "If": {
            "type": "`ax` is an axes or None, `heatmaps_[i, j]` is the partial dependence",
            "description": ""
          },
          "of": {
            "type": "the dataset to be used to plot ICE curves. If int, represents the",
            "description": ""
          },
          "maximum": {
            "type": "absolute number of samples to use.",
            "description": ""
          },
          "Note": {
            "type": "that the full dataset is still used to calculate partial",
            "description": ""
          },
          "dependence": {
            "type": "heatmap corresponding to the i-th item in `ax`. Elements",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Controls": {
            "type": "the randomness of the selected samples when subsamples is not",
            "description": "`None`. See :term:`Glossary <random_state>` for details.\n.. versionadded:: 0.24"
          },
          "is_categorical": {
            "type": "list of (bool,) or list of (bool, bool), default=None",
            "description": ""
          },
          "are": {
            "type": "assumed to be continuous.",
            "description": ".. versionadded:: 1.2\nAttributes\n----------"
          },
          "bounding_ax_": {
            "type": "matplotlib Axes or None",
            "description": ""
          },
          "grid": {
            "type": "of partial dependence plots are drawn. If `ax` is a list of axes",
            "description": ""
          },
          "or": {
            "type": "an axes that does not include a contour plot.",
            "description": ""
          },
          "axes_": {
            "type": "ndarray of matplotlib Axes",
            "description": ""
          },
          "and": {
            "type": "j-th column. If `ax` is a list of axes, `axes_[i]` is the i-th item",
            "description": ""
          },
          "that": {
            "type": "are None correspond to a nonexisting axes or an axes that does not",
            "description": ""
          },
          "lines_": {
            "type": "ndarray of matplotlib Artists",
            "description": ""
          },
          "curve": {
            "type": "on the i-th row and j-th column. If `ax` is a list of axes,",
            "description": "`lines_[i]` is the partial dependence curve corresponding to the i-th"
          },
          "item": {
            "type": "in `ax`. Elements that are None correspond to a nonexisting axes",
            "description": ""
          },
          "deciles_vlines_": {
            "type": "ndarray of matplotlib LineCollection",
            "description": ""
          },
          "representing": {
            "type": "the y axis deciles of the i-th row and j-th column. If",
            "description": "`ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n`ax`. Elements that are None correspond to a nonexisting axes or an"
          },
          "axes": {
            "type": "that does not include a 2-way plot.",
            "description": ".. versionadded:: 0.23"
          },
          "deciles_hlines_": {
            "type": "ndarray of matplotlib LineCollection",
            "description": ""
          },
          "contours_": {
            "type": "ndarray of matplotlib Artists",
            "description": ""
          },
          "bars_": {
            "type": "ndarray of matplotlib Artists",
            "description": ""
          },
          "correspond": {
            "type": "to a nonexisting axes or an axes that does not include a",
            "description": ""
          },
          "bar": {
            "type": "plot.",
            "description": ".. versionadded:: 1.2"
          },
          "heatmaps_": {
            "type": "ndarray of matplotlib Artists",
            "description": ""
          },
          "heatmap": {
            "type": "on the i-th row and j-th column (for a pair of categorical",
            "description": "features) . If `ax` is a list of axes, `heatmaps_[i]` is the partial"
          },
          "include": {
            "type": "a heatmap.",
            "description": ".. versionadded:: 1.2"
          },
          "figure_": {
            "type": "matplotlib Figure",
            "description": ""
          },
          "Figure": {
            "type": "containing partial dependence plots.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "partial_dependence": {
            "type": "Compute Partial Dependence values.",
            "description": "PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.inspection import PartialDependenceDisplay\n>>> from sklearn.inspection import partial_dependence\n>>> X, y = make_friedman1()\n>>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n>>> features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n>>> deciles = {0: np.linspace(0, 1, num=5)}\n>>> pd_results = partial_dependence(\n...     clf, X, features=0, kind=\"average\", grid_resolution=5)\n>>> display = PartialDependenceDisplay(\n...     [pd_results], features=features, feature_names=feature_names,\n...     target_idx=0, deciles=deciles\n... )\n>>> display.plot(pdp_lim={1: (-1.38, 0.66)})\n<...>\n>>> plt.show()"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    partial_dependence : Compute Partial Dependence values.\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> from sklearn.inspection import PartialDependenceDisplay\n    >>> from sklearn.inspection import partial_dependence\n    >>> X, y = make_friedman1()\n    >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    >>> features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n    >>> deciles = {0: np.linspace(0, 1, num=5)}\n    >>> pd_results = partial_dependence(\n    ...     clf, X, features=0, kind=\"average\", grid_resolution=5)\n    >>> display = PartialDependenceDisplay(\n    ...     [pd_results], features=features, feature_names=feature_names,\n    ...     target_idx=0, deciles=deciles\n    ... )\n    >>> display.plot(pdp_lim={1: (-1.38, 0.66)})\n    <...>\n    >>> plt.show()",
        "notes": "that the full dataset is still used to calculate partial\n        dependence when `kind='both'`.\n\n        .. versionadded:: 0.24\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness of the selected samples when subsamples is not\n        `None`. See :term:`Glossary <random_state>` for details.\n\n        .. versionadded:: 0.24\n\n    is_categorical : list of (bool,) or list of (bool, bool), default=None\n        Whether each target feature in `features` is categorical or not.\n        The list should be same size as `features`. If `None`, all features\n        are assumed to be continuous.\n\n        .. versionadded:: 1.2\n\n    Attributes\n    ----------\n    bounding_ax_ : matplotlib Axes or None\n        If `ax` is an axes or None, the `bounding_ax_` is the axes where the\n        grid of partial dependence plots are drawn. If `ax` is a list of axes\n        or a numpy array of axes, `bounding_ax_` is None.\n\n    axes_ : ndarray of matplotlib Axes\n        If `ax` is an axes or None, `axes_[i, j]` is the axes on the i-th row\n        and j-th column. If `ax` is a list of axes, `axes_[i]` is the i-th item\n        in `ax`. Elements that are None correspond to a nonexisting axes in\n        that position.\n\n    lines_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `lines_[i, j]` is the partial dependence\n        curve on the i-th row and j-th column. If `ax` is a list of axes,\n        `lines_[i]` is the partial dependence curve corresponding to the i-th\n        item in `ax`. Elements that are None correspond to a nonexisting axes\n        or an axes that does not include a line plot.\n\n    deciles_vlines_ : ndarray of matplotlib LineCollection\n        If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n        representing the x axis deciles of the i-th row and j-th column. If\n        `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n        `ax`. Elements that are None correspond to a nonexisting axes or an\n        axes that does not include a PDP plot.\n\n        .. versionadded:: 0.23\n\n    deciles_hlines_ : ndarray of matplotlib LineCollection\n        If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n        representing the y axis deciles of the i-th row and j-th column. If\n        `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n        `ax`. Elements that are None correspond to a nonexisting axes or an\n        axes that does not include a 2-way plot.\n\n        .. versionadded:: 0.23\n\n    contours_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `contours_[i, j]` is the partial dependence\n        plot on the i-th row and j-th column. If `ax` is a list of axes,\n        `contours_[i]` is the partial dependence plot corresponding to the i-th\n        item in `ax`. Elements that are None correspond to a nonexisting axes\n        or an axes that does not include a contour plot.\n\n    bars_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `bars_[i, j]` is the partial dependence bar\n        plot on the i-th row and j-th column (for a categorical feature).\n        If `ax` is a list of axes, `bars_[i]` is the partial dependence bar\n        plot corresponding to the i-th item in `ax`. Elements that are None\n        correspond to a nonexisting axes or an axes that does not include a\n        bar plot.\n\n        .. versionadded:: 1.2\n\n    heatmaps_ : ndarray of matplotlib Artists\n        If `ax` is an axes or None, `heatmaps_[i, j]` is the partial dependence\n        heatmap on the i-th row and j-th column (for a pair of categorical\n        features) . If `ax` is a list of axes, `heatmaps_[i]` is the partial\n        dependence heatmap corresponding to the i-th item in `ax`. Elements\n        that are None correspond to a nonexisting axes or an axes that does not\n        include a heatmap.\n\n        .. versionadded:: 1.2\n\n    figure_ : matplotlib Figure\n        Figure containing partial dependence plots.\n\n    See Also\n    --------\n    partial_dependence : Compute Partial Dependence values.\n    PartialDependenceDisplay.from_estimator : Plot Partial Dependence.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> from sklearn.inspection import PartialDependenceDisplay\n    >>> from sklearn.inspection import partial_dependence\n    >>> X, y = make_friedman1()\n    >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    >>> features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n    >>> deciles = {0: np.linspace(0, 1, num=5)}\n    >>> pd_results = partial_dependence(\n    ...     clf, X, features=0, kind=\"average\", grid_resolution=5)\n    >>> display = PartialDependenceDisplay(\n    ...     [pd_results], features=features, feature_names=feature_names,\n    ...     target_idx=0, deciles=deciles\n    ... )\n    >>> display.plot(pdp_lim={1: (-1.38, 0.66)})\n    <...>\n    >>> plt.show()",
        "examples": "--------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> from sklearn.inspection import PartialDependenceDisplay\n    >>> from sklearn.inspection import partial_dependence\n    >>> X, y = make_friedman1()\n    >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    >>> features, feature_names = [(0,)], [f\"Features #{i}\" for i in range(X.shape[1])]\n    >>> deciles = {0: np.linspace(0, 1, num=5)}\n    >>> pd_results = partial_dependence(\n    ...     clf, X, features=0, kind=\"average\", grid_resolution=5)\n    >>> display = PartialDependenceDisplay(\n    ...     [pd_results], features=features, feature_names=feature_names,\n    ...     target_idx=0, deciles=deciles\n    ... )\n    >>> display.plot(pdp_lim={1: (-1.38, 0.66)})\n    <...>\n    >>> plt.show()"
      },
      "methods": [
        {
          "name": "from_estimator",
          "signature": "from_estimator(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, target=None, response_method='auto', n_cols=3, grid_resolution=100, percentiles=(0.05, 0.95), method='auto', n_jobs=None, verbose=0, line_kw=None, ice_lines_kw=None, pd_line_kw=None, contour_kw=None, ax=None, kind='average', centered=False, subsample=1000, random_state=None)",
          "documentation": {
            "description": "Partial dependence (PD) and individual conditional expectation (ICE) plots.\n\n        Partial dependence plots, individual conditional expectation plots or an\n        overlay of both of them can be plotted by setting the ``kind``\n        parameter. The ``len(features)`` plots are arranged in a grid with\n        ``n_cols`` columns. Two-way partial dependence plots are plotted as\n        contour plots. The deciles of the feature values will be shown with tick\n        marks on the x-axes for one-way plots, and on both axes for two-way\n        plots.\n\n        Read more in the :ref:`User Guide <partial_dependence>`.\n\n        .. note::\n\n            :func:`PartialDependenceDisplay.from_estimator` does not support using the\n            same axes with multiple calls. To plot the partial dependence for\n            multiple estimators, please pass the axes created by the first call to the\n            second call::\n\n               >>> from sklearn.inspection import PartialDependenceDisplay\n               >>> from sklearn.datasets import make_friedman1\n               >>> from sklearn.linear_model import LinearRegression\n               >>> from sklearn.ensemble import RandomForestRegressor\n               >>> X, y = make_friedman1()\n               >>> est1 = LinearRegression().fit(X, y)\n               >>> est2 = RandomForestRegressor().fit(X, y)\n               >>> disp1 = PartialDependenceDisplay.from_estimator(est1, X,\n               ...                                                 [1, 2])\n               >>> disp2 = PartialDependenceDisplay.from_estimator(est2, X, [1, 2],\n               ...                                                 ax=disp1.axes_)\n\n        .. warning::\n\n            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the\n            `'recursion'` method (used by default) will not account for the `init`\n            predictor of the boosting process. In practice, this will produce\n            the same values as `'brute'` up to a constant offset in the target\n            response, provided that `init` is a constant estimator (which is the\n            default). However, if `init` is not a constant estimator, the\n            partial dependence values are incorrect for `'recursion'` because the\n            offset will be sample-dependent. It is preferable to use the `'brute'`\n            method. Note that this only applies to\n            :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to\n            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        estimator : BaseEstimator\n            A fitted estimator object implementing :term:`predict`,\n            :term:`predict_proba`, or :term:`decision_function`.\n            Multioutput-multiclass classifiers are not supported.\n\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            ``X`` is used to generate a grid of values for the target\n            ``features`` (where the partial dependence will be evaluated), and\n            also to generate values for the complement features when the\n            `method` is `'brute'`.\n\n        features : list of {int, str, pair of int, pair of str}\n            The target features for which to create the PDPs.\n            If `features[i]` is an integer or a string, a one-way PDP is created;\n            if `features[i]` is a tuple, a two-way PDP is created (only supported\n            with `kind='average'`). Each tuple must be of size 2.\n            If any entry is a string, then it must be in ``feature_names``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights are used to calculate weighted means when averaging the\n            model output. If `None`, then samples are equally weighted. If\n            `sample_weight` is not `None`, then `method` will be set to `'brute'`.\n            Note that `sample_weight` is ignored for `kind='individual'`.\n\n            .. versionadded:: 1.3\n\n        categorical_features : array-like of shape (n_features,) or shape                 (n_categorical_features,), dtype={bool, int, str}, default=None\n            Indicates the categorical features.\n\n            - `None`: no feature will be considered categorical;\n            - boolean array-like: boolean mask of shape `(n_features,)`\n              indicating which features are categorical. Thus, this array has\n              the same shape has `X.shape[1]`;\n            - integer or string array-like: integer indices or strings\n              indicating categorical features.\n\n            .. versionadded:: 1.2\n\n        feature_names : array-like of shape (n_features,), dtype=str, default=None\n            Name of each feature; `feature_names[i]` holds the name of the feature\n            with index `i`.\n            By default, the name of the feature corresponds to their numerical\n            index for NumPy array and their column name for pandas dataframe.\n\n        target : int, default=None\n            - In a multiclass setting, specifies the class for which the PDPs\n              should be computed. Note that for binary classification, the\n              positive class (index 1) is always used.\n            - In a multioutput setting, specifies the task for which the PDPs\n              should be computed.\n\n            Ignored in binary classification or classical regression settings.\n\n        response_method : {'auto', 'predict_proba', 'decision_function'},                 default='auto'\n            Specifies whether to use :term:`predict_proba` or\n            :term:`decision_function` as the target response. For regressors\n            this parameter is ignored and the response is always the output of\n            :term:`predict`. By default, :term:`predict_proba` is tried first\n            and we revert to :term:`decision_function` if it doesn't exist. If\n            ``method`` is `'recursion'`, the response is always the output of\n            :term:`decision_function`.\n\n        n_cols : int, default=3\n            The maximum number of columns in the grid plot. Only active when `ax`\n            is a single axis or `None`.\n\n        grid_resolution : int, default=100\n            The number of equally spaced points on the axes of the plots, for each\n            target feature.\n\n        percentiles : tuple of float, default=(0.05, 0.95)\n            The lower and upper percentile used to create the extreme values\n            for the PDP axes. Must be in [0, 1].\n\n        method : str, default='auto'\n            The method used to calculate the averaged predictions:\n\n            - `'recursion'` is only supported for some tree-based estimators\n              (namely\n              :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n              :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n              :class:`~sklearn.tree.DecisionTreeRegressor`,\n              :class:`~sklearn.ensemble.RandomForestRegressor`\n              but is more efficient in terms of speed.\n              With this method, the target response of a\n              classifier is always the decision function, not the predicted\n              probabilities. Since the `'recursion'` method implicitly computes\n              the average of the ICEs by design, it is not compatible with ICE and\n              thus `kind` must be `'average'`.\n\n            - `'brute'` is supported for any estimator, but is more\n              computationally intensive.\n\n            - `'auto'`: the `'recursion'` is used for estimators that support it,\n              and `'brute'` is used otherwise. If `sample_weight` is not `None`,\n              then `'brute'` is used regardless of the estimator.\n\n            Please see :ref:`this note <pdp_method_differences>` for\n            differences between the `'brute'` and `'recursion'` method.\n\n        n_jobs : int, default=None\n            The number of CPUs to use to compute the partial dependences.\n            Computation is parallelized over features specified by the `features`\n            parameter.\n\n            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n            for more details.\n\n        verbose : int, default=0\n            Verbose output during PD computations.\n\n        line_kw : dict, default=None\n            Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n            For one-way partial dependence plots. It can be used to define common\n            properties for both `ice_lines_kw` and `pdp_line_kw`.\n\n        ice_lines_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For ICE lines in the one-way partial dependence plots.\n            The key value pairs defined in `ice_lines_kw` takes priority over\n            `line_kw`.\n\n        pd_line_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For partial dependence in one-way partial dependence plots.\n            The key value pairs defined in `pd_line_kw` takes priority over\n            `line_kw`.\n\n        contour_kw : dict, default=None\n            Dict with keywords passed to the ``matplotlib.pyplot.contourf`` call.\n            For two-way partial dependence plots.\n\n        ax : Matplotlib axes or array-like of Matplotlib axes, default=None\n            - If a single axis is passed in, it is treated as a bounding axes\n              and a grid of partial dependence plots will be drawn within\n              these bounds. The `n_cols` parameter controls the number of\n              columns in the grid.\n            - If an array-like of axes are passed in, the partial dependence\n              plots will be drawn directly into these axes.\n            - If `None`, a figure and a bounding axes is created and treated\n              as the single axes case.\n\n        kind : {'average', 'individual', 'both'}, default='average'\n            Whether to plot the partial dependence averaged across all the samples\n            in the dataset or one line per sample or both.\n\n            - ``kind='average'`` results in the traditional PD plot;\n            - ``kind='individual'`` results in the ICE plot.\n\n            Note that the fast `method='recursion'` option is only available for\n            `kind='average'` and `sample_weights=None`. Computing individual\n            dependencies and doing weighted averages requires using the slower\n            `method='brute'`.\n\n        centered : bool, default=False\n            If `True`, the ICE and PD lines will start at the origin of the\n            y-axis. By default, no centering is done.\n\n            .. versionadded:: 1.1\n\n        subsample : float, int or None, default=1000\n            Sampling for ICE curves when `kind` is 'individual' or 'both'.\n            If `float`, should be between 0.0 and 1.0 and represent the proportion\n            of the dataset to be used to plot ICE curves. If `int`, represents the\n            absolute number samples to use.\n\n            Note that the full dataset is still used to calculate averaged partial\n            dependence when `kind='both'`.\n\n        random_state : int, RandomState instance or None, default=None\n            Controls the randomness of the selected samples when subsamples is not\n            `None` and `kind` is either `'both'` or `'individual'`.\n            See :term:`Glossary <random_state>` for details.\n\n        Returns\n        -------\n        display : :class:`~sklearn.inspection.PartialDependenceDisplay`\n\n        See Also\n        --------\n        partial_dependence : Compute Partial Dependence values.",
            "parameters": {
              "estimator": {
                "type": "BaseEstimator",
                "description": ""
              },
              "A": {
                "type": "fitted estimator object implementing :term:`predict`,",
                "description": ":term:`predict_proba`, or :term:`decision_function`.\nMultioutput-multiclass classifiers are not supported."
              },
              "X": {
                "type": "{array",
                "description": "like, dataframe} of shape (n_samples, n_features)\n``X`` is used to generate a grid of values for the target\n``features`` (where the partial dependence will be evaluated), and"
              },
              "also": {
                "type": "to generate values for the complement features when the",
                "description": "`method` is `'brute'`."
              },
              "features": {
                "type": "list of {int, str, pair of int, pair of str}",
                "description": ""
              },
              "The": {
                "type": "key value pairs defined in `pd_line_kw` takes priority over",
                "description": "`line_kw`."
              },
              "If": {
                "type": "`float`, should be between 0.0 and 1.0 and represent the proportion",
                "description": ""
              },
              "if": {
                "type": "`features[i]` is a tuple, a two-way PDP is created (only supported",
                "description": ""
              },
              "with": {
                "type": "index `i`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights are used to calculate weighted means when averaging the",
                "description": ""
              },
              "model": {
                "type": "output. If `None`, then samples are equally weighted. If",
                "description": "`sample_weight` is not `None`, then `method` will be set to `'brute'`."
              },
              "Note": {
                "type": "that the full dataset is still used to calculate averaged partial",
                "description": ""
              },
              "categorical_features": {
                "type": "array",
                "description": "like of shape (n_features,) or shape                 (n_categorical_features,), dtype={bool, int, str}, default=None"
              },
              "Indicates": {
                "type": "the categorical features.",
                "description": "- `None`: no feature will be considered categorical;\n- boolean array-like: boolean mask of shape `(n_features,)`"
              },
              "indicating": {
                "type": "categorical features.",
                "description": ".. versionadded:: 1.2"
              },
              "the": {
                "type": "average of the ICEs by design, it is not compatible with ICE and",
                "description": ""
              },
              "feature_names": {
                "type": "array",
                "description": "like of shape (n_features,), dtype=str, default=None"
              },
              "Name": {
                "type": "of each feature; `feature_names[i]` holds the name of the feature",
                "description": ""
              },
              "By": {
                "type": "default, the name of the feature corresponds to their numerical",
                "description": ""
              },
              "index": {
                "type": "for NumPy array and their column name for pandas dataframe.",
                "description": ""
              },
              "target": {
                "type": "feature.",
                "description": ""
              },
              "should": {
                "type": "be computed.",
                "description": ""
              },
              "positive": {
                "type": "class (index 1) is always used.",
                "description": "- In a multioutput setting, specifies the task for which the PDPs"
              },
              "Ignored": {
                "type": "in binary classification or classical regression settings.",
                "description": ""
              },
              "response_method": {
                "type": "{'auto', 'predict_proba', 'decision_function'},                 default='auto'",
                "description": ""
              },
              "Specifies": {
                "type": "whether to use :term:`predict_proba` or",
                "description": ":term:`decision_function` as the target response. For regressors"
              },
              "this": {
                "type": "parameter is ignored and the response is always the output of",
                "description": ":term:`predict`. By default, :term:`predict_proba` is tried first"
              },
              "and": {
                "type": "a grid of partial dependence plots will be drawn within",
                "description": ""
              },
              "n_cols": {
                "type": "int, default=3",
                "description": ""
              },
              "is": {
                "type": "a single axis or `None`.",
                "description": ""
              },
              "grid_resolution": {
                "type": "int, default=100",
                "description": ""
              },
              "percentiles": {
                "type": "tuple of float, default=(0.05, 0.95)",
                "description": ""
              },
              "for": {
                "type": "more details.",
                "description": ""
              },
              "method": {
                "type": "str, default='auto'",
                "description": ""
              },
              "but": {
                "type": "is more efficient in terms of speed.",
                "description": ""
              },
              "With": {
                "type": "this method, the target response of a",
                "description": ""
              },
              "classifier": {
                "type": "is always the decision function, not the predicted",
                "description": "probabilities. Since the `'recursion'` method implicitly computes"
              },
              "thus": {
                "type": "`kind` must be `'average'`.",
                "description": "- `'brute'` is supported for any estimator, but is more"
              },
              "computationally": {
                "type": "intensive.",
                "description": "- `'auto'`: the `'recursion'` is used for estimators that support it,"
              },
              "then": {
                "type": "`'brute'` is used regardless of the estimator.",
                "description": ""
              },
              "Please": {
                "type": "see :ref:`this note <pdp_method_differences>` for",
                "description": ""
              },
              "differences": {
                "type": "between the `'brute'` and `'recursion'` method.",
                "description": ""
              },
              "n_jobs": {
                "type": "int, default=None",
                "description": ""
              },
              "Computation": {
                "type": "is parallelized over features specified by the `features`",
                "description": "parameter.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
              },
              "verbose": {
                "type": "int, default=0",
                "description": ""
              },
              "Verbose": {
                "type": "output during PD computations.",
                "description": ""
              },
              "line_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "Dict": {
                "type": "with keywords passed to the ``matplotlib.pyplot.contourf`` call.",
                "description": ""
              },
              "For": {
                "type": "two-way partial dependence plots.",
                "description": ""
              },
              "properties": {
                "type": "for both `ice_lines_kw` and `pdp_line_kw`.",
                "description": ""
              },
              "ice_lines_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "Dictionary": {
                "type": "with keywords passed to the `matplotlib.pyplot.plot` call.",
                "description": ""
              },
              "pd_line_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "contour_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "ax": {
                "type": "Matplotlib axes or array",
                "description": "like of Matplotlib axes, default=None\n- If a single axis is passed in, it is treated as a bounding axes"
              },
              "these": {
                "type": "bounds. The `n_cols` parameter controls the number of",
                "description": ""
              },
              "columns": {
                "type": "in the grid.",
                "description": "- If an array-like of axes are passed in, the partial dependence"
              },
              "plots": {
                "type": "will be drawn directly into these axes.",
                "description": "- If `None`, a figure and a bounding axes is created and treated"
              },
              "as": {
                "type": "the single axes case.",
                "description": ""
              },
              "kind": {
                "type": "{'average', 'individual', 'both'}, default='average'",
                "description": ""
              },
              "Whether": {
                "type": "to plot the partial dependence averaged across all the samples",
                "description": ""
              },
              "in": {
                "type": "the dataset or one line per sample or both.",
                "description": "- ``kind='average'`` results in the traditional PD plot;\n- ``kind='individual'`` results in the ICE plot."
              },
              "dependencies": {
                "type": "and doing weighted averages requires using the slower",
                "description": "`method='brute'`."
              },
              "centered": {
                "type": "bool, default=False",
                "description": ""
              },
              "subsample": {
                "type": "float, int or None, default=1000",
                "description": ""
              },
              "Sampling": {
                "type": "for ICE curves when `kind` is 'individual' or 'both'.",
                "description": ""
              },
              "of": {
                "type": "the dataset to be used to plot ICE curves. If `int`, represents the",
                "description": ""
              },
              "absolute": {
                "type": "number samples to use.",
                "description": ""
              },
              "dependence": {
                "type": "when `kind='both'`.",
                "description": ""
              },
              "random_state": {
                "type": "int, RandomState instance or None, default=None",
                "description": ""
              },
              "Controls": {
                "type": "the randomness of the selected samples when subsamples is not",
                "description": "`None` and `kind` is either `'both'` or `'individual'`."
              },
              "See": {
                "type": "Also",
                "description": "--------"
              },
              "display": {
                "type": ":class:`~sklearn.inspection.PartialDependenceDisplay`",
                "description": ""
              },
              "partial_dependence": {
                "type": "Compute Partial Dependence values.",
                "description": "Examples\n--------\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.inspection import PartialDependenceDisplay\n>>> X, y = make_friedman1()\n>>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n>>> PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])\n<...>\n>>> plt.show()"
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.inspection.PartialDependenceDisplay`\n\n        See Also\n        --------\n        partial_dependence : Compute Partial Dependence values.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_friedman1\n        >>> from sklearn.ensemble import GradientBoostingRegressor\n        >>> from sklearn.inspection import PartialDependenceDisplay\n        >>> X, y = make_friedman1()\n        >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n        >>> PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])\n        <...>\n        >>> plt.show()",
            "raises": "",
            "see_also": "--------\n        partial_dependence : Compute Partial Dependence values.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_friedman1\n        >>> from sklearn.ensemble import GradientBoostingRegressor\n        >>> from sklearn.inspection import PartialDependenceDisplay\n        >>> X, y = make_friedman1()\n        >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n        >>> PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])\n        <...>\n        >>> plt.show()",
            "notes": "that `sample_weight` is ignored for `kind='individual'`.\n\n            .. versionadded:: 1.3\n\n        categorical_features : array-like of shape (n_features,) or shape                 (n_categorical_features,), dtype={bool, int, str}, default=None\n            Indicates the categorical features.\n\n            - `None`: no feature will be considered categorical;\n            - boolean array-like: boolean mask of shape `(n_features,)`\n              indicating which features are categorical. Thus, this array has\n              the same shape has `X.shape[1]`;\n            - integer or string array-like: integer indices or strings\n              indicating categorical features.\n\n            .. versionadded:: 1.2\n\n        feature_names : array-like of shape (n_features,), dtype=str, default=None\n            Name of each feature; `feature_names[i]` holds the name of the feature\n            with index `i`.\n            By default, the name of the feature corresponds to their numerical\n            index for NumPy array and their column name for pandas dataframe.\n\n        target : int, default=None\n            - In a multiclass setting, specifies the class for which the PDPs\n              should be computed. Note that for binary classification, the\n              positive class (index 1) is always used.\n            - In a multioutput setting, specifies the task for which the PDPs\n              should be computed.\n\n            Ignored in binary classification or classical regression settings.\n\n        response_method : {'auto', 'predict_proba', 'decision_function'},                 default='auto'\n            Specifies whether to use :term:`predict_proba` or\n            :term:`decision_function` as the target response. For regressors\n            this parameter is ignored and the response is always the output of\n            :term:`predict`. By default, :term:`predict_proba` is tried first\n            and we revert to :term:`decision_function` if it doesn't exist. If\n            ``method`` is `'recursion'`, the response is always the output of\n            :term:`decision_function`.\n\n        n_cols : int, default=3\n            The maximum number of columns in the grid plot. Only active when `ax`\n            is a single axis or `None`.\n\n        grid_resolution : int, default=100\n            The number of equally spaced points on the axes of the plots, for each\n            target feature.\n\n        percentiles : tuple of float, default=(0.05, 0.95)\n            The lower and upper percentile used to create the extreme values\n            for the PDP axes. Must be in [0, 1].\n\n        method : str, default='auto'\n            The method used to calculate the averaged predictions:\n\n            - `'recursion'` is only supported for some tree-based estimators\n              (namely\n              :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n              :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n              :class:`~sklearn.tree.DecisionTreeRegressor`,\n              :class:`~sklearn.ensemble.RandomForestRegressor`\n              but is more efficient in terms of speed.\n              With this method, the target response of a\n              classifier is always the decision function, not the predicted\n              probabilities. Since the `'recursion'` method implicitly computes\n              the average of the ICEs by design, it is not compatible with ICE and\n              thus `kind` must be `'average'`.\n\n            - `'brute'` is supported for any estimator, but is more\n              computationally intensive.\n\n            - `'auto'`: the `'recursion'` is used for estimators that support it,\n              and `'brute'` is used otherwise. If `sample_weight` is not `None`,\n              then `'brute'` is used regardless of the estimator.\n\n            Please see :ref:`this note <pdp_method_differences>` for\n            differences between the `'brute'` and `'recursion'` method.\n\n        n_jobs : int, default=None\n            The number of CPUs to use to compute the partial dependences.\n            Computation is parallelized over features specified by the `features`\n            parameter.\n\n            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n            for more details.\n\n        verbose : int, default=0\n            Verbose output during PD computations.\n\n        line_kw : dict, default=None\n            Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n            For one-way partial dependence plots. It can be used to define common\n            properties for both `ice_lines_kw` and `pdp_line_kw`.\n\n        ice_lines_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For ICE lines in the one-way partial dependence plots.\n            The key value pairs defined in `ice_lines_kw` takes priority over\n            `line_kw`.\n\n        pd_line_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For partial dependence in one-way partial dependence plots.\n            The key value pairs defined in `pd_line_kw` takes priority over\n            `line_kw`.\n\n        contour_kw : dict, default=None\n            Dict with keywords passed to the ``matplotlib.pyplot.contourf`` call.\n            For two-way partial dependence plots.\n\n        ax : Matplotlib axes or array-like of Matplotlib axes, default=None\n            - If a single axis is passed in, it is treated as a bounding axes\n              and a grid of partial dependence plots will be drawn within\n              these bounds. The `n_cols` parameter controls the number of\n              columns in the grid.\n            - If an array-like of axes are passed in, the partial dependence\n              plots will be drawn directly into these axes.\n            - If `None`, a figure and a bounding axes is created and treated\n              as the single axes case.\n\n        kind : {'average', 'individual', 'both'}, default='average'\n            Whether to plot the partial dependence averaged across all the samples\n            in the dataset or one line per sample or both.\n\n            - ``kind='average'`` results in the traditional PD plot;\n            - ``kind='individual'`` results in the ICE plot.",
            "examples": "--------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_friedman1\n        >>> from sklearn.ensemble import GradientBoostingRegressor\n        >>> from sklearn.inspection import PartialDependenceDisplay\n        >>> X, y = make_friedman1()\n        >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n        >>> PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])\n        <...>\n        >>> plt.show()"
          }
        },
        {
          "name": "plot",
          "signature": "plot(self, *, ax=None, n_cols=3, line_kw=None, ice_lines_kw=None, pd_line_kw=None, contour_kw=None, bar_kw=None, heatmap_kw=None, pdp_lim=None, centered=False)",
          "documentation": {
            "description": "Plot partial dependence plots.\n\n        Parameters\n        ----------\n        ax : Matplotlib axes or array-like of Matplotlib axes, default=None\n            - If a single axis is passed in, it is treated as a bounding axes\n                and a grid of partial dependence plots will be drawn within\n                these bounds. The `n_cols` parameter controls the number of\n                columns in the grid.\n            - If an array-like of axes are passed in, the partial dependence\n                plots will be drawn directly into these axes.\n            - If `None`, a figure and a bounding axes is created and treated\n                as the single axes case.\n\n        n_cols : int, default=3\n            The maximum number of columns in the grid plot. Only active when\n            `ax` is a single axes or `None`.\n\n        line_kw : dict, default=None\n            Dict with keywords passed to the `matplotlib.pyplot.plot` call.\n            For one-way partial dependence plots.\n\n        ice_lines_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For ICE lines in the one-way partial dependence plots.\n            The key value pairs defined in `ice_lines_kw` takes priority over\n            `line_kw`.\n\n            .. versionadded:: 1.0\n\n        pd_line_kw : dict, default=None\n            Dictionary with keywords passed to the `matplotlib.pyplot.plot` call.\n            For partial dependence in one-way partial dependence plots.\n            The key value pairs defined in `pd_line_kw` takes priority over\n            `line_kw`.\n\n            .. versionadded:: 1.0\n\n        contour_kw : dict, default=None\n            Dict with keywords passed to the `matplotlib.pyplot.contourf`\n            call for two-way partial dependence plots.\n\n        bar_kw : dict, default=None\n            Dict with keywords passed to the `matplotlib.pyplot.bar`\n            call for one-way categorical partial dependence plots.\n\n            .. versionadded:: 1.2\n\n        heatmap_kw : dict, default=None\n            Dict with keywords passed to the `matplotlib.pyplot.imshow`\n            call for two-way categorical partial dependence plots.\n\n            .. versionadded:: 1.2\n\n        pdp_lim : dict, default=None\n            Global min and max average predictions, such that all plots will have the\n            same scale and y limits. `pdp_lim[1]` is the global min and max for single\n            partial dependence curves. `pdp_lim[2]` is the global min and max for\n            two-way partial dependence curves. If `None` (default), the limit will be\n            inferred from the global minimum and maximum of all predictions.\n\n            .. versionadded:: 1.1\n\n        centered : bool, default=False\n            If `True`, the ICE and PD lines will start at the origin of the\n            y-axis. By default, no centering is done.\n\n            .. versionadded:: 1.1",
            "parameters": {
              "ax": {
                "type": "Matplotlib axes or array",
                "description": "like of Matplotlib axes, default=None\n- If a single axis is passed in, it is treated as a bounding axes"
              },
              "and": {
                "type": "a grid of partial dependence plots will be drawn within",
                "description": ""
              },
              "these": {
                "type": "bounds. The `n_cols` parameter controls the number of",
                "description": ""
              },
              "columns": {
                "type": "in the grid.",
                "description": "- If an array-like of axes are passed in, the partial dependence"
              },
              "plots": {
                "type": "will be drawn directly into these axes.",
                "description": "- If `None`, a figure and a bounding axes is created and treated"
              },
              "as": {
                "type": "the single axes case.",
                "description": ""
              },
              "n_cols": {
                "type": "int, default=3",
                "description": ""
              },
              "The": {
                "type": "key value pairs defined in `pd_line_kw` takes priority over",
                "description": "`line_kw`.\n.. versionadded:: 1.0"
              },
              "line_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "Dict": {
                "type": "with keywords passed to the `matplotlib.pyplot.imshow`",
                "description": ""
              },
              "For": {
                "type": "partial dependence in one-way partial dependence plots.",
                "description": ""
              },
              "ice_lines_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "Dictionary": {
                "type": "with keywords passed to the `matplotlib.pyplot.plot` call.",
                "description": ""
              },
              "pd_line_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "contour_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "call": {
                "type": "for two-way categorical partial dependence plots.",
                "description": ".. versionadded:: 1.2"
              },
              "bar_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "heatmap_kw": {
                "type": "dict, default=None",
                "description": ""
              },
              "pdp_lim": {
                "type": "dict, default=None",
                "description": ""
              },
              "Global": {
                "type": "min and max average predictions, such that all plots will have the",
                "description": ""
              },
              "same": {
                "type": "scale and y limits. `pdp_lim[1]` is the global min and max for single",
                "description": ""
              },
              "partial": {
                "type": "dependence curves. `pdp_lim[2]` is the global min and max for",
                "description": "two-way partial dependence curves. If `None` (default), the limit will be"
              },
              "inferred": {
                "type": "from the global minimum and maximum of all predictions.",
                "description": ".. versionadded:: 1.1"
              },
              "centered": {
                "type": "bool, default=False",
                "description": ""
              },
              "If": {
                "type": "`True`, the ICE and PD lines will start at the origin of the",
                "description": "y-axis. By default, no centering is done.\n.. versionadded:: 1.1\nReturns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.inspection.PartialDependenceDisplay`",
                "description": ""
              },
              "Returns": {
                "type": "a :class:`~sklearn.inspection.PartialDependenceDisplay`",
                "description": ""
              },
              "object": {
                "type": "that contains the partial dependence plots.",
                "description": ""
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.inspection.PartialDependenceDisplay`",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}