{
  "description": "Methods and algorithms to robustly estimate covariance.\n\nThey estimate the covariance of features at given sets of points, as well as the\nprecision matrix defined as the inverse of the covariance. Covariance estimation is\nclosely related to the theory of Gaussian graphical models.",
  "functions": [
    {
      "name": "empirical_covariance",
      "signature": "empirical_covariance(X, *, assume_centered=False)",
      "documentation": {
        "description": "Compute the Maximum likelihood covariance estimator.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n        If `True`, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If `False`, data will be centered before computation.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).",
        "parameters": {
          "X": {
            "type": "ndarray of shape (n_samples, n_features)",
            "description": ""
          },
          "Data": {
            "type": "from which to compute the covariance estimate.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "`False`, data will be centered before computation.",
            "description": "Returns\n-------"
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "covariance": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Empirical": {
            "type": "covariance (Maximum Likelihood Estimator).",
            "description": "Examples\n--------\n>>> from sklearn.covariance import empirical_covariance\n>>> X = [[1,1,1],[1,1,1],[1,1,1],\n...      [0,0,0],[0,0,0],[0,0,0]]\n>>> empirical_covariance(X)\narray([[0.25, 0.25, 0.25],\n[0.25, 0.25, 0.25],\n[0.25, 0.25, 0.25]])"
          }
        },
        "returns": "-------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).\n\n    Examples\n    --------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])"
      }
    },
    {
      "name": "fast_mcd",
      "signature": "fast_mcd(X, support_fraction=None, cov_computation_method=<function empirical_covariance at 0x76cab3770b80>, random_state=None)",
      "documentation": {
        "description": "Estimate the Minimum Covariance Determinant matrix.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data matrix, with p features and n samples.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is `None`, which implies that the minimum\n        value of `support_fraction` will be used within the algorithm:\n        `(n_samples + n_features + 1) / 2 * n_samples`. This parameter must be\n        in the range (0, 1).\n\n    cov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n        The function which will be used to compute the covariance.\n        Must return an array of shape (n_features, n_features).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    location : ndarray of shape (n_features,)\n        Robust location of the data.\n\n    covariance : ndarray of shape (n_features, n_features)\n        Robust covariance of the features.\n\n    support : ndarray of shape (n_samples,), dtype=bool\n        A mask of the observations that have been used to compute\n        the robust location and covariance estimates of the data set.\n\n    Notes\n    -----\n    The FastMCD algorithm has been introduced by Rousseuw and Van Driessen\n    in \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n    1999, American Statistical Association and the American Society\n    for Quality, TECHNOMETRICS\".\n    The principle is to compute robust estimates and random subsets before\n    pooling them into a larger subsets, and finally into the full data set.\n    Depending on the size of the initial sample, we have one, two or three\n    such computation levels.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "The": {
            "type": "Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
            "description": ""
          },
          "support_fraction": {
            "type": "float, default=None",
            "description": ""
          },
          "MCD": {
            "type": "estimate. Default is `None`, which implies that the minimum",
            "description": ""
          },
          "value": {
            "type": "of `support_fraction` will be used within the algorithm:",
            "description": "`(n_samples + n_features + 1) / 2 * n_samples`. This parameter must be"
          },
          "in": {
            "type": "\"A Fast Algorithm for the Minimum Covariance Determinant Estimator,",
            "description": "1999, American Statistical Association and the American Society"
          },
          "cov_computation_method": {
            "type": "callable,             default=:func:`sklearn.covariance.empirical_covariance`",
            "description": ""
          },
          "Must": {
            "type": "return an array of shape (n_features, n_features).",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "the pseudo random number generator for shuffling the data.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "term:`Glossary <random_state>`.",
            "description": "Returns\n-------"
          },
          "location": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Robust": {
            "type": "covariance of the features.",
            "description": ""
          },
          "covariance": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "support": {
            "type": "ndarray of shape (n_samples,), dtype=bool",
            "description": ""
          },
          "A": {
            "type": "mask of the observations that have been used to compute",
            "description": ""
          },
          "the": {
            "type": "correction and reweighting steps described in [RouseeuwVan]_,",
            "description": ""
          },
          "for": {
            "type": "Quality, TECHNOMETRICS\".",
            "description": ""
          },
          "pooling": {
            "type": "them into a larger subsets, and finally into the full data set.",
            "description": ""
          },
          "Depending": {
            "type": "on the size of the initial sample, we have one, two or three",
            "description": ""
          },
          "such": {
            "type": "computation levels.",
            "description": ""
          },
          "Note": {
            "type": "that only raw estimates are returned. If one is interested in",
            "description": ""
          },
          "see": {
            "type": "the MinCovDet object.",
            "description": "References\n----------\n.. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance"
          },
          "Determinant": {
            "type": "Estimator, 1999, American Statistical Association",
            "description": ""
          },
          "and": {
            "type": "the American Society for Quality, TECHNOMETRICS",
            "description": ".. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,"
          },
          "Asymptotics": {
            "type": "For The Minimum Covariance Determinant Estimator,",
            "description": ""
          }
        },
        "returns": "-------\n    location : ndarray of shape (n_features,)\n        Robust location of the data.\n\n    covariance : ndarray of shape (n_features, n_features)\n        Robust covariance of the features.\n\n    support : ndarray of shape (n_samples,), dtype=bool\n        A mask of the observations that have been used to compute\n        the robust location and covariance estimates of the data set.\n\n    Notes\n    -----\n    The FastMCD algorithm has been introduced by Rousseuw and Van Driessen\n    in \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n    1999, American Statistical Association and the American Society\n    for Quality, TECHNOMETRICS\".\n    The principle is to compute robust estimates and random subsets before\n    pooling them into a larger subsets, and finally into the full data set.\n    Depending on the size of the initial sample, we have one, two or three\n    such computation levels.\n\n    Note that only raw estimates are returned. If one is interested in\n    the correction and reweighting steps described in [RouseeuwVan]_,\n    see the MinCovDet object.\n\n    References\n    ----------\n\n    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n        Determinant Estimator, 1999, American Statistical Association\n        and the American Society for Quality, TECHNOMETRICS\n\n    .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
        "raises": "",
        "see_also": "",
        "notes": "that only raw estimates are returned. If one is interested in\n    the correction and reweighting steps described in [RouseeuwVan]_,\n    see the MinCovDet object.\n\n    References\n    ----------\n\n    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n        Determinant Estimator, 1999, American Statistical Association\n        and the American Society for Quality, TECHNOMETRICS\n\n    .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
        "examples": ""
      }
    },
    {
      "name": "graphical_lasso",
      "signature": "graphical_lasso(emp_cov, alpha, *, mode='cd', tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, return_costs=False, eps=2.220446049250313e-16, return_n_iter=False)",
      "documentation": {
        "description": "L1-penalized covariance estimator.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        graph_lasso has been renamed to graphical_lasso\n\n    Parameters\n    ----------\n    emp_cov : array-like of shape (n_features, n_features)\n        Empirical covariance from which to compute the covariance estimate.\n\n    alpha : float\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        printed at each iteration.\n\n    return_costs : bool, default=False\n        If return_costs is True, the objective function and dual gap\n        at each iteration are returned.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : ndarray of shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with\n        cross-validated choice of the l1 penalty.\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.",
        "parameters": {
          "emp_cov": {
            "type": "array",
            "description": "like of shape (n_features, n_features)"
          },
          "Empirical": {
            "type": "covariance from which to compute the covariance estimate.",
            "description": ""
          },
          "alpha": {
            "type": "float",
            "description": ""
          },
          "The": {
            "type": "algorithm employed to solve this problem is the GLasso algorithm,",
            "description": ""
          },
          "Range": {
            "type": "is (0, inf].",
            "description": ""
          },
          "mode": {
            "type": "{'cd', 'lars'}, default='cd'",
            "description": ""
          },
          "very": {
            "type": "sparse underlying graphs, where p > n. Elsewhere prefer cd",
            "description": ""
          },
          "which": {
            "type": "is more numerically stable.",
            "description": ""
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "this": {
            "type": "value, iterations are stopped. Range is (0, inf].",
            "description": ""
          },
          "enet_tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "for": {
            "type": "a given column update, not of the overall parameter estimate. Only",
            "description": ""
          },
          "used": {
            "type": "for mode='cd'. Range is (0, inf].",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "return_costs is True, the objective function and dual gap",
            "description": ""
          },
          "printed": {
            "type": "at each iteration.",
            "description": ""
          },
          "return_costs": {
            "type": "bool, default=False",
            "description": ""
          },
          "at": {
            "type": "each iteration are returned.",
            "description": ""
          },
          "eps": {
            "type": "float, default=eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Default is `np.finfo(np.float64).eps`."
          },
          "return_n_iter": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "or not to return the number of iterations.",
            "description": "Returns\n-------"
          },
          "covariance": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "costs": {
            "type": "list of (objective, dual_gap) pairs",
            "description": ""
          },
          "each": {
            "type": "iteration. Returned only if return_costs is True.",
            "description": ""
          },
          "n_iter": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of iterations. Returned only if `return_n_iter` is set to True.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with",
            "description": "cross-validated choice of the l1 penalty.\nNotes\n-----"
          },
          "from": {
            "type": "the Friedman 2008 Biostatistics paper. It is the same algorithm",
            "description": ""
          },
          "as": {
            "type": "in the R `glasso` package.",
            "description": ""
          },
          "One": {
            "type": "possible difference with the `glasso` R package is that the",
            "description": ""
          },
          "diagonal": {
            "type": "coefficients are not penalized.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_spd_matrix\n>>> from sklearn.covariance import empirical_covariance, graphical_lasso\n>>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)\n>>> rng = np.random.RandomState(42)\n>>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)\n>>> emp_cov = empirical_covariance(X, assume_centered=True)\n>>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)\n>>> emp_cov\narray([[ 1.68...,  0.21..., -0.20...],\n[ 0.21...,  0.22..., -0.08...],\n[-0.20..., -0.08...,  0.23...]])"
          }
        },
        "returns": "-------\n    covariance : ndarray of shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : ndarray of shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with\n        cross-validated choice of the l1 penalty.\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_spd_matrix\n    >>> from sklearn.covariance import empirical_covariance, graphical_lasso\n    >>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)\n    >>> emp_cov = empirical_covariance(X, assume_centered=True)\n    >>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)\n    >>> emp_cov\n    array([[ 1.68...,  0.21..., -0.20...],\n           [ 0.21...,  0.22..., -0.08...],\n           [-0.20..., -0.08...,  0.23...]])",
        "raises": "",
        "see_also": "--------\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with\n        cross-validated choice of the l1 penalty.\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_spd_matrix\n    >>> from sklearn.covariance import empirical_covariance, graphical_lasso\n    >>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)\n    >>> emp_cov = empirical_covariance(X, assume_centered=True)\n    >>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)\n    >>> emp_cov\n    array([[ 1.68...,  0.21..., -0.20...],\n           [ 0.21...,  0.22..., -0.08...],\n           [-0.20..., -0.08...,  0.23...]])",
        "notes": "-----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_spd_matrix\n    >>> from sklearn.covariance import empirical_covariance, graphical_lasso\n    >>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)\n    >>> emp_cov = empirical_covariance(X, assume_centered=True)\n    >>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)\n    >>> emp_cov\n    array([[ 1.68...,  0.21..., -0.20...],\n           [ 0.21...,  0.22..., -0.08...],\n           [-0.20..., -0.08...,  0.23...]])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_spd_matrix\n    >>> from sklearn.covariance import empirical_covariance, graphical_lasso\n    >>> true_cov = make_sparse_spd_matrix(n_dim=3,random_state=42)\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.multivariate_normal(mean=np.zeros(3), cov=true_cov, size=3)\n    >>> emp_cov = empirical_covariance(X, assume_centered=True)\n    >>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)\n    >>> emp_cov\n    array([[ 1.68...,  0.21..., -0.20...],\n           [ 0.21...,  0.22..., -0.08...],\n           [-0.20..., -0.08...,  0.23...]])"
      }
    },
    {
      "name": "ledoit_wolf",
      "signature": "ledoit_wolf(X, *, assume_centered=False, block_size=1000)",
      "documentation": {
        "description": "Estimate the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n        This is purely a memory optimization and does not affect results.\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "from which to compute the covariance estimate.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, data will be centered before computation.",
            "description": ""
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "block_size": {
            "type": "int, default=1000",
            "description": ""
          },
          "Size": {
            "type": "of blocks into which the covariance matrix will be split.",
            "description": ""
          },
          "This": {
            "type": "is purely a memory optimization and does not affect results.",
            "description": "Returns\n-------"
          },
          "shrunk_cov": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Shrunk": {
            "type": "covariance.",
            "description": ""
          },
          "shrinkage": {
            "type": "float",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized (shrunk) covariance is:",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "mu = trace(cov) / n_features",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n>>> real_cov = np.array([[.4, .2], [.2, .8]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n>>> covariance, shrinkage = ledoit_wolf(X)\n>>> covariance\narray([[0.44..., 0.16...],\n[0.16..., 0.80...]])\n>>> shrinkage\nnp.float64(0.23...)"
          }
        },
        "returns": "-------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)"
      }
    },
    {
      "name": "ledoit_wolf_shrinkage",
      "signature": "ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000)",
      "documentation": {
        "description": "Estimate the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n\n    Returns\n    -------\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "from which to compute the Ledoit-Wolf shrunk covariance shrinkage.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, data will be centered before computation.",
            "description": ""
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "block_size": {
            "type": "int, default=1000",
            "description": ""
          },
          "Size": {
            "type": "of blocks into which the covariance matrix will be split.",
            "description": "Returns\n-------"
          },
          "shrinkage": {
            "type": "float",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized (shrunk) covariance is:",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "mu = trace(cov) / n_features",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import ledoit_wolf_shrinkage\n>>> real_cov = np.array([[.4, .2], [.2, .8]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n>>> shrinkage_coefficient = ledoit_wolf_shrinkage(X)\n>>> shrinkage_coefficient\nnp.float64(0.23...)"
          }
        },
        "returns": "-------\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ledoit_wolf_shrinkage\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> shrinkage_coefficient = ledoit_wolf_shrinkage(X)\n    >>> shrinkage_coefficient\n    np.float64(0.23...)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ledoit_wolf_shrinkage\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> shrinkage_coefficient = ledoit_wolf_shrinkage(X)\n    >>> shrinkage_coefficient\n    np.float64(0.23...)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ledoit_wolf_shrinkage\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> shrinkage_coefficient = ledoit_wolf_shrinkage(X)\n    >>> shrinkage_coefficient\n    np.float64(0.23...)"
      }
    },
    {
      "name": "log_likelihood",
      "signature": "log_likelihood(emp_cov, precision)",
      "documentation": {
        "description": "Compute the sample mean of the log_likelihood under a covariance model.\n\n    Computes the empirical expected log-likelihood, allowing for universal\n    comparison (beyond this software package), and accounts for normalization\n    terms and scaling.\n\n    Parameters\n    ----------\n    emp_cov : ndarray of shape (n_features, n_features)\n        Maximum Likelihood Estimator of covariance.\n\n    precision : ndarray of shape (n_features, n_features)\n        The precision matrix of the covariance model to be tested.",
        "parameters": {
          "emp_cov": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Maximum": {
            "type": "Likelihood Estimator of covariance.",
            "description": ""
          },
          "precision": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "The": {
            "type": "precision matrix of the covariance model to be tested.",
            "description": "Returns\n-------"
          },
          "log_likelihood_": {
            "type": "float",
            "description": ""
          },
          "Sample": {
            "type": "mean of the log-likelihood.",
            "description": ""
          }
        },
        "returns": "-------\n    log_likelihood_ : float\n        Sample mean of the log-likelihood.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "oas",
      "signature": "oas(X, *, assume_centered=False)",
      "documentation": {
        "description": "Estimate covariance with the Oracle Approximating Shrinkage.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n      If True, data will not be centered before computation.\n      Useful to work with data whose mean is significantly equal to\n      zero but is not exactly zero.\n      If False, data will be centered before computation.\n\n    Returns\n    -------\n    shrunk_cov : array-like of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "from which to compute the covariance estimate.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, data will be centered before computation.",
            "description": "Returns\n-------"
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "shrunk_cov": {
            "type": "array",
            "description": "like of shape (n_features, n_features)"
          },
          "Shrunk": {
            "type": "covariance.",
            "description": ""
          },
          "shrinkage": {
            "type": "float",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "2/p is so small that it doesn't affect the value of the estimator.",
            "description": "References\n----------\n.. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\nChen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O."
          },
          "The": {
            "type": "shrinkage formulation implemented here differs from Eq. 23 in [1]_. In",
            "description": ""
          },
          "where": {
            "type": "mu = trace(cov) / n_features and shrinkage is given by the OAS formula",
            "description": "(see [1]_)."
          },
          "the": {
            "type": "original article, formula (23) states that 2/p (p being the number of",
            "description": "features) is multiplied by Trace(cov*cov) in both the numerator and\ndenominator, but this operation is omitted because for a large p, the value"
          },
          "IEEE": {
            "type": "Transactions on Signal Processing, 58(10), 5016-5029, 2010.",
            "description": "<0907.4698>`\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import oas\n>>> rng = np.random.RandomState(0)\n>>> real_cov = [[.8, .3], [.3, .4]]\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n>>> shrunk_cov, shrinkage = oas(X)\n>>> shrunk_cov\narray([[0.7533..., 0.2763...],\n[0.2763..., 0.3964...]])\n>>> shrinkage\nnp.float64(0.0195...)"
          }
        },
        "returns": "-------\n    shrunk_cov : array-like of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import oas\n    >>> rng = np.random.RandomState(0)\n    >>> real_cov = [[.8, .3], [.3, .4]]\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_cov, shrinkage = oas(X)\n    >>> shrunk_cov\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> shrinkage\n    np.float64(0.0195...)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import oas\n    >>> rng = np.random.RandomState(0)\n    >>> real_cov = [[.8, .3], [.3, .4]]\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_cov, shrinkage = oas(X)\n    >>> shrunk_cov\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> shrinkage\n    np.float64(0.0195...)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import oas\n    >>> rng = np.random.RandomState(0)\n    >>> real_cov = [[.8, .3], [.3, .4]]\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_cov, shrinkage = oas(X)\n    >>> shrunk_cov\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> shrinkage\n    np.float64(0.0195...)"
      }
    },
    {
      "name": "shrunk_covariance",
      "signature": "shrunk_covariance(emp_cov, shrinkage=0.1)",
      "documentation": {
        "description": "Calculate covariance matrices shrunk on the diagonal.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : array-like of shape (..., n_features, n_features)\n        Covariance matrices to be shrunk, at least 2D ndarray.\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (..., n_features, n_features)\n        Shrunk covariance matrices.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.",
        "parameters": {
          "emp_cov": {
            "type": "array",
            "description": "like of shape (..., n_features, n_features)"
          },
          "Covariance": {
            "type": "matrices to be shrunk, at least 2D ndarray.",
            "description": ""
          },
          "shrinkage": {
            "type": "float, default=0.1",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate. Range is [0, 1].",
            "description": "Returns\n-------"
          },
          "shrunk_cov": {
            "type": "ndarray of shape (..., n_features, n_features)",
            "description": ""
          },
          "Shrunk": {
            "type": "covariance matrices.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized (shrunk) covariance is given by::",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "`mu = trace(cov) / n_features`.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n>>> real_cov = np.array([[.8, .3], [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n>>> shrunk_covariance(empirical_covariance(X))\narray([[0.73..., 0.25...],\n[0.25..., 0.41...]])"
          }
        },
        "returns": "-------\n    shrunk_cov : ndarray of shape (..., n_features, n_features)\n        Shrunk covariance matrices.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])"
      }
    }
  ],
  "classes": [
    {
      "name": "EllipticEnvelope",
      "documentation": {
        "description": "An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`.\n        Range is (0, 1).\n\n    contamination : float, default=0.1\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Range is (0, 0.5].\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling\n        the data. Pass an int for reproducible results across multiple function\n        calls. See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specify": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, the robust location and covariance are directly computed",
            "description": ""
          },
          "is": {
            "type": "computed, and a covariance estimate is recomputed from it,",
            "description": ""
          },
          "without": {
            "type": "centering the data.",
            "description": ""
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "support_fraction": {
            "type": "float, default=None",
            "description": ""
          },
          "The": {
            "type": "raw robust estimated covariance before correction and re-weighting.",
            "description": ""
          },
          "MCD": {
            "type": "estimate. If None, the minimum value of support_fraction will",
            "description": ""
          },
          "be": {
            "type": "used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`.",
            "description": ""
          },
          "Range": {
            "type": "is (0, 1).",
            "description": ""
          },
          "contamination": {
            "type": "float, default=0.1",
            "description": ""
          },
          "of": {
            "type": "outliers in the data set. Range is (0, 0.5].",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "the pseudo random number generator for shuffling",
            "description": ""
          },
          "the": {
            "type": "raw robust estimates of location and shape, before correction",
            "description": ""
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "support_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "A": {
            "type": "mask of the observations that have been used to compute",
            "description": ""
          },
          "robust": {
            "type": "estimates of location and shape.",
            "description": ""
          },
          "offset_": {
            "type": "float",
            "description": ""
          },
          "Offset": {
            "type": "used to define the decision function from the raw scores.",
            "description": ""
          },
          "We": {
            "type": "have the relation: ``decision_function = score_samples - offset_``.",
            "description": ""
          },
          "such": {
            "type": "a way we obtain the expected number of outliers (samples with",
            "description": ""
          },
          "decision": {
            "type": "function < 0) in training.",
            "description": ".. versionadded:: 0.20"
          },
          "raw_location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "raw_covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "raw_support_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "and": {
            "type": "re-weighting.",
            "description": ""
          },
          "dist_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Mahalanobis": {
            "type": "distances of the training set (on which :meth:`fit` is",
            "description": "called) observations."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "EmpiricalCovariance": {
            "type": "Maximum likelihood covariance estimator.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "LedoitWolf": {
            "type": "LedoitWolf Estimator.",
            "description": ""
          },
          "MinCovDet": {
            "type": "Minimum Covariance Determinant",
            "description": "(robust estimator of covariance)."
          },
          "OAS": {
            "type": "Oracle Approximating Shrinkage Estimator.",
            "description": ""
          },
          "ShrunkCovariance": {
            "type": "Covariance estimator with shrinkage.",
            "description": "Notes\n-----"
          },
          "Outlier": {
            "type": "detection from covariance estimation may break or not",
            "description": ""
          },
          "perform": {
            "type": "well in high-dimensional settings. In particular, one will",
            "description": ""
          },
          "always": {
            "type": "take care to work with ``n_samples > n_features ** 2``.",
            "description": "References\n----------\n.. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the"
          },
          "minimum": {
            "type": "covariance determinant estimator\" Technometrics 41(3), 212",
            "description": "(1999)\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EllipticEnvelope\n>>> true_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n...                                                  cov=true_cov,\n...                                                  size=500)\n>>> cov = EllipticEnvelope(random_state=0).fit(X)\n>>> # predict returns 1 for an inlier and -1 for an outlier\n>>> cov.predict([[0, 0],\n...              [3, 3]])"
          },
          "array": {
            "type": "[0.0813... , 0.0427...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])",
        "notes": "-----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])"
      },
      "methods": [
        {
          "name": "correct_covariance",
          "signature": "correct_covariance(self, data)",
          "documentation": {
            "description": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.",
            "parameters": {
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data set must be the one which was used to compute",
                "description": ""
              },
              "the": {
                "type": "raw estimates.",
                "description": "Returns\n-------"
              },
              "covariance_corrected": {
                "type": "ndarray of shape (n_features, n_features)",
                "description": ""
              },
              "Corrected": {
                "type": "robust covariance estimate.",
                "description": "References\n----------\n.. [RVD] A Fast Algorithm for the Minimum Covariance"
              },
              "Determinant": {
                "type": "Estimator, 1999, American Statistical Association",
                "description": ""
              },
              "and": {
                "type": "the American Society for Quality, TECHNOMETRICS",
                "description": ""
              }
            },
            "returns": "-------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "threshold for being an outlier is 0, which ensures a",
                "description": ""
              },
              "decision": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Decision": {
                "type": "function of the samples.",
                "description": ""
              },
              "It": {
                "type": "is equal to the shifted Mahalanobis distances.",
                "description": ""
              },
              "compatibility": {
                "type": "with other outlier detection algorithms.",
                "description": ""
              }
            },
            "returns": "-------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_predict",
          "signature": "fit_predict(self, X, y=None, **kwargs)",
          "documentation": {
            "description": "Perform fit on X and returns labels for X.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "input samples.",
                "description": ""
              },
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**kwargs : dict"
              },
              "Arguments": {
                "type": "to be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              },
              "1": {
                "type": "for inliers, -1 for outliers.",
                "description": ""
              },
              "to": {
                "type": "be passed to ``fit``.",
                "description": ".. versionadded:: 1.4\nReturns\n-------"
              }
            },
            "returns": "-1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **kwargs : dict\n            Arguments to be passed to ``fit``.\n\n            .. versionadded:: 1.4",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict labels (1 inlier, -1 outlier) of X according to fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix.",
                "description": "Returns\n-------"
              },
              "is_inlier": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Returns": {
                "type": "-1 for anomalies/outliers and +1 for inliers.",
                "description": ""
              }
            },
            "returns": "-------\n        is_inlier : ndarray of shape (n_samples,)",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reweight_covariance",
          "signature": "reweight_covariance(self, data)",
          "documentation": {
            "description": "Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.",
            "parameters": {
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data set must be the one which was used to compute",
                "description": ""
              },
              "the": {
                "type": "re-weighted robust location and covariance estimates.",
                "description": "References\n----------\n.. [RVDriessen] A Fast Algorithm for the Minimum Covariance"
              },
              "location_reweighted": {
                "type": "ndarray of shape (n_features,)",
                "description": "Re-weighted robust location estimate."
              },
              "covariance_reweighted": {
                "type": "ndarray of shape (n_features, n_features)",
                "description": "Re-weighted robust covariance estimate."
              },
              "support_reweighted": {
                "type": "ndarray of shape (n_samples,), dtype=bool",
                "description": ""
              },
              "A": {
                "type": "mask of the observations that have been used to compute",
                "description": ""
              },
              "Determinant": {
                "type": "Estimator, 1999, American Statistical Association",
                "description": ""
              },
              "and": {
                "type": "the American Society for Quality, TECHNOMETRICS",
                "description": ""
              }
            },
            "returns": "-------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of self.predict(X) w.r.t. y.",
                "description": ""
              }
            },
            "returns": "-------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score_samples",
          "signature": "score_samples(self, X)",
          "documentation": {
            "description": "Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix.",
                "description": "Returns\n-------"
              },
              "negative_mahal_distances": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Opposite": {
                "type": "of the Mahalanobis distances.",
                "description": ""
              }
            },
            "returns": "-------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "EmpiricalCovariance",
      "documentation": {
        "description": "Maximum likelihood covariance estimator.\n\n    Read more in the :ref:`User Guide <covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specifies if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo-inverse matrix.\n        (stored only if store_precision is True)\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specifies": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False (default), data are centered before computation.",
            "description": "Attributes\n----------"
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo-inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "EllipticEnvelope": {
            "type": "An object for detecting outliers in",
            "description": ""
          },
          "a": {
            "type": "Gaussian distributed dataset.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "LedoitWolf": {
            "type": "LedoitWolf Estimator.",
            "description": ""
          },
          "MinCovDet": {
            "type": "Minimum Covariance Determinant",
            "description": "(robust estimator of covariance)."
          },
          "OAS": {
            "type": "Oracle Approximating Shrinkage Estimator.",
            "description": ""
          },
          "ShrunkCovariance": {
            "type": "Covariance estimator with shrinkage.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EmpiricalCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> cov = EmpiricalCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7569..., 0.2818...],\n[0.2818..., 0.3928...]])\n>>> cov.location_"
          },
          "array": {
            "type": "[0.0622..., 0.0193...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])"
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the maximum likelihood covariance estimator to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where `n_samples` is the number of samples and\n          `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GraphicalLasso",
      "documentation": {
        "description": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\n    For a usage example see\n    :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLasso has been renamed to GraphicalLasso\n\n    Parameters\n    ----------\n    alpha : float, default=0.01\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    covariance : \"precomputed\", default=None\n        If covariance is \"precomputed\", the input data in `fit` is assumed\n        to be the covariance matrix. If `None`, the empirical covariance\n        is estimated from the data `X`.\n\n        .. versionadded:: 1.3\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        plotted at each iteration.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n        .. versionadded:: 1.3\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    costs_ : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n        .. versionadded:: 1.3\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLassoCV : Sparse inverse covariance with\n        cross-validated choice of the l1 penalty.",
        "parameters": {
          "alpha": {
            "type": "float, default=0.01",
            "description": ""
          },
          "The": {
            "type": "list of values of the objective function and the dual gap at",
            "description": ""
          },
          "Range": {
            "type": "is (0, inf].",
            "description": ""
          },
          "mode": {
            "type": "{'cd', 'lars'}, default='cd'",
            "description": ""
          },
          "very": {
            "type": "sparse underlying graphs, where p > n. Elsewhere prefer cd",
            "description": ""
          },
          "which": {
            "type": "is more numerically stable.",
            "description": ""
          },
          "covariance": {
            "type": "\"precomputed\", default=None",
            "description": ""
          },
          "If": {
            "type": "False, data are centered before computation.",
            "description": "Attributes\n----------"
          },
          "to": {
            "type": "be the covariance matrix. If `None`, the empirical covariance",
            "description": ""
          },
          "is": {
            "type": "estimated from the data `X`.",
            "description": ".. versionadded:: 1.3"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "this": {
            "type": "value, iterations are stopped. Range is (0, inf].",
            "description": ""
          },
          "enet_tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "for": {
            "type": "a given column update, not of the overall parameter estimate. Only",
            "description": ""
          },
          "used": {
            "type": "for mode='cd'. Range is (0, inf].",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "plotted": {
            "type": "at each iteration.",
            "description": ""
          },
          "eps": {
            "type": "float, default=eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Default is `np.finfo(np.float64).eps`.\n.. versionadded:: 1.3"
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": ""
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "costs_": {
            "type": "list of (objective, dual_gap) pairs",
            "description": ""
          },
          "each": {
            "type": "iteration. Returned only if return_costs is True.",
            "description": ".. versionadded:: 1.3"
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "graphical_lasso": {
            "type": "L1",
            "description": "penalized covariance estimator."
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with",
            "description": "cross-validated choice of the l1 penalty.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLasso\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLasso().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.049, 0.218, 0.019],\n[0.049, 0.364, 0.017, 0.034],\n[0.218, 0.017, 0.322, 0.093],\n[0.019, 0.034, 0.093, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)"
          },
          "array": {
            "type": "[0.073, 0.04 , 0.038, 0.143]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLassoCV : Sparse inverse covariance with\n        cross-validated choice of the l1 penalty.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLasso\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLasso().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.049, 0.218, 0.019],\n           [0.049, 0.364, 0.017, 0.034],\n           [0.218, 0.017, 0.322, 0.093],\n           [0.019, 0.034, 0.093, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLasso\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLasso().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.049, 0.218, 0.019],\n           [0.049, 0.364, 0.017, 0.034],\n           [0.218, 0.017, 0.322, 0.093],\n           [0.019, 0.034, 0.093, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])"
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the GraphicalLasso model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "from which to compute the covariance estimate.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GraphicalLassoCV",
      "documentation": {
        "description": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLassoCV has been renamed to GraphicalLassoCV\n\n    Parameters\n    ----------\n    alphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n        If an integer is given, it fixes the number of points on the\n        grids of alpha to be used. If a list is given, it gives the\n        grid to be used. See the notes in the class docstring for\n        more details. Range is [1, inf) for an integer.\n        Range is (0, inf] for an array-like of floats.\n\n    n_refinements : int, default=4\n        The number of times the grid is refined. Not used if explicit\n        values of alphas are passed. Range is [1, inf).\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        Maximum number of iterations.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where number of features is greater\n        than number of samples. Elsewhere prefer cd which is more numerically\n        stable.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and duality gap are\n        printed at each iteration.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n        .. versionadded:: 1.3\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated precision matrix (inverse covariance).\n\n    costs_ : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n        .. versionadded:: 1.3\n\n    alpha_ : float\n        Penalization parameter selected.\n\n    cv_results_ : dict of ndarrays\n        A dict with keys:\n\n        alphas : ndarray of shape (n_alphas,)\n            All penalization parameters explored.\n\n        split(k)_test_score : ndarray of shape (n_alphas,)\n            Log-likelihood score on left-out data across (k)th fold.\n\n            .. versionadded:: 1.0\n\n        mean_test_score : ndarray of shape (n_alphas,)\n            Mean of scores over the folds.\n\n            .. versionadded:: 1.0\n\n        std_test_score : ndarray of shape (n_alphas,)\n            Standard deviation of scores over the folds.\n\n            .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations run for the optimal alpha.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (`alpha`) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of `alpha` then come out as missing values, but the optimum may\n    be close to these missing values.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.",
        "parameters": {
          "alphas": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "If": {
            "type": "False, data are centered before computation.",
            "description": "Attributes\n----------"
          },
          "grids": {
            "type": "of alpha to be used. If a list is given, it gives the",
            "description": ""
          },
          "grid": {
            "type": "to be used. See the notes in the class docstring for",
            "description": ""
          },
          "more": {
            "type": "details. Range is [1, inf) for an integer.",
            "description": ""
          },
          "Range": {
            "type": "is (0, inf] for an array-like of floats.",
            "description": ""
          },
          "n_refinements": {
            "type": "int, default=4",
            "description": ""
          },
          "The": {
            "type": "search for the optimal penalization parameter (`alpha`) is done on an",
            "description": ""
          },
          "values": {
            "type": "of `alpha` then come out as missing values, but the optimum may",
            "description": ""
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs :class:`~sklearn.model_selection.KFold` is used.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.20\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "this": {
            "type": "value, iterations are stopped. Range is (0, inf].",
            "description": ""
          },
          "enet_tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "for": {
            "type": "more details.",
            "description": ".. versionchanged:: v0.20\n`n_jobs` default changed from 1 to None"
          },
          "used": {
            "type": "for mode='cd'. Range is (0, inf].",
            "description": ""
          },
          "max_iter": {
            "type": "int, default=100",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations.",
            "description": ""
          },
          "mode": {
            "type": "{'cd', 'lars'}, default='cd'",
            "description": ""
          },
          "very": {
            "type": "sparse underlying graphs, where number of features is greater",
            "description": ""
          },
          "than": {
            "type": "number of samples. Elsewhere prefer cd which is more numerically",
            "description": "stable."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "verbose": {
            "type": "bool, default=False",
            "description": ""
          },
          "printed": {
            "type": "at each iteration.",
            "description": ""
          },
          "eps": {
            "type": "float, default=eps",
            "description": ""
          },
          "Cholesky": {
            "type": "diagonal factors. Increase this for very ill-conditioned",
            "description": "systems. Default is `np.finfo(np.float64).eps`.\n.. versionadded:: 1.3"
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "precision matrix (inverse covariance).",
            "description": ""
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "costs_": {
            "type": "list of (objective, dual_gap) pairs",
            "description": ""
          },
          "each": {
            "type": "iteration. Returned only if return_costs is True.",
            "description": ".. versionadded:: 1.3"
          },
          "alpha_": {
            "type": "float",
            "description": ""
          },
          "Penalization": {
            "type": "parameter selected.",
            "description": ""
          },
          "cv_results_": {
            "type": "dict of ndarrays",
            "description": ""
          },
          "A": {
            "type": "dict with keys:",
            "description": ""
          },
          "All": {
            "type": "penalization parameters explored.",
            "description": ""
          },
          "split": {
            "type": "k",
            "description": "_test_score : ndarray of shape (n_alphas,)\nLog-likelihood score on left-out data across (k)th fold.\n.. versionadded:: 1.0"
          },
          "mean_test_score": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "Mean": {
            "type": "of scores over the folds.",
            "description": ".. versionadded:: 1.0"
          },
          "std_test_score": {
            "type": "ndarray of shape (n_alphas,)",
            "description": ""
          },
          "Standard": {
            "type": "deviation of scores over the folds.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "graphical_lasso": {
            "type": "L1",
            "description": "penalized covariance estimator."
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": "Notes\n-----"
          },
          "iteratively": {
            "type": "refined grid: first the cross-validated scores on a grid are",
            "description": "computed, then a new refined grid is centered around the maximum, and so\non."
          },
          "One": {
            "type": "of the challenges which is faced here is that the solvers can",
            "description": ""
          },
          "fail": {
            "type": "to converge to a well-conditioned estimate. The corresponding",
            "description": ""
          },
          "be": {
            "type": "close to these missing values.",
            "description": ""
          },
          "In": {
            "type": "`fit`, once the best parameter `alpha` is found through",
            "description": "cross-validation, the model is fit again using the entire training set.\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLassoCV\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLassoCV().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.051, 0.22 , 0.017],\n[0.051, 0.364, 0.018, 0.036],\n[0.22 , 0.018, 0.322, 0.094],\n[0.017, 0.036, 0.094, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)"
          },
          "array": {
            "type": "[0.073, 0.04 , 0.038, 0.143]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    graphical_lasso : L1-penalized covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (`alpha`) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of `alpha` then come out as missing values, but the optimum may\n    be close to these missing values.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])",
        "notes": "-----\n    The search for the optimal penalization parameter (`alpha`) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of `alpha` then come out as missing values, but the optimum may\n    be close to these missing values.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])"
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, **params)",
          "documentation": {
            "description": "Fit the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        **params : dict, default=None\n            Parameters to be passed to the CV splitter and the\n            cross_val_score function.\n\n            .. versionadded:: 1.5\n                Only available if `enable_metadata_routing=True`,\n                which can be set by using\n                ``sklearn.set_config(enable_metadata_routing=True)``.\n                See :ref:`Metadata Routing User Guide <metadata_routing>` for\n                more details.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Data": {
                "type": "from which to compute the covariance estimate.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "**params : dict, default=None"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        .. versionadded:: 1.5",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LedoitWolf",
      "documentation": {
        "description": "LedoitWolf Estimator.\n\n    Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\n    coefficient is computed using O. Ledoit and M. Wolf's formula as\n    described in \"A Well-Conditioned Estimator for Large-Dimensional\n    Covariance Matrices\", Ledoit and Wolf, Journal of Multivariate\n    Analysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split\n        during its Ledoit-Wolf estimation. This is purely a memory\n        optimization and does not affect results.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specify": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False (default), data will be centered before computation.",
            "description": ""
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "block_size": {
            "type": "int, default=1000",
            "description": ""
          },
          "Size": {
            "type": "of blocks into which the covariance matrix will be split",
            "description": ""
          },
          "during": {
            "type": "its Ledoit-Wolf estimation. This is purely a memory",
            "description": ""
          },
          "optimization": {
            "type": "and does not affect results.",
            "description": "Attributes\n----------"
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "shrinkage_": {
            "type": "float",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate. Range is [0, 1].",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`",
            "description": ""
          },
          "EllipticEnvelope": {
            "type": "An object for detecting outliers in",
            "description": ""
          },
          "a": {
            "type": "Gaussian distributed dataset.",
            "description": ""
          },
          "EmpiricalCovariance": {
            "type": "Maximum likelihood covariance estimator.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with cross",
            "description": "validated"
          },
          "choice": {
            "type": "of the l1 penalty.",
            "description": ""
          },
          "MinCovDet": {
            "type": "Minimum Covariance Determinant",
            "description": "(robust estimator of covariance)."
          },
          "OAS": {
            "type": "Oracle Approximating Shrinkage Estimator.",
            "description": ""
          },
          "ShrunkCovariance": {
            "type": "Covariance estimator with shrinkage.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularised covariance is:",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "mu = trace(cov) / n_features",
            "description": ""
          },
          "and": {
            "type": "shrinkage is given by the Ledoit and Wolf formula (see References)",
            "description": "References\n----------\n\"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\","
          },
          "Ledoit": {
            "type": "and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,",
            "description": ""
          },
          "February": {
            "type": "2004, pages 365-411.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import LedoitWolf\n>>> real_cov = np.array([[.4, .2],\n...                      [.2, .8]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=50)\n>>> cov = LedoitWolf().fit(X)\n>>> cov.covariance_\narray([[0.4406..., 0.1616...],\n[0.1616..., 0.8022...]])\n>>> cov.location_"
          },
          "array": {
            "type": "[ 0.0595... , -0.0075...]",
            "description": ""
          },
          "for": {
            "type": "a more detailed example.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])",
        "notes": "-----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])\n\n    See also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`\n    for a more detailed example.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])\n\n    See also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`\n    for a more detailed example."
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the Ledoit-Wolf shrunk covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MinCovDet",
      "documentation": {
        "description": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\n    The Minimum Covariance Determinant covariance estimator is to be applied\n    on Gaussian-distributed data, but could still be relevant on data\n    drawn from a unimodal, symmetric distribution. It is not meant to be used\n    with multi-modal data (the algorithm used to fit a MinCovDet object is\n    likely to fail in such a case).\n    One should consider projection pursuit methods to deal with multi-modal\n    datasets.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of the robust location and the covariance\n        estimates is computed, and a covariance estimate is recomputed from\n        it, without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is None, which implies that the minimum\n        value of support_fraction will be used within the algorithm:\n        `(n_samples + n_features + 1) / 2 * n_samples`. The parameter must be\n        in the range (0, 1].\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the robust estimates of location and shape.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    References\n    ----------\n\n    .. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n        J. Am Stat Ass, 79:871, 1984.\n    .. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n        Estimator, 1999, American Statistical Association and the American\n        Society for Quality, TECHNOMETRICS\n    .. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specify": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, the robust location and covariance are directly computed",
            "description": ""
          },
          "estimates": {
            "type": "is computed, and a covariance estimate is recomputed from",
            "description": "it, without centering the data."
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "support_fraction": {
            "type": "float, default=None",
            "description": ""
          },
          "The": {
            "type": "Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import MinCovDet\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = MinCovDet(random_state=0).fit(X)\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n[0.2535..., 0.3053...]])\n>>> cov.location_"
          },
          "MCD": {
            "type": "estimate. Default is None, which implies that the minimum",
            "description": ""
          },
          "value": {
            "type": "of support_fraction will be used within the algorithm:",
            "description": "`(n_samples + n_features + 1) / 2 * n_samples`. The parameter must be"
          },
          "in": {
            "type": "the range (0, 1].",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Determines": {
            "type": "the pseudo random number generator for shuffling the data.",
            "description": ""
          },
          "Pass": {
            "type": "an int for reproducible results across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "raw_location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "raw_covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "raw_support_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "A": {
            "type": "mask of the observations that have been used to compute",
            "description": ""
          },
          "the": {
            "type": "robust estimates of location and shape.",
            "description": ""
          },
          "and": {
            "type": "re-weighting.",
            "description": ""
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "support_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "dist_": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Mahalanobis": {
            "type": "distances of the training set (on which :meth:`fit` is",
            "description": "called) observations."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "EllipticEnvelope": {
            "type": "An object for detecting outliers in",
            "description": ""
          },
          "a": {
            "type": "Gaussian distributed dataset.",
            "description": ""
          },
          "EmpiricalCovariance": {
            "type": "Maximum likelihood covariance estimator.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with cross",
            "description": "validated"
          },
          "choice": {
            "type": "of the l1 penalty.",
            "description": ""
          },
          "LedoitWolf": {
            "type": "LedoitWolf Estimator.",
            "description": ""
          },
          "OAS": {
            "type": "Oracle Approximating Shrinkage Estimator.",
            "description": ""
          },
          "ShrunkCovariance": {
            "type": "Covariance estimator with shrinkage.",
            "description": "References\n----------\n.. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\nJ. Am Stat Ass, 79:871, 1984.\n.. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\nEstimator, 1999, American Statistical Association and the American"
          },
          "Society": {
            "type": "for Quality, TECHNOMETRICS",
            "description": ".. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,"
          },
          "Asymptotics": {
            "type": "For The Minimum Covariance Determinant Estimator,",
            "description": ""
          },
          "array": {
            "type": "[0.0813... , 0.0427...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    References\n    ----------\n\n    .. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n        J. Am Stat Ass, 79:871, 1984.\n    .. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n        Estimator, 1999, American Statistical Association and the American\n        Society for Quality, TECHNOMETRICS\n    .. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import MinCovDet\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = MinCovDet(random_state=0).fit(X)\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import MinCovDet\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = MinCovDet(random_state=0).fit(X)\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])"
      },
      "methods": [
        {
          "name": "correct_covariance",
          "signature": "correct_covariance(self, data)",
          "documentation": {
            "description": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.",
            "parameters": {
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data set must be the one which was used to compute",
                "description": ""
              },
              "the": {
                "type": "raw estimates.",
                "description": "Returns\n-------"
              },
              "covariance_corrected": {
                "type": "ndarray of shape (n_features, n_features)",
                "description": ""
              },
              "Corrected": {
                "type": "robust covariance estimate.",
                "description": "References\n----------\n.. [RVD] A Fast Algorithm for the Minimum Covariance"
              },
              "Determinant": {
                "type": "Estimator, 1999, American Statistical Association",
                "description": ""
              },
              "and": {
                "type": "the American Society for Quality, TECHNOMETRICS",
                "description": ""
              }
            },
            "returns": "-------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit a Minimum Covariance Determinant with the FastMCD algorithm.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reweight_covariance",
          "signature": "reweight_covariance(self, data)",
          "documentation": {
            "description": "Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.",
            "parameters": {
              "data": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data set must be the one which was used to compute",
                "description": ""
              },
              "the": {
                "type": "re-weighted robust location and covariance estimates.",
                "description": "References\n----------\n.. [RVDriessen] A Fast Algorithm for the Minimum Covariance"
              },
              "location_reweighted": {
                "type": "ndarray of shape (n_features,)",
                "description": "Re-weighted robust location estimate."
              },
              "covariance_reweighted": {
                "type": "ndarray of shape (n_features, n_features)",
                "description": "Re-weighted robust covariance estimate."
              },
              "support_reweighted": {
                "type": "ndarray of shape (n_samples,), dtype=bool",
                "description": ""
              },
              "A": {
                "type": "mask of the observations that have been used to compute",
                "description": ""
              },
              "Determinant": {
                "type": "Estimator, 1999, American Statistical Association",
                "description": ""
              },
              "and": {
                "type": "the American Society for Quality, TECHNOMETRICS",
                "description": ""
              }
            },
            "returns": "-------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "OAS",
      "documentation": {
        "description": "Oracle Approximating Shrinkage Estimator.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate. Range is [0, 1].\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specify": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False (default), data will be centered before computation.",
            "description": "Attributes\n----------"
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "shrinkage_": {
            "type": "float",
            "description": ""
          },
          "coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "2/p is so small that it doesn't affect the value of the estimator.",
            "description": "References\n----------\n.. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\nChen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O."
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`",
            "description": ""
          },
          "EllipticEnvelope": {
            "type": "An object for detecting outliers in",
            "description": ""
          },
          "a": {
            "type": "Gaussian distributed dataset.",
            "description": ""
          },
          "EmpiricalCovariance": {
            "type": "Maximum likelihood covariance estimator.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with cross",
            "description": "validated"
          },
          "choice": {
            "type": "of the l1 penalty.",
            "description": ""
          },
          "LedoitWolf": {
            "type": "LedoitWolf Estimator.",
            "description": ""
          },
          "MinCovDet": {
            "type": "Minimum Covariance Determinant",
            "description": "(robust estimator of covariance)."
          },
          "ShrunkCovariance": {
            "type": "Covariance estimator with shrinkage.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "shrinkage formulation implemented here differs from Eq. 23 in [1]_. In",
            "description": ""
          },
          "where": {
            "type": "mu = trace(cov) / n_features and shrinkage is given by the OAS formula",
            "description": "(see [1]_)."
          },
          "the": {
            "type": "original article, formula (23) states that 2/p (p being the number of",
            "description": "features) is multiplied by Trace(cov*cov) in both the numerator and\ndenominator, but this operation is omitted because for a large p, the value"
          },
          "IEEE": {
            "type": "Transactions on Signal Processing, 58(10), 5016-5029, 2010.",
            "description": "<0907.4698>`\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import OAS\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> oas = OAS().fit(X)\n>>> oas.covariance_\narray([[0.7533..., 0.2763...],\n[0.2763..., 0.3964...]])\n>>> oas.precision_\narray([[ 1.7833..., -1.2431... ],\n[-1.2431...,  3.3889...]])\n>>> oas.shrinkage_\nnp.float64(0.0195...)"
          },
          "for": {
            "type": "a more detailed example.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    np.float64(0.0195...)",
        "notes": "-----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features),\n\n    where mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n    (see [1]_).\n\n    The shrinkage formulation implemented here differs from Eq. 23 in [1]_. In\n    the original article, formula (23) states that 2/p (p being the number of\n    features) is multiplied by Trace(cov*cov) in both the numerator and\n    denominator, but this operation is omitted because for a large p, the value\n    of 2/p is so small that it doesn't affect the value of the estimator.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n           Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n           IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n           <0907.4698>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    np.float64(0.0195...)\n\n    See also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`\n    for a more detailed example.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    np.float64(0.0195...)\n\n    See also :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`\n    for a more detailed example."
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the Oracle Approximating Shrinkage covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ShrunkCovariance",
      "documentation": {
        "description": "Covariance estimator with shrinkage.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data will be centered before computation.\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n\n    Notes\n    -----\n    The regularized covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features",
        "parameters": {
          "store_precision": {
            "type": "bool, default=True",
            "description": ""
          },
          "Specify": {
            "type": "if the estimated precision is stored.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, data will be centered before computation.",
            "description": ""
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "shrinkage": {
            "type": "float, default=0.1",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate. Range is [0, 1].",
            "description": "Attributes\n----------"
          },
          "covariance_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Estimated": {
            "type": "pseudo inverse matrix.",
            "description": "(stored only if store_precision is True)"
          },
          "location_": {
            "type": "ndarray of shape (n_features,)",
            "description": ""
          },
          "precision_": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "EllipticEnvelope": {
            "type": "An object for detecting outliers in",
            "description": ""
          },
          "a": {
            "type": "Gaussian distributed dataset.",
            "description": ""
          },
          "EmpiricalCovariance": {
            "type": "Maximum likelihood covariance estimator.",
            "description": ""
          },
          "GraphicalLasso": {
            "type": "Sparse inverse covariance estimation",
            "description": ""
          },
          "with": {
            "type": "an l1-penalized estimator.",
            "description": ""
          },
          "GraphicalLassoCV": {
            "type": "Sparse inverse covariance with cross",
            "description": "validated"
          },
          "choice": {
            "type": "of the l1 penalty.",
            "description": ""
          },
          "LedoitWolf": {
            "type": "LedoitWolf Estimator.",
            "description": ""
          },
          "MinCovDet": {
            "type": "Minimum Covariance Determinant",
            "description": "(robust estimator of covariance)."
          },
          "OAS": {
            "type": "Oracle Approximating Shrinkage Estimator.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized covariance is given by:",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "mu = trace(cov) / n_features",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import ShrunkCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = ShrunkCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7387..., 0.2536...],\n[0.2536..., 0.4110...]])\n>>> cov.location_"
          },
          "array": {
            "type": "[0.0622..., 0.0193...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    EmpiricalCovariance : Maximum likelihood covariance estimator.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    GraphicalLassoCV : Sparse inverse covariance with cross-validated\n        choice of the l1 penalty.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n\n    Notes\n    -----\n    The regularized covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ShrunkCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = ShrunkCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7387..., 0.2536...],\n           [0.2536..., 0.4110...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])",
        "notes": "-----\n    The regularized covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ShrunkCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = ShrunkCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7387..., 0.2536...],\n           [0.2536..., 0.4110...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ShrunkCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = ShrunkCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7387..., 0.2536...],\n           [0.2536..., 0.4110...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])"
      },
      "methods": [
        {
          "name": "error_norm",
          "signature": "error_norm(self, comp_cov, norm='frobenius', scaling=True, squared=True)",
          "documentation": {
            "description": "Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.",
            "parameters": {
              "comp_cov": {
                "type": "array",
                "description": "like of shape (n_features, n_features)"
              },
              "The": {
                "type": "Mean Squared Error (in the sense of the Frobenius norm) between",
                "description": "`self` and `comp_cov` covariance estimators."
              },
              "norm": {
                "type": "{\"frobenius\", \"spectral\"}, default=\"frobenius\"",
                "description": ""
              },
              "where": {
                "type": "A is the error ``(comp_cov - self.covariance_)``.",
                "description": ""
              },
              "scaling": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "False, the error norm is returned.",
                "description": "Returns\n-------"
              },
              "squared": {
                "type": "bool, default=True",
                "description": ""
              },
              "Whether": {
                "type": "to compute the squared error norm or the error norm.",
                "description": ""
              },
              "result": {
                "type": "float",
                "description": ""
              }
            },
            "returns": "-------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y=None)",
          "documentation": {
            "description": "Fit the shrunk covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_precision",
          "signature": "get_precision(self)",
          "documentation": {
            "description": "Getter for the precision matrix.",
            "parameters": {},
            "returns": "-------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mahalanobis",
          "signature": "mahalanobis(self, X)",
          "documentation": {
            "description": "Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "observations, the Mahalanobis distances of the which we",
                "description": "compute. Observations are assumed to be drawn from the same"
              },
              "distribution": {
                "type": "than the data used in fit.",
                "description": "Returns\n-------"
              },
              "dist": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Squared": {
                "type": "Mahalanobis distances of the observations.",
                "description": ""
              }
            },
            "returns": "-------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X_test, y=None)",
          "documentation": {
            "description": "Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.",
            "parameters": {
              "X_test": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "data of which we compute the likelihood, where `n_samples` is",
                "description": ""
              },
              "the": {
                "type": "data used in fit (including centering).",
                "description": ""
              },
              "y": {
                "type": "Ignored",
                "description": ""
              },
              "Not": {
                "type": "used, present for API consistency by convention.",
                "description": "Returns\n-------"
              },
              "res": {
                "type": "float",
                "description": ""
              },
              "The": {
                "type": "log-likelihood of `X_test` with `self.location_` and `self.covariance_`",
                "description": ""
              },
              "as": {
                "type": "estimators of the Gaussian model mean and covariance matrix respectively.",
                "description": ""
              }
            },
            "returns": "-------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}