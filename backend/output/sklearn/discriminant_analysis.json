{
  "description": "Linear and quadratic discriminant analysis.",
  "functions": [
    {
      "name": "check_classification_targets",
      "signature": "check_classification_targets(y)",
      "documentation": {
        "description": "Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'",
        "parameters": {
          "y": {
            "type": "array",
            "description": "like"
          },
          "Target": {
            "type": "values.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "documentation": {
        "description": "Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "Estimator": {
            "type": "instance for which the check is performed.",
            "description": ""
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": ""
          },
          "Attribute": {
            "type": "name(s) given as string or a list/tuple of strings",
            "description": "Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"
          },
          "If": {
            "type": "the attributes are not found.",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")"
          },
          "attribute": {
            "type": "that ends with a underscore and does not start with double",
            "description": "underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": ""
          },
          "The": {
            "type": "default error message is, \"This %(name)s instance is not fitted",
            "description": "yet. Call 'fit' with appropriate arguments before using this\nestimator.\""
          },
          "For": {
            "type": "custom messages if \"%(name)s\" is present in the message string,",
            "description": ""
          },
          "it": {
            "type": "is substituted for the estimator name.",
            "description": "Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": ""
          },
          "Specify": {
            "type": "whether all or any of the given attributes must exist.",
            "description": "Raises\n------\nTypeError"
          },
          "Model": {
            "type": "is not fitted yet.",
            "description": ">>> lr.fit([[1, 2], [1, 3]], [1, 0])"
          },
          "LogisticRegression": {
            "type": "",
            "description": ">>> check_is_fitted(lr)"
          }
        },
        "returns": "",
        "raises": "a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)"
      }
    },
    {
      "name": "device",
      "signature": "device(*array_list, remove_none=True, remove_types=(<class 'str'>,))",
      "documentation": {
        "description": "Hardware device where the array data resides on.\n\n    If the hardware device is not the same for all arrays, an error is raised.\n\n    Parameters\n    ----------\n    *array_list : arrays\n        List of array instances from NumPy or an array API compatible library.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in array_list.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in array_list.",
        "parameters": {
          "List": {
            "type": "of array instances from NumPy or an array API compatible library.",
            "description": ""
          },
          "remove_none": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to ignore None objects passed in array_list.",
            "description": ""
          },
          "remove_types": {
            "type": "tuple or list, default=(str,)",
            "description": ""
          },
          "Types": {
            "type": "to ignore in array_list.",
            "description": "Returns\n-------"
          },
          "out": {
            "type": "device",
            "description": "`device` object (see the \"Device Support\" section of the array API spec)."
          }
        },
        "returns": "-------\n    out : device\n        `device` object (see the \"Device Support\" section of the array API spec).",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "empirical_covariance",
      "signature": "empirical_covariance(X, *, assume_centered=False)",
      "documentation": {
        "description": "Compute the Maximum likelihood covariance estimator.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n        If `True`, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If `False`, data will be centered before computation.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).",
        "parameters": {
          "X": {
            "type": "ndarray of shape (n_samples, n_features)",
            "description": ""
          },
          "Data": {
            "type": "from which to compute the covariance estimate.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "`False`, data will be centered before computation.",
            "description": "Returns\n-------"
          },
          "Useful": {
            "type": "when working with data whose mean is almost, but not exactly",
            "description": "zero."
          },
          "covariance": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Empirical": {
            "type": "covariance (Maximum Likelihood Estimator).",
            "description": "Examples\n--------\n>>> from sklearn.covariance import empirical_covariance\n>>> X = [[1,1,1],[1,1,1],[1,1,1],\n...      [0,0,0],[0,0,0],[0,0,0]]\n>>> empirical_covariance(X)\narray([[0.25, 0.25, 0.25],\n[0.25, 0.25, 0.25],\n[0.25, 0.25, 0.25]])"
          }
        },
        "returns": "-------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).\n\n    Examples\n    --------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])"
      }
    },
    {
      "name": "get_namespace",
      "signature": "get_namespace(*arrays, remove_none=True, remove_types=(<class 'str'>,), xp=None)",
      "documentation": {
        "description": "Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.",
        "parameters": {
          "Array": {
            "type": "objects.",
            "description": ""
          },
          "remove_none": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to ignore None objects passed in arrays.",
            "description": ""
          },
          "remove_types": {
            "type": "tuple or list, default=(str,)",
            "description": ""
          },
          "Types": {
            "type": "to ignore in the arrays.",
            "description": ""
          },
          "xp": {
            "type": "module, default=None",
            "description": ""
          },
          "Precomputed": {
            "type": "array namespace module. When passed, typically from a caller",
            "description": ""
          },
          "that": {
            "type": "has already performed inspection of its own inputs, skips array",
            "description": ""
          },
          "namespace": {
            "type": "module",
            "description": ""
          },
          "Namespace": {
            "type": "shared by array objects. If any of the `arrays` are not arrays,",
            "description": ""
          },
          "the": {
            "type": "namespace defaults to NumPy.",
            "description": ""
          },
          "is_array_api_compliant": {
            "type": "bool",
            "description": ""
          },
          "True": {
            "type": "if the arrays are containers that implement the Array API spec.",
            "description": ""
          },
          "Always": {
            "type": "False when array_api_dispatch=False.",
            "description": ""
          }
        },
        "returns": "-------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.",
        "raises": "",
        "see_also": "",
        "notes": "that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, an instance of the `_NumPyAPIWrapper`\n    compatibility wrapper is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise an instance of the `_NumPyAPIWrapper` compatibility wrapper is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.",
        "examples": ""
      }
    },
    {
      "name": "ledoit_wolf",
      "signature": "ledoit_wolf(X, *, assume_centered=False, block_size=1000)",
      "documentation": {
        "description": "Estimate the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n        This is purely a memory optimization and does not affect results.\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of shape (n_samples, n_features)"
          },
          "Data": {
            "type": "from which to compute the covariance estimate.",
            "description": ""
          },
          "assume_centered": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "False, data will be centered before computation.",
            "description": ""
          },
          "Useful": {
            "type": "to work with data whose mean is significantly equal to",
            "description": ""
          },
          "zero": {
            "type": "but is not exactly zero.",
            "description": ""
          },
          "block_size": {
            "type": "int, default=1000",
            "description": ""
          },
          "Size": {
            "type": "of blocks into which the covariance matrix will be split.",
            "description": ""
          },
          "This": {
            "type": "is purely a memory optimization and does not affect results.",
            "description": "Returns\n-------"
          },
          "shrunk_cov": {
            "type": "ndarray of shape (n_features, n_features)",
            "description": ""
          },
          "Shrunk": {
            "type": "covariance.",
            "description": ""
          },
          "shrinkage": {
            "type": "float",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized (shrunk) covariance is:",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "mu = trace(cov) / n_features",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n>>> real_cov = np.array([[.4, .2], [.2, .8]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n>>> covariance, shrinkage = ledoit_wolf(X)\n>>> covariance\narray([[0.44..., 0.16...],\n[0.16..., 0.80...]])\n>>> shrinkage\nnp.float64(0.23...)"
          }
        },
        "returns": "-------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.covariance import empirical_covariance, ledoit_wolf\n    >>> real_cov = np.array([[.4, .2], [.2, .8]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=50)\n    >>> covariance, shrinkage = ledoit_wolf(X)\n    >>> covariance\n    array([[0.44..., 0.16...],\n           [0.16..., 0.80...]])\n    >>> shrinkage\n    np.float64(0.23...)"
      }
    },
    {
      "name": "shrunk_covariance",
      "signature": "shrunk_covariance(emp_cov, shrinkage=0.1)",
      "documentation": {
        "description": "Calculate covariance matrices shrunk on the diagonal.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : array-like of shape (..., n_features, n_features)\n        Covariance matrices to be shrunk, at least 2D ndarray.\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (..., n_features, n_features)\n        Shrunk covariance matrices.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.",
        "parameters": {
          "emp_cov": {
            "type": "array",
            "description": "like of shape (..., n_features, n_features)"
          },
          "Covariance": {
            "type": "matrices to be shrunk, at least 2D ndarray.",
            "description": ""
          },
          "shrinkage": {
            "type": "float, default=0.1",
            "description": ""
          },
          "Coefficient": {
            "type": "in the convex combination used for the computation",
            "description": ""
          },
          "of": {
            "type": "the shrunk estimate. Range is [0, 1].",
            "description": "Returns\n-------"
          },
          "shrunk_cov": {
            "type": "ndarray of shape (..., n_features, n_features)",
            "description": ""
          },
          "Shrunk": {
            "type": "covariance matrices.",
            "description": "Notes\n-----"
          },
          "The": {
            "type": "regularized (shrunk) covariance is given by::",
            "description": "(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)"
          },
          "where": {
            "type": "`mu = trace(cov) / n_features`.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n>>> real_cov = np.array([[.8, .3], [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n>>> shrunk_covariance(empirical_covariance(X))\narray([[0.73..., 0.25...],\n[0.25..., 0.41...]])"
          }
        },
        "returns": "-------\n    shrunk_cov : ndarray of shape (..., n_features, n_features)\n        Shrunk covariance matrices.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    The regularized (shrunk) covariance is given by::\n\n        (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where `mu = trace(cov) / n_features`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> from sklearn.covariance import empirical_covariance, shrunk_covariance\n    >>> real_cov = np.array([[.8, .3], [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0], cov=real_cov, size=500)\n    >>> shrunk_covariance(empirical_covariance(X))\n    array([[0.73..., 0.25...],\n           [0.25..., 0.41...]])"
      }
    },
    {
      "name": "size",
      "signature": "size(x)",
      "documentation": {
        "description": "Return the total number of elements of x.\n\n    Parameters\n    ----------\n    x : array\n        Array instance from NumPy or an array API compatible library.",
        "parameters": {
          "x": {
            "type": "array",
            "description": ""
          },
          "Array": {
            "type": "instance from NumPy or an array API compatible library.",
            "description": "Returns\n-------"
          },
          "out": {
            "type": "int",
            "description": ""
          },
          "Total": {
            "type": "number of elements.",
            "description": ""
          }
        },
        "returns": "-------\n    out : int\n        Total number of elements.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "softmax",
      "signature": "softmax(X, copy=True)",
      "documentation": {
        "description": "Calculate the softmax function.\n\n    The softmax function is calculated by\n    np.exp(X) / np.sum(np.exp(X), axis=1)\n\n    This will cause overflow when large values are exponentiated.\n    Hence the largest value in each row is subtracted from each data\n    point to prevent this.\n\n    Parameters\n    ----------\n    X : array-like of float of shape (M, N)\n        Argument to the logistic function.\n\n    copy : bool, default=True\n        Copy X or not.",
        "parameters": {
          "X": {
            "type": "array",
            "description": "like of float of shape (M, N)"
          },
          "Argument": {
            "type": "to the logistic function.",
            "description": ""
          },
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "Copy": {
            "type": "X or not.",
            "description": "Returns\n-------"
          },
          "out": {
            "type": "ndarray of shape (M, N)",
            "description": ""
          },
          "Softmax": {
            "type": "function evaluated at every point in x.",
            "description": ""
          }
        },
        "returns": "-------\n    out : ndarray of shape (M, N)\n        Softmax function evaluated at every point in x.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "unique_labels",
      "signature": "unique_labels(*ys)",
      "documentation": {
        "description": "Extract an ordered array of unique labels.\n\n    We don't allow:\n        - mix of multilabel and multiclass (single label) targets\n        - mix of label indicator matrix and anything else,\n          because there are no explicit labels)\n        - mix of label indicator matrices of different sizes\n        - mix of string and integer labels\n\n    At the moment, we also don't allow \"multiclass-multioutput\" input type.\n\n    Parameters\n    ----------\n    *ys : array-likes\n        Label values.\n\n    Returns\n    -------\n    out : ndarray of shape (n_unique_labels,)\n        An ordered array of unique labels.",
        "parameters": {
          "Label": {
            "type": "values.",
            "description": "Returns\n-------"
          },
          "out": {
            "type": "ndarray of shape (n_unique_labels,)",
            "description": ""
          },
          "An": {
            "type": "ordered array of unique labels.",
            "description": "Examples\n--------\n>>> from sklearn.utils.multiclass import unique_labels\n>>> unique_labels([3, 5, 5, 5, 7, 7])"
          },
          "array": {
            "type": "[ 1,  2,  5, 10, 11]",
            "description": ""
          }
        },
        "returns": "-------\n    out : ndarray of shape (n_unique_labels,)\n        An ordered array of unique labels.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import unique_labels\n    >>> unique_labels([3, 5, 5, 5, 7, 7])\n    array([3, 5, 7])\n    >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\n    array([1, 2, 3, 4])\n    >>> unique_labels([1, 2, 10], [5, 11])\n    array([ 1,  2,  5, 10, 11])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.multiclass import unique_labels\n    >>> unique_labels([3, 5, 5, 5, 7, 7])\n    array([3, 5, 7])\n    >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\n    array([1, 2, 3, 4])\n    >>> unique_labels([1, 2, 10], [5, 11])\n    array([ 1,  2,  5, 10, 11])"
      }
    },
    {
      "name": "validate_data",
      "signature": "validate_data(_estimator, /, X='no_validation', y='no_validation', reset=True, validate_separately=False, skip_check_array=False, **check_params)",
      "documentation": {
        "description": "Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape             (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.",
        "parameters": {
          "_estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "The": {
            "type": "targets.",
            "description": "- If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If"
          },
          "X": {
            "type": "{array",
            "description": "like, sparse matrix, dataframe} of shape             (n_samples, n_features), default='no validation'"
          },
          "If": {
            "type": "`True`, `X` and `y` are unchanged and only `feature_names_in_` and",
            "description": "`n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`"
          },
          "useful": {
            "type": "for meta-estimator which can delegate input validation to",
            "description": ""
          },
          "their": {
            "type": "underlying estimator(s). In that case `y` must be passed and",
            "description": ""
          },
          "the": {
            "type": "estimator's `requires_y` tag is True, then an error will be raised.",
            "description": "- If `'no_validation'`, :func:`~sklearn.utils.check_array` is called"
          },
          "y": {
            "type": "array",
            "description": "like of shape (n_samples,), default='no_validation'"
          },
          "on": {
            "type": "`X` and the estimator's `requires_y` tag is ignored. This is a default",
            "description": ""
          },
          "placeholder": {
            "type": "and is never meant to be explicitly set. In that case `X` must be",
            "description": "passed.\n- Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with"
          },
          "either": {
            "type": "func:`~sklearn.utils.check_array` or",
            "description": ":func:`~sklearn.utils.check_X_y` depending on `validate_separately`."
          },
          "reset": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "to reset the `n_features_in_` attribute.",
            "description": ""
          },
          "provided": {
            "type": "when reset was last True.",
            "description": ".. note::"
          },
          "It": {
            "type": "is recommended to call `reset=True` in `fit` and in the first",
            "description": ""
          },
          "call": {
            "type": "to `partial_fit`. All other methods that validate `X`",
            "description": ""
          },
          "should": {
            "type": "set `reset=False`.",
            "description": ""
          },
          "validate_separately": {
            "type": "False or tuple of dicts, default=False",
            "description": ""
          },
          "Only": {
            "type": "used if `y` is not `None`.",
            "description": ""
          },
          "kwargs": {
            "type": "to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`",
            "description": "respectively.\n`estimator=self` is automatically added to these dicts to generate"
          },
          "more": {
            "type": "informative error message in case of invalid input data.",
            "description": ""
          },
          "skip_check_array": {
            "type": "bool, default=False",
            "description": ""
          },
          "is": {
            "type": "called on `X` and `y`.",
            "description": "**check_params : kwargs"
          }
        },
        "returns": "-------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "BaseEstimator",
      "documentation": {
        "description": "Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).",
        "parameters": {
          "array": {
            "type": "[3, 3, 3]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])"
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ClassNamePrefixFeaturesOutMixin",
      "documentation": {
        "description": "Mixin class for transformers that generate their own names by prefixing.\n\n    This mixin is useful when the transformer needs to generate its own feature\n    names out, such as :class:`~sklearn.decomposition.PCA`. For example, if\n    :class:`~sklearn.decomposition.PCA` outputs 3 features, then the generated feature\n    names out are: `[\"pca0\", \"pca1\", \"pca2\"]`.\n\n    This mixin assumes that a `_n_features_out` attribute is defined when the\n    transformer is fitted. `_n_features_out` is the number of output features\n    that the transformer will return in `transform` of `fit_transform`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import ClassNamePrefixFeaturesOutMixin, BaseEstimator\n    >>> class MyEstimator(ClassNamePrefixFeaturesOutMixin, BaseEstimator):\n    ...     def fit(self, X, y=None):\n    ...         self._n_features_out = X.shape[1]\n    ...         return self\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> MyEstimator().fit(X).get_feature_names_out()\n    array(['myestimator0', 'myestimator1'], dtype=object)"
      },
      "methods": [
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ClassifierMixin",
      "documentation": {
        "description": "Mixin class for all classifiers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - set estimator type to `\"classifier\"` through the `estimator_type` tag;\n    - `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n      which is done by setting the classifier type tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClassifierMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(ClassifierMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=1)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([1, 1, 1])\n    >>> estimator.score(X, y)\n    0.66..."
      },
      "methods": [
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DiscriminantAnalysisPredictionMixin",
      "documentation": {
        "description": "Mixin class for QuadraticDiscriminantAnalysis and NearestCentroid.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Apply decision function to an array of samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array of samples (test vectors).",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples (test vectors).",
                "description": "Returns\n-------"
              },
              "y_scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Decision": {
                "type": "function values related to each class, per sample.",
                "description": ""
              },
              "In": {
                "type": "the two-class case, the shape is `(n_samples,)`, giving the",
                "description": ""
              },
              "log": {
                "type": "likelihood ratio of the positive class.",
                "description": ""
              }
            },
            "returns": "-------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Perform classification on an array of vectors `X`.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "vectors, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Class": {
                "type": "label for each sample.",
                "description": ""
              }
            },
            "returns": "the class label for each sample.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Estimate log class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y_log_proba": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Estimated": {
                "type": "log probabilities.",
                "description": ""
              }
            },
            "returns": "-------\n        y_log_proba : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Estimate class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "y_proba": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Probability": {
                "type": "estimate of the sample for each class in the",
                "description": "model, where classes are ordered as they are in `self.classes_`."
              }
            },
            "returns": "-------\n        y_proba : ndarray of shape (n_samples, n_classes)\n            Probability estimate of the sample for each class in the\n            model, where classes are ordered as they are in `self.classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HasMethods",
      "documentation": {
        "description": "Constraint representing objects that expose specific methods.\n\n    It is useful for parameters following a protocol and where we don't want to impose\n    an affiliation to a specific module or class.",
        "parameters": {
          "methods": {
            "type": "str or list of str",
            "description": ""
          },
          "The": {
            "type": "method(s) that the object is expected to expose.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Integral",
      "documentation": {
        "description": "Integral adds methods that work on integral numbers.\n\n    In short, these are conversion to int, pow with modulus, and the\n    bit-string operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interval",
      "documentation": {
        "description": "Constraint representing a typed interval.\n\n    Parameters\n    ----------\n    type : {numbers.Integral, numbers.Real, RealNotInt}\n        The set of numbers in which to set the interval.\n\n        If RealNotInt, only reals that don't have the integer type\n        are allowed. For example 1.0 is allowed but 1 is not.\n\n    left : float or int or None\n        The left bound of the interval. None means left bound is -∞.\n\n    right : float, int or None\n        The right bound of the interval. None means right bound is +∞.\n\n    closed : {\"left\", \"right\", \"both\", \"neither\"}\n        Whether the interval is open or closed. Possible choices are:\n\n        - `\"left\"`: the interval is closed on the left and open on the right.\n          It is equivalent to the interval `[ left, right )`.\n        - `\"right\"`: the interval is closed on the right and open on the left.\n          It is equivalent to the interval `( left, right ]`.\n        - `\"both\"`: the interval is closed.\n          It is equivalent to the interval `[ left, right ]`.\n        - `\"neither\"`: the interval is open.\n          It is equivalent to the interval `( left, right )`.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": ""
          },
          "The": {
            "type": "right bound of the interval. None means right bound is +∞.",
            "description": ""
          },
          "If": {
            "type": "RealNotInt, only reals that don't have the integer type",
            "description": ""
          },
          "are": {
            "type": "allowed. For example 1.0 is allowed but 1 is not.",
            "description": ""
          },
          "left": {
            "type": "float or int or None",
            "description": ""
          },
          "right": {
            "type": "float, int or None",
            "description": ""
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": ""
          },
          "Whether": {
            "type": "the interval is open or closed. Possible choices are:",
            "description": "- `\"left\"`: the interval is closed on the left and open on the right."
          },
          "It": {
            "type": "is equivalent to the interval `( left, right )`.",
            "description": "Notes\n-----"
          },
          "Setting": {
            "type": "a bound to `None` and setting the interval closed is valid. For instance,",
            "description": ""
          },
          "strictly": {
            "type": "speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to",
            "description": "`[0, +∞) U {+∞}`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    Setting a bound to `None` and setting the interval closed is valid. For instance,\n    strictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n    `[0, +∞) U {+∞}`.",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LinearClassifierMixin",
      "documentation": {
        "description": "Mixin for linear classifiers.\n\n    Handles prediction for sparse and dense X.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LinearDiscriminantAnalysis",
      "documentation": {
        "description": "Linear Discriminant Analysis.\n\n    A classifier with a linear decision boundary, generated by fitting class\n    conditional densities to the data and using Bayes' rule.\n\n    The model fits a Gaussian density to each class, assuming that all classes\n    share the same covariance matrix.\n\n    The fitted model can also be used to reduce the dimensionality of the input\n    by projecting it to the most discriminative directions, using the\n    `transform` method.\n\n    .. versionadded:: 0.17\n\n    For a comparison between\n    :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`\n    and :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`, see\n    :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`.\n\n    Read more in the :ref:`User Guide <lda_qda>`.\n\n    Parameters\n    ----------\n    solver : {'svd', 'lsqr', 'eigen'}, default='svd'\n        Solver to use, possible values:\n          - 'svd': Singular value decomposition (default).\n            Does not compute the covariance matrix, therefore this solver is\n            recommended for data with a large number of features.\n          - 'lsqr': Least squares solution.\n            Can be combined with shrinkage or custom covariance estimator.\n          - 'eigen': Eigenvalue decomposition.\n            Can be combined with shrinkage or custom covariance estimator.\n\n        .. versionchanged:: 1.2\n            `solver=\"svd\"` now has experimental Array API support. See the\n            :ref:`Array API User Guide <array_api>` for more details.\n\n    shrinkage : 'auto' or float, default=None\n        Shrinkage parameter, possible values:\n          - None: no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        This should be left to None if `covariance_estimator` is used.\n        Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\n\n        For a usage example, see\n        :ref:`sphx_glr_auto_examples_classification_plot_lda.py`.\n\n    priors : array-like of shape (n_classes,), default=None\n        The class prior probabilities. By default, the class proportions are\n        inferred from the training data.\n\n    n_components : int, default=None\n        Number of components (<= min(n_classes - 1, n_features)) for\n        dimensionality reduction. If None, will be set to\n        min(n_classes - 1, n_features). This parameter only affects the\n        `transform` method.\n\n        For a usage example, see\n        :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`.\n\n    store_covariance : bool, default=False\n        If True, explicitly compute the weighted within-class covariance\n        matrix when solver is 'svd'. The matrix is always computed\n        and stored for the other solvers.\n\n        .. versionadded:: 0.17\n\n    tol : float, default=1.0e-4\n        Absolute threshold for a singular value of X to be considered\n        significant, used to estimate the rank of X. Dimensions whose\n        singular values are non-significant are discarded. Only used if\n        solver is 'svd'.\n\n        .. versionadded:: 0.17\n\n    covariance_estimator : covariance estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying on the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in :mod:`sklearn.covariance`.\n        if None the shrinkage parameter drives the estimate.\n\n        This should be left to None if `shrinkage` is used.\n        Note that `covariance_estimator` works only with 'lsqr' and 'eigen'\n        solvers.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (n_features,) or (n_classes, n_features)\n        Weight vector(s).\n\n    intercept_ : ndarray of shape (n_classes,)\n        Intercept term.\n\n    covariance_ : array-like of shape (n_features, n_features)\n        Weighted within-class covariance matrix. It corresponds to\n        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the\n        samples in class `k`. The `C_k` are estimated using the (potentially\n        shrunk) biased estimator of covariance. If solver is 'svd', only\n        exists when `store_covariance` is True.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If ``n_components`` is not set then all components are stored and the\n        sum of explained variances is equal to 1.0. Only available when eigen\n        or svd solver is used.\n\n    means_ : array-like of shape (n_classes, n_features)\n        Class-wise means.\n\n    priors_ : array-like of shape (n_classes,)\n        Class priors (sum to 1).\n\n    scalings_ : array-like of shape (rank, n_classes - 1)\n        Scaling of the features in the space spanned by the class centroids.\n        Only available for 'svd' and 'eigen' solvers.\n\n    xbar_ : array-like of shape (n_features,)\n        Overall mean. Only present if solver is 'svd'.\n\n    classes_ : array-like of shape (n_classes,)\n        Unique class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    QuadraticDiscriminantAnalysis : Quadratic Discriminant Analysis.",
        "parameters": {
          "solver": {
            "type": "is 'svd'.",
            "description": ".. versionadded:: 0.17"
          },
          "Solver": {
            "type": "to use, possible values:",
            "description": "- 'svd': Singular value decomposition (default)."
          },
          "Does": {
            "type": "not compute the covariance matrix, therefore this solver is",
            "description": ""
          },
          "recommended": {
            "type": "for data with a large number of features.",
            "description": "- 'lsqr': Least squares solution."
          },
          "Can": {
            "type": "be combined with shrinkage or custom covariance estimator.",
            "description": ".. versionchanged:: 1.2\n`solver=\"svd\"` now has experimental Array API support. See the\n:ref:`Array API User Guide <array_api>` for more details."
          },
          "shrinkage": {
            "type": "'auto' or float, default=None",
            "description": ""
          },
          "Shrinkage": {
            "type": "parameter, possible values:",
            "description": "- None: no shrinkage (default).\n- 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n- float between 0 and 1: fixed shrinkage parameter."
          },
          "This": {
            "type": "should be left to None if `shrinkage` is used.",
            "description": ""
          },
          "Note": {
            "type": "that `covariance_estimator` works only with 'lsqr' and 'eigen'",
            "description": "solvers.\n.. versionadded:: 0.24\nAttributes\n----------"
          },
          "For": {
            "type": "a usage example, see",
            "description": ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`."
          },
          "priors": {
            "type": "array",
            "description": "like of shape (n_classes,), default=None"
          },
          "The": {
            "type": "object should have a fit method and a ``covariance_`` attribute",
            "description": ""
          },
          "inferred": {
            "type": "from the training data.",
            "description": ""
          },
          "n_components": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "dimensionality": {
            "type": "reduction. If None, will be set to",
            "description": ""
          },
          "min": {
            "type": "n_classes - 1, n_features",
            "description": ". This parameter only affects the\n`transform` method."
          },
          "store_covariance": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "``n_components`` is not set then all components are stored and the",
            "description": ""
          },
          "matrix": {
            "type": "when solver is 'svd'. The matrix is always computed",
            "description": ""
          },
          "and": {
            "type": "stored for the other solvers.",
            "description": ".. versionadded:: 0.17"
          },
          "tol": {
            "type": "float, default=1.0e",
            "description": "4"
          },
          "Absolute": {
            "type": "threshold for a singular value of X to be considered",
            "description": "significant, used to estimate the rank of X. Dimensions whose"
          },
          "singular": {
            "type": "values are non-significant are discarded. Only used if",
            "description": ""
          },
          "covariance_estimator": {
            "type": "covariance estimator, default=None",
            "description": ""
          },
          "the": {
            "type": "covariance matrices instead of relying on the empirical",
            "description": ""
          },
          "covariance": {
            "type": "estimator (with potential shrinkage).",
            "description": ""
          },
          "like": {
            "type": "the estimators in :mod:`sklearn.covariance`.",
            "description": ""
          },
          "if": {
            "type": "None the shrinkage parameter drives the estimate.",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (n_features,) or (n_classes, n_features)",
            "description": ""
          },
          "Weight": {
            "type": "vector(s).",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "Intercept": {
            "type": "term.",
            "description": ""
          },
          "covariance_": {
            "type": "array",
            "description": "like of shape (n_features, n_features)"
          },
          "Weighted": {
            "type": "within-class covariance matrix. It corresponds to",
            "description": "`sum_k prior_k * C_k` where `C_k` is the covariance matrix of the"
          },
          "samples": {
            "type": "in class `k`. The `C_k` are estimated using the (potentially",
            "description": "shrunk) biased estimator of covariance. If solver is 'svd', only"
          },
          "exists": {
            "type": "when `store_covariance` is True.",
            "description": ""
          },
          "explained_variance_ratio_": {
            "type": "ndarray of shape (n_components,)",
            "description": ""
          },
          "Percentage": {
            "type": "of variance explained by each of the selected components.",
            "description": ""
          },
          "sum": {
            "type": "of explained variances is equal to 1.0. Only available when eigen",
            "description": ""
          },
          "or": {
            "type": "svd solver is used.",
            "description": ""
          },
          "means_": {
            "type": "array",
            "description": "like of shape (n_classes, n_features)\nClass-wise means."
          },
          "priors_": {
            "type": "array",
            "description": "like of shape (n_classes,)"
          },
          "Class": {
            "type": "priors (sum to 1).",
            "description": ""
          },
          "scalings_": {
            "type": "array",
            "description": "like of shape (rank, n_classes - 1)"
          },
          "Scaling": {
            "type": "of the features in the space spanned by the class centroids.",
            "description": ""
          },
          "Only": {
            "type": "available for 'svd' and 'eigen' solvers.",
            "description": ""
          },
          "xbar_": {
            "type": "array",
            "description": "like of shape (n_features,)"
          },
          "Overall": {
            "type": "mean. Only present if solver is 'svd'.",
            "description": ""
          },
          "classes_": {
            "type": "array",
            "description": "like of shape (n_classes,)"
          },
          "Unique": {
            "type": "class labels.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "QuadraticDiscriminantAnalysis": {
            "type": "Quadratic Discriminant Analysis.",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = LinearDiscriminantAnalysis()\n>>> clf.fit(X, y)"
          },
          "LinearDiscriminantAnalysis": {
            "type": "",
            "description": ">>> print(clf.predict([[-0.8, -1]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    QuadraticDiscriminantAnalysis : Quadratic Discriminant Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = LinearDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    LinearDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]",
        "notes": "that shrinkage works only with 'lsqr' and 'eigen' solvers.\n\n        For a usage example, see\n        :ref:`sphx_glr_auto_examples_classification_plot_lda.py`.\n\n    priors : array-like of shape (n_classes,), default=None\n        The class prior probabilities. By default, the class proportions are\n        inferred from the training data.\n\n    n_components : int, default=None\n        Number of components (<= min(n_classes - 1, n_features)) for\n        dimensionality reduction. If None, will be set to\n        min(n_classes - 1, n_features). This parameter only affects the\n        `transform` method.\n\n        For a usage example, see\n        :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`.\n\n    store_covariance : bool, default=False\n        If True, explicitly compute the weighted within-class covariance\n        matrix when solver is 'svd'. The matrix is always computed\n        and stored for the other solvers.\n\n        .. versionadded:: 0.17\n\n    tol : float, default=1.0e-4\n        Absolute threshold for a singular value of X to be considered\n        significant, used to estimate the rank of X. Dimensions whose\n        singular values are non-significant are discarded. Only used if\n        solver is 'svd'.\n\n        .. versionadded:: 0.17\n\n    covariance_estimator : covariance estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying on the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in :mod:`sklearn.covariance`.\n        if None the shrinkage parameter drives the estimate.\n\n        This should be left to None if `shrinkage` is used.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = LinearDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    LinearDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples (test vectors).",
                "description": "Returns\n-------"
              },
              "y_scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Decision": {
                "type": "function values related to each class, per sample.",
                "description": ""
              },
              "In": {
                "type": "the two-class case, the shape is `(n_samples,)`, giving the",
                "description": ""
              },
              "log": {
                "type": "likelihood ratio of the positive class.",
                "description": ""
              }
            },
            "returns": "-------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the Linear Discriminant Analysis model.\n\n        .. versionchanged:: 0.19\n            `store_covariance` and `tol` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        The feature names out will prefixed by the lowercased class name. For\n        example, if the transformer outputs 3 features, then the feature names\n        out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Only used to validate feature names with the names seen in `fit`.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Only": {
                "type": "used to validate feature names with the names seen in `fit`.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Transformed": {
                "type": "feature names.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Estimate log probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Estimated": {
                "type": "log probabilities.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Estimate probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Estimated": {
                "type": "probabilities.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.discriminant_analysis.LinearDiscriminantAnalysis, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X)",
          "documentation": {
            "description": "Project data to maximize class separation.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "data.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray of shape (n_samples, n_components) or             (n_samples, min(rank, n_components))",
                "description": ""
              },
              "Transformed": {
                "type": "data. In the case of the 'svd' solver, the shape",
                "description": ""
              },
              "is": {
                "type": "n_samples, min(rank, n_components",
                "description": ")."
              }
            },
            "returns": "-------\n        X_new : ndarray of shape (n_samples, n_components) or             (n_samples, min(rank, n_components))\n            Transformed data. In the case of the 'svd' solver, the shape\n            is (n_samples, min(rank, n_components)).",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "QuadraticDiscriminantAnalysis",
      "documentation": {
        "description": "Quadratic Discriminant Analysis.\n\n    A classifier with a quadratic decision boundary, generated\n    by fitting class conditional densities to the data\n    and using Bayes' rule.\n\n    The model fits a Gaussian density to each class.\n\n    .. versionadded:: 0.17\n\n    For a comparison between\n    :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`\n    and :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, see\n    :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`.\n\n    Read more in the :ref:`User Guide <lda_qda>`.\n\n    Parameters\n    ----------\n    priors : array-like of shape (n_classes,), default=None\n        Class priors. By default, the class proportions are inferred from the\n        training data.\n\n    reg_param : float, default=0.0\n        Regularizes the per-class covariance estimates by transforming S2 as\n        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,\n        where S2 corresponds to the `scaling_` attribute of a given class.\n\n    store_covariance : bool, default=False\n        If True, the class covariance matrices are explicitly computed and\n        stored in the `self.covariance_` attribute.\n\n        .. versionadded:: 0.17\n\n    tol : float, default=1.0e-4\n        Absolute threshold for the covariance matrix to be considered rank\n        deficient after applying some regularization (see `reg_param`) to each\n        `Sk` where `Sk` represents covariance matrix for k-th class. This\n        parameter does not affect the predictions. It controls when a warning\n        is raised if the covariance matrix is not full rank.\n\n        .. versionadded:: 0.17\n\n    Attributes\n    ----------\n    covariance_ : list of len n_classes of ndarray             of shape (n_features, n_features)\n        For each class, gives the covariance matrix estimated using the\n        samples of that class. The estimations are unbiased. Only present if\n        `store_covariance` is True.\n\n    means_ : array-like of shape (n_classes, n_features)\n        Class-wise means.\n\n    priors_ : array-like of shape (n_classes,)\n        Class priors (sum to 1).\n\n    rotations_ : list of len n_classes of ndarray of shape (n_features, n_k)\n        For each class k an array of shape (n_features, n_k), where\n        ``n_k = min(n_features, number of elements in class k)``\n        It is the rotation of the Gaussian distribution, i.e. its\n        principal axis. It corresponds to `V`, the matrix of eigenvectors\n        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered\n        matrix of samples from class k.\n\n    scalings_ : list of len n_classes of ndarray of shape (n_k,)\n        For each class, contains the scaling of\n        the Gaussian distributions along its principal axes, i.e. the\n        variance in the rotated coordinate system. It corresponds to `S^2 /\n        (n_samples - 1)`, where `S` is the diagonal matrix of singular values\n        from the SVD of `Xk`, where `Xk` is the centered matrix of samples\n        from class k.\n\n    classes_ : ndarray of shape (n_classes,)\n        Unique class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    LinearDiscriminantAnalysis : Linear Discriminant Analysis.",
        "parameters": {
          "priors": {
            "type": "array",
            "description": "like of shape (n_classes,), default=None"
          },
          "Class": {
            "type": "priors (sum to 1).",
            "description": ""
          },
          "training": {
            "type": "data.",
            "description": ""
          },
          "reg_param": {
            "type": "float, default=0.0",
            "description": ""
          },
          "Regularizes": {
            "type": "the per-class covariance estimates by transforming S2 as",
            "description": "``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,"
          },
          "where": {
            "type": "S2 corresponds to the `scaling_` attribute of a given class.",
            "description": ""
          },
          "store_covariance": {
            "type": "bool, default=False",
            "description": ""
          },
          "If": {
            "type": "True, the class covariance matrices are explicitly computed and",
            "description": ""
          },
          "stored": {
            "type": "in the `self.covariance_` attribute.",
            "description": ".. versionadded:: 0.17"
          },
          "tol": {
            "type": "float, default=1.0e",
            "description": "4"
          },
          "Absolute": {
            "type": "threshold for the covariance matrix to be considered rank",
            "description": ""
          },
          "deficient": {
            "type": "after applying some regularization (see `reg_param`) to each",
            "description": "`Sk` where `Sk` represents covariance matrix for k-th class. This"
          },
          "parameter": {
            "type": "does not affect the predictions. It controls when a warning",
            "description": ""
          },
          "is": {
            "type": "raised if the covariance matrix is not full rank.",
            "description": ".. versionadded:: 0.17\nAttributes\n----------"
          },
          "covariance_": {
            "type": "list of len n_classes of ndarray             of shape (n_features, n_features)",
            "description": ""
          },
          "For": {
            "type": "each class, contains the scaling of",
            "description": ""
          },
          "samples": {
            "type": "of that class. The estimations are unbiased. Only present if",
            "description": "`store_covariance` is True."
          },
          "means_": {
            "type": "array",
            "description": "like of shape (n_classes, n_features)\nClass-wise means."
          },
          "priors_": {
            "type": "array",
            "description": "like of shape (n_classes,)"
          },
          "rotations_": {
            "type": "list of len n_classes of ndarray of shape (n_features, n_k)",
            "description": ""
          },
          "It": {
            "type": "is the rotation of the Gaussian distribution, i.e. its",
            "description": ""
          },
          "principal": {
            "type": "axis. It corresponds to `V`, the matrix of eigenvectors",
            "description": ""
          },
          "coming": {
            "type": "from the SVD of `Xk = U S Vt` where `Xk` is the centered",
            "description": ""
          },
          "matrix": {
            "type": "of samples from class k.",
            "description": ""
          },
          "scalings_": {
            "type": "list of len n_classes of ndarray of shape (n_k,)",
            "description": ""
          },
          "the": {
            "type": "Gaussian distributions along its principal axes, i.e. the",
            "description": ""
          },
          "variance": {
            "type": "in the rotated coordinate system. It corresponds to `S^2 /",
            "description": "(n_samples - 1)`, where `S` is the diagonal matrix of singular values"
          },
          "from": {
            "type": "class k.",
            "description": ""
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "Unique": {
            "type": "class labels.",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "LinearDiscriminantAnalysis": {
            "type": "Linear Discriminant Analysis.",
            "description": "Examples\n--------\n>>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = QuadraticDiscriminantAnalysis()\n>>> clf.fit(X, y)"
          },
          "QuadraticDiscriminantAnalysis": {
            "type": "",
            "description": ">>> print(clf.predict([[-0.8, -1]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    LinearDiscriminantAnalysis : Linear Discriminant Analysis.\n\n    Examples\n    --------\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    QuadraticDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    QuadraticDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples (test vectors).",
                "description": "Returns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Decision": {
                "type": "function values related to each class, per sample.",
                "description": ""
              },
              "In": {
                "type": "the two-class case, the shape is `(n_samples,)`, giving the",
                "description": ""
              },
              "log": {
                "type": "likelihood ratio of the positive class.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y)",
          "documentation": {
            "description": "Fit the model according to the given training data and parameters.\n\n        .. versionchanged:: 0.19\n            ``store_covariances`` has been moved to main constructor as\n            ``store_covariance``.\n\n        .. versionchanged:: 0.19\n            ``tol`` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers).",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values (integers).",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Perform classification on an array of test vectors X.\n\n        The predicted class C for each sample in X is returned.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Vector": {
                "type": "to be scored, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features.\nReturns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Estimated": {
                "type": "probabilities.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples,)\n            Estimated probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "documentation": {
            "description": "Return log of posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples/test vectors.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples/test vectors.",
                "description": "Returns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Posterior": {
                "type": "log-probabilities of classification per class.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples, n_classes)\n            Posterior log-probabilities of classification per class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Return posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples/test vectors.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Array": {
                "type": "of samples/test vectors.",
                "description": "Returns\n-------"
              },
              "C": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              },
              "Posterior": {
                "type": "probabilities of classification per class.",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples, n_classes)\n            Posterior probabilities of classification per class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Real",
      "documentation": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\n    In short, those are: a conversion to float, trunc(), divmod,\n    %, <, <=, >, and >=.\n\n    Real also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StandardScaler",
      "documentation": {
        "description": "Standardize features by removing the mean and scaling to unit variance.\n\n    The standard score of a sample `x` is calculated as:\n\n    .. code-block:: text\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using\n    :meth:`transform`.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    than others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    `StandardScaler` is sensitive to outliers, and the features may scale\n    differently from each other in the presence of outliers. For an example\n    visualization, refer to :ref:`Compare StandardScaler with other scalers\n    <plot_all_scaling_standard_scaler_section>`.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    with_mean : bool, default=True\n        If True, center the data before scaling.\n        This does not work (and will raise an exception) when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_std : bool, default=True\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    Attributes\n    ----------\n    scale_ : ndarray of shape (n_features,) or None\n        Per feature relative scaling of the data to achieve zero mean and unit\n        variance. Generally this is calculated using `np.sqrt(var_)`. If a\n        variance is zero, we can't achieve unit variance, and the data is left\n        as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n        when `with_std=False`.\n\n        .. versionadded:: 0.17\n           *scale_*\n\n    mean_ : ndarray of shape (n_features,) or None\n        The mean value for each feature in the training set.\n        Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\n    var_ : ndarray of shape (n_features,) or None\n        The variance for each feature in the training set. Used to compute\n        `scale_`. Equal to ``None`` when ``with_mean=False`` and\n        ``with_std=False``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_seen_ : int or ndarray of shape (n_features,)\n        The number of samples processed by the estimator for each feature.\n        If there are no missing samples, the ``n_samples_seen`` will be an\n        integer, otherwise it will be an array of dtype int. If\n        `sample_weights` are used it will be a float (if no missing data)\n        or an array of dtype float that sums the weights seen so far.\n        Will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    See Also\n    --------\n    scale : Equivalent function without the estimator API.\n\n    :class:`~sklearn.decomposition.PCA` : Further removes the linear\n        correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.",
        "parameters": {
          "copy": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "there are no missing samples, the ``n_samples_seen`` will be an",
            "description": "integer, otherwise it will be an array of dtype int. If\n`sample_weights` are used it will be a float (if no missing data)"
          },
          "This": {
            "type": "does not work (and will raise an exception) when attempted on",
            "description": ""
          },
          "not": {
            "type": "a NumPy array or scipy.sparse CSR matrix, a copy may still be",
            "description": "returned."
          },
          "with_mean": {
            "type": "bool, default=True",
            "description": ""
          },
          "sparse": {
            "type": "matrices, because centering them entails building a dense",
            "description": ""
          },
          "matrix": {
            "type": "which in common use cases is likely to be too large to fit in",
            "description": "memory."
          },
          "with_std": {
            "type": "bool, default=True",
            "description": ""
          },
          "unit": {
            "type": "standard deviation).",
            "description": "Attributes\n----------"
          },
          "scale_": {
            "type": "ndarray of shape (n_features,) or None",
            "description": ""
          },
          "Per": {
            "type": "feature relative scaling of the data to achieve zero mean and unit",
            "description": "variance. Generally this is calculated using `np.sqrt(var_)`. If a"
          },
          "variance": {
            "type": "is zero, we can't achieve unit variance, and the data is left",
            "description": "as-is, giving a scaling factor of 1. `scale_` is equal to `None`"
          },
          "when": {
            "type": "`with_std=False`.",
            "description": ".. versionadded:: 0.17\n*scale_*"
          },
          "mean_": {
            "type": "ndarray of shape (n_features,) or None",
            "description": ""
          },
          "The": {
            "type": "number of samples processed by the estimator for each feature.",
            "description": ""
          },
          "Equal": {
            "type": "to ``None`` when ``with_mean=False`` and ``with_std=False``.",
            "description": ""
          },
          "var_": {
            "type": "ndarray of shape (n_features,) or None",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_samples_seen_": {
            "type": "int or ndarray of shape (n_features,)",
            "description": ""
          },
          "or": {
            "type": "an array of dtype float that sums the weights seen so far.",
            "description": ""
          },
          "Will": {
            "type": "be reset on new calls to fit, but increments across",
            "description": "``partial_fit`` calls."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "scale": {
            "type": "Equivalent function without the estimator API.",
            "description": ":class:`~sklearn.decomposition.PCA` : Further removes the linear"
          },
          "correlation": {
            "type": "across features with 'whiten=True'.",
            "description": "Notes\n-----"
          },
          "NaNs": {
            "type": "are treated as missing values: disregarded in fit, and maintained in",
            "description": "transform."
          },
          "We": {
            "type": "use a biased estimator for the standard deviation, equivalent to",
            "description": "`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to"
          },
          "affect": {
            "type": "model performance.",
            "description": "Examples\n--------\n>>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))"
          },
          "StandardScaler": {
            "type": "",
            "description": ">>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n[-1. -1.]\n[ 1.  1.]\n[ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    scale : Equivalent function without the estimator API.\n\n    :class:`~sklearn.decomposition.PCA` : Further removes the linear\n        correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]",
        "notes": "-----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]",
        "examples": "--------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Compute the mean and std to be used for later scaling.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n\n        y : None\n            Ignored.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.24\n               parameter *sample_weight* support to StandardScaler.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data used to compute the mean and standard deviation",
                "description": ""
              },
              "used": {
                "type": "for later scaling along the features axis.",
                "description": ""
              },
              "y": {
                "type": "None",
                "description": "Ignored."
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Individual": {
                "type": "weights for each sample.",
                "description": ".. versionadded:: 0.24"
              },
              "parameter": {
                "type": "*sample_weight* support to StandardScaler.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "scaler.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted scaler.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Input features.\n\n            - If `input_features` is `None`, then `feature_names_in_` is\n              used as feature names in. If `feature_names_in_` is not defined,\n              then the following input feature names are generated:\n              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n            - If `input_features` is an array-like, then `input_features` must\n              match `feature_names_in_` if `feature_names_in_` is defined.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None"
              },
              "Input": {
                "type": "features.",
                "description": "- If `input_features` is `None`, then `feature_names_in_` is"
              },
              "used": {
                "type": "as feature names in. If `feature_names_in_` is not defined,",
                "description": ""
              },
              "then": {
                "type": "the following input feature names are generated:",
                "description": "`[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n- If `input_features` is an array-like, then `input_features` must"
              },
              "match": {
                "type": "`feature_names_in_` if `feature_names_in_` is defined.",
                "description": "Returns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "Same": {
                "type": "as input features.",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            Same as input features.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, X, copy=None)",
          "documentation": {
            "description": "Scale back the data to the original representation.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to scale along the features axis.\n        copy : bool, default=None\n            Copy the input X or not.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data used to scale along the features axis.",
                "description": ""
              },
              "copy": {
                "type": "bool, default=None",
                "description": ""
              },
              "Copy": {
                "type": "the input X or not.",
                "description": "Returns\n-------"
              },
              "X_tr": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_features)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y=None, sample_weight=None)",
          "documentation": {
            "description": "Online computation of mean and std on X for later scaling.\n\n        All of X is processed as a single batch. This is intended for cases\n        when :meth:`fit` is not feasible due to very large number of\n        `n_samples` or because X is read from a continuous stream.\n\n        The algorithm for incremental mean and std is given in Equation 1.5a,b\n        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n        for computing the sample variance: Analysis and recommendations.\"\n        The American Statistician 37.3 (1983): 242-247:\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n\n        y : None\n            Ignored.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.24\n               parameter *sample_weight* support to StandardScaler.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data used to compute the mean and standard deviation",
                "description": ""
              },
              "used": {
                "type": "for later scaling along the features axis.",
                "description": ""
              },
              "y": {
                "type": "None",
                "description": "Ignored."
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Individual": {
                "type": "weights for each sample.",
                "description": ".. versionadded:: 0.24"
              },
              "parameter": {
                "type": "*sample_weight* support to StandardScaler.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Fitted": {
                "type": "scaler.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            Fitted scaler.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_inverse_transform_request",
          "signature": "set_inverse_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler",
          "documentation": {
            "description": "Request metadata passed to the ``inverse_transform`` method.",
            "parameters": {
              "copy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``copy`` parameter in ``inverse_transform``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``copy`` parameter in ``inverse_transform``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler",
          "documentation": {
            "description": "Request metadata passed to the ``partial_fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``partial_fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_transform_request",
          "signature": "set_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler",
          "documentation": {
            "description": "Request metadata passed to the ``transform`` method.",
            "parameters": {
              "copy": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``copy`` parameter in ``transform``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``copy`` parameter in ``transform``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, X, copy=None)",
          "documentation": {
            "description": "Perform standardization by centering and scaling.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix of shape (n_samples, n_features)\n            The data used to scale along the features axis.\n        copy : bool, default=None\n            Copy the input X or not.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data used to scale along the features axis.",
                "description": ""
              },
              "copy": {
                "type": "bool, default=None",
                "description": ""
              },
              "Copy": {
                "type": "the input X or not.",
                "description": "Returns\n-------"
              },
              "X_tr": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_features)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StrOptions",
      "documentation": {
        "description": "Constraint representing a finite set of strings.",
        "parameters": {
          "options": {
            "type": "set of str",
            "description": ""
          },
          "The": {
            "type": "set of valid strings.",
            "description": ""
          },
          "deprecated": {
            "type": "set of str or None, default=None",
            "description": ""
          },
          "A": {
            "type": "subset of the `options` to mark as deprecated in the string",
            "description": ""
          },
          "representation": {
            "type": "of the constraint.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "TransformerMixin",
      "documentation": {
        "description": "Mixin class for all transformers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - a `fit_transform` method that delegates to `fit` and `transform`;\n    - a `set_output` method to output `X` as a specific container type.\n\n    If :term:`get_feature_names_out` is defined, then :class:`BaseEstimator` will\n    automatically wrap `transform` and `fit_transform` to follow the `set_output`\n    API. See the :ref:`developer_api_set_output` for details.\n\n    :class:`OneToOneFeatureMixin` and\n    :class:`ClassNamePrefixFeaturesOutMixin` are helpful mixins for\n    defining :term:`get_feature_names_out`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, TransformerMixin\n    >>> class MyTransformer(TransformerMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         return self\n    ...     def transform(self, X):\n    ...         return np.full(shape=len(X), fill_value=self.param)\n    >>> transformer = MyTransformer()\n    >>> X = [[1, 2], [2, 3], [3, 4]]\n    >>> transformer.fit_transform(X)\n    array([1, 1, 1])"
      },
      "methods": [
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}