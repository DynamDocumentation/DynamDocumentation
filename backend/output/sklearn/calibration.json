{
  "description": "Methods for calibrating predicted probabilities.",
  "functions": [
    {
      "name": "calibration_curve",
      "signature": "calibration_curve(y_true, y_prob, *, pos_label=None, n_bins=5, strategy='uniform')",
      "documentation": {
        "description": "Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int, float, bool or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).",
        "parameters": {
          "y_true": {
            "type": "array",
            "description": "like of shape (n_samples,)"
          },
          "True": {
            "type": "targets.",
            "description": ""
          },
          "y_prob": {
            "type": "array",
            "description": "like of shape (n_samples,)"
          },
          "Probabilities": {
            "type": "With Supervised Learning, in Proceedings of the 22nd",
            "description": ""
          },
          "pos_label": {
            "type": "int, float, bool or str, default=None",
            "description": ""
          },
          "The": {
            "type": "mean predicted probability in each bin.",
            "description": "References\n----------"
          },
          "n_bins": {
            "type": "int, default=5",
            "description": ""
          },
          "Number": {
            "type": "of bins to discretize the [0, 1] interval. A bigger number",
            "description": ""
          },
          "requires": {
            "type": "more data. Bins with no samples (i.e. without",
            "description": ""
          },
          "corresponding": {
            "type": "values in `y_prob`) will not be returned, thus the",
            "description": ""
          },
          "returned": {
            "type": "arrays may have less than `n_bins` values.",
            "description": ""
          },
          "strategy": {
            "type": "{'uniform', 'quantile'}, default='uniform'",
            "description": ""
          },
          "Strategy": {
            "type": "used to define the widths of the bins.",
            "description": "uniform"
          },
          "prob_true": {
            "type": "ndarray of shape (n_bins,) or smaller",
            "description": ""
          },
          "bin": {
            "type": "fraction of positives",
            "description": "."
          },
          "prob_pred": {
            "type": "ndarray of shape (n_bins,) or smaller",
            "description": ""
          },
          "Alexandru": {
            "type": "Niculescu-Mizil and Rich Caruana (2005) Predicting Good",
            "description": ""
          },
          "International": {
            "type": "Conference on Machine Learning (ICML).",
            "description": ""
          },
          "See": {
            "type": "section 4 (Qualitative Analysis of Predictions).",
            "description": "Examples\n--------\n>>> import numpy as np\n>>> from sklearn.calibration import calibration_curve\n>>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n>>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n>>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n>>> prob_true"
          },
          "array": {
            "type": "[0.2  , 0.525, 0.85 ]",
            "description": ""
          }
        },
        "returns": "-------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])"
      }
    },
    {
      "name": "check_classification_targets",
      "signature": "check_classification_targets(y)",
      "documentation": {
        "description": "Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'",
        "parameters": {
          "y": {
            "type": "array",
            "description": "like"
          },
          "Target": {
            "type": "values.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "check_consistent_length",
      "signature": "check_consistent_length(*arrays)",
      "documentation": {
        "description": "Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.",
        "parameters": {
          "Objects": {
            "type": "that will be checked for consistent length.",
            "description": "Examples\n--------\n>>> from sklearn.utils.validation import check_consistent_length\n>>> a = [1, 2, 3]\n>>> b = [2, 3, 4]\n>>> check_consistent_length(a, b)"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.validation import check_consistent_length\n    >>> a = [1, 2, 3]\n    >>> b = [2, 3, 4]\n    >>> check_consistent_length(a, b)"
      }
    },
    {
      "name": "check_cv",
      "signature": "check_cv(cv=5, y=None, *, classifier=False)",
      "documentation": {
        "description": "Input checker utility for building a cross-validator.\n\n    Parameters\n    ----------\n    cv : int, cross-validation generator, iterable or None, default=5\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable that generates (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if classifier is True and ``y`` is either\n        binary or multiclass, :class:`StratifiedKFold` is used. In all other\n        cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value changed from 3-fold to 5-fold.\n\n    y : array-like, default=None\n        The target variable for supervised learning problems.\n\n    classifier : bool, default=False\n        Whether the task is a classification task, in which case\n        stratified KFold will be used.\n\n    Returns\n    -------\n    checked_cv : a cross-validator instance.\n        The return value is a cross-validator which generates the train/test\n        splits via the ``split`` method.",
        "parameters": {
          "cv": {
            "type": "int, cross",
            "description": "validation generator, iterable or None, default=5"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable that generates (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, if classifier is True and ``y`` is either",
            "description": ""
          },
          "binary": {
            "type": "or multiclass, :class:`StratifiedKFold` is used. In all other",
            "description": "cases, :class:`KFold` is used."
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value changed from 3-fold to 5-fold."
          },
          "y": {
            "type": "array",
            "description": "like, default=None"
          },
          "The": {
            "type": "return value is a cross-validator which generates the train/test",
            "description": ""
          },
          "classifier": {
            "type": "bool, default=False",
            "description": ""
          },
          "Whether": {
            "type": "the task is a classification task, in which case",
            "description": ""
          },
          "stratified": {
            "type": "KFold will be used.",
            "description": "Returns\n-------"
          },
          "checked_cv": {
            "type": "a cross",
            "description": "validator instance."
          },
          "splits": {
            "type": "via the ``split`` method.",
            "description": "Examples\n--------\n>>> from sklearn.model_selection import check_cv\n>>> check_cv(cv=5, y=None, classifier=False)"
          },
          "KFold": {
            "type": "...",
            "description": ">>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)"
          },
          "StratifiedKFold": {
            "type": "...",
            "description": ""
          }
        },
        "returns": "-------\n    checked_cv : a cross-validator instance.\n        The return value is a cross-validator which generates the train/test\n        splits via the ``split`` method.\n\n    Examples\n    --------\n    >>> from sklearn.model_selection import check_cv\n    >>> check_cv(cv=5, y=None, classifier=False)\n    KFold(...)\n    >>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)\n    StratifiedKFold(...)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.model_selection import check_cv\n    >>> check_cv(cv=5, y=None, classifier=False)\n    KFold(...)\n    >>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)\n    StratifiedKFold(...)"
      }
    },
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "documentation": {
        "description": "Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": ""
          },
          "Estimator": {
            "type": "instance for which the check is performed.",
            "description": ""
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": ""
          },
          "Attribute": {
            "type": "name(s) given as string or a list/tuple of strings",
            "description": "Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"
          },
          "If": {
            "type": "the attributes are not found.",
            "description": "Examples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")"
          },
          "attribute": {
            "type": "that ends with a underscore and does not start with double",
            "description": "underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": ""
          },
          "The": {
            "type": "default error message is, \"This %(name)s instance is not fitted",
            "description": "yet. Call 'fit' with appropriate arguments before using this\nestimator.\""
          },
          "For": {
            "type": "custom messages if \"%(name)s\" is present in the message string,",
            "description": ""
          },
          "it": {
            "type": "is substituted for the estimator name.",
            "description": "Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": ""
          },
          "Specify": {
            "type": "whether all or any of the given attributes must exist.",
            "description": "Raises\n------\nTypeError"
          },
          "Model": {
            "type": "is not fitted yet.",
            "description": ">>> lr.fit([[1, 2], [1, 3]], [1, 0])"
          },
          "LogisticRegression": {
            "type": "",
            "description": ">>> check_is_fitted(lr)"
          }
        },
        "returns": "",
        "raises": "a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)"
      }
    },
    {
      "name": "clone",
      "signature": "clone(estimator, *, safe=True)",
      "documentation": {
        "description": "Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.",
        "parameters": {
          "estimator": {
            "type": "doesn't have a `random_state` parameter), an *exact clone* is",
            "description": ""
          },
          "The": {
            "type": "deep copy of the input, an estimator if input is an estimator.",
            "description": "Notes\n-----"
          },
          "safe": {
            "type": "bool, default=True",
            "description": ""
          },
          "If": {
            "type": "the estimator's `random_state` parameter is an integer (or if the",
            "description": ""
          },
          "that": {
            "type": "are not estimators. Ignored if `estimator.__sklearn_clone__`",
            "description": "exists.\nReturns\n-------"
          },
          "returned": {
            "type": "the clone and the original estimator will give the exact same",
            "description": "results. Otherwise, *statistical clone* is returned: the clone might"
          },
          "return": {
            "type": "different results from the original estimator. More details can be",
            "description": ""
          },
          "found": {
            "type": "in :ref:`randomness`.",
            "description": "Examples\n--------\n>>> from sklearn.base import clone\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n>>> y = [0, 0, 1, 1]\n>>> classifier = LogisticRegression().fit(X, y)\n>>> cloned_classifier = clone(classifier)\n>>> hasattr(classifier, \"classes_\")\nTrue\n>>> hasattr(cloned_classifier, \"classes_\")\nFalse\n>>> classifier is cloned_classifier\nFalse"
          }
        },
        "returns": "different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False",
        "examples": "--------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False"
      }
    },
    {
      "name": "column_or_1d",
      "signature": "column_or_1d(y, *, dtype=None, warn=False, device=None)",
      "documentation": {
        "description": "Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n       Input data.\n\n    dtype : data-type, default=None\n        Data type for `y`.\n\n        .. versionadded:: 1.2\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    device : device, default=None\n        `device` object.\n        See the :ref:`Array API User Guide <array_api>` for more details.\n\n        .. versionadded:: 1.6\n\n    Returns\n    -------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.",
        "parameters": {
          "y": {
            "type": "ndarray",
            "description": ""
          },
          "Input": {
            "type": "data.",
            "description": ""
          },
          "dtype": {
            "type": "data",
            "description": "type, default=None"
          },
          "Data": {
            "type": "type for `y`.",
            "description": ".. versionadded:: 1.2"
          },
          "warn": {
            "type": "bool, default=False",
            "description": ""
          },
          "To": {
            "type": "control display of warnings.",
            "description": ""
          },
          "device": {
            "type": "device, default=None",
            "description": "`device` object."
          },
          "See": {
            "type": "the :ref:`Array API User Guide <array_api>` for more details.",
            "description": ".. versionadded:: 1.6\nReturns\n-------"
          },
          "Output": {
            "type": "data.",
            "description": "Raises\n------\nValueError"
          },
          "If": {
            "type": "`y` is not a 1D array or a 2D array with a single row or column.",
            "description": "Examples\n--------\n>>> from sklearn.utils.validation import column_or_1d\n>>> column_or_1d([1, 1])"
          },
          "array": {
            "type": "[1, 1]",
            "description": ""
          }
        },
        "returns": "-------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import column_or_1d\n    >>> column_or_1d([1, 1])\n    array([1, 1])",
        "raises": "------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import column_or_1d\n    >>> column_or_1d([1, 1])\n    array([1, 1])",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils.validation import column_or_1d\n    >>> column_or_1d([1, 1])\n    array([1, 1])"
      }
    },
    {
      "name": "cross_val_predict",
      "signature": "cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, params=None, pre_dispatch='2*n_jobs', method='predict')",
      "documentation": {
        "description": "Generate cross-validated estimates for each input data point.\n\n    The data is split according to the cv parameter. Each sample belongs\n    to exactly one test set, and its prediction is computed with an\n    estimator fitted on the corresponding training set.\n\n    Passing these predictions into an evaluation metric may not be a valid\n    way to measure generalization performance. Results can differ from\n    :func:`cross_validate` and :func:`cross_val_score` unless all tests sets\n    have equal size and the metric decomposes over samples.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator\n        The estimator instance to use to fit the data. It must implement a `fit`\n        method and the method given by the `method` parameter.\n\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to fit. Can be, for example a list, or an array at least 2d.\n\n    y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs),             default=None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like of shape (n_samples,), default=None\n        Group labels for the samples used while splitting the dataset into\n        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n        instance (e.g., :class:`GroupKFold`).\n\n        .. versionchanged:: 1.4\n            ``groups`` can only be passed if metadata routing is not enabled\n            via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n            is enabled, pass ``groups`` alongside other metadata via the ``params``\n            argument instead. E.g.:\n            ``cross_val_predict(..., params={'groups': groups})``.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - int, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable that generates (train, test) splits as arrays of indices.\n\n        For int/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel. Training the estimator and\n        predicting are parallelized over the cross-validation splits.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    params : dict, default=None\n        Parameters to pass to the underlying estimator's ``fit`` and the CV\n        splitter.\n\n        .. versionadded:: 1.4\n\n    pre_dispatch : int or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately created and spawned. Use\n          this for lightweight and fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n        - An int, giving the exact number of total jobs that are spawned\n        - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n\n    method : {'predict', 'predict_proba', 'predict_log_proba',               'decision_function'}, default='predict'\n        The method to be invoked by `estimator`.\n\n    Returns\n    -------\n    predictions : ndarray\n        This is the result of calling `method`. Shape:\n\n        - When `method` is 'predict' and in special case where `method` is\n          'decision_function' and the target is binary: (n_samples,)\n        - When `method` is one of {'predict_proba', 'predict_log_proba',\n          'decision_function'} (unless special case above):\n          (n_samples, n_classes)\n        - If `estimator` is :term:`multioutput`, an extra dimension\n          'n_outputs' is added to the end of each shape above.\n\n    See Also\n    --------\n    cross_val_score : Calculate score for each CV split.\n    cross_validate : Calculate one or more scores and timings for each CV\n        split.\n\n    Notes\n    -----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.",
        "parameters": {
          "estimator": {
            "type": "estimator",
            "description": ""
          },
          "The": {
            "type": "verbosity level.",
            "description": ""
          },
          "method": {
            "type": "and the method given by the `method` parameter.",
            "description": ""
          },
          "X": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples, n_features)"
          },
          "y": {
            "type": "{array",
            "description": "like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs),             default=None"
          },
          "supervised": {
            "type": "learning.",
            "description": ""
          },
          "groups": {
            "type": "array",
            "description": "like of shape (n_samples,), default=None"
          },
          "Group": {
            "type": "labels for the samples used while splitting the dataset into",
            "description": "train/test set. Only used in conjunction with a \"Group\" :term:`cv`"
          },
          "instance": {
            "type": "e.g., :class:`GroupKFold`",
            "description": ".\n.. versionchanged:: 1.4\n``groups`` can only be passed if metadata routing is not enabled"
          },
          "via": {
            "type": "``sklearn.set_config(enable_metadata_routing=True)``. When routing",
            "description": ""
          },
          "is": {
            "type": "enabled, pass ``groups`` alongside other metadata via the ``params``",
            "description": ""
          },
          "argument": {
            "type": "instead. E.g.:",
            "description": "``cross_val_predict(..., params={'groups': groups})``."
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator or an iterable, default=None"
          },
          "Determines": {
            "type": "the cross-validation splitting strategy.",
            "description": ""
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross validation,\n- int, to specify the number of folds in a `(Stratified)KFold`,\n- :term:`CV splitter`,\n- An iterable that generates (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "int/None inputs, if the estimator is a classifier and ``y`` is",
            "description": ""
          },
          "either": {
            "type": "binary or multiclass, :class:`StratifiedKFold` is used. In all",
            "description": ""
          },
          "other": {
            "type": "cases, :class:`KFold` is used. These splitters are instantiated",
            "description": ""
          },
          "with": {
            "type": "`shuffle=False` so the splits will be the same across calls.",
            "description": ""
          },
          "Refer": {
            "type": "ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of jobs to run in parallel. Training the estimator and",
            "description": ""
          },
          "predicting": {
            "type": "are parallelized over the cross-validation splits.",
            "description": "``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`"
          },
          "for": {
            "type": "more details.",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "params": {
            "type": "dict, default=None",
            "description": ""
          }
        },
        "returns": "-------\n    predictions : ndarray\n        This is the result of calling `method`. Shape:\n\n        - When `method` is 'predict' and in special case where `method` is\n          'decision_function' and the target is binary: (n_samples,)\n        - When `method` is one of {'predict_proba', 'predict_log_proba',\n          'decision_function'} (unless special case above):\n          (n_samples, n_classes)\n        - If `estimator` is :term:`multioutput`, an extra dimension\n          'n_outputs' is added to the end of each shape above.\n\n    See Also\n    --------\n    cross_val_score : Calculate score for each CV split.\n    cross_validate : Calculate one or more scores and timings for each CV\n        split.\n\n    Notes\n    -----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)",
        "raises": "",
        "see_also": "--------\n    cross_val_score : Calculate score for each CV split.\n    cross_validate : Calculate one or more scores and timings for each CV\n        split.\n\n    Notes\n    -----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)",
        "notes": "-----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)",
        "examples": "--------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)"
      }
    },
    {
      "name": "delayed",
      "signature": "delayed(function)",
      "documentation": {
        "description": "Decorator used to capture the arguments of a function.\n\n    This alternative to `joblib.delayed` is meant to be used in conjunction\n    with `sklearn.utils.parallel.Parallel`. The latter captures the scikit-\n    learn configuration by calling `sklearn.get_config()` in the current\n    thread, prior to dispatching the first task. The captured configuration is\n    then propagated and enabled for the duration of the execution of the\n    delayed function in the joblib workers.\n\n    .. versionchanged:: 1.3\n       `delayed` was moved from `sklearn.utils.fixes` to `sklearn.utils.parallel`\n       in scikit-learn 1.3.\n\n    Parameters\n    ----------\n    function : callable\n        The function to be delayed.",
        "parameters": {
          "function": {
            "type": "callable",
            "description": ""
          },
          "The": {
            "type": "function to be delayed.",
            "description": "Returns\n-------"
          },
          "output": {
            "type": "tuple",
            "description": ""
          },
          "Tuple": {
            "type": "containing the delayed function, the positional arguments, and the",
            "description": ""
          },
          "keyword": {
            "type": "arguments.",
            "description": ""
          }
        },
        "returns": "-------\n    output: tuple\n        Tuple containing the delayed function, the positional arguments, and the\n        keyword arguments.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "expit",
      "signature": "expit(*args, **kwargs)",
      "documentation": {
        "description": "expit(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\n\n    expit(x, out=None)\n\n    Expit (a.k.a. logistic sigmoid) ufunc for ndarrays.\n\n    The expit function, also known as the logistic sigmoid function, is\n    defined as ``expit(x) = 1/(1+exp(-x))``.  It is the inverse of the\n    logit function.\n\n    Parameters\n    ----------\n    x : ndarray\n        The ndarray to apply expit to element-wise.\n    out : ndarray, optional\n        Optional output array for the function values\n\n    Returns\n    -------\n    scalar or ndarray\n        An ndarray of the same shape as x. Its entries\n        are `expit` of the corresponding entry of x.\n\n    See Also\n    --------\n    logit\n\n    Notes\n    -----\n    As a ufunc expit takes a number of optional\n    keyword arguments. For more information\n    see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n\n    .. versionadded:: 0.10.0",
        "parameters": {
          "x": {
            "type": "ndarray",
            "description": ""
          },
          "The": {
            "type": "ndarray to apply expit to element-wise.",
            "description": ""
          },
          "out": {
            "type": "ndarray, optional",
            "description": ""
          },
          "Optional": {
            "type": "output array for the function values",
            "description": "Returns\n-------"
          },
          "scalar": {
            "type": "or ndarray",
            "description": ""
          },
          "An": {
            "type": "ndarray of the same shape as x. Its entries",
            "description": ""
          },
          "are": {
            "type": "`expit` of the corresponding entry of x.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nlogit\nNotes\n-----"
          },
          "As": {
            "type": "a ufunc expit takes a number of optional",
            "description": ""
          },
          "keyword": {
            "type": "arguments. For more information",
            "description": ""
          },
          "see": {
            "type": "`ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_",
            "description": ".. versionadded:: 0.10.0\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.special import expit, logit\n>>> expit([-np.inf, -1.5, 0, 1.5, np.inf])"
          },
          "array": {
            "type": "[-2.5,  0. ,  3.1,  5. ]",
            "description": ""
          },
          "Plot": {
            "type": "expit(x) for x in [-6, 6]:",
            "description": ">>> import matplotlib.pyplot as plt\n>>> x = np.linspace(-6, 6, 121)\n>>> y = expit(x)\n>>> plt.plot(x, y)\n>>> plt.grid()\n>>> plt.xlim(-6, 6)\n>>> plt.xlabel('x')\n>>> plt.title('expit(x)')\n>>> plt.show()"
          }
        },
        "returns": "-------\n    scalar or ndarray\n        An ndarray of the same shape as x. Its entries\n        are `expit` of the corresponding entry of x.\n\n    See Also\n    --------\n    logit\n\n    Notes\n    -----\n    As a ufunc expit takes a number of optional\n    keyword arguments. For more information\n    see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n\n    .. versionadded:: 0.10.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.special import expit, logit\n\n    >>> expit([-np.inf, -1.5, 0, 1.5, np.inf])\n    array([ 0.        ,  0.18242552,  0.5       ,  0.81757448,  1.        ])\n\n    `logit` is the inverse of `expit`:\n\n    >>> logit(expit([-2.5, 0, 3.1, 5.0]))\n    array([-2.5,  0. ,  3.1,  5. ])\n\n    Plot expit(x) for x in [-6, 6]:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(-6, 6, 121)\n    >>> y = expit(x)\n    >>> plt.plot(x, y)\n    >>> plt.grid()\n    >>> plt.xlim(-6, 6)\n    >>> plt.xlabel('x')\n    >>> plt.title('expit(x)')\n    >>> plt.show()",
        "raises": "",
        "see_also": "--------\n    logit\n\n    Notes\n    -----\n    As a ufunc expit takes a number of optional\n    keyword arguments. For more information\n    see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n\n    .. versionadded:: 0.10.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.special import expit, logit\n\n    >>> expit([-np.inf, -1.5, 0, 1.5, np.inf])\n    array([ 0.        ,  0.18242552,  0.5       ,  0.81757448,  1.        ])\n\n    `logit` is the inverse of `expit`:\n\n    >>> logit(expit([-2.5, 0, 3.1, 5.0]))\n    array([-2.5,  0. ,  3.1,  5. ])\n\n    Plot expit(x) for x in [-6, 6]:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(-6, 6, 121)\n    >>> y = expit(x)\n    >>> plt.plot(x, y)\n    >>> plt.grid()\n    >>> plt.xlim(-6, 6)\n    >>> plt.xlabel('x')\n    >>> plt.title('expit(x)')\n    >>> plt.show()",
        "notes": "-----\n    As a ufunc expit takes a number of optional\n    keyword arguments. For more information\n    see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n\n    .. versionadded:: 0.10.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.special import expit, logit\n\n    >>> expit([-np.inf, -1.5, 0, 1.5, np.inf])\n    array([ 0.        ,  0.18242552,  0.5       ,  0.81757448,  1.        ])\n\n    `logit` is the inverse of `expit`:\n\n    >>> logit(expit([-2.5, 0, 3.1, 5.0]))\n    array([-2.5,  0. ,  3.1,  5. ])\n\n    Plot expit(x) for x in [-6, 6]:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(-6, 6, 121)\n    >>> y = expit(x)\n    >>> plt.plot(x, y)\n    >>> plt.grid()\n    >>> plt.xlim(-6, 6)\n    >>> plt.xlabel('x')\n    >>> plt.title('expit(x)')\n    >>> plt.show()",
        "examples": "--------\n    >>> import numpy as np\n    >>> from scipy.special import expit, logit\n\n    >>> expit([-np.inf, -1.5, 0, 1.5, np.inf])\n    array([ 0.        ,  0.18242552,  0.5       ,  0.81757448,  1.        ])\n\n    `logit` is the inverse of `expit`:\n\n    >>> logit(expit([-2.5, 0, 3.1, 5.0]))\n    array([-2.5,  0. ,  3.1,  5. ])\n\n    Plot expit(x) for x in [-6, 6]:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(-6, 6, 121)\n    >>> y = expit(x)\n    >>> plt.plot(x, y)\n    >>> plt.grid()\n    >>> plt.xlim(-6, 6)\n    >>> plt.xlabel('x')\n    >>> plt.title('expit(x)')\n    >>> plt.show()"
      }
    },
    {
      "name": "get_tags",
      "signature": "get_tags(estimator) -> 'Tags'",
      "documentation": {
        "description": "Get estimator tags.\n\n    :class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\n    However, if an estimator does not inherit from this base class, we should\n    fall-back to the default tags.\n\n    For scikit-learn built-in estimators, we should still rely on\n    `self.__sklearn_tags__()`. `get_tags(est)` should be used when we\n    are not sure where `est` comes from: typically\n    `get_tags(self.estimator)` where `self` is a meta-estimator, or in\n    the common checks.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    estimator : estimator object\n        The estimator from which to get the tag.",
        "parameters": {
          "estimator": {
            "type": "estimator object",
            "description": ""
          },
          "The": {
            "type": "estimator tags.",
            "description": ""
          },
          "tags": {
            "type": ":class:`~.sklearn.utils.Tags`",
            "description": ""
          }
        },
        "returns": "-------\n    tags : :class:`~.sklearn.utils.Tags`\n        The estimator tags.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "indexable",
      "signature": "indexable(*iterables)",
      "documentation": {
        "description": "Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-iterable objects to arrays.\n\n    Parameters\n    ----------\n    *iterables : {lists, dataframes, ndarrays, sparse matrices}\n        List of objects to ensure sliceability.\n\n    Returns\n    -------\n    result : list of {ndarray, sparse matrix, dataframe} or None\n        Returns a list containing indexable arrays (i.e. NumPy array,\n        sparse matrix, or dataframe) or `None`.",
        "parameters": {
          "List": {
            "type": "of objects to ensure sliceability.",
            "description": "Returns\n-------"
          },
          "result": {
            "type": "list of {ndarray, sparse matrix, dataframe} or None",
            "description": ""
          },
          "Returns": {
            "type": "a list containing indexable arrays (i.e. NumPy array,",
            "description": ""
          },
          "sparse": {
            "type": "matrix, or dataframe) or `None`.",
            "description": "Examples\n--------\n>>> from sklearn.utils import indexable\n>>> from scipy.sparse import csr_matrix\n>>> import numpy as np\n>>> iterables = [\n...     [1, 2, 3], np.array([2, 3, 4]), None, csr_matrix([[5], [6], [7]])\n... ]\n>>> indexable(*iterables)\n[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]"
          }
        },
        "returns": "-------\n    result : list of {ndarray, sparse matrix, dataframe} or None",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils import indexable\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> iterables = [\n    ...     [1, 2, 3], np.array([2, 3, 4]), None, csr_matrix([[5], [6], [7]])\n    ... ]\n    >>> indexable(*iterables)\n    [[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]"
      }
    },
    {
      "name": "label_binarize",
      "signature": "label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False)",
      "documentation": {
        "description": "Binarize labels in a one-vs-all fashion.\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    This function makes it possible to compute this transformation for a\n    fixed set of class labels known ahead of time.\n\n    Parameters\n    ----------\n    y : array-like or sparse matrix\n        Sequence of integer labels or multilabel data to encode.\n\n    classes : array-like of shape (n_classes,)\n        Uniquely holds the label for each class.\n\n    neg_label : int, default=0\n        Value with which negative labels must be encoded.\n\n    pos_label : int, default=1\n        Value with which positive labels must be encoded.\n\n    sparse_output : bool, default=False,\n        Set to true if output binary array is desired in CSR sparse format.\n\n    Returns\n    -------\n    Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n        Shape will be (n_samples, 1) for binary problems. Sparse matrix will\n        be of CSR format.\n\n    See Also\n    --------\n    LabelBinarizer : Class used to wrap the functionality of label_binarize and\n        allow for fitting to classes independently of the transform operation.",
        "parameters": {
          "y": {
            "type": "array",
            "description": "like or sparse matrix"
          },
          "Sequence": {
            "type": "of integer labels or multilabel data to encode.",
            "description": ""
          },
          "classes": {
            "type": "array",
            "description": "like of shape (n_classes,)"
          },
          "Uniquely": {
            "type": "holds the label for each class.",
            "description": ""
          },
          "neg_label": {
            "type": "int, default=0",
            "description": ""
          },
          "Value": {
            "type": "with which positive labels must be encoded.",
            "description": ""
          },
          "pos_label": {
            "type": "int, default=1",
            "description": ""
          },
          "sparse_output": {
            "type": "bool, default=False,",
            "description": ""
          },
          "Set": {
            "type": "to true if output binary array is desired in CSR sparse format.",
            "description": "Returns\n-------"
          },
          "Y": {
            "type": "{ndarray, sparse matrix} of shape (n_samples, n_classes)",
            "description": ""
          },
          "Shape": {
            "type": "will be (n_samples, 1) for binary problems. Sparse matrix will",
            "description": ""
          },
          "be": {
            "type": "of CSR format.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "LabelBinarizer": {
            "type": "Class used to wrap the functionality of label_binarize and",
            "description": ""
          },
          "allow": {
            "type": "for fitting to classes independently of the transform operation.",
            "description": "Examples\n--------\n>>> from sklearn.preprocessing import label_binarize\n>>> label_binarize([1, 6], classes=[1, 2, 4, 6])\narray([[1, 0, 0, 0],\n[0, 0, 0, 1]])"
          },
          "The": {
            "type": "class ordering is preserved:",
            "description": ">>> label_binarize([1, 6], classes=[1, 6, 4, 2])\narray([[1, 0, 0, 0],\n[0, 1, 0, 0]])"
          },
          "Binary": {
            "type": "targets transform to a column vector",
            "description": ">>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\narray([[1],\n[0],\n[0],\n[1]])"
          }
        },
        "returns": "-------\n    Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n        Shape will be (n_samples, 1) for binary problems. Sparse matrix will\n        be of CSR format.\n\n    See Also\n    --------\n    LabelBinarizer : Class used to wrap the functionality of label_binarize and\n        allow for fitting to classes independently of the transform operation.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import label_binarize\n    >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    The class ordering is preserved:\n\n    >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n    array([[1, 0, 0, 0],\n           [0, 1, 0, 0]])\n\n    Binary targets transform to a column vector\n\n    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])",
        "raises": "",
        "see_also": "--------\n    LabelBinarizer : Class used to wrap the functionality of label_binarize and\n        allow for fitting to classes independently of the transform operation.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import label_binarize\n    >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    The class ordering is preserved:\n\n    >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n    array([[1, 0, 0, 0],\n           [0, 1, 0, 0]])\n\n    Binary targets transform to a column vector\n\n    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.preprocessing import label_binarize\n    >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    The class ordering is preserved:\n\n    >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n    array([[1, 0, 0, 0],\n           [0, 1, 0, 0]])\n\n    Binary targets transform to a column vector\n\n    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])"
      }
    },
    {
      "name": "log",
      "signature": "log(x, [base=math.e])",
      "documentation": {
        "description": "log(x, [base=math.e])",
        "parameters": {},
        "returns": "the logarithm of x to the given base.\n\nIf the base is not specified, returns the natural logarithm (base e) of x.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "minimize",
      "signature": "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)",
      "documentation": {
        "description": "Minimization of scalar function of one or more variables.\n\n    Parameters\n    ----------\n    fun : callable\n        The objective function to be minimized::\n\n            fun(x, *args) -> float\n\n        where ``x`` is a 1-D array with shape (n,) and ``args``\n        is a tuple of the fixed parameters needed to completely\n        specify the function.\n\n        Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n        ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n        Rather than passing ``f0`` as the callable, wrap it to accept\n        only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n        callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n        gathered before invoking this function.\n    x0 : ndarray, shape (n,)\n        Initial guess. Array of real elements of size (n,),\n        where ``n`` is the number of independent variables.\n    args : tuple, optional\n        Extra arguments passed to the objective function and its\n        derivatives (`fun`, `jac` and `hess` functions).\n    method : str or callable, optional\n        Type of solver.  Should be one of\n\n        - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n        - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n        - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n        - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n        - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n        - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n        - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n        - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n        - 'COBYQA'      :ref:`(see here) <optimize.minimize-cobyqa>`\n        - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n        - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n        - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n        - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n        - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n        - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n        - custom - a callable object, see below for description.\n\n        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n        depending on whether or not the problem has constraints or bounds.\n    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n        Method for computing the gradient vector. Only for CG, BFGS,\n        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n        trust-exact and trust-constr.\n        If it is a callable, it should be a function that returns the gradient\n        vector::\n\n            jac(x, *args) -> array_like, shape (n,)\n\n        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n        assumed to return a tuple ``(f, g)`` containing the objective\n        function and the gradient.\n        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n        'trust-krylov' require that either a callable be supplied, or that\n        `fun` return the objective and gradient.\n        If None or False, the gradient will be estimated using 2-point finite\n        difference estimation with an absolute step size.\n        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n        to select a finite difference scheme for numerical estimation of the\n        gradient with a relative step size. These finite difference schemes\n        obey any specified `bounds`.\n    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n        trust-ncg, trust-krylov, trust-exact and trust-constr.\n        If it is callable, it should return the Hessian matrix::\n\n            hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)\n\n        where ``x`` is a (n,) ndarray and ``args`` is a tuple with the fixed\n        parameters.\n        The keywords {'2-point', '3-point', 'cs'} can also be used to select\n        a finite difference scheme for numerical estimation of the hessian.\n        Alternatively, objects implementing the `HessianUpdateStrategy`\n        interface can be used to approximate the Hessian. Available\n        quasi-Newton methods implementing this interface are:\n\n        - `BFGS`\n        - `SR1`\n\n        Not all of the options are available for each of the methods; for\n        availability refer to the notes.\n    hessp : callable, optional\n        Hessian of objective function times an arbitrary vector p. Only for\n        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n        Only one of `hessp` or `hess` needs to be given. If `hess` is\n        provided, then `hessp` will be ignored. `hessp` must compute the\n        Hessian times an arbitrary vector::\n\n            hessp(x, p, *args) ->  ndarray shape (n,)\n\n        where ``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with\n        dimension (n,) and ``args`` is a tuple with the fixed\n        parameters.\n    bounds : sequence or `Bounds`, optional\n        Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell,\n        trust-constr, COBYLA, and COBYQA methods. There are two ways to specify\n        the bounds:\n\n        1. Instance of `Bounds` class.\n        2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n           is used to specify no bound.\n\n    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n        Constraints definition. Only for COBYLA, COBYQA, SLSQP and trust-constr.\n\n        Constraints for 'trust-constr' and 'cobyqa' are defined as a single object\n        or a list of objects specifying constraints to the optimization problem.\n        Available constraints are:\n\n        - `LinearConstraint`\n        - `NonlinearConstraint`\n\n        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n        Each dictionary with fields:\n\n        type : str\n            Constraint type: 'eq' for equality, 'ineq' for inequality.\n        fun : callable\n            The function defining the constraint.\n        jac : callable, optional\n            The Jacobian of `fun` (only for SLSQP).\n        args : sequence, optional\n            Extra arguments to be passed to the function and Jacobian.\n\n        Equality constraint means that the constraint function result is to\n        be zero whereas inequality means that it is to be non-negative.\n        Note that COBYLA only supports inequality constraints.\n\n    tol : float, optional\n        Tolerance for termination. When `tol` is specified, the selected\n        minimization algorithm sets some relevant solver-specific tolerance(s)\n        equal to `tol`. For detailed control, use solver-specific\n        options.\n    options : dict, optional\n        A dictionary of solver options. All methods except `TNC` accept the\n        following generic options:\n\n        maxiter : int\n            Maximum number of iterations to perform. Depending on the\n            method each iteration may use several function evaluations.\n\n            For `TNC` use `maxfun` instead of `maxiter`.\n        disp : bool\n            Set to True to print convergence messages.\n\n        For method-specific options, see :func:`show_options()`.\n    callback : callable, optional\n        A callable called after each iteration.\n\n        All methods except TNC, SLSQP, and COBYLA support a callable with\n        the signature::\n\n            callback(intermediate_result: OptimizeResult)\n\n        where ``intermediate_result`` is a keyword parameter containing an\n        `OptimizeResult` with attributes ``x`` and ``fun``, the present values\n        of the parameter vector and objective function. Note that the name\n        of the parameter must be ``intermediate_result`` for the callback\n        to be passed an `OptimizeResult`. These methods will also terminate if\n        the callback raises ``StopIteration``.\n\n        All methods except trust-constr (also) support a signature like::\n\n            callback(xk)\n\n        where ``xk`` is the current parameter vector.\n\n        Introspection is used to determine which of the signatures above to\n        invoke.\n\n    Returns\n    -------\n    res : OptimizeResult\n        The optimization result represented as a ``OptimizeResult`` object.\n        Important attributes are: ``x`` the solution array, ``success`` a\n        Boolean flag indicating if the optimizer exited successfully and\n        ``message`` which describes the cause of the termination. See\n        `OptimizeResult` for a description of other attributes.\n\n    See also\n    --------\n    minimize_scalar : Interface to minimization algorithms for scalar\n        univariate functions\n    show_options : Additional options accepted by the solvers\n\n    Notes\n    -----\n    This section describes the available solvers that can be selected by the\n    'method' parameter. The default method is *BFGS*.\n\n    **Unconstrained minimization**\n\n    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n    gradient algorithm by Polak and Ribiere, a variant of the\n    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n    first derivatives are used.\n\n    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n    pp. 136. It uses the first derivatives only. BFGS has proven good\n    performance even for non-smooth optimizations. This method also\n    returns an approximation of the Hessian inverse, stored as\n    `hess_inv` in the OptimizeResult object.\n\n    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n    Newton method). It uses a CG method to the compute the search\n    direction. See also *TNC* method for a box-constrained\n    minimization with a similar algorithm. Suitable for large-scale\n    problems.\n\n    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n    trust-region algorithm [5]_ for unconstrained minimization. This\n    algorithm requires the gradient and Hessian; furthermore the\n    Hessian is required to be positive definite.\n\n    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n    Newton conjugate gradient trust-region algorithm [5]_ for\n    unconstrained minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n\n    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n    minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n    On indefinite problems it requires usually less iterations than the\n    `trust-ncg` method and is recommended for medium and large-scale problems.\n\n    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n    is a trust-region method for unconstrained minimization in which\n    quadratic subproblems are solved almost exactly [13]_. This\n    algorithm requires the gradient and the Hessian (which is\n    *not* required to be positive definite). It is, in many\n    situations, the Newton method to converge in fewer iterations\n    and the most recommended for small and medium-size problems.\n\n    **Bound-Constrained minimization**\n\n    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n    applications. However, if numerical computation of derivative can be\n    trusted, other algorithms using the first and/or second derivatives\n    information might be preferred for their better performance in\n    general.\n\n    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n    algorithm [6]_, [7]_ for bound constrained minimization.\n\n    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n    of Powell's method [3]_, [4]_ which is a conjugate direction\n    method. It performs sequential one-dimensional minimizations along\n    each vector of the directions set (`direc` field in `options` and\n    `info`), which is updated at each iteration of the main\n    minimization loop. The function need not be differentiable, and no\n    derivatives are taken. If bounds are not provided, then an\n    unbounded line search will be used. If bounds are provided and\n    the initial guess is within the bounds, then every function\n    evaluation throughout the minimization procedure will be within\n    the bounds. If bounds are provided, the initial guess is outside\n    the bounds, and `direc` is full rank (default has full rank), then\n    some function evaluations during the first iteration may be\n    outside the bounds, but every function evaluation after the first\n    iteration will be within the bounds. If `direc` is not full rank,\n    then some parameters may not be optimized and the solution is not\n    guaranteed to be within the bounds.\n\n    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n    algorithm [5]_, [8]_ to minimize a function with variables subject\n    to bounds. This algorithm uses gradient information; it is also\n    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n    method described above as it wraps a C implementation and allows\n    each variable to be given upper and lower bounds.\n\n    **Constrained Minimization**\n\n    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n    Constrained Optimization BY Linear Approximation (COBYLA) method\n    [9]_, [10]_, [11]_. The algorithm is based on linear\n    approximations to the objective function and each constraint. The\n    method wraps a FORTRAN implementation of the algorithm. The\n    constraints functions 'fun' may return either a single number\n    or an array or list of numbers.\n\n    Method :ref:`COBYQA <optimize.minimize-cobyqa>` uses the Constrained\n    Optimization BY Quadratic Approximations (COBYQA) method [18]_. The\n    algorithm is a derivative-free trust-region SQP method based on quadratic\n    approximations to the objective function and each nonlinear constraint. The\n    bounds are treated as unrelaxable constraints, in the sense that the\n    algorithm always respects them throughout the optimization process.\n\n    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n    Least SQuares Programming to minimize a function of several\n    variables with any combination of bounds, equality and inequality\n    constraints. The method wraps the SLSQP Optimization subroutine\n    originally implemented by Dieter Kraft [12]_. Note that the\n    wrapper handles infinite values in bounds by converting them into\n    large floating values.\n\n    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n    trust-region algorithm for constrained optimization. It switches\n    between two implementations depending on the problem definition.\n    It is the most versatile constrained minimization algorithm\n    implemented in SciPy and the most appropriate for large-scale problems.\n    For equality constrained problems it is an implementation of Byrd-Omojokun\n    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n    inequality constraints are imposed as well, it switches to the trust-region\n    interior point method described in [16]_. This interior point algorithm,\n    in turn, solves inequality constraints by introducing slack variables\n    and solving a sequence of equality-constrained barrier problems\n    for progressively smaller values of the barrier parameter.\n    The previously described equality constrained SQP method is\n    used to solve the subproblems with increasing levels of accuracy\n    as the iterate gets closer to a solution.\n\n    **Finite-Difference Options**\n\n    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n    the gradient and the Hessian may be approximated using\n    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n    The scheme 'cs' is, potentially, the most accurate but it\n    requires the function to correctly handle complex inputs and to\n    be differentiable in the complex plane. The scheme '3-point' is more\n    accurate than '2-point' but requires twice as many operations. If the\n    gradient is estimated via finite-differences the Hessian must be\n    estimated using one of the quasi-Newton strategies.\n\n    **Method specific options for the** `hess` **keyword**\n\n    +--------------+------+----------+-------------------------+-----+\n    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n    +==============+======+==========+=========================+=====+\n    | Newton-CG    | x    | (n, n)   | x                       | x   |\n    |              |      | LO       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | dogleg       |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-ncg    |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-krylov |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-exact  |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-constr | x    | (n, n)   |  x                      | x   |\n    |              |      | LO       |                         |     |\n    |              |      | sp       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n\n    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n\n    **Custom minimizers**\n\n    It may be useful to pass a custom minimization method, for example\n    when using a frontend to this method such as `scipy.optimize.basinhopping`\n    or a different library.  You can simply pass a callable as the ``method``\n    parameter.\n\n    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n    its contents also passed as `method` parameters pair by pair.  Also, if\n    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n    `fun` returns just the function values and `jac` is converted to a function\n    returning the Jacobian.  The method shall return an `OptimizeResult`\n    object.\n\n    The provided `method` callable must be able to accept (and possibly ignore)\n    arbitrary parameters; the set of parameters accepted by `minimize` may\n    expand in future versions and then these parameters will be passed to\n    the method.  You can find an example in the scipy.optimize tutorial.\n\n    References\n    ----------\n    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n        Minimization. The Computer Journal 7: 308-13.\n    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n        191-208.\n    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n       a function of several variables without calculating derivatives. The\n       Computer Journal 7: 155-162.\n    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n       Numerical Recipes (any edition), Cambridge University Press.\n    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n       Springer New York.\n    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n       Algorithm for Bound Constrained Optimization. SIAM Journal on\n       Scientific and Statistical Computing 16 (5): 1190-1208.\n    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n       optimization. ACM Transactions on Mathematical Software 23 (4):\n       550-560.\n    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n    .. [9] Powell, M J D. A direct search optimization method that models\n       the objective and constraint functions by linear interpolation.\n       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n    .. [10] Powell M J D. Direct search algorithms for optimization\n       calculations. 1998. Acta Numerica 7: 287-336.\n    .. [11] Powell M J D. A view of algorithms for optimization without\n       derivatives. 2007.Cambridge University Technical Report DAMTP\n       2007/NA03\n    .. [12] Kraft, D. A software package for sequential quadratic\n       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n       Center -- Institute for Flight Mechanics, Koln, Germany.\n    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n       Trust region methods. 2000. Siam. pp. 169-200.\n    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n       implementation of the GLTR method for iterative solution of\n       the trust region problem\", :arxiv:`1611.04718`\n    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n       Trust-Region Subproblem using the Lanczos Method\",\n       SIAM J. Optim., 9(2), 504--525, (1999).\n    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n        An interior point algorithm for large-scale nonlinear  programming.\n        SIAM Journal on Optimization 9.4: 877-900.\n    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the\n        implementation of an algorithm for large-scale equality constrained\n        optimization. SIAM Journal on Optimization 8.3: 682-706.\n    .. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods\n        and Software*. PhD thesis, Department of Applied Mathematics, The Hong\n        Kong Polytechnic University, Hong Kong, China, 2022. URL:\n        https://theses.lib.polyu.edu.hk/handle/200/12294.\n\n    Examples\n    --------\n    Let us consider the problem of minimizing the Rosenbrock function. This\n    function (and its respective derivatives) is implemented in `rosen`\n    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n\n    >>> from scipy.optimize import minimize, rosen, rosen_der\n\n    A simple application of the *Nelder-Mead* method is:\n\n    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n\n    Now using the *BFGS* algorithm, using the first derivative and a few\n    options:\n\n    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n    ...                options={'gtol': 1e-6, 'disp': True})\n    Optimization terminated successfully.\n             Current function value: 0.000000\n             Iterations: 26\n             Function evaluations: 31\n             Gradient evaluations: 31\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n    >>> print(res.message)\n    Optimization terminated successfully.\n    >>> res.hess_inv\n    array([\n        [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n        [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n        [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n        [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n        [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n    ])\n\n\n    Next, consider a minimization problem with several constraints (namely",
        "parameters": {
          "fun": {
            "type": "callable",
            "description": ""
          },
          "The": {
            "type": "Jacobian of `fun` (only for SLSQP).",
            "description": ""
          },
          "where": {
            "type": "``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with",
            "description": ""
          },
          "is": {
            "type": "used to specify no bound.",
            "description": ""
          },
          "specify": {
            "type": "the function.",
            "description": ""
          },
          "Suppose": {
            "type": "the callable has signature ``f0(x, *my_args, **my_kwargs)``, where",
            "description": "``my_args`` and ``my_kwargs`` are required positional and keyword arguments."
          },
          "Rather": {
            "type": "than passing ``f0`` as the callable, wrap it to accept",
            "description": ""
          },
          "only": {
            "type": "``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the",
            "description": "callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been"
          },
          "gathered": {
            "type": "before invoking this function.",
            "description": ""
          },
          "x0": {
            "type": "ndarray, shape (n,)",
            "description": ""
          },
          "Initial": {
            "type": "guess. Array of real elements of size (n,),",
            "description": ""
          },
          "args": {
            "type": "sequence, optional",
            "description": ""
          },
          "Extra": {
            "type": "arguments passed to the objective function and its",
            "description": ""
          },
          "derivatives": {
            "type": "`fun`, `jac` and `hess` functions",
            "description": "."
          },
          "method": {
            "type": "str or callable, optional",
            "description": ""
          },
          "Type": {
            "type": "of solver.  Should be one of",
            "description": "- 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n- 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n- 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n- 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n- 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n- 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n- 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n- 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n- 'COBYQA'      :ref:`(see here) <optimize.minimize-cobyqa>`\n- 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n- 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n- 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n- 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n- 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n- 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n- custom - a callable object, see below for description."
          },
          "If": {
            "type": "it is callable, it should return the Hessian matrix::",
            "description": ""
          },
          "depending": {
            "type": "on whether or not the problem has constraints or bounds.",
            "description": ""
          },
          "jac": {
            "type": "callable, optional",
            "description": ""
          },
          "Method": {
            "type": "for computing the Hessian matrix. Only for Newton-CG, dogleg,",
            "description": "trust-ncg, trust-krylov, trust-exact and trust-constr."
          },
          "vector": {
            "type": ":",
            "description": ""
          },
          "the": {
            "type": "bounds:",
            "description": "1. Instance of `Bounds` class.\n2. Sequence of ``(min, max)`` pairs for each element in `x`. None"
          },
          "assumed": {
            "type": "to return a tuple ``(f, g)`` containing the objective",
            "description": ""
          },
          "function": {
            "type": "and the gradient.",
            "description": ""
          },
          "Methods": {
            "type": "'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and",
            "description": "'trust-krylov' require that either a callable be supplied, or that\n`fun` return the objective and gradient."
          },
          "difference": {
            "type": "estimation with an absolute step size.",
            "description": "Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used"
          },
          "to": {
            "type": "select a finite difference scheme for numerical estimation of the",
            "description": ""
          },
          "gradient": {
            "type": "with a relative step size. These finite difference schemes",
            "description": ""
          },
          "obey": {
            "type": "any specified `bounds`.",
            "description": ""
          },
          "hess": {
            "type": "x, *args",
            "description": "-> {LinearOperator, spmatrix, array}, (n, n)"
          },
          "a": {
            "type": "finite difference scheme for numerical estimation of the hessian.",
            "description": "Alternatively, objects implementing the `HessianUpdateStrategy`"
          },
          "interface": {
            "type": "can be used to approximate the Hessian. Available",
            "description": "quasi-Newton methods implementing this interface are:\n- `BFGS`\n- `SR1`"
          },
          "Not": {
            "type": "all of the options are available for each of the methods; for",
            "description": ""
          },
          "availability": {
            "type": "refer to the notes.",
            "description": ""
          },
          "hessp": {
            "type": "x, p, *args",
            "description": "->  ndarray shape (n,)"
          },
          "Hessian": {
            "type": "times an arbitrary vector::",
            "description": ""
          },
          "Only": {
            "type": "one of `hessp` or `hess` needs to be given. If `hess` is",
            "description": "provided, then `hessp` will be ignored. `hessp` must compute the"
          },
          "dimension": {
            "type": "n,",
            "description": "and ``args`` is a tuple with the fixed\nparameters."
          },
          "bounds": {
            "type": "sequence or `Bounds`, optional",
            "description": ""
          },
          "Bounds": {
            "type": "on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell,",
            "description": "trust-constr, COBYLA, and COBYQA methods. There are two ways to specify"
          },
          "constraints": {
            "type": "{Constraint, dict} or List of {Constraint, dict}, optional",
            "description": ""
          },
          "Constraints": {
            "type": "for COBYLA, SLSQP are defined as a list of dictionaries.",
            "description": ""
          },
          "or": {
            "type": "a list of objects specifying constraints to the optimization problem.",
            "description": ""
          },
          "Available": {
            "type": "constraints are:",
            "description": "- `LinearConstraint`\n- `NonlinearConstraint`"
          },
          "Each": {
            "type": "dictionary with fields:",
            "description": ""
          },
          "type": {
            "type": "str",
            "description": ""
          },
          "Constraint": {
            "type": "type: 'eq' for equality, 'ineq' for inequality.",
            "description": ""
          },
          "Equality": {
            "type": "constraint means that the constraint function result is to",
            "description": ""
          },
          "be": {
            "type": "differentiable in the complex plane. The scheme '3-point' is more",
            "description": ""
          },
          "Note": {
            "type": "that COBYLA only supports inequality constraints.",
            "description": ""
          },
          "tol": {
            "type": "float, optional",
            "description": ""
          },
          "Tolerance": {
            "type": "for termination. When `tol` is specified, the selected",
            "description": ""
          },
          "minimization": {
            "type": "loop. The function need not be differentiable, and no",
            "description": ""
          },
          "equal": {
            "type": "to `tol`. For detailed control, use solver-specific",
            "description": "options."
          },
          "options": {
            "type": "",
            "description": ">>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n...                options={'gtol': 1e-6, 'disp': True})"
          },
          "A": {
            "type": "simple application of the *Nelder-Mead* method is:",
            "description": ">>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n>>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n>>> res.x"
          },
          "following": {
            "type": "generic options:",
            "description": ""
          },
          "maxiter": {
            "type": "int",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations to perform. Depending on the",
            "description": ""
          },
          "For": {
            "type": "Method :ref:`trust-constr <optimize.minimize-trustconstr>`",
            "description": ""
          },
          "disp": {
            "type": "bool",
            "description": ""
          },
          "Set": {
            "type": "to True to print convergence messages.",
            "description": ""
          },
          "callback": {
            "type": "xk",
            "description": ""
          },
          "All": {
            "type": "methods except trust-constr (also) support a signature like::",
            "description": ""
          },
          "of": {
            "type": "Powell's method [3]_, [4]_ which is a conjugate direction",
            "description": "method. It performs sequential one-dimensional minimizations along"
          },
          "Introspection": {
            "type": "is used to determine which of the signatures above to",
            "description": "invoke.\nReturns\n-------"
          },
          "res": {
            "type": "OptimizeResult",
            "description": ""
          },
          "Important": {
            "type": "attributes are: ``x`` the solution array, ``success`` a",
            "description": ""
          },
          "Boolean": {
            "type": "flag indicating if the optimizer exited successfully and",
            "description": "``message`` which describes the cause of the termination. See\n`OptimizeResult` for a description of other attributes."
          },
          "See": {
            "type": "also",
            "description": "--------"
          },
          "minimize_scalar": {
            "type": "Interface to minimization algorithms for scalar",
            "description": ""
          },
          "univariate": {
            "type": "functions",
            "description": ""
          },
          "show_options": {
            "type": "Additional options accepted by the solvers",
            "description": "Notes\n-----"
          },
          "This": {
            "type": "section describes the available solvers that can be selected by the",
            "description": "'method' parameter. The default method is *BFGS*.\n**Unconstrained minimization**"
          },
          "first": {
            "type": "derivatives are used.",
            "description": ""
          },
          "performance": {
            "type": "even for non-smooth optimizations. This method also",
            "description": ""
          },
          "returns": {
            "type": "an approximation of the Hessian inverse, stored as",
            "description": "`hess_inv` in the OptimizeResult object."
          },
          "Newton": {
            "type": "conjugate gradient trust-region algorithm [5]_ for",
            "description": ""
          },
          "algorithm": {
            "type": "always respects them throughout the optimization process.",
            "description": ""
          },
          "unconstrained": {
            "type": "minimization. This algorithm requires the gradient",
            "description": ""
          },
          "and": {
            "type": "Software*. PhD thesis, Department of Applied Mathematics, The Hong",
            "description": ""
          },
          "On": {
            "type": "indefinite problems it requires usually less iterations than the",
            "description": "`trust-ncg` method and is recommended for medium and large-scale problems."
          },
          "quadratic": {
            "type": "subproblems are solved almost exactly [13]_. This",
            "description": ""
          },
          "Simplex": {
            "type": "algorithm [1]_, [2]_. This algorithm is robust in many",
            "description": "applications. However, if numerical computation of derivative can be\ntrusted, other algorithms using the first and/or second derivatives"
          },
          "information": {
            "type": "might be preferred for their better performance in",
            "description": "general."
          },
          "each": {
            "type": "variable to be given upper and lower bounds.",
            "description": "**Constrained Minimization**"
          },
          "unbounded": {
            "type": "line search will be used. If bounds are provided and",
            "description": ""
          },
          "evaluation": {
            "type": "throughout the minimization procedure will be within",
            "description": ""
          },
          "some": {
            "type": "function evaluations during the first iteration may be",
            "description": ""
          },
          "outside": {
            "type": "the bounds, but every function evaluation after the first",
            "description": ""
          },
          "iteration": {
            "type": "will be within the bounds. If `direc` is not full rank,",
            "description": ""
          },
          "then": {
            "type": "some parameters may not be optimized and the solution is not",
            "description": ""
          },
          "guaranteed": {
            "type": "to be within the bounds.",
            "description": ""
          },
          "called": {
            "type": "Newton Conjugate-Gradient. It differs from the *Newton-CG*",
            "description": ""
          },
          "Constrained": {
            "type": "Optimization BY Linear Approximation (COBYLA) method",
            "description": "[9]_, [10]_, [11]_. The algorithm is based on linear"
          },
          "approximations": {
            "type": "to the objective function and each nonlinear constraint. The",
            "description": ""
          },
          "Optimization": {
            "type": "terminated successfully.",
            "description": ">>> res.hess_inv\narray([\n[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n[ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n[ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n[ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n[ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n])\nNext, consider a minimization problem with several constraints (namely"
          },
          "Least": {
            "type": "SQuares Programming to minimize a function of several",
            "description": ""
          },
          "variables": {
            "type": "with any combination of bounds, equality and inequality",
            "description": "constraints. The method wraps the SLSQP Optimization subroutine"
          },
          "originally": {
            "type": "implemented by Dieter Kraft [12]_. Note that the",
            "description": ""
          },
          "wrapper": {
            "type": "handles infinite values in bounds by converting them into",
            "description": ""
          },
          "large": {
            "type": "floating values.",
            "description": ""
          },
          "between": {
            "type": "two implementations depending on the problem definition.",
            "description": ""
          },
          "It": {
            "type": "should converge to the theoretical solution (1.4 ,1.7).",
            "description": ""
          },
          "implemented": {
            "type": "in SciPy and the most appropriate for large-scale problems.",
            "description": ""
          },
          "inequality": {
            "type": "constraints are imposed as well, it switches to the trust-region",
            "description": ""
          },
          "interior": {
            "type": "point method described in [16]_. This interior point algorithm,",
            "description": ""
          },
          "in": {
            "type": "turn, solves inequality constraints by introducing slack variables",
            "description": ""
          },
          "for": {
            "type": "progressively smaller values of the barrier parameter.",
            "description": ""
          },
          "used": {
            "type": "to solve the subproblems with increasing levels of accuracy",
            "description": ""
          },
          "as": {
            "type": "the iterate gets closer to a solution.",
            "description": "**Finite-Difference Options**"
          },
          "three": {
            "type": "finite-difference schemes: {'2-point', '3-point', 'cs'}.",
            "description": ""
          },
          "requires": {
            "type": "the function to correctly handle complex inputs and to",
            "description": ""
          },
          "accurate": {
            "type": "than '2-point' but requires twice as many operations. If the",
            "description": ""
          },
          "estimated": {
            "type": "using one of the quasi-Newton strategies.",
            "description": "**Method specific options for the** `hess` **keyword**\n+--------------+------+----------+-------------------------+-----+\n| method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n+==============+======+==========+=========================+=====+\n| Newton-CG    | x    | (n, n)   | x                       | x   |\n|              |      | LO       |                         |     |\n+--------------+------+----------+-------------------------+-----+\n| dogleg       |      | (n, n)   |                         |     |\n+--------------+------+----------+-------------------------+-----+\n| trust-ncg    |      | (n, n)   | x                       | x   |\n+--------------+------+----------+-------------------------+-----+\n| trust-krylov |      | (n, n)   | x                       | x   |\n+--------------+------+----------+-------------------------+-----+\n| trust-exact  |      | (n, n)   |                         |     |\n+--------------+------+----------+-------------------------+-----+\n| trust-constr | x    | (n, n)   |  x                      | x   |\n|              |      | LO       |                         |     |\n|              |      | sp       |                         |     |\n+--------------+------+----------+-------------------------+-----+"
          },
          "when": {
            "type": "using a frontend to this method such as `scipy.optimize.basinhopping`",
            "description": ""
          },
          "its": {
            "type": "contents also passed as `method` parameters pair by pair.  Also, if",
            "description": "`jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n`fun` returns just the function values and `jac` is converted to a function"
          },
          "returning": {
            "type": "the Jacobian.  The method shall return an `OptimizeResult`",
            "description": "object."
          },
          "arbitrary": {
            "type": "parameters; the set of parameters accepted by `minimize` may",
            "description": ""
          },
          "expand": {
            "type": "in future versions and then these parameters will be passed to",
            "description": ""
          },
          "Dundee": {
            "type": "Biennial Conference in Numerical Analysis (Eds. D F",
            "description": ""
          },
          "Griffiths": {
            "type": "and G A Watson). Addison Wesley Longman, Harlow, UK.",
            "description": "191-208.\n.. [3] Powell, M J D. 1964. An efficient method for finding the minimum of"
          },
          "Computer": {
            "type": "Journal 7: 155-162.",
            "description": ".. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery."
          },
          "Numerical": {
            "type": "Recipes (any edition), Cambridge University Press.",
            "description": ".. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization."
          },
          "Springer": {
            "type": "New York.",
            "description": ".. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory"
          },
          "Algorithm": {
            "type": "for Bound Constrained Optimization. SIAM Journal on",
            "description": ""
          },
          "Scientific": {
            "type": "and Statistical Computing 16 (5): 1190-1208.",
            "description": ".. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm"
          },
          "778": {
            "type": "L",
            "description": "BFGS-B, FORTRAN routines for large scale bound constrained\noptimization. ACM Transactions on Mathematical Software 23 (4):\n550-560.\n.. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n1984. SIAM Journal of Numerical Analysis 21: 770-778.\n.. [9] Powell, M J D. A direct search optimization method that models"
          },
          "Center": {
            "type": "",
            "description": "Institute for Flight Mechanics, Koln, Germany.\n.. [13] Conn, A. R., Gould, N. I., and Toint, P. L."
          },
          "Trust": {
            "type": "region methods. 2000. Siam. pp. 169-200.",
            "description": ".. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free"
          },
          "implementation": {
            "type": "of an algorithm for large-scale equality constrained",
            "description": "optimization. SIAM Journal on Optimization 8.3: 682-706.\n.. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods"
          },
          "SIAM": {
            "type": "Journal on Optimization 9.4: 877-900.",
            "description": ".. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the"
          },
          "An": {
            "type": "interior point algorithm for large-scale nonlinear  programming.",
            "description": ""
          },
          "Kong": {
            "type": "Polytechnic University, Hong Kong, China, 2022. URL:",
            "description": ""
          },
          "https": {
            "type": "//theses.lib.polyu.edu.hk/handle/200/12294.",
            "description": "Examples\n--------"
          },
          "Let": {
            "type": "us consider the problem of minimizing the Rosenbrock function. This",
            "description": ""
          },
          "array": {
            "type": "[ 1.,  1.,  1.,  1.,  1.]",
            "description": ">>> print(res.message)"
          },
          "Now": {
            "type": "using the *BFGS* algorithm, using the first derivative and a few",
            "description": ""
          },
          "Current": {
            "type": "function value: 0.000000",
            "description": ""
          },
          "Iterations": {
            "type": "26",
            "description": ""
          },
          "Function": {
            "type": "evaluations: 31",
            "description": ""
          },
          "Gradient": {
            "type": "evaluations: 31",
            "description": ">>> res.x"
          },
          "Example": {
            "type": "16.4 from [5]_). The objective function is:",
            "description": ">>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2"
          },
          "There": {
            "type": "are three constraints defined as:",
            "description": ">>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})"
          },
          "And": {
            "type": "variables must be positive, hence the following bounds:",
            "description": ">>> bnds = ((0, None), (0, None))"
          }
        },
        "returns": "-------\n    res : OptimizeResult\n        The optimization result represented as a ``OptimizeResult`` object.\n        Important attributes are: ``x`` the solution array, ``success`` a\n        Boolean flag indicating if the optimizer exited successfully and\n        ``message`` which describes the cause of the termination. See\n        `OptimizeResult` for a description of other attributes.\n\n    See also\n    --------\n    minimize_scalar : Interface to minimization algorithms for scalar\n        univariate functions\n    show_options : Additional options accepted by the solvers\n\n    Notes\n    -----\n    This section describes the available solvers that can be selected by the\n    'method' parameter. The default method is *BFGS*.\n\n    **Unconstrained minimization**\n\n    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n    gradient algorithm by Polak and Ribiere, a variant of the\n    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n    first derivatives are used.\n\n    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n    pp. 136. It uses the first derivatives only. BFGS has proven good\n    performance even for non-smooth optimizations. This method also",
        "raises": "",
        "see_also": "--------\n    minimize_scalar : Interface to minimization algorithms for scalar\n        univariate functions\n    show_options : Additional options accepted by the solvers\n\n    Notes\n    -----\n    This section describes the available solvers that can be selected by the\n    'method' parameter. The default method is *BFGS*.\n\n    **Unconstrained minimization**\n\n    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n    gradient algorithm by Polak and Ribiere, a variant of the\n    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n    first derivatives are used.\n\n    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n    pp. 136. It uses the first derivatives only. BFGS has proven good\n    performance even for non-smooth optimizations. This method also\n    returns an approximation of the Hessian inverse, stored as\n    `hess_inv` in the OptimizeResult object.\n\n    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n    Newton method). It uses a CG method to the compute the search\n    direction. See also *TNC* method for a box-constrained\n    minimization with a similar algorithm. Suitable for large-scale\n    problems.\n\n    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n    trust-region algorithm [5]_ for unconstrained minimization. This\n    algorithm requires the gradient and Hessian; furthermore the\n    Hessian is required to be positive definite.\n\n    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n    Newton conjugate gradient trust-region algorithm [5]_ for\n    unconstrained minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n\n    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n    minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n    On indefinite problems it requires usually less iterations than the\n    `trust-ncg` method and is recommended for medium and large-scale problems.\n\n    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n    is a trust-region method for unconstrained minimization in which\n    quadratic subproblems are solved almost exactly [13]_. This\n    algorithm requires the gradient and the Hessian (which is\n    *not* required to be positive definite). It is, in many\n    situations, the Newton method to converge in fewer iterations\n    and the most recommended for small and medium-size problems.\n\n    **Bound-Constrained minimization**\n\n    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n    applications. However, if numerical computation of derivative can be\n    trusted, other algorithms using the first and/or second derivatives\n    information might be preferred for their better performance in\n    general.\n\n    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n    algorithm [6]_, [7]_ for bound constrained minimization.\n\n    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n    of Powell's method [3]_, [4]_ which is a conjugate direction\n    method. It performs sequential one-dimensional minimizations along\n    each vector of the directions set (`direc` field in `options` and\n    `info`), which is updated at each iteration of the main\n    minimization loop. The function need not be differentiable, and no\n    derivatives are taken. If bounds are not provided, then an\n    unbounded line search will be used. If bounds are provided and\n    the initial guess is within the bounds, then every function\n    evaluation throughout the minimization procedure will be within\n    the bounds. If bounds are provided, the initial guess is outside\n    the bounds, and `direc` is full rank (default has full rank), then\n    some function evaluations during the first iteration may be\n    outside the bounds, but every function evaluation after the first\n    iteration will be within the bounds. If `direc` is not full rank,\n    then some parameters may not be optimized and the solution is not\n    guaranteed to be within the bounds.\n\n    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n    algorithm [5]_, [8]_ to minimize a function with variables subject\n    to bounds. This algorithm uses gradient information; it is also\n    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n    method described above as it wraps a C implementation and allows\n    each variable to be given upper and lower bounds.\n\n    **Constrained Minimization**\n\n    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n    Constrained Optimization BY Linear Approximation (COBYLA) method\n    [9]_, [10]_, [11]_. The algorithm is based on linear\n    approximations to the objective function and each constraint. The\n    method wraps a FORTRAN implementation of the algorithm. The\n    constraints functions 'fun' may return either a single number\n    or an array or list of numbers.\n\n    Method :ref:`COBYQA <optimize.minimize-cobyqa>` uses the Constrained\n    Optimization BY Quadratic Approximations (COBYQA) method [18]_. The\n    algorithm is a derivative-free trust-region SQP method based on quadratic\n    approximations to the objective function and each nonlinear constraint. The\n    bounds are treated as unrelaxable constraints, in the sense that the\n    algorithm always respects them throughout the optimization process.\n\n    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n    Least SQuares Programming to minimize a function of several\n    variables with any combination of bounds, equality and inequality\n    constraints. The method wraps the SLSQP Optimization subroutine\n    originally implemented by Dieter Kraft [12]_. Note that the\n    wrapper handles infinite values in bounds by converting them into\n    large floating values.\n\n    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n    trust-region algorithm for constrained optimization. It switches\n    between two implementations depending on the problem definition.\n    It is the most versatile constrained minimization algorithm\n    implemented in SciPy and the most appropriate for large-scale problems.\n    For equality constrained problems it is an implementation of Byrd-Omojokun\n    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n    inequality constraints are imposed as well, it switches to the trust-region\n    interior point method described in [16]_. This interior point algorithm,\n    in turn, solves inequality constraints by introducing slack variables\n    and solving a sequence of equality-constrained barrier problems\n    for progressively smaller values of the barrier parameter.\n    The previously described equality constrained SQP method is\n    used to solve the subproblems with increasing levels of accuracy\n    as the iterate gets closer to a solution.\n\n    **Finite-Difference Options**\n\n    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n    the gradient and the Hessian may be approximated using\n    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n    The scheme 'cs' is, potentially, the most accurate but it\n    requires the function to correctly handle complex inputs and to\n    be differentiable in the complex plane. The scheme '3-point' is more\n    accurate than '2-point' but requires twice as many operations. If the\n    gradient is estimated via finite-differences the Hessian must be\n    estimated using one of the quasi-Newton strategies.\n\n    **Method specific options for the** `hess` **keyword**\n\n    +--------------+------+----------+-------------------------+-----+\n    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n    +==============+======+==========+=========================+=====+\n    | Newton-CG    | x    | (n, n)   | x                       | x   |\n    |              |      | LO       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | dogleg       |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-ncg    |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-krylov |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-exact  |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-constr | x    | (n, n)   |  x                      | x   |\n    |              |      | LO       |                         |     |\n    |              |      | sp       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n\n    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n\n    **Custom minimizers**\n\n    It may be useful to pass a custom minimization method, for example\n    when using a frontend to this method such as `scipy.optimize.basinhopping`\n    or a different library.  You can simply pass a callable as the ``method``\n    parameter.\n\n    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n    its contents also passed as `method` parameters pair by pair.  Also, if\n    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n    `fun` returns just the function values and `jac` is converted to a function\n    returning the Jacobian.  The method shall return an `OptimizeResult`\n    object.\n\n    The provided `method` callable must be able to accept (and possibly ignore)\n    arbitrary parameters; the set of parameters accepted by `minimize` may\n    expand in future versions and then these parameters will be passed to\n    the method.  You can find an example in the scipy.optimize tutorial.\n\n    References\n    ----------\n    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n        Minimization. The Computer Journal 7: 308-13.\n    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n        191-208.\n    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n       a function of several variables without calculating derivatives. The\n       Computer Journal 7: 155-162.\n    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n       Numerical Recipes (any edition), Cambridge University Press.\n    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n       Springer New York.\n    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n       Algorithm for Bound Constrained Optimization. SIAM Journal on\n       Scientific and Statistical Computing 16 (5): 1190-1208.\n    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n       optimization. ACM Transactions on Mathematical Software 23 (4):\n       550-560.\n    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n    .. [9] Powell, M J D. A direct search optimization method that models\n       the objective and constraint functions by linear interpolation.\n       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n    .. [10] Powell M J D. Direct search algorithms for optimization\n       calculations. 1998. Acta Numerica 7: 287-336.\n    .. [11] Powell M J D. A view of algorithms for optimization without\n       derivatives. 2007.Cambridge University Technical Report DAMTP\n       2007/NA03\n    .. [12] Kraft, D. A software package for sequential quadratic\n       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n       Center -- Institute for Flight Mechanics, Koln, Germany.\n    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n       Trust region methods. 2000. Siam. pp. 169-200.\n    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n       implementation of the GLTR method for iterative solution of\n       the trust region problem\", :arxiv:`1611.04718`\n    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n       Trust-Region Subproblem using the Lanczos Method\",\n       SIAM J. Optim., 9(2), 504--525, (1999).\n    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n        An interior point algorithm for large-scale nonlinear  programming.\n        SIAM Journal on Optimization 9.4: 877-900.\n    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the\n        implementation of an algorithm for large-scale equality constrained\n        optimization. SIAM Journal on Optimization 8.3: 682-706.\n    .. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods\n        and Software*. PhD thesis, Department of Applied Mathematics, The Hong\n        Kong Polytechnic University, Hong Kong, China, 2022. URL:\n        https://theses.lib.polyu.edu.hk/handle/200/12294.\n\n    Examples\n    --------\n    Let us consider the problem of minimizing the Rosenbrock function. This\n    function (and its respective derivatives) is implemented in `rosen`\n    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n\n    >>> from scipy.optimize import minimize, rosen, rosen_der\n\n    A simple application of the *Nelder-Mead* method is:\n\n    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n\n    Now using the *BFGS* algorithm, using the first derivative and a few\n    options:\n\n    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n    ...                options={'gtol': 1e-6, 'disp': True})\n    Optimization terminated successfully.\n             Current function value: 0.000000\n             Iterations: 26\n             Function evaluations: 31\n             Gradient evaluations: 31\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n    >>> print(res.message)\n    Optimization terminated successfully.\n    >>> res.hess_inv\n    array([\n        [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n        [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n        [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n        [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n        [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n    ])\n\n\n    Next, consider a minimization problem with several constraints (namely\n    Example 16.4 from [5]_). The objective function is:\n\n    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n\n    There are three constraints defined as:\n\n    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n\n    And variables must be positive, hence the following bounds:\n\n    >>> bnds = ((0, None), (0, None))\n\n    The optimization problem is solved using the SLSQP method as:\n\n    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n    ...                constraints=cons)\n\n    It should converge to the theoretical solution (1.4 ,1.7).",
        "notes": "that COBYLA only supports inequality constraints.\n\n    tol : float, optional\n        Tolerance for termination. When `tol` is specified, the selected\n        minimization algorithm sets some relevant solver-specific tolerance(s)\n        equal to `tol`. For detailed control, use solver-specific\n        options.\n    options : dict, optional\n        A dictionary of solver options. All methods except `TNC` accept the\n        following generic options:\n\n        maxiter : int\n            Maximum number of iterations to perform. Depending on the\n            method each iteration may use several function evaluations.\n\n            For `TNC` use `maxfun` instead of `maxiter`.\n        disp : bool\n            Set to True to print convergence messages.\n\n        For method-specific options, see :func:`show_options()`.\n    callback : callable, optional\n        A callable called after each iteration.\n\n        All methods except TNC, SLSQP, and COBYLA support a callable with\n        the signature::\n\n            callback(intermediate_result: OptimizeResult)\n\n        where ``intermediate_result`` is a keyword parameter containing an\n        `OptimizeResult` with attributes ``x`` and ``fun``, the present values\n        of the parameter vector and objective function. Note that the name\n        of the parameter must be ``intermediate_result`` for the callback\n        to be passed an `OptimizeResult`. These methods will also terminate if\n        the callback raises ``StopIteration``.\n\n        All methods except trust-constr (also) support a signature like::\n\n            callback(xk)\n\n        where ``xk`` is the current parameter vector.\n\n        Introspection is used to determine which of the signatures above to\n        invoke.\n\n    Returns\n    -------\n    res : OptimizeResult\n        The optimization result represented as a ``OptimizeResult`` object.\n        Important attributes are: ``x`` the solution array, ``success`` a\n        Boolean flag indicating if the optimizer exited successfully and\n        ``message`` which describes the cause of the termination. See\n        `OptimizeResult` for a description of other attributes.\n\n    See also\n    --------\n    minimize_scalar : Interface to minimization algorithms for scalar\n        univariate functions\n    show_options : Additional options accepted by the solvers\n\n    Notes\n    -----\n    This section describes the available solvers that can be selected by the\n    'method' parameter. The default method is *BFGS*.\n\n    **Unconstrained minimization**\n\n    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n    gradient algorithm by Polak and Ribiere, a variant of the\n    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n    first derivatives are used.\n\n    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n    pp. 136. It uses the first derivatives only. BFGS has proven good\n    performance even for non-smooth optimizations. This method also\n    returns an approximation of the Hessian inverse, stored as\n    `hess_inv` in the OptimizeResult object.\n\n    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n    Newton method). It uses a CG method to the compute the search\n    direction. See also *TNC* method for a box-constrained\n    minimization with a similar algorithm. Suitable for large-scale\n    problems.\n\n    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n    trust-region algorithm [5]_ for unconstrained minimization. This\n    algorithm requires the gradient and Hessian; furthermore the\n    Hessian is required to be positive definite.\n\n    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n    Newton conjugate gradient trust-region algorithm [5]_ for\n    unconstrained minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n\n    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n    minimization. This algorithm requires the gradient\n    and either the Hessian or a function that computes the product of\n    the Hessian with a given vector. Suitable for large-scale problems.\n    On indefinite problems it requires usually less iterations than the\n    `trust-ncg` method and is recommended for medium and large-scale problems.\n\n    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n    is a trust-region method for unconstrained minimization in which\n    quadratic subproblems are solved almost exactly [13]_. This\n    algorithm requires the gradient and the Hessian (which is\n    *not* required to be positive definite). It is, in many\n    situations, the Newton method to converge in fewer iterations\n    and the most recommended for small and medium-size problems.\n\n    **Bound-Constrained minimization**\n\n    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n    applications. However, if numerical computation of derivative can be\n    trusted, other algorithms using the first and/or second derivatives\n    information might be preferred for their better performance in\n    general.\n\n    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n    algorithm [6]_, [7]_ for bound constrained minimization.\n\n    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n    of Powell's method [3]_, [4]_ which is a conjugate direction\n    method. It performs sequential one-dimensional minimizations along\n    each vector of the directions set (`direc` field in `options` and\n    `info`), which is updated at each iteration of the main\n    minimization loop. The function need not be differentiable, and no\n    derivatives are taken. If bounds are not provided, then an\n    unbounded line search will be used. If bounds are provided and\n    the initial guess is within the bounds, then every function\n    evaluation throughout the minimization procedure will be within\n    the bounds. If bounds are provided, the initial guess is outside\n    the bounds, and `direc` is full rank (default has full rank), then\n    some function evaluations during the first iteration may be\n    outside the bounds, but every function evaluation after the first\n    iteration will be within the bounds. If `direc` is not full rank,\n    then some parameters may not be optimized and the solution is not\n    guaranteed to be within the bounds.\n\n    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n    algorithm [5]_, [8]_ to minimize a function with variables subject\n    to bounds. This algorithm uses gradient information; it is also\n    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n    method described above as it wraps a C implementation and allows\n    each variable to be given upper and lower bounds.\n\n    **Constrained Minimization**\n\n    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n    Constrained Optimization BY Linear Approximation (COBYLA) method\n    [9]_, [10]_, [11]_. The algorithm is based on linear\n    approximations to the objective function and each constraint. The\n    method wraps a FORTRAN implementation of the algorithm. The\n    constraints functions 'fun' may return either a single number\n    or an array or list of numbers.\n\n    Method :ref:`COBYQA <optimize.minimize-cobyqa>` uses the Constrained\n    Optimization BY Quadratic Approximations (COBYQA) method [18]_. The\n    algorithm is a derivative-free trust-region SQP method based on quadratic\n    approximations to the objective function and each nonlinear constraint. The\n    bounds are treated as unrelaxable constraints, in the sense that the\n    algorithm always respects them throughout the optimization process.\n\n    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n    Least SQuares Programming to minimize a function of several\n    variables with any combination of bounds, equality and inequality\n    constraints. The method wraps the SLSQP Optimization subroutine\n    originally implemented by Dieter Kraft [12]_. Note that the\n    wrapper handles infinite values in bounds by converting them into\n    large floating values.\n\n    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n    trust-region algorithm for constrained optimization. It switches\n    between two implementations depending on the problem definition.\n    It is the most versatile constrained minimization algorithm\n    implemented in SciPy and the most appropriate for large-scale problems.\n    For equality constrained problems it is an implementation of Byrd-Omojokun\n    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n    inequality constraints are imposed as well, it switches to the trust-region\n    interior point method described in [16]_. This interior point algorithm,\n    in turn, solves inequality constraints by introducing slack variables\n    and solving a sequence of equality-constrained barrier problems\n    for progressively smaller values of the barrier parameter.\n    The previously described equality constrained SQP method is\n    used to solve the subproblems with increasing levels of accuracy\n    as the iterate gets closer to a solution.\n\n    **Finite-Difference Options**\n\n    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n    the gradient and the Hessian may be approximated using\n    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n    The scheme 'cs' is, potentially, the most accurate but it\n    requires the function to correctly handle complex inputs and to\n    be differentiable in the complex plane. The scheme '3-point' is more\n    accurate than '2-point' but requires twice as many operations. If the\n    gradient is estimated via finite-differences the Hessian must be\n    estimated using one of the quasi-Newton strategies.\n\n    **Method specific options for the** `hess` **keyword**\n\n    +--------------+------+----------+-------------------------+-----+\n    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n    +==============+======+==========+=========================+=====+\n    | Newton-CG    | x    | (n, n)   | x                       | x   |\n    |              |      | LO       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | dogleg       |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-ncg    |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-krylov |      | (n, n)   | x                       | x   |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-exact  |      | (n, n)   |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n    | trust-constr | x    | (n, n)   |  x                      | x   |\n    |              |      | LO       |                         |     |\n    |              |      | sp       |                         |     |\n    +--------------+------+----------+-------------------------+-----+\n\n    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n\n    **Custom minimizers**\n\n    It may be useful to pass a custom minimization method, for example\n    when using a frontend to this method such as `scipy.optimize.basinhopping`\n    or a different library.  You can simply pass a callable as the ``method``\n    parameter.\n\n    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n    its contents also passed as `method` parameters pair by pair.  Also, if\n    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n    `fun` returns just the function values and `jac` is converted to a function\n    returning the Jacobian.  The method shall return an `OptimizeResult`\n    object.\n\n    The provided `method` callable must be able to accept (and possibly ignore)\n    arbitrary parameters; the set of parameters accepted by `minimize` may\n    expand in future versions and then these parameters will be passed to\n    the method.  You can find an example in the scipy.optimize tutorial.\n\n    References\n    ----------\n    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n        Minimization. The Computer Journal 7: 308-13.\n    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n        191-208.\n    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n       a function of several variables without calculating derivatives. The\n       Computer Journal 7: 155-162.\n    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n       Numerical Recipes (any edition), Cambridge University Press.\n    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n       Springer New York.\n    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n       Algorithm for Bound Constrained Optimization. SIAM Journal on\n       Scientific and Statistical Computing 16 (5): 1190-1208.\n    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n       optimization. ACM Transactions on Mathematical Software 23 (4):\n       550-560.\n    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n    .. [9] Powell, M J D. A direct search optimization method that models\n       the objective and constraint functions by linear interpolation.\n       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n    .. [10] Powell M J D. Direct search algorithms for optimization\n       calculations. 1998. Acta Numerica 7: 287-336.\n    .. [11] Powell M J D. A view of algorithms for optimization without\n       derivatives. 2007.Cambridge University Technical Report DAMTP\n       2007/NA03\n    .. [12] Kraft, D. A software package for sequential quadratic\n       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n       Center -- Institute for Flight Mechanics, Koln, Germany.\n    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n       Trust region methods. 2000. Siam. pp. 169-200.\n    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n       implementation of the GLTR method for iterative solution of\n       the trust region problem\", :arxiv:`1611.04718`\n    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n       Trust-Region Subproblem using the Lanczos Method\",\n       SIAM J. Optim., 9(2), 504--525, (1999).\n    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n        An interior point algorithm for large-scale nonlinear  programming.\n        SIAM Journal on Optimization 9.4: 877-900.\n    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the\n        implementation of an algorithm for large-scale equality constrained\n        optimization. SIAM Journal on Optimization 8.3: 682-706.\n    .. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods\n        and Software*. PhD thesis, Department of Applied Mathematics, The Hong\n        Kong Polytechnic University, Hong Kong, China, 2022. URL:\n        https://theses.lib.polyu.edu.hk/handle/200/12294.\n\n    Examples\n    --------\n    Let us consider the problem of minimizing the Rosenbrock function. This\n    function (and its respective derivatives) is implemented in `rosen`\n    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n\n    >>> from scipy.optimize import minimize, rosen, rosen_der\n\n    A simple application of the *Nelder-Mead* method is:\n\n    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n\n    Now using the *BFGS* algorithm, using the first derivative and a few\n    options:\n\n    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n    ...                options={'gtol': 1e-6, 'disp': True})\n    Optimization terminated successfully.\n             Current function value: 0.000000\n             Iterations: 26\n             Function evaluations: 31\n             Gradient evaluations: 31\n    >>> res.x\n    array([ 1.,  1.,  1.,  1.,  1.])\n    >>> print(res.message)\n    Optimization terminated successfully.\n    >>> res.hess_inv\n    array([\n        [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n        [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n        [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n        [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n        [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n    ])\n\n\n    Next, consider a minimization problem with several constraints (namely\n    Example 16.4 from [5]_). The objective function is:\n\n    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n\n    There are three constraints defined as:\n\n    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n\n    And variables must be positive, hence the following bounds:\n\n    >>> bnds = ((0, None), (0, None))\n\n    The optimization problem is solved using the SLSQP method as:\n\n    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n    ...                constraints=cons)\n\n    It should converge to the theoretical solution (1.4 ,1.7).",
        "examples": "16.4 from [5]_). The objective function is:\n\n    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n\n    There are three constraints defined as:\n\n    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n\n    And variables must be positive, hence the following bounds:\n\n    >>> bnds = ((0, None), (0, None))\n\n    The optimization problem is solved using the SLSQP method as:\n\n    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n    ...                constraints=cons)\n\n    It should converge to the theoretical solution (1.4 ,1.7)."
      }
    },
    {
      "name": "process_routing",
      "signature": "process_routing(_obj, _method, /, **kwargs)",
      "documentation": {
        "description": "Validate and route input parameters.\n\n    This function is used inside a router's method, e.g. :term:`fit`,\n    to validate the metadata and handle the routing.\n\n    Assuming this signature of a router's fit method:\n    ``fit(self, X, y, sample_weight=None, **fit_params)``,\n    a call to this function would be:\n    ``process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params)``.",
        "parameters": {
          "_obj": {
            "type": "object",
            "description": ""
          },
          "An": {
            "type": "object implementing ``get_metadata_routing``. Typically a",
            "description": "meta-estimator."
          },
          "_method": {
            "type": "str",
            "description": ""
          },
          "The": {
            "type": "name of the router's method in which this function is called.",
            "description": "**kwargs : dict"
          },
          "Metadata": {
            "type": "to be routed.",
            "description": "Returns\n-------"
          },
          "routed_params": {
            "type": "Bunch",
            "description": ""
          },
          "A": {
            "type": "class:`~sklearn.utils.Bunch` of the form ``{\"object_name\": {\"method_name\":",
            "description": "{params: value}}}`` which can be used to pass the required metadata to"
          },
          "corresponding": {
            "type": "methods or corresponding child objects. The object names",
            "description": ""
          },
          "are": {
            "type": "those defined in `obj.get_metadata_routing()`.",
            "description": ""
          }
        },
        "returns": "an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``\n    is always an empty dictionary.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    _obj : object\n        An object implementing ``get_metadata_routing``. Typically a\n        meta-estimator.\n\n    _method : str\n        The name of the router's method in which this function is called.\n\n    **kwargs : dict\n        Metadata to be routed.",
        "raises": "",
        "see_also": "",
        "notes": "that if routing is not enabled and ``kwargs`` is empty, then it\n    returns an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``\n    is always an empty dictionary.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    _obj : object\n        An object implementing ``get_metadata_routing``. Typically a\n        meta-estimator.\n\n    _method : str\n        The name of the router's method in which this function is called.\n\n    **kwargs : dict\n        Metadata to be routed.\n\n    Returns\n    -------\n    routed_params : Bunch\n        A :class:`~utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {params: value}}}`` which can be used to pass the required metadata to\n        A :class:`~sklearn.utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {params: value}}}`` which can be used to pass the required metadata to\n        corresponding methods or corresponding child objects. The object names\n        are those defined in `obj.get_metadata_routing()`.",
        "examples": ""
      }
    },
    {
      "name": "signature",
      "signature": "signature(obj, *, follow_wrapped=True, globals=None, locals=None, eval_str=False)",
      "documentation": {
        "description": "Get a signature object for the passed callable.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "validate_params",
      "signature": "validate_params(parameter_constraints, *, prefer_skip_nested_validation)",
      "documentation": {
        "description": "Decorator to validate types and values of functions and methods.\n\n    Parameters\n    ----------\n    parameter_constraints : dict\n        A dictionary `param_name: list of constraints`. See the docstring of\n        `validate_parameter_constraints` for a description of the accepted constraints.",
        "parameters": {
          "parameter_constraints": {
            "type": "dict",
            "description": ""
          },
          "A": {
            "type": "dictionary `param_name: list of constraints`. See the docstring of",
            "description": "`validate_parameter_constraints` for a description of the accepted constraints."
          },
          "Note": {
            "type": "that the *args and **kwargs parameters are not validated and must not be",
            "description": ""
          },
          "present": {
            "type": "in the parameter_constraints dictionary.",
            "description": ""
          },
          "prefer_skip_nested_validation": {
            "type": "bool",
            "description": ""
          },
          "If": {
            "type": "True, the validation of parameters of inner estimators or functions",
            "description": ""
          },
          "called": {
            "type": "by the decorated function will be skipped.",
            "description": ""
          },
          "This": {
            "type": "is useful to avoid validating many times the parameters passed by the",
            "description": ""
          },
          "user": {
            "type": "from the public facing API. It's also useful to avoid validating",
            "description": ""
          }
        },
        "returns": "-------\n    decorated_function : function or method\n        The decorated function.",
        "raises": "",
        "see_also": "",
        "notes": "that the *args and **kwargs parameters are not validated and must not be\n        present in the parameter_constraints dictionary.\n\n    prefer_skip_nested_validation : bool\n        If True, the validation of parameters of inner estimators or functions\n        called by the decorated function will be skipped.\n\n        This is useful to avoid validating many times the parameters passed by the\n        user from the public facing API. It's also useful to avoid validating\n        parameters that we pass internally to inner functions that are guaranteed to\n        be valid by the test suite.\n\n        It should be set to True for most functions, except for those that receive\n        non-validated objects as parameters or that are just wrappers around classes\n        because they only perform a partial validation.\n\n    Returns\n    -------\n    decorated_function : function or method\n        The decorated function.",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "BaseEstimator",
      "documentation": {
        "description": "Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).",
        "parameters": {
          "array": {
            "type": "[3, 3, 3]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])"
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Bunch",
      "documentation": {
        "description": "Container object exposing keys as attributes.\n\n    Bunch objects are sometimes used as an output for functions and methods.\n    They extend dictionaries by enabling values to be accessed by key,\n    `bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.utils import Bunch\n    >>> b = Bunch(a=1, b=2)\n    >>> b['b']\n    2\n    >>> b.b\n    2\n    >>> b.a = 3\n    >>> b['a']\n    3\n    >>> b.c = 6\n    >>> b['c']\n    6"
      },
      "methods": [
        {
          "name": "clear",
          "signature": "clear()",
          "documentation": {
            "description": "D.clear() -> None.  Remove all items from D.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy",
          "signature": "copy()",
          "documentation": {
            "description": "D.copy() -> a shallow copy of D",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fromkeys",
          "signature": "fromkeys(iterable, value=None, /)",
          "documentation": {
            "description": "Create a new dictionary with keys from iterable and values set to value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get",
          "signature": "get(self, key, default=None, /)",
          "documentation": {
            "description": "Return the value for key if key is in the dictionary, else default.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "items",
          "signature": "items()",
          "documentation": {
            "description": "D.items() -> a set-like object providing a view on D's items",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "keys",
          "signature": "keys()",
          "documentation": {
            "description": "D.keys() -> a set-like object providing a view on D's keys",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "pop",
          "signature": "pop(k[,d])",
          "documentation": {
            "description": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\nIf the key is not found, return the default if given; otherwise,\nraise a KeyError.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "popitem",
          "signature": "popitem(self, /)",
          "documentation": {
            "description": "Remove and return a (key, value) pair as a 2-tuple.\n\nPairs are returned in LIFO (last-in, first-out) order.",
            "parameters": {},
            "returns": "",
            "raises": "KeyError if the dict is empty.",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "setdefault",
          "signature": "setdefault(self, key, default=None, /)",
          "documentation": {
            "description": "Insert key with a value of default if key is not in the dictionary.",
            "parameters": {},
            "returns": "the value for key if key is in the dictionary, else default.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "update",
          "signature": "update([E, ]**F)",
          "documentation": {
            "description": "D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\nIf E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\nIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\nIn either case, this is followed by: for k in F:  D[k] = F[k]",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "values",
          "signature": "values()",
          "documentation": {
            "description": "D.values() -> an object providing a view on D's values",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "CalibratedClassifierCV",
      "documentation": {
        "description": "Probability calibration with isotonic regression or logistic regression.\n\n    This class uses cross-validation to both estimate the parameters of a\n    classifier and subsequently calibrate a classifier. With default\n    `ensemble=True`, for each cv split it\n    fits a copy of the base estimator to the training subset, and calibrates it\n    using the testing subset. For prediction, predicted probabilities are\n    averaged across these individual calibrated classifiers. When\n    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n    used for calibration. For prediction, the base estimator, trained using all\n    the data, is used. This is the prediction method implemented when\n    `probabilities=True` for :class:`~sklearn.svm.SVC` and :class:`~sklearn.svm.NuSVC`\n    estimators (see :ref:`User Guide <scores_probabilities>` for details).\n\n    Already fitted classifiers can be calibrated by wrapping the model in a\n    :class:`~sklearn.frozen.FrozenEstimator`. In this case all provided\n    data is used for calibration. The user has to take care manually that data\n    for model fitting and calibration are disjoint.\n\n    The calibration is based on the :term:`decision_function` method of the\n    `estimator` if it exists, else on :term:`predict_proba`.\n\n    Read more in the :ref:`User Guide <calibration>`.\n    In order to learn more on the CalibratedClassifierCV class, see the\n    following calibration examples:\n    :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`,\n    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`, and\n    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_multiclass.py`.\n\n    Parameters\n    ----------\n    estimator : estimator instance, default=None\n        The classifier whose output need to be calibrated to provide more\n        accurate `predict_proba` outputs. The default classifier is\n        a :class:`~sklearn.svm.LinearSVC`.\n\n        .. versionadded:: 1.2\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method (i.e. a logistic regression model) or\n        'isotonic' which is a non-parametric approach. It is not advised to\n        use isotonic calibration with too few calibration samples\n        ``(<<1000)`` since it tends to overfit.\n\n    cv : int, cross-validation generator, or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n        is used.\n\n        Refer to the :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n        .. versionchanged:: 1.6\n            `\"prefit\"` is deprecated. Use :class:`~sklearn.frozen.FrozenEstimator`\n            instead.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors.\n\n        Base estimator clones are fitted in parallel across cross-validation\n        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        .. versionadded:: 0.24\n\n    ensemble : bool, or \"auto\", default=\"auto\"\n        Determines how the calibrator is fitted.\n\n        \"auto\" will use `False` if the `estimator` is a\n        :class:`~sklearn.frozen.FrozenEstimator`, and `True` otherwise.\n\n        If `True`, the `estimator` is fitted using training data, and\n        calibrated using testing data, for each `cv` fold. The final estimator\n        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n        `n_cv` is the number of cross-validation folds. The output is the\n        average predicted probabilities of all pairs.\n\n        If `False`, `cv` is used to compute unbiased predictions, via\n        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n        used for calibration. At prediction time, the classifier used is the\n        `estimator` trained on all the data.\n        Note that this method is also internally implemented  in\n        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n        .. versionadded:: 0.24\n\n        .. versionchanged:: 1.6\n            `\"auto\"` option is added and is the default.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if `ensemble=False`)\n        The list of classifier and calibrator pairs.\n\n        - When `ensemble=True`, `n_cv` fitted `estimator` and calibrator pairs.\n          `n_cv` is the number of cross-validation folds.\n        - When `ensemble=False`, the `estimator`, fitted on all the data, and fitted\n          calibrator.\n\n        .. versionchanged:: 0.24\n            Single calibrated classifier case when `ensemble=False`.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005",
        "parameters": {
          "estimator": {
            "type": "estimator instance, default=None",
            "description": ""
          },
          "The": {
            "type": "list of classifier and calibrator pairs.",
            "description": "- When `ensemble=True`, `n_cv` fitted `estimator` and calibrator pairs.\n`n_cv` is the number of cross-validation folds.\n- When `ensemble=False`, the `estimator`, fitted on all the data, and fitted\ncalibrator.\n.. versionchanged:: 0.24"
          },
          "accurate": {
            "type": "`predict_proba` outputs. The default classifier is",
            "description": ""
          },
          "a": {
            "type": "class:`~sklearn.svm.LinearSVC`.",
            "description": ".. versionadded:: 1.2"
          },
          "method": {
            "type": "{'sigmoid', 'isotonic'}, default='sigmoid'",
            "description": ""
          },
          "corresponds": {
            "type": "to Platt's method (i.e. a logistic regression model) or",
            "description": "'isotonic' which is a non-parametric approach. It is not advised to"
          },
          "use": {
            "type": "isotonic calibration with too few calibration samples",
            "description": "``(<<1000)`` since it tends to overfit."
          },
          "cv": {
            "type": "int, cross",
            "description": "validation generator, or iterable, default=None"
          },
          "Determines": {
            "type": "how the calibrator is fitted.",
            "description": "\"auto\" will use `False` if the `estimator` is a\n:class:`~sklearn.frozen.FrozenEstimator`, and `True` otherwise."
          },
          "Possible": {
            "type": "inputs for cv are:",
            "description": "- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices."
          },
          "For": {
            "type": "integer/None inputs, if ``y`` is binary or multiclass,",
            "description": ":class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is"
          },
          "neither": {
            "type": "binary nor multiclass, :class:`~sklearn.model_selection.KFold`",
            "description": ""
          },
          "is": {
            "type": "an ensemble of `n_cv` fitted classifier and calibrator pairs, where",
            "description": "`n_cv` is the number of cross-validation folds. The output is the"
          },
          "Refer": {
            "type": "to the :ref:`User Guide <cross_validation>` for the various",
            "description": "cross-validation strategies that can be used here.\n.. versionchanged:: 0.22\n``cv`` default value if None changed from 3-fold to 5-fold.\n.. versionchanged:: 1.6\n`\"prefit\"` is deprecated. Use :class:`~sklearn.frozen.FrozenEstimator`\ninstead."
          },
          "n_jobs": {
            "type": "int, default=None",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`. Only defined if the",
            "description": ""
          },
          "Base": {
            "type": "estimator clones are fitted in parallel across cross-validation",
            "description": "iterations. Therefore parallelism happens only when `cv != \"prefit\"`."
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "ensemble": {
            "type": "bool, or \"auto\", default=\"auto\"",
            "description": ""
          },
          "If": {
            "type": "`False`, `cv` is used to compute unbiased predictions, via",
            "description": ":func:`~sklearn.model_selection.cross_val_predict`, which are then"
          },
          "calibrated": {
            "type": "using testing data, for each `cv` fold. The final estimator",
            "description": ""
          },
          "average": {
            "type": "predicted probabilities of all pairs.",
            "description": ""
          },
          "used": {
            "type": "for calibration. At prediction time, the classifier used is the",
            "description": "`estimator` trained on all the data."
          },
          "Note": {
            "type": "that this method is also internally implemented  in",
            "description": ":mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n.. versionadded:: 0.24\n.. versionchanged:: 1.6\n`\"auto\"` option is added and is the default.\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "underlying": {
            "type": "estimator exposes such an attribute when fit.",
            "description": ".. versionadded:: 1.0"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Only defined if the",
            "description": ""
          },
          "calibrated_classifiers_": {
            "type": "list (len() equal to cv or 1 if `ensemble=False`)",
            "description": ""
          },
          "Single": {
            "type": "calibrated classifier case when `ensemble=False`.",
            "description": ""
          },
          "calibration_curve": {
            "type": "Compute true and predicted probabilities",
            "description": ""
          },
          "for": {
            "type": "a calibration curve.",
            "description": "References\n----------\n.. [1] Obtaining calibrated probability estimates from decision trees"
          },
          "and": {
            "type": "naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001",
            "description": ".. [2] Transforming Classifier Scores into Accurate Multiclass"
          },
          "Probability": {
            "type": "Estimates, B. Zadrozny & C. Elkan, (KDD 2002)",
            "description": ".. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to"
          },
          "Regularized": {
            "type": "Likelihood Methods, J. Platt, (1999)",
            "description": ".. [4] Predicting Good Probabilities with Supervised Learning,\nA. Niculescu-Mizil & R. Caruana, ICML 2005\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.calibration import CalibratedClassifierCV\n>>> X, y = make_classification(n_samples=100, n_features=2,\n...                            n_redundant=0, random_state=42)\n>>> base_clf = GaussianNB()\n>>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n>>> calibrated_clf.fit(X, y)"
          },
          "CalibratedClassifierCV": {
            "type": "...",
            "description": ">>> len(calibrated_clf.calibrated_classifiers_)\n1\n>>> calibrated_clf.predict_proba([[-0.5, 0.5]])"
          },
          "GaussianNB": {
            "type": "",
            "description": ">>> from sklearn.frozen import FrozenEstimator\n>>> calibrated_clf = CalibratedClassifierCV(FrozenEstimator(base_clf))\n>>> calibrated_clf.fit(X_calib, y_calib)"
          },
          "array": {
            "type": "[[0.936..., 0.063...]]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> from sklearn.frozen import FrozenEstimator\n    >>> calibrated_clf = CalibratedClassifierCV(FrozenEstimator(base_clf))\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])",
        "notes": "that this method is also internally implemented  in\n        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n        .. versionadded:: 0.24\n\n        .. versionchanged:: 1.6\n            `\"auto\"` option is added and is the default.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if `ensemble=False`)\n        The list of classifier and calibrator pairs.\n\n        - When `ensemble=True`, `n_cv` fitted `estimator` and calibrator pairs.\n          `n_cv` is the number of cross-validation folds.\n        - When `ensemble=False`, the `estimator`, fitted on all the data, and fitted\n          calibrator.\n\n        .. versionchanged:: 0.24\n            Single calibrated classifier case when `ensemble=False`.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> from sklearn.frozen import FrozenEstimator\n    >>> calibrated_clf = CalibratedClassifierCV(FrozenEstimator(base_clf))\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])",
        "examples": "--------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> from sklearn.frozen import FrozenEstimator\n    >>> calibrated_clf = CalibratedClassifierCV(FrozenEstimator(base_clf))\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None, **fit_params)",
          "documentation": {
            "description": "Fit the calibrated model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        **fit_params : dict\n            Parameters to pass to the `fit` method of the underlying\n            classifier.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights. If None, then samples are equally weighted.",
                "description": "**fit_params : dict"
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRouter\n            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict the target of new samples.\n\n        The predicted class is the class that has the highest probability,\n        and can thus be different from the prediction of the uncalibrated classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "predicted class.",
                "description": ""
              },
              "C": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples,)\n            The predicted class.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "documentation": {
            "description": "Calibrated probabilities of classification.\n\n        This function returns calibrated probabilities of classification\n        according to each class on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict_proba`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "The": {
                "type": "predicted probas.",
                "description": ""
              },
              "C": {
                "type": "ndarray of shape (n_samples, n_classes)",
                "description": ""
              }
            },
            "returns": "-------\n        C : ndarray of shape (n_samples, n_classes)\n            The predicted probas.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.calibration.CalibratedClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.calibration.CalibratedClassifierCV",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.calibration.CalibratedClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.calibration.CalibratedClassifierCV",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "CalibrationDisplay",
      "documentation": {
        "description": "Calibration curve (also known as reliability diagram) visualization.\n\n    It is recommended to use\n    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or\n    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions`\n    to create a `CalibrationDisplay`. All parameters are stored as attributes.\n\n    Read more about calibration in the :ref:`User Guide <calibration>` and\n    more about the scikit-learn visualization API in :ref:`visualizations`.\n\n    For an example on how to use the visualization, see\n    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    prob_true : ndarray of shape (n_bins,)\n        The proportion of samples whose class is the positive class (fraction\n        of positives), in each bin.\n\n    prob_pred : ndarray of shape (n_bins,)\n        The mean predicted probability in each bin.\n\n    y_prob : ndarray of shape (n_samples,)\n        Probability estimates for the positive class, for each sample.\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n    pos_label : int, float, bool or str, default=None\n        The positive class when computing the calibration curve.\n        By default, `pos_label` is set to `estimators.classes_[1]` when using\n        `from_estimator` and set to 1 when using `from_predictions`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist\n        Calibration curve.\n\n    ax_ : matplotlib Axes\n        Axes with calibration curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities for a\n        calibration curve.\n    CalibrationDisplay.from_predictions : Plot calibration curve using true\n        and predicted labels.\n    CalibrationDisplay.from_estimator : Plot calibration curve using an\n        estimator and data.",
        "parameters": {
          "prob_true": {
            "type": "ndarray of shape (n_bins,)",
            "description": ""
          },
          "The": {
            "type": "positive class when computing the calibration curve.",
            "description": ""
          },
          "of": {
            "type": "positives), in each bin.",
            "description": ""
          },
          "prob_pred": {
            "type": "ndarray of shape (n_bins,)",
            "description": ""
          },
          "y_prob": {
            "type": "ndarray of shape (n_samples,)",
            "description": ""
          },
          "Probability": {
            "type": "estimates for the positive class, for each sample.",
            "description": ""
          },
          "estimator_name": {
            "type": "str, default=None",
            "description": ""
          },
          "Name": {
            "type": "of estimator. If None, the estimator name is not shown.",
            "description": ""
          },
          "pos_label": {
            "type": "int, float, bool or str, default=None",
            "description": ""
          },
          "By": {
            "type": "default, `pos_label` is set to `estimators.classes_[1]` when using",
            "description": "`from_estimator` and set to 1 when using `from_predictions`.\n.. versionadded:: 1.1\nAttributes\n----------"
          },
          "line_": {
            "type": "matplotlib Artist",
            "description": ""
          },
          "Calibration": {
            "type": "curve.",
            "description": ""
          },
          "ax_": {
            "type": "matplotlib Axes",
            "description": ""
          },
          "Axes": {
            "type": "with calibration curve.",
            "description": ""
          },
          "figure_": {
            "type": "matplotlib Figure",
            "description": ""
          },
          "Figure": {
            "type": "containing the curve.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "calibration_curve": {
            "type": "Compute true and predicted probabilities for a",
            "description": ""
          },
          "calibration": {
            "type": "curve.",
            "description": "CalibrationDisplay.from_predictions : Plot calibration curve using true"
          },
          "and": {
            "type": "predicted labels.",
            "description": "CalibrationDisplay.from_estimator : Plot calibration curve using an"
          },
          "estimator": {
            "type": "and data.",
            "description": "Examples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> clf = LogisticRegression(random_state=0)\n>>> clf.fit(X_train, y_train)"
          },
          "LogisticRegression": {
            "type": "random_state=0",
            "description": ">>> y_prob = clf.predict_proba(X_test)[:, 1]\n>>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n>>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n>>> disp.plot()\n<...>"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    calibration_curve : Compute true and predicted probabilities for a\n        calibration curve.\n    CalibrationDisplay.from_predictions : Plot calibration curve using true\n        and predicted labels.\n    CalibrationDisplay.from_estimator : Plot calibration curve using an\n        estimator and data.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>"
      },
      "methods": [
        {
          "name": "from_estimator",
          "signature": "from_estimator(estimator, X, y, *, n_bins=5, strategy='uniform', pos_label=None, name=None, ref_line=True, ax=None, **kwargs)",
          "documentation": {
            "description": "Plot calibration curve using a binary classifier and data.\n\n        A calibration curve, also known as a reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        estimator : estimator instance\n            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n            in which the last estimator is a classifier. The classifier must\n            have a :term:`predict_proba` method.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input values.\n\n        y : array-like of shape (n_samples,)\n            Binary target values.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : int, float, bool or str, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve. If `None`, the name of the estimator is\n            used.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.",
            "parameters": {
              "estimator": {
                "type": "estimator instance",
                "description": ""
              },
              "Fitted": {
                "type": "classifier or a fitted :class:`~sklearn.pipeline.Pipeline`",
                "description": ""
              },
              "in": {
                "type": "which the last estimator is a classifier. The classifier must",
                "description": ""
              },
              "have": {
                "type": "a :term:`predict_proba` method.",
                "description": ""
              },
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "values.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Binary": {
                "type": "target values.",
                "description": ""
              },
              "n_bins": {
                "type": "int, default=5",
                "description": ""
              },
              "Number": {
                "type": "of bins to discretize the [0, 1] interval into when",
                "description": ""
              },
              "calculating": {
                "type": "the calibration curve. A bigger number requires more",
                "description": "data."
              },
              "strategy": {
                "type": "{'uniform', 'quantile'}, default='uniform'",
                "description": ""
              },
              "Strategy": {
                "type": "used to define the widths of the bins.",
                "description": "- `'uniform'`: The bins have identical widths.\n- `'quantile'`: The bins have the same number of samples and depend"
              },
              "on": {
                "type": "predicted probabilities.",
                "description": ""
              },
              "pos_label": {
                "type": "int, float, bool or str, default=None",
                "description": ""
              },
              "The": {
                "type": "positive class when computing the calibration curve.",
                "description": ""
              },
              "By": {
                "type": "default, `estimators.classes_[1]` is considered as the",
                "description": ""
              },
              "positive": {
                "type": "class.",
                "description": ".. versionadded:: 1.1"
              },
              "name": {
                "type": "str, default=None",
                "description": ""
              },
              "Name": {
                "type": "for labeling curve. If `None`, the name of the estimator is",
                "description": "used."
              },
              "ref_line": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "`True`, plots a reference line representing a perfectly",
                "description": ""
              },
              "calibrated": {
                "type": "classifier.",
                "description": ""
              },
              "ax": {
                "type": "matplotlib axes, default=None",
                "description": ""
              },
              "Axes": {
                "type": "object to plot on. If `None`, a new figure and axes is",
                "description": "created.\n**kwargs : dict"
              },
              "Keyword": {
                "type": "arguments to be passed to :func:`matplotlib.pyplot.plot`.",
                "description": "Returns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.calibration.CalibrationDisplay`.",
                "description": ""
              },
              "Object": {
                "type": "that stores computed values.",
                "description": ""
              },
              "See": {
                "type": "Also",
                "description": "--------\nCalibrationDisplay.from_predictions : Plot calibration curve using true"
              },
              "and": {
                "type": "predicted labels.",
                "description": "Examples\n--------\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.calibration import CalibrationDisplay\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> clf = LogisticRegression(random_state=0)\n>>> clf.fit(X_train, y_train)"
              },
              "LogisticRegression": {
                "type": "random_state=0",
                "description": ">>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n>>> plt.show()"
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()",
            "raises": "",
            "see_also": "--------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()",
            "notes": "",
            "examples": "--------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()"
          }
        },
        {
          "name": "from_predictions",
          "signature": "from_predictions(y_true, y_prob, *, n_bins=5, strategy='uniform', pos_label=None, name=None, ref_line=True, ax=None, **kwargs)",
          "documentation": {
            "description": "Plot calibration curve using true labels and predicted probabilities.\n\n        Calibration curve, also known as reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            True labels.\n\n        y_prob : array-like of shape (n_samples,)\n            The predicted probabilities of the positive class.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : int, float, bool or str, default=None\n            The positive class when computing the calibration curve.\n            By default `pos_label` is set to 1.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.",
            "parameters": {
              "y_true": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "True": {
                "type": "labels.",
                "description": ""
              },
              "y_prob": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "The": {
                "type": "positive class when computing the calibration curve.",
                "description": ""
              },
              "n_bins": {
                "type": "int, default=5",
                "description": ""
              },
              "Number": {
                "type": "of bins to discretize the [0, 1] interval into when",
                "description": ""
              },
              "calculating": {
                "type": "the calibration curve. A bigger number requires more",
                "description": "data."
              },
              "strategy": {
                "type": "{'uniform', 'quantile'}, default='uniform'",
                "description": ""
              },
              "Strategy": {
                "type": "used to define the widths of the bins.",
                "description": "- `'uniform'`: The bins have identical widths.\n- `'quantile'`: The bins have the same number of samples and depend"
              },
              "on": {
                "type": "predicted probabilities.",
                "description": ""
              },
              "pos_label": {
                "type": "int, float, bool or str, default=None",
                "description": ""
              },
              "By": {
                "type": "default `pos_label` is set to 1.",
                "description": ".. versionadded:: 1.1"
              },
              "name": {
                "type": "str, default=None",
                "description": ""
              },
              "Name": {
                "type": "for labeling curve.",
                "description": ""
              },
              "ref_line": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "`True`, plots a reference line representing a perfectly",
                "description": ""
              },
              "calibrated": {
                "type": "classifier.",
                "description": ""
              },
              "ax": {
                "type": "matplotlib axes, default=None",
                "description": ""
              },
              "Axes": {
                "type": "object to plot on. If `None`, a new figure and axes is",
                "description": "created.\n**kwargs : dict"
              },
              "Keyword": {
                "type": "arguments to be passed to :func:`matplotlib.pyplot.plot`.",
                "description": "Returns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.calibration.CalibrationDisplay`.",
                "description": ""
              },
              "Object": {
                "type": "that stores computed values.",
                "description": ""
              },
              "See": {
                "type": "Also",
                "description": "--------\nCalibrationDisplay.from_estimator : Plot calibration curve using an"
              },
              "estimator": {
                "type": "and data.",
                "description": "Examples\n--------\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.calibration import CalibrationDisplay\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> clf = LogisticRegression(random_state=0)\n>>> clf.fit(X_train, y_train)"
              },
              "LogisticRegression": {
                "type": "random_state=0",
                "description": ">>> y_prob = clf.predict_proba(X_test)[:, 1]\n>>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n>>> plt.show()"
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()",
            "raises": "",
            "see_also": "--------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()",
            "notes": "",
            "examples": "--------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()"
          }
        },
        {
          "name": "plot",
          "signature": "plot(self, *, ax=None, name=None, ref_line=True, **kwargs)",
          "documentation": {
            "description": "Plot visualization.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Parameters\n        ----------\n        ax : Matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        name : str, default=None\n            Name for labeling curve. If `None`, use `estimator_name` if\n            not `None`, otherwise no labeling is shown.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.",
            "parameters": {
              "ax": {
                "type": "Matplotlib Axes, default=None",
                "description": ""
              },
              "Axes": {
                "type": "object to plot on. If `None`, a new figure and axes is",
                "description": "created."
              },
              "name": {
                "type": "str, default=None",
                "description": ""
              },
              "Name": {
                "type": "for labeling curve. If `None`, use `estimator_name` if",
                "description": ""
              },
              "not": {
                "type": "`None`, otherwise no labeling is shown.",
                "description": ""
              },
              "ref_line": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "`True`, plots a reference line representing a perfectly",
                "description": ""
              },
              "calibrated": {
                "type": "classifier.",
                "description": "**kwargs : dict"
              },
              "Keyword": {
                "type": "arguments to be passed to :func:`matplotlib.pyplot.plot`.",
                "description": "Returns\n-------"
              },
              "display": {
                "type": ":class:`~sklearn.calibration.CalibrationDisplay`",
                "description": ""
              },
              "Object": {
                "type": "that stores computed values.",
                "description": ""
              }
            },
            "returns": "-------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`\n            Object that stores computed values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ClassifierMixin",
      "documentation": {
        "description": "Mixin class for all classifiers in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - set estimator type to `\"classifier\"` through the `estimator_type` tag;\n    - `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n      which is done by setting the classifier type tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, ClassifierMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(ClassifierMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=1)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([1, 1, 1])\n    >>> estimator.score(X, y)\n    0.66..."
      },
      "methods": [
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "FrozenEstimator",
      "documentation": {
        "description": "Estimator that wraps a fitted estimator to prevent re-fitting.\n\n    This meta-estimator takes an estimator and freezes it, in the sense that calling\n    `fit` on it has no effect. `fit_predict` and `fit_transform` are also disabled.\n    All other methods are delegated to the original estimator and original estimator's\n    attributes are accessible as well.\n\n    This is particularly useful when you have a fitted or a pre-trained model as a\n    transformer in a pipeline, and you'd like `pipeline.fit` to have no effect on this\n    step.\n\n    Parameters\n    ----------\n    estimator : estimator\n        The estimator which is to be kept frozen.\n\n    See Also\n    --------\n    None: No similar entry in the scikit-learn documentation.",
        "parameters": {
          "estimator": {
            "type": "estimator",
            "description": ""
          },
          "The": {
            "type": "estimator which is to be kept frozen.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "None": {
            "type": "No similar entry in the scikit",
            "description": "learn documentation.\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.frozen import FrozenEstimator\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = make_classification(random_state=0)\n>>> clf = LogisticRegression(random_state=0).fit(X, y)\n>>> frozen_clf = FrozenEstimator(clf)\n>>> frozen_clf.fit(X, y)  # No-op"
          },
          "FrozenEstimator": {
            "type": "estimator=LogisticRegression(random_state=0",
            "description": ")\n>>> frozen_clf.predict(X)  # Predictions from `clf.predict`"
          },
          "array": {
            "type": "...",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    None: No similar entry in the scikit-learn documentation.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.frozen import FrozenEstimator\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = make_classification(random_state=0)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> frozen_clf = FrozenEstimator(clf)\n    >>> frozen_clf.fit(X, y)  # No-op\n    FrozenEstimator(estimator=LogisticRegression(random_state=0))\n    >>> frozen_clf.predict(X)  # Predictions from `clf.predict`\n    array(...)",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.frozen import FrozenEstimator\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = make_classification(random_state=0)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> frozen_clf = FrozenEstimator(clf)\n    >>> frozen_clf.fit(X, y)  # No-op\n    FrozenEstimator(estimator=LogisticRegression(random_state=0))\n    >>> frozen_clf.predict(X)  # Predictions from `clf.predict`\n    array(...)"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, *args, **kwargs)",
          "documentation": {
            "description": "No-op.\n\n        As a frozen estimator, calling `fit` has no effect.\n\n        Parameters\n        ----------\n        X : object\n            Ignored.\n\n        y : object\n            Ignored.\n\n        *args : tuple\n            Additional positional arguments. Ignored, but present for API compatibility\n            with `self.estimator`.\n\n        **kwargs : dict\n            Additional keyword arguments. Ignored, but present for API compatibility\n            with `self.estimator`.",
            "parameters": {
              "X": {
                "type": "object",
                "description": "Ignored."
              },
              "y": {
                "type": "object",
                "description": "Ignored.\n*args : tuple"
              },
              "Additional": {
                "type": "keyword arguments. Ignored, but present for API compatibility",
                "description": ""
              },
              "with": {
                "type": "`self.estimator`.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "the instance itself.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "Ignored.\nReturns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "a `{\"estimator\": estimator}` dict. The parameters of the inner\n        estimator are not included.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            Ignored.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **kwargs)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The only valid key here is `estimator`. You cannot set the parameters of the\n        inner estimator.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "parameters.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "FrozenEstimator",
                "description": ""
              },
              "This": {
                "type": "estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : FrozenEstimator\n            This estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HalfBinomialLoss",
      "documentation": {
        "description": "Half Binomial deviance loss with logit link, for binary classification.\n\n    This is also know as binary cross entropy, log-loss and logistic loss.\n\n    Domain:\n    y_true in [0, 1], i.e. regression on the unit interval\n    y_pred in (0, 1), i.e. boundaries excluded\n\n    Link:\n    y_pred = expit(raw_prediction)\n\n    For a given sample x_i, half Binomial deviance is defined as the negative\n    log-likelihood of the Binomial/Bernoulli distribution and can be expressed\n    as::\n\n        loss(x_i) = log(1 + exp(raw_pred_i)) - y_true_i * raw_pred_i\n\n    See The Elements of Statistical Learning, by Hastie, Tibshirani, Friedman,\n    section 4.4.1 (about logistic regression).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "that the formulation works for classification, y = {0, 1}, as well as\n    logistic regression, y = [0, 1].\n    If you add `constant_to_optimal_zero` to the loss, you get half the\n    Bernoulli/binomial deviance.\n\n    More details: Inserting the predicted probability y_pred = expit(raw_prediction)\n    in the loss gives the well known::\n\n        loss(x_i) = - y_true_i * log(y_pred_i) - (1 - y_true_i) * log(1 - y_pred_i)",
        "examples": ""
      },
      "methods": [
        {
          "name": "constant_to_optimal_zero",
          "signature": "constant_to_optimal_zero(self, y_true, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_intercept_only",
          "signature": "fit_intercept_only(self, y_true, sample_weight=None)",
          "documentation": {
            "description": "Compute raw_prediction of an intercept-only model.\n\n        This can be used as initial estimates of predictions, i.e. before the\n        first iteration in fit.\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            Observed, true target values.\n        sample_weight : None or array of shape (n_samples,)\n            Sample weights.",
            "parameters": {
              "y_true": {
                "type": "array",
                "description": "like of shape (n_samples,)\nObserved, true target values."
              },
              "sample_weight": {
                "type": "None or array of shape (n_samples,)",
                "description": ""
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "raw_prediction": {
                "type": "numpy scalar or array of shape (n_classes,)",
                "description": ""
              },
              "Raw": {
                "type": "predictions of an intercept-only model.",
                "description": ""
              }
            },
            "returns": "-------\n        raw_prediction : numpy scalar or array of shape (n_classes,)\n            Raw predictions of an intercept-only model.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gradient",
          "signature": "gradient(self, y_true, raw_prediction, sample_weight=None, gradient_out=None, n_threads=1)",
          "documentation": {
            "description": "Compute gradient of loss w.r.t raw_prediction for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the result is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.",
            "parameters": {
              "y_true": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,)\nObserved, true target values."
              },
              "raw_prediction": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)"
              },
              "Raw": {
                "type": "prediction values (in link space).",
                "description": ""
              },
              "sample_weight": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "Sample": {
                "type": "weights.",
                "description": ""
              },
              "gradient_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)"
              },
              "A": {
                "type": "location into which the result is stored. If None, a new array",
                "description": ""
              },
              "might": {
                "type": "be created.",
                "description": ""
              },
              "n_threads": {
                "type": "int, default=1",
                "description": ""
              },
              "Might": {
                "type": "use openmp thread parallelism.",
                "description": "Returns\n-------"
              },
              "gradient": {
                "type": "array of shape (n_samples,) or (n_samples, n_classes)",
                "description": "Element-wise gradients."
              }
            },
            "returns": "-------\n        gradient : array of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "gradient_hessian",
          "signature": "gradient_hessian(self, y_true, raw_prediction, sample_weight=None, gradient_out=None, hessian_out=None, n_threads=1)",
          "documentation": {
            "description": "Compute gradient and hessian of loss w.r.t raw_prediction.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the gradient is stored. If None, a new array\n            might be created.\n        hessian_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the hessian is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.",
            "parameters": {
              "y_true": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,)\nObserved, true target values."
              },
              "raw_prediction": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)"
              },
              "Raw": {
                "type": "prediction values (in link space).",
                "description": ""
              },
              "sample_weight": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "Sample": {
                "type": "weights.",
                "description": ""
              },
              "gradient_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)"
              },
              "A": {
                "type": "location into which the hessian is stored. If None, a new array",
                "description": ""
              },
              "might": {
                "type": "be created.",
                "description": ""
              },
              "hessian_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)"
              },
              "n_threads": {
                "type": "int, default=1",
                "description": ""
              },
              "Might": {
                "type": "use openmp thread parallelism.",
                "description": "Returns\n-------"
              },
              "gradient": {
                "type": "arrays of shape (n_samples,) or (n_samples, n_classes)",
                "description": "Element-wise gradients."
              },
              "hessian": {
                "type": "arrays of shape (n_samples,) or (n_samples, n_classes)",
                "description": "Element-wise hessians."
              }
            },
            "returns": "-------\n        gradient : arrays of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.\n\n        hessian : arrays of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise hessians.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "in_y_pred_range",
          "signature": "in_y_pred_range(self, y)",
          "documentation": {
            "description": "Return True if y is in the valid range of y_pred.",
            "parameters": {
              "y": {
                "type": "ndarray",
                "description": ""
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "in_y_true_range",
          "signature": "in_y_true_range(self, y)",
          "documentation": {
            "description": "Return True if y is in the valid range of y_true.",
            "parameters": {
              "y": {
                "type": "ndarray",
                "description": ""
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "init_gradient_and_hessian",
          "signature": "init_gradient_and_hessian(self, n_samples, dtype=<class 'numpy.float64'>, order='F')",
          "documentation": {
            "description": "Initialize arrays for gradients and hessians.\n\n        Unless hessians are constant, arrays are initialized with undefined values.\n\n        Parameters\n        ----------\n        n_samples : int\n            The number of samples, usually passed to `fit()`.\n        dtype : {np.float64, np.float32}, default=np.float64\n            The dtype of the arrays gradient and hessian.\n        order : {'C', 'F'}, default='F'\n            Order of the arrays gradient and hessian. The default 'F' makes the arrays\n            contiguous along samples.",
            "parameters": {
              "n_samples": {
                "type": "int",
                "description": ""
              },
              "The": {
                "type": "dtype of the arrays gradient and hessian.",
                "description": ""
              },
              "dtype": {
                "type": "{np.float64, np.float32}, default=np.float64",
                "description": ""
              },
              "order": {
                "type": "{'C', 'F'}, default='F'",
                "description": ""
              },
              "Order": {
                "type": "of the arrays gradient and hessian. The default 'F' makes the arrays",
                "description": ""
              },
              "contiguous": {
                "type": "along samples.",
                "description": "Returns\n-------"
              },
              "gradient": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,) or array of shape             (n_samples, n_classes)"
              },
              "Empty": {
                "type": "allocated but not initialized",
                "description": "array to be used as argument\nhessian_out."
              },
              "hessian": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,), array of shape\n(n_samples, n_classes) or shape (1,)"
              },
              "If": {
                "type": "constant_hessian is True (e.g. `HalfSquaredError`), the array is",
                "description": ""
              },
              "initialized": {
                "type": "to ``1``.",
                "description": ""
              }
            },
            "returns": "-------\n        gradient : C-contiguous array of shape (n_samples,) or array of shape             (n_samples, n_classes)\n            Empty array (allocated but not initialized) to be used as argument\n            gradient_out.\n        hessian : C-contiguous array of shape (n_samples,), array of shape\n            (n_samples, n_classes) or shape (1,)\n            Empty (allocated but not initialized) array to be used as argument\n            hessian_out.\n            If constant_hessian is True (e.g. `HalfSquaredError`), the array is\n            initialized to ``1``.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "loss",
          "signature": "loss(self, y_true, raw_prediction, sample_weight=None, loss_out=None, n_threads=1)",
          "documentation": {
            "description": "Compute the pointwise loss value for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        loss_out : None or C-contiguous array of shape (n_samples,)\n            A location into which the result is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.",
            "parameters": {
              "y_true": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,)\nObserved, true target values."
              },
              "raw_prediction": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)"
              },
              "Raw": {
                "type": "prediction values (in link space).",
                "description": ""
              },
              "sample_weight": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "Sample": {
                "type": "weights.",
                "description": ""
              },
              "loss_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "A": {
                "type": "location into which the result is stored. If None, a new array",
                "description": ""
              },
              "might": {
                "type": "be created.",
                "description": ""
              },
              "n_threads": {
                "type": "int, default=1",
                "description": ""
              },
              "Might": {
                "type": "use openmp thread parallelism.",
                "description": "Returns\n-------"
              },
              "loss": {
                "type": "array of shape (n_samples,)",
                "description": "Element-wise loss function."
              }
            },
            "returns": "-------\n        loss : array of shape (n_samples,)\n            Element-wise loss function.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "loss_gradient",
          "signature": "loss_gradient(self, y_true, raw_prediction, sample_weight=None, loss_out=None, gradient_out=None, n_threads=1)",
          "documentation": {
            "description": "Compute loss and gradient w.r.t. raw_prediction for each input.\n\n        Parameters\n        ----------\n        y_true : C-contiguous array of shape (n_samples,)\n            Observed, true target values.\n        raw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\n            Raw prediction values (in link space).\n        sample_weight : None or C-contiguous array of shape (n_samples,)\n            Sample weights.\n        loss_out : None or C-contiguous array of shape (n_samples,)\n            A location into which the loss is stored. If None, a new array\n            might be created.\n        gradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\n            A location into which the gradient is stored. If None, a new array\n            might be created.\n        n_threads : int, default=1\n            Might use openmp thread parallelism.",
            "parameters": {
              "y_true": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,)\nObserved, true target values."
              },
              "raw_prediction": {
                "type": "C",
                "description": "contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)"
              },
              "Raw": {
                "type": "prediction values (in link space).",
                "description": ""
              },
              "sample_weight": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "Sample": {
                "type": "weights.",
                "description": ""
              },
              "loss_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,)"
              },
              "A": {
                "type": "location into which the gradient is stored. If None, a new array",
                "description": ""
              },
              "might": {
                "type": "be created.",
                "description": ""
              },
              "gradient_out": {
                "type": "None or C",
                "description": "contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)"
              },
              "n_threads": {
                "type": "int, default=1",
                "description": ""
              },
              "Might": {
                "type": "use openmp thread parallelism.",
                "description": "Returns\n-------"
              },
              "loss": {
                "type": "array of shape (n_samples,)",
                "description": "Element-wise loss function."
              },
              "gradient": {
                "type": "array of shape (n_samples,) or (n_samples, n_classes)",
                "description": "Element-wise gradients."
              }
            },
            "returns": "-------\n        loss : array of shape (n_samples,)\n            Element-wise loss function.\n\n        gradient : array of shape (n_samples,) or (n_samples, n_classes)\n            Element-wise gradients.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, raw_prediction)",
          "documentation": {
            "description": "Predict probabilities.\n\n        Parameters\n        ----------\n        raw_prediction : array of shape (n_samples,) or (n_samples, 1)\n            Raw prediction values (in link space).",
            "parameters": {
              "raw_prediction": {
                "type": "array of shape (n_samples,) or (n_samples, 1)",
                "description": ""
              },
              "Raw": {
                "type": "prediction values (in link space).",
                "description": "Returns\n-------"
              },
              "proba": {
                "type": "array of shape (n_samples, 2)",
                "description": "Element-wise class probabilities."
              }
            },
            "returns": "-------\n        proba : array of shape (n_samples, 2)\n            Element-wise class probabilities.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "HasMethods",
      "documentation": {
        "description": "Constraint representing objects that expose specific methods.\n\n    It is useful for parameters following a protocol and where we don't want to impose\n    an affiliation to a specific module or class.",
        "parameters": {
          "methods": {
            "type": "str or list of str",
            "description": ""
          },
          "The": {
            "type": "method(s) that the object is expected to expose.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Hidden",
      "documentation": {
        "description": "Class encapsulating a constraint not meant to be exposed to the user.",
        "parameters": {
          "constraint": {
            "type": "str or _Constraint instance",
            "description": ""
          },
          "The": {
            "type": "constraint to be used internally.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Integral",
      "documentation": {
        "description": "Integral adds methods that work on integral numbers.\n\n    In short, these are conversion to int, pow with modulus, and the\n    bit-string operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interval",
      "documentation": {
        "description": "Constraint representing a typed interval.\n\n    Parameters\n    ----------\n    type : {numbers.Integral, numbers.Real, RealNotInt}\n        The set of numbers in which to set the interval.\n\n        If RealNotInt, only reals that don't have the integer type\n        are allowed. For example 1.0 is allowed but 1 is not.\n\n    left : float or int or None\n        The left bound of the interval. None means left bound is -∞.\n\n    right : float, int or None\n        The right bound of the interval. None means right bound is +∞.\n\n    closed : {\"left\", \"right\", \"both\", \"neither\"}\n        Whether the interval is open or closed. Possible choices are:\n\n        - `\"left\"`: the interval is closed on the left and open on the right.\n          It is equivalent to the interval `[ left, right )`.\n        - `\"right\"`: the interval is closed on the right and open on the left.\n          It is equivalent to the interval `( left, right ]`.\n        - `\"both\"`: the interval is closed.\n          It is equivalent to the interval `[ left, right ]`.\n        - `\"neither\"`: the interval is open.\n          It is equivalent to the interval `( left, right )`.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": ""
          },
          "The": {
            "type": "right bound of the interval. None means right bound is +∞.",
            "description": ""
          },
          "If": {
            "type": "RealNotInt, only reals that don't have the integer type",
            "description": ""
          },
          "are": {
            "type": "allowed. For example 1.0 is allowed but 1 is not.",
            "description": ""
          },
          "left": {
            "type": "float or int or None",
            "description": ""
          },
          "right": {
            "type": "float, int or None",
            "description": ""
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": ""
          },
          "Whether": {
            "type": "the interval is open or closed. Possible choices are:",
            "description": "- `\"left\"`: the interval is closed on the left and open on the right."
          },
          "It": {
            "type": "is equivalent to the interval `( left, right )`.",
            "description": "Notes\n-----"
          },
          "Setting": {
            "type": "a bound to `None` and setting the interval closed is valid. For instance,",
            "description": ""
          },
          "strictly": {
            "type": "speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to",
            "description": "`[0, +∞) U {+∞}`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "-----\n    Setting a bound to `None` and setting the interval closed is valid. For instance,\n    strictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n    `[0, +∞) U {+∞}`.",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "IsotonicRegression",
      "documentation": {
        "description": "Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977",
        "parameters": {
          "y_min": {
            "type": "float, default=None",
            "description": ""
          },
          "Lower": {
            "type": "bound on the lowest predicted value (the minimum value may",
            "description": ""
          },
          "still": {
            "type": "be higher). If not set, defaults to -inf.",
            "description": ""
          },
          "y_max": {
            "type": "float, default=None",
            "description": ""
          },
          "Upper": {
            "type": "bound on the highest predicted value (the maximum may still be",
            "description": "lower). If not set, defaults to +inf."
          },
          "increasing": {
            "type": "bool or 'auto', default=True",
            "description": ""
          },
          "Determines": {
            "type": "whether the predictions should be constrained to increase",
            "description": ""
          },
          "or": {
            "type": "decrease with `X`. 'auto' will decide based on the Spearman",
            "description": ""
          },
          "correlation": {
            "type": "estimate's sign.",
            "description": ""
          },
          "out_of_bounds": {
            "type": "{'nan', 'clip', 'raise'}, default='nan'",
            "description": ""
          },
          "Handles": {
            "type": "how `X` values outside of the training domain are handled",
            "description": ""
          },
          "during": {
            "type": "prediction.",
            "description": "- 'nan', predictions will be NaN.\n- 'clip', predictions will be set to the value corresponding to"
          },
          "the": {
            "type": "y = f(X) monotonic function.",
            "description": ".. versionadded:: 0.24"
          },
          "X_min_": {
            "type": "float",
            "description": ""
          },
          "Minimum": {
            "type": "value of input array `X_` for left bound.",
            "description": ""
          },
          "X_max_": {
            "type": "float",
            "description": ""
          },
          "Maximum": {
            "type": "value of input array `X_` for right bound.",
            "description": ""
          },
          "X_thresholds_": {
            "type": "ndarray of shape (n_thresholds,)",
            "description": ""
          },
          "Unique": {
            "type": "ascending `X` values used to interpolate",
            "description": ""
          },
          "y_thresholds_": {
            "type": "ndarray of shape (n_thresholds,)",
            "description": "De-duplicated `y` values suitable to interpolate the y = f(X)"
          },
          "monotonic": {
            "type": "function.",
            "description": ".. versionadded:: 0.24"
          },
          "f_": {
            "type": "function",
            "description": ""
          },
          "The": {
            "type": "stepwise interpolating function that covers the input domain ``X``.",
            "description": ""
          },
          "increasing_": {
            "type": "bool",
            "description": ""
          },
          "Inferred": {
            "type": "value for ``increasing``.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------\nsklearn.linear_model.LinearRegression : Ordinary least squares Linear\nRegression.\nsklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that"
          },
          "is": {
            "type": "a non-parametric model accepting monotonicity constraints.",
            "description": ""
          },
          "isotonic_regression": {
            "type": "Function to solve the isotonic regression model.",
            "description": "Notes\n-----"
          },
          "Ties": {
            "type": "are broken using the secondary method from de Leeuw, 1977.",
            "description": "References\n----------"
          },
          "Isotonic": {
            "type": "Median Regression: A Linear Programming Approach",
            "description": ""
          },
          "Nilotpal": {
            "type": "Chakravarti",
            "description": ""
          },
          "Mathematics": {
            "type": "of Operations Research",
            "description": "Vol. 14, No. 2 (May, 1989), pp. 303-308"
          },
          "Isotone": {
            "type": "Optimization in R : Pool-Adjacent-Violators",
            "description": ""
          },
          "Algorithm": {
            "type": "PAVA",
            "description": "and Active Set Methods"
          },
          "de": {
            "type": "Leeuw, Psychometrica, 1977",
            "description": "Examples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.isotonic import IsotonicRegression\n>>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n>>> iso_reg = IsotonicRegression().fit(X, y)\n>>> iso_reg.predict([.1, .2])"
          },
          "Journal": {
            "type": "of Statistical Software 2009",
            "description": ""
          },
          "Correctness": {
            "type": "of Kruskal's algorithms for monotone regression with ties",
            "description": ""
          },
          "array": {
            "type": "[1.8628..., 3.7256...]",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])",
        "notes": "-----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])",
        "examples": "--------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.",
            "parameters": {
              "X": {
                "type": "is stored for future use, as :meth:`transform` needs X to interpolate",
                "description": ""
              },
              "Training": {
                "type": "target.",
                "description": ""
              },
              "Also": {
                "type": "accepts 2d array with 1 feature.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None\nWeights. If set to None, all weights will be set to 1 (equal\nweights).\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "Returns": {
                "type": "an instance of self.",
                "description": "Notes\n-----"
              },
              "new": {
                "type": "input data.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, X, y=None, **fit_params)",
          "documentation": {
            "description": "Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Input": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs),                 default=None"
              },
              "Target": {
                "type": "values (None for unsupervised transformations).",
                "description": "**fit_params : dict"
              },
              "Additional": {
                "type": "fit parameters.",
                "description": "Returns\n-------"
              },
              "X_new": {
                "type": "ndarray array of shape (n_samples, n_features_new)",
                "description": ""
              },
              "Transformed": {
                "type": "array.",
                "description": ""
              }
            },
            "returns": "-------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_feature_names_out",
          "signature": "get_feature_names_out(self, input_features=None)",
          "documentation": {
            "description": "Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.",
            "parameters": {
              "input_features": {
                "type": "array",
                "description": "like of str or None, default=None\nIgnored.\nReturns\n-------"
              },
              "feature_names_out": {
                "type": "ndarray of str objects",
                "description": ""
              },
              "An": {
                "type": "ndarray with one string i.e. [\"isotonicregression0\"].",
                "description": ""
              }
            },
            "returns": "-------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, T)",
          "documentation": {
            "description": "Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.",
            "parameters": {
              "T": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, 1)"
              },
              "Data": {
                "type": "to transform.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Transformed": {
                "type": "data.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.isotonic.IsotonicRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.isotonic.IsotonicRegression",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.isotonic.IsotonicRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.isotonic.IsotonicRegression",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, T)",
          "documentation": {
            "description": "Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.",
            "parameters": {
              "T": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, 1)"
              },
              "Data": {
                "type": "to transform.",
                "description": ".. versionchanged:: 0.24"
              },
              "Also": {
                "type": "accepts 2d array with 1 feature.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "The": {
                "type": "transformed data.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LabelEncoder",
      "documentation": {
        "description": "Encode target labels with value between 0 and n_classes-1.\n\n    This transformer should be used to encode target values, *i.e.* `y`, and\n    not the input `X`.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    .. versionadded:: 0.12\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Holds the label for each class.\n\n    See Also\n    --------\n    OrdinalEncoder : Encode categorical features using an ordinal encoding\n        scheme.\n    OneHotEncoder : Encode categorical features as a one-hot numeric array.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "--------\n    OrdinalEncoder : Encode categorical features using an ordinal encoding\n        scheme.\n    OneHotEncoder : Encode categorical features as a one-hot numeric array.\n\n    Examples\n    --------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn.preprocessing import LabelEncoder\n    >>> le = LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6])\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    [np.str_('amsterdam'), np.str_('paris'), np.str_('tokyo')]\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    [np.str_('tokyo'), np.str_('tokyo'), np.str_('paris')]",
        "notes": "",
        "examples": "--------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn.preprocessing import LabelEncoder\n    >>> le = LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6])\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    [np.str_('amsterdam'), np.str_('paris'), np.str_('tokyo')]\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    [np.str_('tokyo'), np.str_('tokyo'), np.str_('paris')]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, y)",
          "documentation": {
            "description": "Fit label encoder.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.",
            "parameters": {
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "returns an instance of self.",
                "description": ""
              },
              "Fitted": {
                "type": "label encoder.",
                "description": ""
              }
            },
            "returns": "-------\n        self : returns an instance of self.\n            Fitted label encoder.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, y)",
          "documentation": {
            "description": "Fit label encoder and return encoded labels.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.",
            "parameters": {
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "Encoded": {
                "type": "labels.",
                "description": ""
              }
            },
            "returns": "-------\n        y : array-like of shape (n_samples,)\n            Encoded labels.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, y)",
          "documentation": {
            "description": "Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.",
            "parameters": {
              "y": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "Original": {
                "type": "encoding.",
                "description": ""
              }
            },
            "returns": "-------\n        y : ndarray of shape (n_samples,)\n            Original encoding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "documentation": {
            "description": "Set output container.\n\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n        for an example on how to use the API.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\", \"polars\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `\"polars\"`: Polars output\n            - `None`: Transform configuration is unchanged\n\n            .. versionadded:: 1.4\n                `\"polars\"` option was added.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": ""
              },
              "Configure": {
                "type": "output of `transform` and `fit_transform`.",
                "description": "- `\"default\"`: Default output format of a transformer\n- `\"pandas\"`: DataFrame output\n- `\"polars\"`: Polars output\n- `None`: Transform configuration is unchanged\n.. versionadded:: 1.4\n`\"polars\"` option was added.\nReturns\n-------"
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "Estimator": {
                "type": "instance.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, y)",
          "documentation": {
            "description": "Transform labels to normalized encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.",
            "parameters": {
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "values.",
                "description": "Returns\n-------"
              },
              "Labels": {
                "type": "as normalized encodings.",
                "description": ""
              }
            },
            "returns": "-------\n        y : array-like of shape (n_samples,)\n            Labels as normalized encodings.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LeaveOneOut",
      "documentation": {
        "description": "Leave-One-Out cross-validator.\n\n    Provides train/test indices to split data in train/test sets. Each\n    sample is used once as a test set (singleton) while the remaining\n    samples form the training set.\n\n    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n    ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n\n    Due to the high number of test sets (which is the same as the\n    number of samples) this cross-validation method can be very costly.\n    For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n    or :class:`StratifiedKFold`.\n\n    Read more in the :ref:`User Guide <leave_one_out>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "--------\n    LeaveOneGroupOut : For splitting the data according to explicit,\n        domain-specific stratification of the dataset.\n    GroupKFold : K-fold iterator variant with non-overlapping groups.",
        "notes": "``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n    ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n\n    Due to the high number of test sets (which is the same as the\n    number of samples) this cross-validation method can be very costly.\n    For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n    or :class:`StratifiedKFold`.\n\n    Read more in the :ref:`User Guide <leave_one_out>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneOut\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> y = np.array([1, 2])\n    >>> loo = LeaveOneOut()\n    >>> loo.get_n_splits(X)\n    2\n    >>> print(loo)\n    LeaveOneOut()\n    >>> for i, (train_index, test_index) in enumerate(loo.split(X)):\n    ...     print(f\"Fold {i}:\")\n    ...     print(f\"  Train: index={train_index}\")\n    ...     print(f\"  Test:  index={test_index}\")\n    Fold 0:\n      Train: index=[1]\n      Test:  index=[0]\n    Fold 1:\n      Train: index=[0]\n      Test:  index=[1]\n\n    See Also\n    --------\n    LeaveOneGroupOut : For splitting the data according to explicit,\n        domain-specific stratification of the dataset.\n    GroupKFold : K-fold iterator variant with non-overlapping groups.",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneOut\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> y = np.array([1, 2])\n    >>> loo = LeaveOneOut()\n    >>> loo.get_n_splits(X)\n    2\n    >>> print(loo)\n    LeaveOneOut()\n    >>> for i, (train_index, test_index) in enumerate(loo.split(X)):\n    ...     print(f\"Fold {i}:\")\n    ...     print(f\"  Train: index={train_index}\")\n    ...     print(f\"  Test:  index={test_index}\")\n    Fold 0:\n      Train: index=[1]\n      Test:  index=[0]\n    Fold 1:\n      Train: index=[0]\n      Test:  index=[1]\n\n    See Also\n    --------\n    LeaveOneGroupOut : For splitting the data according to explicit,\n        domain-specific stratification of the dataset.\n    GroupKFold : K-fold iterator variant with non-overlapping groups."
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_n_splits",
          "signature": "get_n_splits(self, X, y=None, groups=None)",
          "documentation": {
            "description": "Returns the number of splitting iterations in the cross-validator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "object",
                "description": ""
              },
              "Always": {
                "type": "ignored, exists for compatibility.",
                "description": "Returns\n-------"
              },
              "groups": {
                "type": "object",
                "description": ""
              },
              "n_splits": {
                "type": "int",
                "description": ""
              },
              "Returns": {
                "type": "the number of splitting iterations in the cross-validator.",
                "description": ""
              }
            },
            "returns": "-------\n        n_splits : int",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "split",
          "signature": "split(self, X, y=None, groups=None)",
          "documentation": {
            "description": "Generate indices to split data into training and test set.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "data, where `n_samples` is the number of samples",
                "description": ""
              },
              "and": {
                "type": "`n_features` is the number of features.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "The": {
                "type": "testing set indices for that split.",
                "description": ""
              },
              "groups": {
                "type": "object",
                "description": ""
              },
              "Always": {
                "type": "ignored, exists for compatibility.",
                "description": "Yields\n------"
              },
              "train": {
                "type": "ndarray",
                "description": ""
              },
              "test": {
                "type": "ndarray",
                "description": ""
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LinearSVC",
      "documentation": {
        "description": "Linear Support Vector Classification.\n\n    Similar to SVC with parameter kernel='linear', but implemented in terms of\n    liblinear rather than libsvm, so it has more flexibility in the choice of\n    penalties and loss functions and should scale better to large numbers of\n    samples.\n\n    The main differences between :class:`~sklearn.svm.LinearSVC` and\n    :class:`~sklearn.svm.SVC` lie in the loss function used by default, and in\n    the handling of intercept regularization between those two implementations.\n\n    This class supports both dense and sparse input and the multiclass support\n    is handled according to a one-vs-the-rest scheme.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    penalty : {'l1', 'l2'}, default='l2'\n        Specifies the norm used in the penalization. The 'l2'\n        penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n        vectors that are sparse.\n\n    loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n        Specifies the loss function. 'hinge' is the standard SVM loss\n        (used e.g. by the SVC class) while 'squared_hinge' is the\n        square of the hinge loss. The combination of ``penalty='l1'``\n        and ``loss='hinge'`` is not supported.\n\n    dual : \"auto\" or bool, default=\"auto\"\n        Select the algorithm to either solve the dual or primal\n        optimization problem. Prefer dual=False when n_samples > n_features.\n        `dual=\"auto\"` will choose the value of the parameter automatically,\n        based on the values of `n_samples`, `n_features`, `loss`, `multi_class`\n        and `penalty`. If `n_samples` < `n_features` and optimizer supports\n        chosen `loss`, `multi_class` and `penalty`, then dual will be set to True,\n        otherwise it will be set to False.\n\n        .. versionchanged:: 1.3\n           The `\"auto\"` option is added in version 1.3 and will be the default\n           in version 1.5.\n\n    tol : float, default=1e-4\n        Tolerance for stopping criteria.\n\n    C : float, default=1.0\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive.\n        For an intuitive visualization of the effects of scaling\n        the regularization parameter C, see\n        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.\n\n    multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n        Determines the multi-class strategy if `y` contains more than\n        two classes.\n        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n        ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n        While `crammer_singer` is interesting from a theoretical perspective\n        as it is consistent, it is seldom used in practice as it rarely leads\n        to better accuracy and is more expensive to compute.\n        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n        will be ignored.\n\n    fit_intercept : bool, default=True\n        Whether or not to fit an intercept. If set to True, the feature vector\n        is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where\n        1 corresponds to the intercept. If set to False, no intercept will be\n        used in calculations (i.e. data is expected to be already centered).\n\n    intercept_scaling : float, default=1.0\n        When `fit_intercept` is True, the instance vector x becomes ``[x_1,\n        ..., x_n, intercept_scaling]``, i.e. a \"synthetic\" feature with a\n        constant value equal to `intercept_scaling` is appended to the instance\n        vector. The intercept becomes intercept_scaling * synthetic feature\n        weight. Note that liblinear internally penalizes the intercept,\n        treating it like any other term in the feature vector. To reduce the\n        impact of the regularization on the intercept, the `intercept_scaling`\n        parameter can be set to a value greater than 1; the higher the value of\n        `intercept_scaling`, the lower the impact of regularization on it.\n        Then, the weights become `[w_x_1, ..., w_x_n,\n        w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent\n        the feature weights and the intercept weight is scaled by\n        `intercept_scaling`. This scaling allows the intercept term to have a\n        different regularization behavior compared to the other features.\n\n    class_weight : dict or 'balanced', default=None\n        Set the parameter C of class i to ``class_weight[i]*C`` for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : int, default=0\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in liblinear that, if enabled, may not work\n        properly in a multithreaded context.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n        underlying implementation of :class:`LinearSVC` is not random and\n        ``random_state`` has no effect on the results.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_iter : int, default=1000\n        The maximum number of iterations to be run.\n\n    Attributes\n    ----------\n    coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem).\n\n        ``coef_`` is a readonly property derived from ``raw_coef_`` that\n        follows the internal memory layout of liblinear.\n\n    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    classes_ : ndarray of shape (n_classes,)\n        The unique classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Maximum number of iterations run across all classes.\n\n    See Also\n    --------\n    SVC : Implementation of Support Vector Machine classifier using libsvm:\n        the kernel can be non-linear but its SMO algorithm does not\n        scale to large number of samples as LinearSVC does.\n\n        Furthermore SVC multi-class mode is implemented using one\n        vs one scheme while LinearSVC uses one vs the rest. It is\n        possible to implement one vs the rest with SVC by using the\n        :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n\n        Finally SVC can fit dense data without memory copy if the input\n        is C-contiguous. Sparse data will still incur memory copy though.\n\n    sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n        cost function as LinearSVC\n        by adjusting the penalty and loss parameters. In addition it requires\n        less memory, allows incremental (online) learning, and implements\n        various loss functions and regularization regimes.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller ``tol`` parameter.\n\n    The underlying implementation, liblinear, uses a sparse internal\n    representation for the data that will incur a memory copy.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n    `LIBLINEAR: A Library for Large Linear Classification\n    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__",
        "parameters": {
          "penalty": {
            "type": "is the standard used in SVC. The 'l1' leads to ``coef_``",
            "description": ""
          },
          "Specifies": {
            "type": "the loss function. 'hinge' is the standard SVM loss",
            "description": "(used e.g. by the SVC class) while 'squared_hinge' is the"
          },
          "vectors": {
            "type": "that are sparse.",
            "description": ""
          },
          "loss": {
            "type": "{'hinge', 'squared_hinge'}, default='squared_hinge'",
            "description": ""
          },
          "square": {
            "type": "of the hinge loss. The combination of ``penalty='l1'``",
            "description": ""
          },
          "and": {
            "type": "`penalty`. If `n_samples` < `n_features` and optimizer supports",
            "description": ""
          },
          "dual": {
            "type": "\"auto\" or bool, default=\"auto\"",
            "description": ""
          },
          "Select": {
            "type": "the algorithm to either solve the dual or primal",
            "description": ""
          },
          "optimization": {
            "type": "problem. Prefer dual=False when n_samples > n_features.",
            "description": "`dual=\"auto\"` will choose the value of the parameter automatically,"
          },
          "based": {
            "type": "on the values of `n_samples`, `n_features`, `loss`, `multi_class`",
            "description": ""
          },
          "chosen": {
            "type": "`loss`, `multi_class` and `penalty`, then dual will be set to True,",
            "description": ""
          },
          "otherwise": {
            "type": "it will be set to False.",
            "description": ".. versionchanged:: 1.3"
          },
          "The": {
            "type": "underlying implementation, liblinear, uses a sparse internal",
            "description": ""
          },
          "in": {
            "type": "the narrative documentation.",
            "description": "References\n----------\n`LIBLINEAR: A Library for Large Linear Classification\n<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\nExamples\n--------\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = make_pipeline(StandardScaler(),\n...                     LinearSVC(random_state=0, tol=1e-5))\n>>> clf.fit(X, y)"
          },
          "tol": {
            "type": "float, default=1e",
            "description": "4"
          },
          "Tolerance": {
            "type": "for stopping criteria.",
            "description": ""
          },
          "C": {
            "type": "float, default=1.0",
            "description": ""
          },
          "Regularization": {
            "type": "parameter. The strength of the regularization is",
            "description": ""
          },
          "inversely": {
            "type": "proportional to C. Must be strictly positive.",
            "description": ""
          },
          "For": {
            "type": "an intuitive visualization of the effects of scaling",
            "description": ""
          },
          "the": {
            "type": "kernel can be non-linear but its SMO algorithm does not",
            "description": ""
          },
          "multi_class": {
            "type": "{'ovr', 'crammer_singer'}, default='ovr'",
            "description": ""
          },
          "Determines": {
            "type": "the multi-class strategy if `y` contains more than",
            "description": ""
          },
          "two": {
            "type": "classes.",
            "description": "``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n``\"crammer_singer\"`` optimizes a joint objective over all classes."
          },
          "While": {
            "type": "`crammer_singer` is interesting from a theoretical perspective",
            "description": ""
          },
          "as": {
            "type": "``n_samples / (n_classes * np.bincount(y))``.",
            "description": ""
          },
          "to": {
            "type": "have slightly different results for the same input data. If",
            "description": ""
          },
          "If": {
            "type": "``\"crammer_singer\"`` is chosen, the options loss, penalty and dual",
            "description": ""
          },
          "will": {
            "type": "be ignored.",
            "description": ""
          },
          "fit_intercept": {
            "type": "bool, default=True",
            "description": ""
          },
          "Whether": {
            "type": "or not to fit an intercept. If set to True, the feature vector",
            "description": ""
          },
          "is": {
            "type": "C-contiguous. Sparse data will still incur memory copy though.",
            "description": "sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same"
          },
          "1": {
            "type": "corresponds to the intercept. If set to False, no intercept will be",
            "description": ""
          },
          "used": {
            "type": "in calculations (i.e. data is expected to be already centered).",
            "description": ""
          },
          "intercept_scaling": {
            "type": "float, default=1.0",
            "description": ""
          },
          "When": {
            "type": "`fit_intercept` is True, the instance vector x becomes ``[x_1,",
            "description": "..., x_n, intercept_scaling]``, i.e. a \"synthetic\" feature with a"
          },
          "constant": {
            "type": "value equal to `intercept_scaling` is appended to the instance",
            "description": "vector. The intercept becomes intercept_scaling * synthetic feature\nweight. Note that liblinear internally penalizes the intercept,"
          },
          "treating": {
            "type": "it like any other term in the feature vector. To reduce the",
            "description": ""
          },
          "impact": {
            "type": "of the regularization on the intercept, the `intercept_scaling`",
            "description": ""
          },
          "parameter": {
            "type": "can be set to a value greater than 1; the higher the value of",
            "description": "`intercept_scaling`, the lower the impact of regularization on it.\nThen, the weights become `[w_x_1, ..., w_x_n,\nw_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent"
          },
          "different": {
            "type": "regularization behavior compared to the other features.",
            "description": ""
          },
          "class_weight": {
            "type": "dict or 'balanced', default=None",
            "description": ""
          },
          "Set": {
            "type": "the parameter C of class i to ``class_weight[i]*C`` for",
            "description": "SVC. If not given, all classes are supposed to have"
          },
          "weight": {
            "type": "one.",
            "description": ""
          },
          "weights": {
            "type": "inversely proportional to class frequencies in the input data",
            "description": ""
          },
          "verbose": {
            "type": "int, default=0",
            "description": ""
          },
          "Enable": {
            "type": "verbose output. Note that this setting takes advantage of a",
            "description": "per-process runtime setting in liblinear that, if enabled, may not work"
          },
          "properly": {
            "type": "in a multithreaded context.",
            "description": ""
          },
          "random_state": {
            "type": "int, RandomState instance or None, default=None",
            "description": ""
          },
          "Controls": {
            "type": "the pseudo random number generation for shuffling the data for",
            "description": ""
          },
          "underlying": {
            "type": "implementation of :class:`LinearSVC` is not random and",
            "description": "``random_state`` has no effect on the results."
          },
          "Pass": {
            "type": "an int for reproducible output across multiple function calls.",
            "description": ""
          },
          "See": {
            "type": "Also",
            "description": "--------"
          },
          "max_iter": {
            "type": "int, default=1000",
            "description": ""
          },
          "coef_": {
            "type": "ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)",
            "description": ""
          },
          "Weights": {
            "type": "assigned to the features (coefficients in the primal",
            "description": "problem).\n``coef_`` is a readonly property derived from ``raw_coef_`` that"
          },
          "follows": {
            "type": "the internal memory layout of liblinear.",
            "description": ""
          },
          "intercept_": {
            "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
            "description": ""
          },
          "Constants": {
            "type": "in decision function.",
            "description": ""
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": ""
          },
          "n_features_in_": {
            "type": "int",
            "description": ""
          },
          "Number": {
            "type": "of features seen during :term:`fit`.",
            "description": ".. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": ""
          },
          "Names": {
            "type": "of features seen during :term:`fit`. Defined only when `X`",
            "description": ""
          },
          "has": {
            "type": "feature names that are all strings.",
            "description": ".. versionadded:: 1.0"
          },
          "n_iter_": {
            "type": "int",
            "description": ""
          },
          "Maximum": {
            "type": "number of iterations run across all classes.",
            "description": ""
          },
          "SVC": {
            "type": "Implementation of Support Vector Machine classifier using libsvm:",
            "description": ""
          },
          "scale": {
            "type": "to large number of samples as LinearSVC does.",
            "description": ""
          },
          "Furthermore": {
            "type": "SVC multi-class mode is implemented using one",
            "description": ""
          },
          "vs": {
            "type": "one scheme while LinearSVC uses one vs the rest. It is",
            "description": ""
          },
          "possible": {
            "type": "to implement one vs the rest with SVC by using the",
            "description": ":class:`~sklearn.multiclass.OneVsRestClassifier` wrapper."
          },
          "Finally": {
            "type": "SVC can fit dense data without memory copy if the input",
            "description": ""
          },
          "cost": {
            "type": "function as LinearSVC",
            "description": ""
          },
          "by": {
            "type": "adjusting the penalty and loss parameters. In addition it requires",
            "description": ""
          },
          "less": {
            "type": "memory, allows incremental (online) learning, and implements",
            "description": ""
          },
          "various": {
            "type": "loss functions and regularization regimes.",
            "description": "Notes\n-----"
          },
          "select": {
            "type": "features when fitting the model. It is thus not uncommon",
            "description": ""
          },
          "that": {
            "type": "happens, try with a smaller ``tol`` parameter.",
            "description": ""
          },
          "representation": {
            "type": "for the data that will incur a memory copy.",
            "description": ""
          },
          "Predict": {
            "type": "output may not match that of standalone liblinear in certain",
            "description": "cases. See :ref:`differences from liblinear <liblinear_differences>`"
          },
          "Pipeline": {
            "type": "steps=[('standardscaler', StandardScaler(",
            "description": "),\n('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n>>> print(clf.named_steps['linearsvc'].coef_)\n[[0.141...   0.526... 0.679... 0.493...]]\n>>> print(clf.named_steps['linearsvc'].intercept_)\n[0.1693...]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "--------\n    SVC : Implementation of Support Vector Machine classifier using libsvm:\n        the kernel can be non-linear but its SMO algorithm does not\n        scale to large number of samples as LinearSVC does.\n\n        Furthermore SVC multi-class mode is implemented using one\n        vs one scheme while LinearSVC uses one vs the rest. It is\n        possible to implement one vs the rest with SVC by using the\n        :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n\n        Finally SVC can fit dense data without memory copy if the input\n        is C-contiguous. Sparse data will still incur memory copy though.\n\n    sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n        cost function as LinearSVC\n        by adjusting the penalty and loss parameters. In addition it requires\n        less memory, allows incremental (online) learning, and implements\n        various loss functions and regularization regimes.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller ``tol`` parameter.\n\n    The underlying implementation, liblinear, uses a sparse internal\n    representation for the data that will incur a memory copy.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n    `LIBLINEAR: A Library for Large Linear Classification\n    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n\n    Examples\n    --------\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = make_pipeline(StandardScaler(),\n    ...                     LinearSVC(random_state=0, tol=1e-5))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n\n    >>> print(clf.named_steps['linearsvc'].coef_)\n    [[0.141...   0.526... 0.679... 0.493...]]\n\n    >>> print(clf.named_steps['linearsvc'].intercept_)\n    [0.1693...]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]",
        "notes": "-----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller ``tol`` parameter.\n\n    The underlying implementation, liblinear, uses a sparse internal\n    representation for the data that will incur a memory copy.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n    `LIBLINEAR: A Library for Large Linear Classification\n    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n\n    Examples\n    --------\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = make_pipeline(StandardScaler(),\n    ...                     LinearSVC(random_state=0, tol=1e-5))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n\n    >>> print(clf.named_steps['linearsvc'].coef_)\n    [[0.141...   0.526... 0.679... 0.493...]]\n\n    >>> print(clf.named_steps['linearsvc'].intercept_)\n    [0.1693...]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]",
        "examples": "--------\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = make_pipeline(StandardScaler(),\n    ...                     LinearSVC(random_state=0, tol=1e-5))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n\n    >>> print(clf.named_steps['linearsvc'].coef_)\n    [[0.141...   0.526... 0.679... 0.493...]]\n\n    >>> print(clf.named_steps['linearsvc'].intercept_)\n    [0.1693...]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]"
      },
      "methods": [
        {
          "name": "decision_function",
          "signature": "decision_function(self, X)",
          "documentation": {
            "description": "Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the confidence scores.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the confidence scores.",
                "description": "Returns\n-------"
              },
              "scores": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": ""
              },
              "Confidence": {
                "type": "scores per `(n_samples, n_classes)` combination. In the",
                "description": ""
              },
              "binary": {
                "type": "case, confidence score for `self.classes_[1]` where >0 means",
                "description": ""
              },
              "this": {
                "type": "class would be predicted.",
                "description": ""
              }
            },
            "returns": "-------\n        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Confidence scores per `(n_samples, n_classes)` combination. In the\n            binary case, confidence score for `self.classes_[1]` where >0 means\n            this class would be predicted.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "densify",
          "signature": "densify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Array of weights that are assigned to individual\n            samples. If not provided,\n            then each sample is given unit weight.\n\n            .. versionadded:: 0.18",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "Training": {
                "type": "vector, where `n_samples` is the number of samples and",
                "description": "`n_features` is the number of features."
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,)"
              },
              "Target": {
                "type": "vector relative to X.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Array": {
                "type": "of weights that are assigned to individual",
                "description": "samples. If not provided,"
              },
              "then": {
                "type": "each sample is given unit weight.",
                "description": ".. versionadded:: 0.18\nReturns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "An": {
                "type": "instance of the estimator.",
                "description": ""
              }
            },
            "returns": "-------\n        self : object\n            An instance of the estimator.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "documentation": {
            "description": "Get metadata routing of this object.\n\n        Please check :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.",
            "parameters": {},
            "returns": "-------\n        routing : MetadataRequest\n            A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n            routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "documentation": {
            "description": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": ""
              },
              "If": {
                "type": "True, will return the parameters for this estimator and",
                "description": ""
              },
              "contained": {
                "type": "subobjects that are estimators.",
                "description": "Returns\n-------"
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "Parameter": {
                "type": "names mapped to their values.",
                "description": ""
              }
            },
            "returns": "-------\n        params : dict\n            Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "documentation": {
            "description": "Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data matrix for which we want to get the predictions.",
            "parameters": {
              "X": {
                "type": "{array",
                "description": "like, sparse matrix} of shape (n_samples, n_features)"
              },
              "The": {
                "type": "data matrix for which we want to get the predictions.",
                "description": "Returns\n-------"
              },
              "y_pred": {
                "type": "ndarray of shape (n_samples,)",
                "description": ""
              },
              "Vector": {
                "type": "containing the class labels for each sample.",
                "description": ""
              }
            },
            "returns": "-------\n        y_pred : ndarray of shape (n_samples,)\n            Vector containing the class labels for each sample.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "labels for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ""
              },
              "Mean": {
                "type": "accuracy of ``self.predict(X)`` w.r.t. `y`.",
                "description": ""
              }
            },
            "returns": "the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.svm._classes.LinearSVC, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.svm._classes.LinearSVC",
          "documentation": {
            "description": "Request metadata passed to the ``fit`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``fit``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``fit``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "documentation": {
            "description": "Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.",
            "parameters": {
              "Estimator": {
                "type": "instance.",
                "description": ""
              },
              "self": {
                "type": "estimator instance",
                "description": ""
              },
              "of": {
                "type": "the form ``<component>__<parameter>`` so that it's",
                "description": ""
              },
              "possible": {
                "type": "to update each component of a nested object.",
                "description": ""
              }
            },
            "returns": "-------\n        self : estimator instance\n            Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.svm._classes.LinearSVC, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.svm._classes.LinearSVC",
          "documentation": {
            "description": "Request metadata passed to the ``score`` method.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": ""
              },
              "Metadata": {
                "type": "routing for ``sample_weight`` parameter in ``score``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "object",
                "description": ""
              },
              "The": {
                "type": "updated object.",
                "description": ""
              },
              "and": {
                "type": "not others.",
                "description": ".. versionadded:: 1.3\n.. note::"
              },
              "This": {
                "type": "method is only relevant if this estimator is used as a",
                "description": "sub-estimator of a meta-estimator, e.g. used inside a\n:class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect."
              }
            },
            "returns": "-------\n        self : object\n            The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "that this method is only relevant if\n        ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n        Please see :ref:`User Guide <metadata_routing>` on how the routing\n        mechanism works.\n\n        The options for each parameter are:\n\n        - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n        - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n        - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n        - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\n        The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n        existing request. This allows you to change the request for some\n        parameters and not others.\n\n        .. versionadded:: 1.3\n\n        .. note::\n            This method is only relevant if this estimator is used as a\n            sub-estimator of a meta-estimator, e.g. used inside a\n            :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n\n        Parameters\n        ----------\n        sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n            Metadata routing for ``sample_weight`` parameter in ``score``.\n\n        Returns\n        -------\n        self : object\n            The updated object.",
            "examples": ""
          }
        },
        {
          "name": "sparsify",
          "signature": "sparsify(self)",
          "documentation": {
            "description": "Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.",
            "parameters": {},
            "returns": "-------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MetaEstimatorMixin",
      "documentation": {
        "description": "Mixin class for all meta estimators in scikit-learn.\n\n    This mixin is empty, and only exists to indicate that the estimator is a\n    meta-estimator.\n\n    .. versionchanged:: 1.6\n        The `_required_parameters` is now removed and is unnecessary since tests are\n        refactored and don't use this anymore.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> from sklearn.base import MetaEstimatorMixin\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> class MyEstimator(MetaEstimatorMixin):\n    ...     def __init__(self, *, estimator=None):\n    ...         self.estimator = estimator\n    ...     def fit(self, X, y=None):\n    ...         if self.estimator is None:\n    ...             self.estimator_ = LogisticRegression()\n    ...         else:\n    ...             self.estimator_ = self.estimator\n    ...         return self\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimator = MyEstimator().fit(X, y)\n    >>> estimator.estimator_\n    LogisticRegression()"
      },
      "methods": []
    },
    {
      "name": "MetadataRouter",
      "documentation": {
        "description": "Stores and handles metadata routing for a router object.\n\n    This class is used by router objects to store and handle metadata routing.\n    Routing information is stored as a dictionary of the form ``{\"object_name\":\n    RouteMappingPair(method_mapping, routing_info)}``, where ``method_mapping``\n    is an instance of :class:`~sklearn.utils.metadata_routing.MethodMapping` and\n    ``routing_info`` is either a\n    :class:`~sklearn.utils.metadata_routing.MetadataRequest` or a\n    :class:`~sklearn.utils.metadata_routing.MetadataRouter` instance.\n\n    .. versionadded:: 1.3",
        "parameters": {
          "owner": {
            "type": "str",
            "description": ""
          },
          "The": {
            "type": "name of the object to which these requests belong.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self, *, method_mapping, **objs)",
          "documentation": {
            "description": "Add named objects with their corresponding method mapping.\n\n        Parameters\n        ----------\n        method_mapping : MethodMapping\n            The mapping between the child and the parent's methods.\n\n        **objs : dict\n            A dictionary of objects from which metadata is extracted by calling\n            :func:`~sklearn.utils.metadata_routing.get_routing_for_object` on them.",
            "parameters": {
              "method_mapping": {
                "type": "MethodMapping",
                "description": ""
              },
              "The": {
                "type": "mapping between the child and the parent's methods.",
                "description": "**objs : dict"
              },
              "A": {
                "type": "dictionary of objects from which metadata is extracted by calling",
                "description": ":func:`~sklearn.utils.metadata_routing.get_routing_for_object` on them.\nReturns\n-------"
              },
              "self": {
                "type": "MetadataRouter",
                "description": ""
              },
              "Returns": {
                "type": "`self`.",
                "description": ""
              }
            },
            "returns": "-------\n        self : MetadataRouter",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "add_self_request",
          "signature": "add_self_request(self, obj)",
          "documentation": {
            "description": "Add `self` (as a consumer) to the routing.\n\n        This method is used if the router is also a consumer, and hence the\n        router itself needs to be included in the routing. The passed object\n        can be an estimator or a\n        :class:`~sklearn.utils.metadata_routing.MetadataRequest`.\n\n        A router should add itself using this method instead of `add` since it\n        should be treated differently than the other objects to which metadata\n        is routed by the router.\n\n        Parameters\n        ----------\n        obj : object\n            This is typically the router instance, i.e. `self` in a\n            ``get_metadata_routing()`` implementation. It can also be a\n            ``MetadataRequest`` instance.",
            "parameters": {
              "obj": {
                "type": "object",
                "description": ""
              },
              "This": {
                "type": "is typically the router instance, i.e. `self` in a",
                "description": "``get_metadata_routing()`` implementation. It can also be a\n``MetadataRequest`` instance.\nReturns\n-------"
              },
              "self": {
                "type": "MetadataRouter",
                "description": ""
              },
              "Returns": {
                "type": "`self`.",
                "description": ""
              }
            },
            "returns": "-------\n        self : MetadataRouter",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "consumes",
          "signature": "consumes(self, method, params)",
          "documentation": {
            "description": "Check whether the given parameters are consumed by the given method.\n\n        .. versionadded:: 1.4\n\n        Parameters\n        ----------\n        method : str\n            The name of the method to check.\n\n        params : iterable of str\n            An iterable of parameters to check.",
            "parameters": {
              "method": {
                "type": "str",
                "description": ""
              },
              "The": {
                "type": "name of the method to check.",
                "description": ""
              },
              "params": {
                "type": "iterable of str",
                "description": ""
              },
              "An": {
                "type": "iterable of parameters to check.",
                "description": "Returns\n-------"
              },
              "consumed": {
                "type": "set of str",
                "description": ""
              },
              "A": {
                "type": "set of parameters which are consumed by the given method.",
                "description": ""
              }
            },
            "returns": "-------\n        consumed : set of str\n            A set of parameters which are consumed by the given method.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "route_params",
          "signature": "route_params(self, *, caller, params)",
          "documentation": {
            "description": "Return the input parameters requested by child objects.\n\n        The output of this method is a :class:`~sklearn.utils.Bunch`, which includes the\n        metadata for all methods of each child object that is used in the router's\n        `caller` method.\n\n        If the router is also a consumer, it also checks for warnings of\n        `self`'s/consumer's requested metadata.\n\n        Parameters\n        ----------\n        caller : str\n            The name of the method for which the parameters are requested and\n            routed. If called inside the :term:`fit` method of a router, it\n            would be `\"fit\"`.\n\n        params : dict\n            A dictionary of provided metadata.",
            "parameters": {
              "caller": {
                "type": "str",
                "description": ""
              },
              "The": {
                "type": "name of the method for which the parameters are requested and",
                "description": "routed. If called inside the :term:`fit` method of a router, it"
              },
              "would": {
                "type": "be `\"fit\"`.",
                "description": ""
              },
              "params": {
                "type": "Bunch",
                "description": ""
              },
              "A": {
                "type": "class:`~sklearn.utils.Bunch` of the form",
                "description": "``{\"object_name\": {\"method_name\": {params: value}}}`` which can be"
              },
              "used": {
                "type": "to pass the required metadata to corresponding methods or",
                "description": ""
              },
              "corresponding": {
                "type": "child objects.",
                "description": ""
              }
            },
            "returns": "-------\n        params : Bunch\n            A :class:`~sklearn.utils.Bunch` of the form\n            ``{\"object_name\": {\"method_name\": {params: value}}}`` which can be\n            used to pass the required metadata to corresponding methods or\n            corresponding child objects.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "validate_metadata",
          "signature": "validate_metadata(self, *, method, params)",
          "documentation": {
            "description": "Validate given metadata for a method.\n\n        This raises a ``TypeError`` if some of the passed metadata are not\n        understood by child objects.",
            "parameters": {
              "method": {
                "type": "str",
                "description": ""
              },
              "The": {
                "type": "name of the method for which the parameters are requested and",
                "description": "routed. If called inside the :term:`fit` method of a router, it"
              },
              "would": {
                "type": "be `\"fit\"`.",
                "description": ""
              },
              "params": {
                "type": "dict",
                "description": ""
              },
              "A": {
                "type": "dictionary of provided metadata.",
                "description": ""
              }
            },
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MethodMapping",
      "documentation": {
        "description": "Stores the mapping between caller and callee methods for a router.\n\n    This class is primarily used in a ``get_metadata_routing()`` of a router\n    object when defining the mapping between the router's methods and a sub-object (a\n    sub-estimator or a scorer).\n\n    Iterating through an instance of this class yields\n    ``MethodPair(caller, callee)`` instances.\n\n    .. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "add",
          "signature": "add(self, *, caller, callee)",
          "documentation": {
            "description": "Add a method mapping.\n\n        Parameters\n        ----------\n\n        caller : str\n            Parent estimator's method name in which the ``callee`` is called.\n\n        callee : str\n            Child object's method name. This method is called in ``caller``.",
            "parameters": {
              "caller": {
                "type": "str",
                "description": ""
              },
              "Parent": {
                "type": "estimator's method name in which the ``callee`` is called.",
                "description": ""
              },
              "callee": {
                "type": "str",
                "description": ""
              },
              "Child": {
                "type": "object's method name. This method is called in ``caller``.",
                "description": "Returns\n-------"
              },
              "self": {
                "type": "MethodMapping",
                "description": ""
              },
              "Returns": {
                "type": "self.",
                "description": ""
              }
            },
            "returns": "-------\n        self : MethodMapping",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Parallel",
      "documentation": {
        "description": "Tweak of :class:`joblib.Parallel` that propagates the scikit-learn configuration.\n\n    This subclass of :class:`joblib.Parallel` ensures that the active configuration\n    (thread-local) of scikit-learn is propagated to the parallel workers for the\n    duration of the execution of the parallel tasks.\n\n    The API does not change and you can refer to :class:`joblib.Parallel`\n    documentation for more details.\n\n    .. versionadded:: 1.3",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "debug",
          "signature": "debug(self, msg)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "dispatch_next",
          "signature": "dispatch_next(self)",
          "documentation": {
            "description": "Dispatch more data for parallel processing\n\n        This method is meant to be called concurrently by the multiprocessing\n        callback. We rely on the thread-safety of dispatch_one_batch to protect\n        against concurrent consumption of the unprotected iterator.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "dispatch_one_batch",
          "signature": "dispatch_one_batch(self, iterator)",
          "documentation": {
            "description": "Prefetch the tasks for the next batch and dispatch them.\n\n        The effective size of the batch is computed here.\n        If there are no more jobs to dispatch, return False, else return True.\n\n        The iterator consumption and dispatching is protected by the same\n        lock so calling this function should be thread safe.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "format",
          "signature": "format(self, obj, indent=0)",
          "documentation": {
            "description": "Return the formatted representation of the object.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "info",
          "signature": "info(self, msg)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "print_progress",
          "signature": "print_progress(self)",
          "documentation": {
            "description": "Display the process of the parallel execution only a fraction\n           of time, controlled by self.verbose.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "warn",
          "signature": "warn(self, msg)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Real",
      "documentation": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\n    In short, those are: a conversion to float, trunc(), divmod,\n    %, <, <=, >, and >=.\n\n    Real also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "documentation": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RegressorMixin",
      "documentation": {
        "description": "Mixin class for all regression estimators in scikit-learn.\n\n    This mixin defines the following functionality:\n\n    - set estimator type to `\"regressor\"` through the `estimator_type` tag;\n    - `score` method that default to :func:`~sklearn.metrics.r2_score`.\n    - enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n      which is done by setting the regressor type tag.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "--------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator, RegressorMixin\n    >>> # Mixin classes should always be on the left-hand side for a correct MRO\n    >>> class MyEstimator(RegressorMixin, BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=0)\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([-1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([0, 0, 0])\n    >>> estimator.score(X, y)\n    0.0"
      },
      "methods": [
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "documentation": {
            "description": "Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.",
            "parameters": {
              "X": {
                "type": "array",
                "description": "like of shape (n_samples, n_features)"
              },
              "Test": {
                "type": "samples. For some estimators this may be a precomputed",
                "description": ""
              },
              "kernel": {
                "type": "matrix or a list of generic objects instead with shape",
                "description": "``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``"
              },
              "is": {
                "type": "the number of samples used in the fitting for the estimator.",
                "description": ""
              },
              "y": {
                "type": "array",
                "description": "like of shape (n_samples,) or (n_samples, n_outputs)"
              },
              "True": {
                "type": "values for `X`.",
                "description": ""
              },
              "sample_weight": {
                "type": "array",
                "description": "like of shape (n_samples,), default=None"
              },
              "Sample": {
                "type": "weights.",
                "description": "Returns\n-------"
              },
              "score": {
                "type": "float",
                "description": ":math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\nNotes\n-----"
              },
              "The": {
                "type": "math:`R^2` score used when calling ``score`` on a regressor uses",
                "description": "``multioutput='uniform_average'`` from version 0.23 to keep consistent"
              },
              "with": {
                "type": "default value of :func:`~sklearn.metrics.r2_score`.",
                "description": ""
              },
              "This": {
                "type": "influences the ``score`` method of all the multioutput",
                "description": ""
              },
              "regressors": {
                "type": "(except for",
                "description": ":class:`~sklearn.multioutput.MultiOutputRegressor`)."
              }
            },
            "returns": "-------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "raises": "",
            "see_also": "",
            "notes": "-----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "StrOptions",
      "documentation": {
        "description": "Constraint representing a finite set of strings.",
        "parameters": {
          "options": {
            "type": "set of str",
            "description": ""
          },
          "The": {
            "type": "set of valid strings.",
            "description": ""
          },
          "deprecated": {
            "type": "set of str or None, default=None",
            "description": ""
          },
          "A": {
            "type": "subset of the `options` to mark as deprecated in the string",
            "description": ""
          },
          "representation": {
            "type": "of the constraint.",
            "description": ""
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}