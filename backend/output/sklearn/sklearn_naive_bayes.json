{
  "description": "Naive Bayes algorithms.\n\nThese are supervised learning methods based on applying Bayes' theorem with strong\n(naive) feature independence assumptions.",
  "functions": [
    {
      "name": "ABCMeta",
      "signature": "ABCMeta(name, bases, namespace, /, **kwargs)",
      "docstring": {
        "description": "Metaclass for defining Abstract Base Classes (ABCs).\n\nUse this metaclass to create an ABC.  An ABC can be subclassed\ndirectly, and then acts as a mix-in class.  You can also register\nunrelated concrete classes (even built-in classes) and unrelated\nABCs as 'virtual subclasses' -- these and their descendants will\nbe considered subclasses of the registering ABC by the built-in\nissubclass() function, but the registering ABC won't show up in\ntheir MRO (Method Resolution Order) nor will method\nimplementations defined by the registering ABC be callable (not\neven via super()).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "BaseEstimator",
      "signature": "BaseEstimator()",
      "docstring": {
        "description": "Base class for all estimators in scikit-learn.\n\nInheriting from this class provides default implementations of:\n\n- setting and getting parameters used by `GridSearchCV` and friends;\n- textual and HTML representation displayed in terminals and IDEs;\n- estimator serialization;\n- parameters validation;\n- data validation;\n- feature names validation.\n\nRead more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "All estimators should specify all the parameters that can be set\nat the class level in their ``__init__`` as explicit keyword\narguments (no ``*args`` or ``**kwargs``).",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator\n>>> class MyEstimator(BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         self.is_fitted_ = True\n...         return self\n...     def predict(self, X):\n...         return np.full(shape=X.shape[0], fill_value=self.param)\n>>> estimator = MyEstimator(param=2)\n>>> estimator.get_params()\n{'param': 2}\n>>> X = np.array([[1, 2], [2, 3], [3, 4]])\n>>> y = np.array([1, 0, 1])\n>>> estimator.fit(X, y).predict(X)\narray([2, 2, 2])\n>>> estimator.set_params(param=3).fit(X, y).predict(X)\narray([3, 3, 3])"
      }
    },
    {
      "name": "BernoulliNB",
      "signature": "BernoulliNB(*, alpha=1.0, force_alpha=True, binarize=0.0, fit_prior=True, class_prior=None)",
      "docstring": {
        "description": "Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "binarize": {
            "type": "float or None, default=0.0",
            "description": "Threshold for binarizing (mapping to booleans) of sample features.\n    If None, input is presumed to already consist of binary vectors."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Log probability of each class (smoothed)."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical log probability of features given a class, P(x_i|y)."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "CategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : The Complement Naive Bayes classifier\n    described in Rennie et al. (2003).\nGaussianNB : Gaussian Naive Bayes (GaussianNB).\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n\nA. McCallum and K. Nigam (1998). A comparison of event models for naive\nBayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\nText Categorization, pp. 41-48.\n\nV. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\nnaive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> Y = np.array([1, 2, 3, 4, 4, 5])\n>>> from sklearn.naive_bayes import BernoulliNB\n>>> clf = BernoulliNB()\n>>> clf.fit(X, Y)\nBernoulliNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      }
    },
    {
      "name": "CategoricalNB",
      "signature": "CategoricalNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, min_categories=None)",
      "docstring": {
        "description": "Naive Bayes classifier for categorical features.\n\nThe categorical Naive Bayes classifier is suitable for classification with\ndiscrete features that are categorically distributed. The categories of\neach feature are drawn from a categorical distribution.\n\nRead more in the :ref:`User Guide <categorical_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data."
          },
          "min_categories": {
            "type": "int or array-like of shape (n_features,), default=None",
            "description": "Minimum number of categories per feature.\n\n    - integer: Sets the minimum number of categories per feature to\n      `n_categories` for each features.\n    - array-like: shape (n_features,) where `n_categories[i]` holds the\n      minimum number of categories for the ith column of the input.\n    - None (default): Determines the number of categories automatically\n      from the training data.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "category_count_": {
            "type": "list of arrays of shape (n_features,)",
            "description": "Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the number of samples\n    encountered for each class and category of the specific feature."
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_log_prob_": {
            "type": "list of arrays of shape (n_features,)",
            "description": "Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the empirical log probability\n    of categories given the respective feature and class, ``P(x_i|y)``."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_categories_": {
            "type": "ndarray of shape (n_features,), dtype=np.int64",
            "description": "Number of categories for each feature. This value is\n    inferred from the data or set by the minimum number of categories.\n\n    .. versionadded:: 0.24"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import CategoricalNB\n>>> clf = CategoricalNB()\n>>> clf.fit(X, y)\nCategoricalNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      }
    },
    {
      "name": "ClassifierMixin",
      "signature": "ClassifierMixin()",
      "docstring": {
        "description": "Mixin class for all classifiers in scikit-learn.\n\nThis mixin defines the following functionality:\n\n- set estimator type to `\"classifier\"` through the `estimator_type` tag;\n- `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n- enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n  which is done by setting the classifier type tag.\n\nRead more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator, ClassifierMixin\n>>> # Mixin classes should always be on the left-hand side for a correct MRO\n>>> class MyEstimator(ClassifierMixin, BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         self.is_fitted_ = True\n...         return self\n...     def predict(self, X):\n...         return np.full(shape=X.shape[0], fill_value=self.param)\n>>> estimator = MyEstimator(param=1)\n>>> X = np.array([[1, 2], [2, 3], [3, 4]])\n>>> y = np.array([1, 0, 1])\n>>> estimator.fit(X, y).predict(X)\narray([1, 1, 1])\n>>> estimator.score(X, y)\n0.66..."
      }
    },
    {
      "name": "ComplementNB",
      "signature": "ComplementNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, norm=False)",
      "docstring": {
        "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003).\n\nThe Complement Naive Bayes classifier was designed to correct the \"severe\nassumptions\" made by the standard Multinomial Naive Bayes classifier. It is\nparticularly suited for imbalanced data sets.\n\nRead more in the :ref:`User Guide <complement_naive_bayes>`.\n\n.. versionadded:: 0.20",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Only used in edge case with a single class in the training set."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. Not used."
          },
          "norm": {
            "type": "bool, default=False",
            "description": "Whether or not a second normalization of the weights is performed. The\n    default behavior mirrors the implementations found in Mahout and Weka,\n    which do not follow the full algorithm described in Table 9 of the\n    paper.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class. Only used in edge\n    case with a single class in the training set."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_all_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Number of samples encountered for each feature during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature) during fitting.\n    This value is weighted by the sample weight when provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical weights for class complements."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\nTackling the poor assumptions of naive bayes text classifiers. In ICML\n(Vol. 3, pp. 616-623).\nhttps://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import ComplementNB\n>>> clf = ComplementNB()\n>>> clf.fit(X, y)\nComplementNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      }
    },
    {
      "name": "GaussianNB",
      "signature": "GaussianNB(*, priors=None, var_smoothing=1e-09)",
      "docstring": {
        "description": "Gaussian Naive Bayes (GaussianNB).\n\nCan perform online updates to model parameters via :meth:`partial_fit`.\nFor details on algorithm used to update feature means and variance online,\nsee `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque\n<http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.\n\nRead more in the :ref:`User Guide <gaussian_naive_bayes>`.",
        "parameters": {
          "priors": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data."
          },
          "var_smoothing": {
            "type": "float, default=1e-9",
            "description": "Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n\n    .. versionadded:: 0.20\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "number of training samples observed in each class."
          },
          "class_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "probability of each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "class labels known to the classifier."
          },
          "epsilon_": {
            "type": "float",
            "description": "absolute additive value to variances."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "var_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Variance of each feature per class.\n\n    .. versionadded:: 1.0"
          },
          "theta_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "mean of each feature per class."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nMultinomialNB : Naive Bayes classifier for multinomial models.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> Y = np.array([1, 1, 1, 2, 2, 2])\n>>> from sklearn.naive_bayes import GaussianNB\n>>> clf = GaussianNB()\n>>> clf.fit(X, Y)\nGaussianNB()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n>>> clf_pf = GaussianNB()\n>>> clf_pf.partial_fit(X, Y, np.unique(Y))\nGaussianNB()\n>>> print(clf_pf.predict([[-0.8, -1]]))\n[1]"
      }
    },
    {
      "name": "Integral",
      "signature": "Integral()",
      "docstring": {
        "description": "Integral adds methods that work on integral numbers.\n\nIn short, these are conversion to int, pow with modulus, and the\nbit-string operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "Interval",
      "signature": "Interval(type, left, right, *, closed)",
      "docstring": {
        "description": "Constraint representing a typed interval.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": "The set of numbers in which to set the interval.\n\n    If RealNotInt, only reals that don't have the integer type\n    are allowed. For example 1.0 is allowed but 1 is not."
          },
          "left": {
            "type": "float or int or None",
            "description": "The left bound of the interval. None means left bound is -\u221e."
          },
          "right": {
            "type": "float, int or None",
            "description": "The right bound of the interval. None means right bound is +\u221e."
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": "Whether the interval is open or closed. Possible choices are:\n\n    - `\"left\"`: the interval is closed on the left and open on the right.\n      It is equivalent to the interval `[ left, right )`.\n    - `\"right\"`: the interval is closed on the right and open on the left.\n      It is equivalent to the interval `( left, right ]`.\n    - `\"both\"`: the interval is closed.\n      It is equivalent to the interval `[ left, right ]`.\n    - `\"neither\"`: the interval is open.\n      It is equivalent to the interval `( left, right )`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "Setting a bound to `None` and setting the interval closed is valid. For instance,\nstrictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n`[0, +\u221e) U {+\u221e}`.",
        "examples": ""
      }
    },
    {
      "name": "LabelBinarizer",
      "signature": "LabelBinarizer(*, neg_label=0, pos_label=1, sparse_output=False)",
      "docstring": {
        "description": "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor\nor binary classifier per class. In doing so, one needs to convert\nmulti-class labels to binary labels (belong or does not belong\nto the class). `LabelBinarizer` makes this process easy with the\ntransform method.\n\nAt prediction time, one assigns the class for which the corresponding\nmodel gave the greatest confidence. `LabelBinarizer` makes this easy\nwith the :meth:`inverse_transform` method.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.",
        "parameters": {
          "neg_label": {
            "type": "int, default=0",
            "description": "Value with which negative labels must be encoded."
          },
          "pos_label": {
            "type": "int, default=1",
            "description": "Value with which positive labels must be encoded."
          },
          "sparse_output": {
            "type": "bool, default=False",
            "description": "True if the returned array from transform is desired to be in sparse\n    CSR format.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Holds the label for each class."
          },
          "y_type_": {
            "type": "str",
            "description": "Represents the type of the target data as evaluated by\n    :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are\n    'continuous', 'continuous-multioutput', 'binary', 'multiclass',\n    'multiclass-multioutput', 'multilabel-indicator', and 'unknown'."
          },
          "sparse_input_": {
            "type": "bool",
            "description": "`True` if the input data to transform is given as a sparse matrix,\n     `False` otherwise."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "label_binarize : Function to perform the transform operation of\n    LabelBinarizer with fixed classes.\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme.",
        "notes": "",
        "examples": ">>> from sklearn.preprocessing import LabelBinarizer\n>>> lb = LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer()\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nBinary targets transform to a column vector\n\n>>> lb = LabelBinarizer()\n>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])\n\nPassing a 2D matrix for multilabel classification\n\n>>> import numpy as np\n>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\nLabelBinarizer()\n>>> lb.classes_\narray([0, 1, 2])\n>>> lb.transform([0, 1, 2, 1])\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
      }
    },
    {
      "name": "MultinomialNB",
      "signature": "MultinomialNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None)",
      "docstring": {
        "description": "Naive Bayes classifier for multinomial models.\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical log probability of features\n    given a class, ``P(x_i|y)``."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> clf = MultinomialNB()\n>>> clf.fit(X, y)\nMultinomialNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      }
    },
    {
      "name": "Real",
      "signature": "Real()",
      "docstring": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\nIn short, those are: a conversion to float, trunc(), divmod,\n%, <, <=, >, and >=.\n\nReal also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "abstractmethod",
      "signature": "abstractmethod(funcobj)",
      "docstring": {
        "description": "A decorator indicating abstract methods.\n\nRequires that the metaclass is ABCMeta or derived from it.  A\nclass that has a metaclass derived from ABCMeta cannot be\ninstantiated unless all of its abstract methods are overridden.\nThe abstract methods can be called using any of the normal\n'super' call mechanisms.  abstractmethod() may be used to declare\nabstract methods for properties and descriptors.\n\nUsage:\n\n    class C(metaclass=ABCMeta):\n        @abstractmethod\n        def my_abstract_method(self, arg1, arg2, argN):\n            ...",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "binarize",
      "signature": "binarize(X, *, threshold=0.0, copy=True)",
      "docstring": {
        "description": "Boolean thresholding of array-like or scipy.sparse matrix.\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "description": "The data to binarize, element by element.\n    scipy.sparse matrices should be in CSR or CSC format to avoid an\n    un-necessary copy."
          },
          "threshold": {
            "type": "float, default=0.0",
            "description": "Feature values below or equal to this are replaced by 0, above it by 1.\n    Threshold may not be less than 0 for operations on sparse matrices."
          },
          "copy": {
            "type": "bool, default=True",
            "description": "If False, try to avoid a copy and binarize in place.\n    This is not guaranteed to always work in place; e.g. if the data is\n    a numpy array with an object dtype, a copy will be returned even with\n    copy=False."
          }
        },
        "returns": "X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.",
        "raises": "",
        "see_also": "Binarizer : Performs binarization using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).",
        "notes": "",
        "examples": ">>> from sklearn.preprocessing import binarize\n>>> X = [[0.4, 0.6, 0.5], [0.6, 0.1, 0.2]]\n>>> binarize(X, threshold=0.5)\narray([[0., 1., 0.],\n       [1., 0., 0.]])"
      }
    },
    {
      "name": "check_is_fitted",
      "signature": "check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=<built-in function all>)",
      "docstring": {
        "description": "Perform is_fitted validation for estimator.\n\nChecks if the estimator is fitted by verifying the presence of\nfitted attributes (ending with a trailing underscore) and otherwise\nraises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\nIf an estimator does not set any attributes with a trailing underscore, it\ncan define a ``__sklearn_is_fitted__`` method returning a boolean to\nspecify if the estimator is fitted or not. See\n:ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\nfor an example on how to use the API.\n\nIf no `attributes` are passed, this fuction will pass if an estimator is stateless.\nAn estimator can indicate it's stateless by setting the `requires_fit` tag. See\n:ref:`estimator_tags` for more information. Note that the `requires_fit` tag\nis ignored if `attributes` are passed.",
        "parameters": {
          "estimator": {
            "type": "estimator instance",
            "description": "Estimator instance for which the check is performed."
          },
          "attributes": {
            "type": "str, list or tuple of str, default=None",
            "description": "Attribute name(s) given as string or a list/tuple of strings\n    Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n    If `None`, `estimator` is considered fitted if there exist an\n    attribute that ends with a underscore and does not start with double\n    underscore."
          },
          "msg": {
            "type": "str, default=None",
            "description": "The default error message is, \"This %(name)s instance is not fitted\n    yet. Call 'fit' with appropriate arguments before using this\n    estimator.\"\n\n    For custom messages if \"%(name)s\" is present in the message string,\n    it is substituted for the estimator name.\n\n    Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\"."
          },
          "all_or_any": {
            "type": "callable, {all, any}, default=all",
            "description": "Specify whether all or any of the given attributes must exist."
          }
        },
        "returns": "",
        "raises": "TypeError\n    If the estimator is a class or not an estimator instance\n\nNotFittedError\n    If the attributes are not found.",
        "see_also": "",
        "notes": "",
        "examples": ">>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.utils.validation import check_is_fitted\n>>> from sklearn.exceptions import NotFittedError\n>>> lr = LogisticRegression()\n>>> try:\n...     check_is_fitted(lr)\n... except NotFittedError as exc:\n...     print(f\"Model is not fitted yet.\")\nModel is not fitted yet.\n>>> lr.fit([[1, 2], [1, 3]], [1, 0])\nLogisticRegression()\n>>> check_is_fitted(lr)"
      }
    },
    {
      "name": "check_non_negative",
      "signature": "check_non_negative(X, whom)",
      "docstring": {
        "description": "Check if there is any negative value in an array.",
        "parameters": {
          "X": {
            "type": "{array-like, sparse matrix}",
            "description": "Input data."
          },
          "whom": {
            "type": "str",
            "description": "Who passed X to this function."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "label_binarize",
      "signature": "label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False)",
      "docstring": {
        "description": "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nThis function makes it possible to compute this transformation for a\nfixed set of class labels known ahead of time.",
        "parameters": {
          "y": {
            "type": "array-like or sparse matrix",
            "description": "Sequence of integer labels or multilabel data to encode."
          },
          "classes": {
            "type": "array-like of shape (n_classes,)",
            "description": "Uniquely holds the label for each class."
          },
          "neg_label": {
            "type": "int, default=0",
            "description": "Value with which negative labels must be encoded."
          },
          "pos_label": {
            "type": "int, default=1",
            "description": "Value with which positive labels must be encoded."
          },
          "sparse_output": {
            "type": "bool, default=False,",
            "description": "Set to true if output binary array is desired in CSR sparse format."
          }
        },
        "returns": "Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix will\n    be of CSR format.",
        "raises": "",
        "see_also": "LabelBinarizer : Class used to wrap the functionality of label_binarize and\n    allow for fitting to classes independently of the transform operation.",
        "notes": "",
        "examples": ">>> from sklearn.preprocessing import label_binarize\n>>> label_binarize([1, 6], classes=[1, 2, 4, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nThe class ordering is preserved:\n\n>>> label_binarize([1, 6], classes=[1, 6, 4, 2])\narray([[1, 0, 0, 0],\n       [0, 1, 0, 0]])\n\nBinary targets transform to a column vector\n\n>>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])"
      }
    },
    {
      "name": "logsumexp",
      "signature": "logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False)",
      "docstring": {
        "description": "Compute the log of the sum of exponentials of input elements.",
        "parameters": {
          "a": {
            "type": "array_like",
            "description": "Input array."
          },
          "axis": {
            "type": "None or int or tuple of ints, optional",
            "description": "Axis or axes over which the sum is taken. By default `axis` is None,\n    and all elements are summed.\n\n    .. versionadded:: 0.11.0"
          },
          "b": {
            "type": "array-like, optional",
            "description": "Scaling factor for exp(`a`) must be of the same shape as `a` or\n    broadcastable to `a`. These values may be negative in order to\n    implement subtraction.\n\n    .. versionadded:: 0.12.0"
          },
          "keepdims": {
            "type": "bool, optional",
            "description": "If this is set to True, the axes which are reduced are left in the\n    result as dimensions with size one. With this option, the result\n    will broadcast correctly against the original array.\n\n    .. versionadded:: 0.15.0"
          },
          "return_sign": {
            "type": "bool, optional",
            "description": "If this is set to True, the result will be a pair containing sign\n    information; if False, results that are negative will be returned\n    as NaN. Default is False (no sign information).\n\n    .. versionadded:: 0.16.0"
          }
        },
        "returns": "res : ndarray\n    The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n    more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n    is returned. If ``return_sign`` is True, ``res`` contains the log of\n    the absolute value of the argument.\nsgn : ndarray\n    If ``return_sign`` is True, this will be an array of floating-point\n    numbers matching res containing +1, 0, -1 (for real-valued inputs)\n    or a complex phase (for complex inputs). This gives the sign of the\n    argument of the logarithm in ``res``.\n    If ``return_sign`` is False, only one result is returned.",
        "raises": "",
        "see_also": "numpy.logaddexp, numpy.logaddexp2",
        "notes": "NumPy has a logaddexp function which is very similar to `logsumexp`, but\nonly handles two arguments. `logaddexp.reduce` is similar to this\nfunction, but may be less stable.\n\nThe logarithm is a multivalued function: for each :math:`x` there is an\ninfinite number of :math:`z` such that :math:`exp(z) = x`. The convention\nis to return the :math:`z` whose imaginary part lies in :math:`(-pi, pi]`.",
        "examples": ">>> import numpy as np\n>>> from scipy.special import logsumexp\n>>> a = np.arange(10)\n>>> logsumexp(a)\n9.4586297444267107\n>>> np.log(np.sum(np.exp(a)))\n9.4586297444267107\n\nWith weights\n\n>>> a = np.arange(10)\n>>> b = np.arange(10, 0, -1)\n>>> logsumexp(a, b=b)\n9.9170178533034665\n>>> np.log(np.sum(b*np.exp(a)))\n9.9170178533034647\n\nReturning a sign flag\n\n>>> logsumexp([1,2],b=[1,-1],return_sign=True)\n(1.5413248546129181, -1.0)\n\nNotice that `logsumexp` does not directly support masked arrays. To use it\non a masked array, convert the mask into zero weights:\n\n>>> a = np.ma.array([np.log(2), 2, np.log(3)],\n...                  mask=[False, True, False])\n>>> b = (~a.mask).astype(int)\n>>> logsumexp(a.data, b=b), np.log(5)\n1.6094379124341005, 1.6094379124341005"
      }
    },
    {
      "name": "safe_sparse_dot",
      "signature": "safe_sparse_dot(a, b, *, dense_output=False)",
      "docstring": {
        "description": "Dot product that handle the sparse matrix case correctly.",
        "parameters": {
          "a": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "b": {
            "type": "{ndarray, sparse matrix}",
            "description": ""
          },
          "dense_output": {
            "type": "bool, default=False",
            "description": "When False, ``a`` and ``b`` both being sparse will yield sparse output.\n    When True, output will always be a dense array."
          }
        },
        "returns": "dot_product : {ndarray, sparse matrix}\n    Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from scipy.sparse import csr_matrix\n>>> from sklearn.utils.extmath import safe_sparse_dot\n>>> X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n>>> dot_product = safe_sparse_dot(X, X.T)\n>>> dot_product.toarray()\narray([[ 5, 11, 17],\n       [11, 25, 39],\n       [17, 39, 61]])"
      }
    },
    {
      "name": "validate_data",
      "signature": "validate_data(_estimator, /, X='no_validation', y='no_validation', reset=True, validate_separately=False, skip_check_array=False, **check_params)",
      "docstring": {
        "description": "Validate input data and set or check feature names and counts of the input.\n\nThis helper function should be used in an estimator that requires input\nvalidation. This mutates the estimator and sets the `n_features_in_` and\n`feature_names_in_` attributes if `reset=True`.\n\n.. versionadded:: 1.6",
        "parameters": {
          "_estimator": {
            "type": "estimator instance",
            "description": "The estimator to validate the input for."
          },
          "X": {
            "type": "{array-like, sparse matrix, dataframe} of shape             (n_samples, n_features), default='no validation'",
            "description": "The input samples.\n    If `'no_validation'`, no validation is performed on `X`. This is\n    useful for meta-estimator which can delegate input validation to\n    their underlying estimator(s). In that case `y` must be passed and\n    the only accepted `check_params` are `multi_output` and\n    `y_numeric`."
          },
          "y": {
            "type": "array-like of shape (n_samples,), default='no_validation'",
            "description": "The targets.\n\n    - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n      the estimator's `requires_y` tag is True, then an error will be raised.\n    - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n      on `X` and the estimator's `requires_y` tag is ignored. This is a default\n      placeholder and is never meant to be explicitly set. In that case `X` must be\n      passed.\n    - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n      either :func:`~sklearn.utils.check_array` or\n      :func:`~sklearn.utils.check_X_y` depending on `validate_separately`."
          },
          "reset": {
            "type": "bool, default=True",
            "description": "Whether to reset the `n_features_in_` attribute.\n    If False, the input will be checked for consistency with data\n    provided when reset was last True.\n\n    .. note::\n\n       It is recommended to call `reset=True` in `fit` and in the first\n       call to `partial_fit`. All other methods that validate `X`\n       should set `reset=False`."
          },
          "validate_separately": {
            "type": "False or tuple of dicts, default=False",
            "description": "Only used if `y` is not `None`.\n    If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n    kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n    respectively.\n\n    `estimator=self` is automatically added to these dicts to generate\n    more informative error message in case of invalid input data."
          },
          "skip_check_array": {
            "type": "bool, default=False",
            "description": "If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n    `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n    is called on `X` and `y`."
          },
          "**check_params": {
            "type": "kwargs",
            "description": "Parameters passed to :func:`~sklearn.utils.check_array` or\n    :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n    is not False.\n\n    `estimator=self` is automatically added to these params to generate\n    more informative error message in case of invalid input data."
          }
        },
        "returns": "out : {ndarray, sparse matrix} or tuple of these\n    The validated input. A tuple is returned if both `X` and `y` are\n    validated.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "ABCMeta",
      "docstring": {
        "description": "Metaclass for defining Abstract Base Classes (ABCs).\n\nUse this metaclass to create an ABC.  An ABC can be subclassed\ndirectly, and then acts as a mix-in class.  You can also register\nunrelated concrete classes (even built-in classes) and unrelated\nABCs as 'virtual subclasses' -- these and their descendants will\nbe considered subclasses of the registering ABC by the built-in\nissubclass() function, but the registering ABC won't show up in\ntheir MRO (Method Resolution Order) nor will method\nimplementations defined by the registering ABC be callable (not\neven via super()).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "mro",
          "signature": "mro(self, /)",
          "docstring": {
            "description": "Return a type's method resolution order.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "register",
          "signature": "register(cls, subclass)",
          "docstring": {
            "description": "Register a virtual subclass of an ABC.\n\nReturns the subclass, to allow usage as a class decorator.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BaseEstimator",
      "docstring": {
        "description": "Base class for all estimators in scikit-learn.\n\nInheriting from this class provides default implementations of:\n\n- setting and getting parameters used by `GridSearchCV` and friends;\n- textual and HTML representation displayed in terminals and IDEs;\n- estimator serialization;\n- parameters validation;\n- data validation;\n- feature names validation.\n\nRead more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "All estimators should specify all the parameters that can be set\nat the class level in their ``__init__`` as explicit keyword\narguments (no ``*args`` or ``**kwargs``).",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator\n>>> class MyEstimator(BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         self.is_fitted_ = True\n...         return self\n...     def predict(self, X):\n...         return np.full(shape=X.shape[0], fill_value=self.param)\n>>> estimator = MyEstimator(param=2)\n>>> estimator.get_params()\n{'param': 2}\n>>> X = np.array([[1, 2], [2, 3], [3, 4]])\n>>> y = np.array([1, 0, 1])\n>>> estimator.fit(X, y).predict(X)\narray([2, 2, 2])\n>>> estimator.set_params(param=3).fit(X, y).predict(X)\narray([3, 3, 3])"
      },
      "methods": [
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "BernoulliNB",
      "docstring": {
        "description": "Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "binarize": {
            "type": "float or None, default=0.0",
            "description": "Threshold for binarizing (mapping to booleans) of sample features.\n    If None, input is presumed to already consist of binary vectors."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Log probability of each class (smoothed)."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical log probability of features given a class, P(x_i|y)."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "CategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : The Complement Naive Bayes classifier\n    described in Rennie et al. (2003).\nGaussianNB : Gaussian Naive Bayes (GaussianNB).\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n\nA. McCallum and K. Nigam (1998). A comparison of event models for naive\nBayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\nText Categorization, pp. 41-48.\n\nV. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\nnaive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> Y = np.array([1, 2, 3, 4, 4, 5])\n>>> from sklearn.naive_bayes import BernoulliNB\n>>> clf = BernoulliNB()\n>>> clf.fit(X, Y)\nBernoulliNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Fit Naive Bayes classifier according to X, y.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "docstring": {
            "description": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "description": "List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples,)\n    Predicted target values for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_joint_log_proba",
          "signature": "predict_joint_log_proba(self, X)",
          "docstring": {
            "description": "Return joint log probability estimates for the test vector X.\n\nFor each row x of X and class y, the joint log probability is given by\n``log P(x, y) = log P(y) + log P(x|y),``\nwhere ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\nthe class-conditional probability.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples, n_classes)\n    Returns the joint log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Return log-probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.naive_bayes.BernoulliNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.BernoulliNB",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.naive_bayes.BernoulliNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.BernoulliNB",
          "docstring": {
            "description": "Request metadata passed to the ``partial_fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``classes`` parameter in ``partial_fit``."
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``partial_fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.naive_bayes.BernoulliNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.BernoulliNB",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "CategoricalNB",
      "docstring": {
        "description": "Naive Bayes classifier for categorical features.\n\nThe categorical Naive Bayes classifier is suitable for classification with\ndiscrete features that are categorically distributed. The categories of\neach feature are drawn from a categorical distribution.\n\nRead more in the :ref:`User Guide <categorical_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float, default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data."
          },
          "min_categories": {
            "type": "int or array-like of shape (n_features,), default=None",
            "description": "Minimum number of categories per feature.\n\n    - integer: Sets the minimum number of categories per feature to\n      `n_categories` for each features.\n    - array-like: shape (n_features,) where `n_categories[i]` holds the\n      minimum number of categories for the ith column of the input.\n    - None (default): Determines the number of categories automatically\n      from the training data.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------"
          },
          "category_count_": {
            "type": "list of arrays of shape (n_features,)",
            "description": "Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the number of samples\n    encountered for each class and category of the specific feature."
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_log_prob_": {
            "type": "list of arrays of shape (n_features,)",
            "description": "Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the empirical log probability\n    of categories given the respective feature and class, ``P(x_i|y)``."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "n_categories_": {
            "type": "ndarray of shape (n_features,), dtype=np.int64",
            "description": "Number of categories for each feature. This value is\n    inferred from the data or set by the minimum number of categories.\n\n    .. versionadded:: 0.24"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import CategoricalNB\n>>> clf = CategoricalNB()\n>>> clf.fit(X, y)\nCategoricalNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Fit Naive Bayes classifier according to X, y.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features. Here, each feature of X is\n    assumed to be from a different categorical distribution.\n    It is further assumed that all categories of each feature are\n    represented by the numbers 0, ..., n - 1, where n refers to the\n    total number of categories for the given feature. This can, for\n    instance, be achieved with the help of OrdinalEncoder."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "docstring": {
            "description": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features. Here, each feature of X is\n    assumed to be from a different categorical distribution.\n    It is further assumed that all categories of each feature are\n    represented by the numbers 0, ..., n - 1, where n refers to the\n    total number of categories for the given feature. This can, for\n    instance, be achieved with the help of OrdinalEncoder."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "description": "List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples,)\n    Predicted target values for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_joint_log_proba",
          "signature": "predict_joint_log_proba(self, X)",
          "docstring": {
            "description": "Return joint log probability estimates for the test vector X.\n\nFor each row x of X and class y, the joint log probability is given by\n``log P(x, y) = log P(y) + log P(x|y),``\nwhere ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\nthe class-conditional probability.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples, n_classes)\n    Returns the joint log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Return log-probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.naive_bayes.CategoricalNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.CategoricalNB",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.naive_bayes.CategoricalNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.CategoricalNB",
          "docstring": {
            "description": "Request metadata passed to the ``partial_fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``classes`` parameter in ``partial_fit``."
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``partial_fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.naive_bayes.CategoricalNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.CategoricalNB",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ClassifierMixin",
      "docstring": {
        "description": "Mixin class for all classifiers in scikit-learn.\n\nThis mixin defines the following functionality:\n\n- set estimator type to `\"classifier\"` through the `estimator_type` tag;\n- `score` method that default to :func:`~sklearn.metrics.accuracy_score`.\n- enforce that `fit` requires `y` to be passed through the `requires_y` tag,\n  which is done by setting the classifier type tag.\n\nRead more in the :ref:`User Guide <rolling_your_own_estimator>`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> from sklearn.base import BaseEstimator, ClassifierMixin\n>>> # Mixin classes should always be on the left-hand side for a correct MRO\n>>> class MyEstimator(ClassifierMixin, BaseEstimator):\n...     def __init__(self, *, param=1):\n...         self.param = param\n...     def fit(self, X, y=None):\n...         self.is_fitted_ = True\n...         return self\n...     def predict(self, X):\n...         return np.full(shape=X.shape[0], fill_value=self.param)\n>>> estimator = MyEstimator(param=1)\n>>> X = np.array([[1, 2], [2, 3], [3, 4]])\n>>> y = np.array([1, 0, 1])\n>>> estimator.fit(X, y).predict(X)\narray([1, 1, 1])\n>>> estimator.score(X, y)\n0.66..."
      },
      "methods": [
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ComplementNB",
      "docstring": {
        "description": "The Complement Naive Bayes classifier described in Rennie et al. (2003).\n\nThe Complement Naive Bayes classifier was designed to correct the \"severe\nassumptions\" made by the standard Multinomial Naive Bayes classifier. It is\nparticularly suited for imbalanced data sets.\n\nRead more in the :ref:`User Guide <complement_naive_bayes>`.\n\n.. versionadded:: 0.20",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Only used in edge case with a single class in the training set."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. Not used."
          },
          "norm": {
            "type": "bool, default=False",
            "description": "Whether or not a second normalization of the weights is performed. The\n    default behavior mirrors the implementations found in Mahout and Weka,\n    which do not follow the full algorithm described in Table 9 of the\n    paper.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class. Only used in edge\n    case with a single class in the training set."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_all_": {
            "type": "ndarray of shape (n_features,)",
            "description": "Number of samples encountered for each feature during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature) during fitting.\n    This value is weighted by the sample weight when provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical weights for class complements."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\nTackling the poor assumptions of naive bayes text classifiers. In ICML\n(Vol. 3, pp. 616-623).\nhttps://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import ComplementNB\n>>> clf = ComplementNB()\n>>> clf.fit(X, y)\nComplementNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Fit Naive Bayes classifier according to X, y.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "docstring": {
            "description": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "description": "List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples,)\n    Predicted target values for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_joint_log_proba",
          "signature": "predict_joint_log_proba(self, X)",
          "docstring": {
            "description": "Return joint log probability estimates for the test vector X.\n\nFor each row x of X and class y, the joint log probability is given by\n``log P(x, y) = log P(y) + log P(x|y),``\nwhere ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\nthe class-conditional probability.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples, n_classes)\n    Returns the joint log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Return log-probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.naive_bayes.ComplementNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.ComplementNB",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.naive_bayes.ComplementNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.ComplementNB",
          "docstring": {
            "description": "Request metadata passed to the ``partial_fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``classes`` parameter in ``partial_fit``."
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``partial_fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.naive_bayes.ComplementNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.ComplementNB",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GaussianNB",
      "docstring": {
        "description": "Gaussian Naive Bayes (GaussianNB).\n\nCan perform online updates to model parameters via :meth:`partial_fit`.\nFor details on algorithm used to update feature means and variance online,\nsee `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque\n<http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.\n\nRead more in the :ref:`User Guide <gaussian_naive_bayes>`.",
        "parameters": {
          "priors": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data."
          },
          "var_smoothing": {
            "type": "float, default=1e-9",
            "description": "Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n\n    .. versionadded:: 0.20\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "number of training samples observed in each class."
          },
          "class_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "probability of each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "class labels known to the classifier."
          },
          "epsilon_": {
            "type": "float",
            "description": "absolute additive value to variances."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          },
          "var_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Variance of each feature per class.\n\n    .. versionadded:: 1.0"
          },
          "theta_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "mean of each feature per class."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nMultinomialNB : Naive Bayes classifier for multinomial models.",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> Y = np.array([1, 1, 1, 2, 2, 2])\n>>> from sklearn.naive_bayes import GaussianNB\n>>> clf = GaussianNB()\n>>> clf.fit(X, Y)\nGaussianNB()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n>>> clf_pf = GaussianNB()\n>>> clf_pf.partial_fit(X, Y, np.unique(Y))\nGaussianNB()\n>>> print(clf_pf.predict([[-0.8, -1]]))\n[1]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Fit Gaussian Naive Bayes according to X, y.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples\n    and `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted).\n\n    .. versionadded:: 0.17\n       Gaussian Naive Bayes supports fitting with *sample_weight*."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "docstring": {
            "description": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance and numerical stability overhead,\nhence it is better to call partial_fit on chunks of data that are\nas large as possible (as long as fitting in the memory budget) to\nhide the overhead.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "description": "List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted).\n\n    .. versionadded:: 0.17"
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples,)\n    Predicted target values for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_joint_log_proba",
          "signature": "predict_joint_log_proba(self, X)",
          "docstring": {
            "description": "Return joint log probability estimates for the test vector X.\n\nFor each row x of X and class y, the joint log probability is given by\n``log P(x, y) = log P(y) + log P(x|y),``\nwhere ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\nthe class-conditional probability.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples, n_classes)\n    Returns the joint log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Return log-probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.naive_bayes.GaussianNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.naive_bayes.GaussianNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB",
          "docstring": {
            "description": "Request metadata passed to the ``partial_fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``classes`` parameter in ``partial_fit``."
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``partial_fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.naive_bayes.GaussianNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Integral",
      "docstring": {
        "description": "Integral adds methods that work on integral numbers.\n\nIn short, these are conversion to int, pow with modulus, and the\nbit-string operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "docstring": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Interval",
      "docstring": {
        "description": "Constraint representing a typed interval.",
        "parameters": {
          "type": {
            "type": "{numbers.Integral, numbers.Real, RealNotInt}",
            "description": "The set of numbers in which to set the interval.\n\n    If RealNotInt, only reals that don't have the integer type\n    are allowed. For example 1.0 is allowed but 1 is not."
          },
          "left": {
            "type": "float or int or None",
            "description": "The left bound of the interval. None means left bound is -\u221e."
          },
          "right": {
            "type": "float, int or None",
            "description": "The right bound of the interval. None means right bound is +\u221e."
          },
          "closed": {
            "type": "{\"left\", \"right\", \"both\", \"neither\"}",
            "description": "Whether the interval is open or closed. Possible choices are:\n\n    - `\"left\"`: the interval is closed on the left and open on the right.\n      It is equivalent to the interval `[ left, right )`.\n    - `\"right\"`: the interval is closed on the right and open on the left.\n      It is equivalent to the interval `( left, right ]`.\n    - `\"both\"`: the interval is closed.\n      It is equivalent to the interval `[ left, right ]`.\n    - `\"neither\"`: the interval is open.\n      It is equivalent to the interval `( left, right )`."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "Setting a bound to `None` and setting the interval closed is valid. For instance,\nstrictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n`[0, +\u221e) U {+\u221e}`.",
        "examples": ""
      },
      "methods": [
        {
          "name": "is_satisfied_by",
          "signature": "is_satisfied_by(self, val)",
          "docstring": {
            "description": "Whether or not a value satisfies the constraint.",
            "parameters": {
              "val": {
                "type": "object",
                "description": "The value to check."
              }
            },
            "returns": "is_satisfied : bool\n    Whether or not the constraint is satisfied by this value.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "LabelBinarizer",
      "docstring": {
        "description": "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor\nor binary classifier per class. In doing so, one needs to convert\nmulti-class labels to binary labels (belong or does not belong\nto the class). `LabelBinarizer` makes this process easy with the\ntransform method.\n\nAt prediction time, one assigns the class for which the corresponding\nmodel gave the greatest confidence. `LabelBinarizer` makes this easy\nwith the :meth:`inverse_transform` method.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.",
        "parameters": {
          "neg_label": {
            "type": "int, default=0",
            "description": "Value with which negative labels must be encoded."
          },
          "pos_label": {
            "type": "int, default=1",
            "description": "Value with which positive labels must be encoded."
          },
          "sparse_output": {
            "type": "bool, default=False",
            "description": "True if the returned array from transform is desired to be in sparse\n    CSR format.\n\nAttributes\n----------"
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Holds the label for each class."
          },
          "y_type_": {
            "type": "str",
            "description": "Represents the type of the target data as evaluated by\n    :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are\n    'continuous', 'continuous-multioutput', 'binary', 'multiclass',\n    'multiclass-multioutput', 'multilabel-indicator', and 'unknown'."
          },
          "sparse_input_": {
            "type": "bool",
            "description": "`True` if the input data to transform is given as a sparse matrix,\n     `False` otherwise."
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "label_binarize : Function to perform the transform operation of\n    LabelBinarizer with fixed classes.\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme.",
        "notes": "",
        "examples": ">>> from sklearn.preprocessing import LabelBinarizer\n>>> lb = LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer()\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nBinary targets transform to a column vector\n\n>>> lb = LabelBinarizer()\n>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])\n\nPassing a 2D matrix for multilabel classification\n\n>>> import numpy as np\n>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\nLabelBinarizer()\n>>> lb.classes_\narray([0, 1, 2])\n>>> lb.transform([0, 1, 2, 1])\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, y)",
          "docstring": {
            "description": "Fit label binarizer.",
            "parameters": {
              "y": {
                "type": "ndarray of shape (n_samples,) or (n_samples, n_classes)",
                "description": "Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "fit_transform",
          "signature": "fit_transform(self, y)",
          "docstring": {
            "description": "Fit label binarizer/transform multi-class labels to binary labels.\n\nThe output of transform is sometimes referred to as\nthe 1-of-K coding scheme.",
            "parameters": {
              "y": {
                "type": "{ndarray, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)",
                "description": "Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification. Sparse matrix can be\n    CSR, CSC, COO, DOK, or LIL."
              }
            },
            "returns": "Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix\n    will be of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "inverse_transform",
          "signature": "inverse_transform(self, Y, threshold=None)",
          "docstring": {
            "description": "Transform binary labels back to multi-class labels.",
            "parameters": {
              "Y": {
                "type": "{ndarray, sparse matrix} of shape (n_samples, n_classes)",
                "description": "Target values. All sparse matrices are converted to CSR before\n    inverse transformation."
              },
              "threshold": {
                "type": "float, default=None",
                "description": "Threshold used in the binary and multi-label cases.\n\n    Use 0 when ``Y`` contains the output of :term:`decision_function`\n    (classifier).\n    Use 0.5 when ``Y`` contains the output of :term:`predict_proba`.\n\n    If None, the threshold is assumed to be half way between\n    neg_label and pos_label."
              }
            },
            "returns": "y : {ndarray, sparse matrix} of shape (n_samples,)\n    Target values. Sparse matrix will be of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "In the case when the binary labels are fractional\n(probabilistic), :meth:`inverse_transform` chooses the class with the\ngreatest value. Typically, this allows to use the output of a\nlinear model's :term:`decision_function` method directly as the input\nof :meth:`inverse_transform`.",
            "examples": ""
          }
        },
        {
          "name": "set_inverse_transform_request",
          "signature": "set_inverse_transform_request(self: sklearn.preprocessing._label.LabelBinarizer, *, threshold: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._label.LabelBinarizer",
          "docstring": {
            "description": "Request metadata passed to the ``inverse_transform`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "threshold": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``threshold`` parameter in ``inverse_transform``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_output",
          "signature": "set_output(self, *, transform=None)",
          "docstring": {
            "description": "Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an example on how to use the API.",
            "parameters": {
              "transform": {
                "type": "{\"default\", \"pandas\", \"polars\"}, default=None",
                "description": "Configure output of `transform` and `fit_transform`.\n\n    - `\"default\"`: Default output format of a transformer\n    - `\"pandas\"`: DataFrame output\n    - `\"polars\"`: Polars output\n    - `None`: Transform configuration is unchanged\n\n    .. versionadded:: 1.4\n        `\"polars\"` option was added."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transform",
          "signature": "transform(self, y)",
          "docstring": {
            "description": "Transform multi-class labels to binary labels.\n\nThe output of transform is sometimes referred to by some authors as\nthe 1-of-K coding scheme.",
            "parameters": {
              "y": {
                "type": "{array, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)",
                "description": "Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification. Sparse matrix can be\n    CSR, CSC, COO, DOK, or LIL."
              }
            },
            "returns": "Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix\n    will be of CSR format.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "MultinomialNB",
      "docstring": {
        "description": "Naive Bayes classifier for multinomial models.\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.",
        "parameters": {
          "alpha": {
            "type": "float or array-like of shape (n_features,), default=1.0",
            "description": "Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing)."
          },
          "force_alpha": {
            "type": "bool, default=True",
            "description": "If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`."
          },
          "fit_prior": {
            "type": "bool, default=True",
            "description": "Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used."
          },
          "class_prior": {
            "type": "array-like of shape (n_classes,), default=None",
            "description": "Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------"
          },
          "class_count_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided."
          },
          "class_log_prior_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Smoothed empirical log probability for each class."
          },
          "classes_": {
            "type": "ndarray of shape (n_classes,)",
            "description": "Class labels known to the classifier"
          },
          "feature_count_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided."
          },
          "feature_log_prob_": {
            "type": "ndarray of shape (n_classes, n_features)",
            "description": "Empirical log probability of features\n    given a class, ``P(x_i|y)``."
          },
          "n_features_in_": {
            "type": "int",
            "description": "Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24"
          },
          "feature_names_in_": {
            "type": "ndarray of shape (`n_features_in_`,)",
            "description": "Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0"
          }
        },
        "returns": "",
        "raises": "",
        "see_also": "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html",
        "notes": "",
        "examples": ">>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> clf = MultinomialNB()\n>>> clf.fit(X, y)\nMultinomialNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
      },
      "methods": [
        {
          "name": "fit",
          "signature": "fit(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Fit Naive Bayes classifier according to X, y.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_metadata_routing",
          "signature": "get_metadata_routing(self)",
          "docstring": {
            "description": "Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.",
            "parameters": {},
            "returns": "routing : MetadataRequest\n    A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n    routing information.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_params",
          "signature": "get_params(self, deep=True)",
          "docstring": {
            "description": "Get parameters for this estimator.",
            "parameters": {
              "deep": {
                "type": "bool, default=True",
                "description": "If True, will return the parameters for this estimator and\n    contained subobjects that are estimators."
              }
            },
            "returns": "params : dict\n    Parameter names mapped to their values.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "partial_fit",
          "signature": "partial_fit(self, X, y, classes=None, sample_weight=None)",
          "docstring": {
            "description": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.",
            "parameters": {
              "X": {
                "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
                "description": "Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features."
              },
              "y": {
                "type": "array-like of shape (n_samples,)",
                "description": "Target values."
              },
              "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "description": "List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Weights applied to individual samples (1. for unweighted)."
              }
            },
            "returns": "self : object\n    Returns the instance itself.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict",
          "signature": "predict(self, X)",
          "docstring": {
            "description": "Perform classification on an array of test vectors X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples,)\n    Predicted target values for X.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_joint_log_proba",
          "signature": "predict_joint_log_proba(self, X)",
          "docstring": {
            "description": "Return joint log probability estimates for the test vector X.\n\nFor each row x of X and class y, the joint log probability is given by\n``log P(x, y) = log P(y) + log P(x|y),``\nwhere ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\nthe class-conditional probability.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : ndarray of shape (n_samples, n_classes)\n    Returns the joint log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_log_proba",
          "signature": "predict_log_proba(self, X)",
          "docstring": {
            "description": "Return log-probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "predict_proba",
          "signature": "predict_proba(self, X)",
          "docstring": {
            "description": "Return probability estimates for the test vector X.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "The input samples."
              }
            },
            "returns": "C : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "score",
          "signature": "score(self, X, y, sample_weight=None)",
          "docstring": {
            "description": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
            "parameters": {
              "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "description": "Test samples."
              },
              "y": {
                "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
                "description": "True labels for `X`."
              },
              "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "description": "Sample weights."
              }
            },
            "returns": "score : float\n    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_fit_request",
          "signature": "set_fit_request(self: sklearn.naive_bayes.MultinomialNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.MultinomialNB",
          "docstring": {
            "description": "Request metadata passed to the ``fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_params",
          "signature": "set_params(self, **params)",
          "docstring": {
            "description": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.",
            "parameters": {
              "**params": {
                "type": "dict",
                "description": "Estimator parameters."
              }
            },
            "returns": "self : estimator instance\n    Estimator instance.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_partial_fit_request",
          "signature": "set_partial_fit_request(self: sklearn.naive_bayes.MultinomialNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.MultinomialNB",
          "docstring": {
            "description": "Request metadata passed to the ``partial_fit`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "classes": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``classes`` parameter in ``partial_fit``."
              },
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``partial_fit``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "set_score_request",
          "signature": "set_score_request(self: sklearn.naive_bayes.MultinomialNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.MultinomialNB",
          "docstring": {
            "description": "Request metadata passed to the ``score`` method.\n\nNote that this method is only relevant if\n``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\nPlease see :ref:`User Guide <metadata_routing>` on how the routing\nmechanism works.\n\nThe options for each parameter are:\n\n- ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n\n- ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n\n- ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n\n- ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n\nThe default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\nexisting request. This allows you to change the request for some\nparameters and not others.\n\n.. versionadded:: 1.3\n\n.. note::\n    This method is only relevant if this estimator is used as a\n    sub-estimator of a meta-estimator, e.g. used inside a\n    :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.",
            "parameters": {
              "sample_weight": {
                "type": "str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED",
                "description": "Metadata routing for ``sample_weight`` parameter in ``score``."
              }
            },
            "returns": "self : object\n    The updated object.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Real",
      "docstring": {
        "description": "To Complex, Real adds the operations that work on real numbers.\n\nIn short, those are: a conversion to float, trunc(), divmod,\n%, <, <=, >, and >=.\n\nReal also provides defaults for the derived operations.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "conjugate",
          "signature": "conjugate(self)",
          "docstring": {
            "description": "Conjugate is a no-op for Reals.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ],
  "constants": []
}