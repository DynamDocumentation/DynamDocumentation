{
  "description": "No description available",
  "functions": [
    {
      "name": "get_abstract_mesh",
      "signature": "get_abstract_mesh()",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "set_mesh",
      "signature": "set_mesh(mesh: 'mesh_lib.Mesh | None') -> 'mesh_lib.Mesh | None'",
      "documentation": {
        "description": "Sets the given concrete mesh globally and returns the previous concrete\nmesh.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "use_mesh",
      "signature": "use_mesh(mesh: 'mesh_lib.Mesh')",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "AbstractMesh",
      "documentation": {
        "description": "AbstractMesh contains only axis names and axis sizes.\nIt does not contain concrete devices compared to `jax.sharding.Mesh`. You\nshould use this as an input to the sharding passed to with_sharding_constraint\nand mesh passed to shard_map to avoid tracing and lowering cache misses when\nyour mesh shape and axis names stay the same but the devices change.\nSee the description of https://github.com/jax-ml/jax/pull/23022 for more\ndetails.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "update_axis_types",
          "signature": "update_axis_types(self, name_to_type: 'dict[MeshAxisName, AxisType]')",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "AxisType",
      "documentation": {
        "description": "Create a collection of name/value pairs.\nExample enumeration:\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\nAccess them by:\n- attribute access:\n>>> Color.RED\n<Color.RED: 1>\n- value lookup:\n>>> Color(1)\n<Color.RED: 1>\n- name lookup:\n>>> Color['RED']\n<Color.RED: 1>\nEnumerations can be iterated over, and know how many members they have:\n>>> len(Color)\n3\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "GSPMDSharding",
      "documentation": {
        "description": "Describes how a :class:`jax.Array` is laid out across devices.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check_compatible_aval",
          "signature": "check_compatible_aval(self, aval_shape: 'Shape') -> 'None'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_replicated",
          "signature": "get_replicated(device_assignment, *, memory_kind: 'str | None' = None)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'GSPMDSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Mesh",
      "documentation": {
        "description": "Declare the hardware resources available in the scope of this manager.\nIn particular, all ``axis_names`` become valid resource names inside the\nmanaged block and can be used e.g. in the ``in_axis_resources`` argument of\n:py:func:`jax.experimental.pjit.pjit`. Also see JAX's multi-process programming\nmodel (https://jax.readthedocs.io/en/latest/multi_process.html)\nand the Distributed arrays and automatic parallelization tutorial\n(https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\nIf you are compiling in multiple threads, make sure that the\n``with Mesh`` context manager is inside the function that the threads will\nexecute.\nArgs:\ndevices: A NumPy ndarray object containing JAX device objects (as\nobtained e.g. from :py:func:`jax.devices`).\naxis_names: A sequence of resource axis names to be assigned to the\ndimensions of the ``devices`` argument. Its length should match the\nrank of ``devices``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from jax.experimental.pjit import pjit\n>>> from jax.sharding import Mesh\n>>> from jax.sharding import PartitionSpec as P\n>>> import numpy as np\n...\n>>> inp = np.arange(16).reshape((8, 2))\n>>> devices = np.array(jax.devices()).reshape(4, 2)\n...\n>>> # Declare a 2D mesh with axes `x` and `y`.\n>>> global_mesh = Mesh(devices, ('x', 'y'))\n>>> # Use the mesh object directly as a context manager.\n>>> with global_mesh:\n...   out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)\n>>> # Initialize the Mesh and use the mesh as the context manager.\n>>> with Mesh(devices, ('x', 'y')) as global_mesh:\n...   out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)\n>>> # Also you can use it as `with ... as ...`.\n>>> global_mesh = Mesh(devices, ('x', 'y'))\n>>> with global_mesh as m:\n...   out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)\n>>> # You can also use it as `with Mesh(...)`.\n>>> with Mesh(devices, ('x', 'y')):\n...   out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      "methods": []
    },
    {
      "name": "NamedSharding",
      "documentation": {
        "description": "A :class:`NamedSharding` expresses sharding using named axes.\nA :class:`NamedSharding` is a pair of a :class:`Mesh` of devices and\n:class:`PartitionSpec` which describes how to shard an array across that\nmesh.\nA :class:`Mesh` is a multidimensional NumPy array of JAX devices,\nwhere each axis of the mesh has a name, e.g. ``'x'`` or ``'y'``.\nA :class:`PartitionSpec` is a tuple, whose elements can be a ``None``,\na mesh axis, or a tuple of mesh axes. Each element describes how an input\ndimension is partitioned across zero or more mesh dimensions. For example,\n``PartitionSpec('x', 'y')`` says that the first dimension of data\nis sharded across ``x`` axis of the mesh, and the second dimension is sharded\nacross ``y`` axis of the mesh.\nThe Distributed arrays and automatic parallelization\n(https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#namedsharding-gives-a-way-to-express-shardings-with-names)\ntutorial has more details and diagrams that explain how\n:class:`Mesh` and :class:`PartitionSpec` are used.\nArgs:\nmesh: A :class:`jax.sharding.Mesh` object.\nspec: A :class:`jax.sharding.PartitionSpec` object.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from jax.sharding import Mesh\n>>> from jax.sharding import PartitionSpec as P\n>>> mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\n>>> spec = P('x', 'y')\n>>> named_sharding = jax.sharding.NamedSharding(mesh, spec)"
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check_compatible_aval",
          "signature": "check_compatible_aval(self, aval_shape: 'Shape') -> 'None'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'NamedSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_spec",
          "signature": "with_spec(self, spec: 'PartitionSpec | Sequence[Any]') -> 'NamedSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PartitionSpec",
      "documentation": {
        "description": "Tuple describing how to partition an array across a mesh of devices.\nEach element is either ``None``, a string, or a tuple of strings.\nSee the documentation of :class:`jax.sharding.NamedSharding` for more details.\nThis class exists so JAX's pytree utilities can distinguish a partition\nspecifications from tuples that should be treated as pytrees.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value)",
          "documentation": {
            "description": "Return first index of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PmapSharding",
      "documentation": {
        "description": "Describes a sharding used by :func:`jax.pmap`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "default",
          "signature": "default(shape: 'Shape', sharded_dim: 'int | None' = 0, devices: 'Sequence[xc.Device] | None' = None) -> 'PmapSharding'",
          "documentation": {
            "description": "Creates a :class:`PmapSharding` which matches the default placement\nused by :func:`jax.pmap`.\nArgs:\nshape: The shape of the input array.\nsharded_dim: Dimension the input array is sharded on. Defaults to 0.\ndevices: Optional sequence of devices to use. If omitted, the implicit\ndevice order used by pmap is used, which is the order of\n:func:`jax.local_devices`.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'PmapSharding', other: 'PmapSharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str')",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "PositionalSharding",
      "documentation": {
        "description": "Describes how a :class:`jax.Array` is laid out across devices.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check_compatible_aval",
          "signature": "check_compatible_aval(self, aval_shape: 'Shape') -> 'None'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "replicate",
          "signature": "replicate(self, axis=None, keepdims=True) -> 'PositionalSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reshape",
          "signature": "reshape(self, *shape) -> 'PositionalSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transpose",
          "signature": "transpose(self, *axes) -> 'PositionalSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'PositionalSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Sharding",
      "documentation": {
        "description": "Describes how a :class:`jax.Array` is laid out across devices.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'Sharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "SingleDeviceSharding",
      "documentation": {
        "description": "A :class:`Sharding` that places its data on a single device.\nArgs:\ndevice: A single :py:class:`Device`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> single_device_sharding = jax.sharding.SingleDeviceSharding(\n...     jax.devices()[0])"
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'SingleDeviceSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}