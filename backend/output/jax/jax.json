{
  "description": "No description available",
  "functions": [
    {
      "name": "block_until_ready",
      "signature": "block_until_ready(x)",
      "documentation": {
        "description": "Tries to call a ``block_until_ready`` method on pytree leaves.\nArgs:\nx: a pytree, usually with at least some JAX array instances at its leaves.",
        "parameters": {},
        "returns": "A pytree with the same structure and values of the input, where the values\nof all JAX array leaves are ready.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "check_tracer_leaks",
      "signature": "check_tracer_leaks(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_check_tracer_leaks` config option.\nTurn on checking for leaked tracers as soon as a trace completes. Enabling leak checking may have performance impacts: some caching is disabled, and other overheads may be added. Additionally, be aware that some Python debuggers can cause false positives, so it is recommended to disable any debuggers while leak checking is enabled.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "checkpoint",
      "signature": "checkpoint(fun: 'Callable', *, prevent_cse: 'bool' = True, policy: 'Callable[..., bool] | None' = None, static_argnums: 'int | tuple[int, ...]' = ()) -> 'Callable'",
      "documentation": {
        "description": "Make ``fun`` recompute internal linearization points when differentiated.\nThe :func:`jax.checkpoint` decorator, aliased to :func:`jax.remat`, provides a\nway to trade off computation time and memory cost in the context of automatic\ndifferentiation, especially with reverse-mode autodiff like :func:`jax.grad`\nand :func:`jax.vjp` but also with :func:`jax.linearize`.\nWhen differentiating a function in reverse-mode, by default all the\nlinearization points (e.g. inputs to elementwise nonlinear primitive\noperations) are stored when evaluating the forward pass so that they can be\nreused on the backward pass. This evaluation strategy can lead to a high\nmemory cost, or even to poor performance on hardware accelerators where memory\naccess is much more expensive than FLOPs.\nAn alternative evaluation strategy is for some of the linearization points to\nbe recomputed (i.e. rematerialized) rather than stored. This approach can\nreduce memory usage at the cost of increased computation.\nThis function decorator produces a new version of ``fun`` which follows\nthe rematerialization strategy rather than the default store-everything\nstrategy. That is, it returns a new version of ``fun`` which, when\ndifferentiated, doesn't store any of its intermediate linearization points.\nInstead, these linearization points are recomputed from the function's saved\ninputs.\nSee the examples below.\nArgs:\nfun: Function for which the autodiff evaluation strategy is to be changed\nfrom the default of storing all intermediate linearization points to\nrecomputing them. Its arguments and return value should be arrays,\nscalars, or (nested) standard Python containers (tuple/list/dict) thereof.\nprevent_cse: Optional, boolean keyword-only argument indicating whether to\nprevent common subexpression elimination (CSE) optimizations in the HLO\ngenerated from differentiation. This CSE prevention has costs because it\ncan foil other optimizations, and because it can incur high overheads on\nsome backends, especially GPU. The default is True because otherwise,\nunder a :func:`~jax.jit` or :func:`~jax.pmap`, CSE can defeat the purpose\nof this decorator.\nBut in some settings, like when used inside a :func:`~jax.lax.scan`, this\nCSE prevention mechanism is unnecessary, in which case ``prevent_cse`` can\nbe set to False.\nstatic_argnums: Optional, int or sequence of ints, a keyword-only argument\nindicating which argument values on which to specialize for tracing and\ncaching purposes. Specifying arguments as static can avoid\nConcretizationTypeErrors when tracing, but at the cost of more retracing\noverheads. See the example below.\npolicy: Optional, callable keyword-only argument. It should be one of the\nattributes of ``jax.checkpoint_policies``. The callable takes as input a\ntype-level specification of a first-order primitive application and\nreturns a boolean indicating whether the corresponding output value(s) can\nbe saved as residuals (or instead must be recomputed in the (co)tangent\ncomputation if needed).",
        "parameters": {},
        "returns": "A function (callable) with the same input/output behavior as ``fun`` but\nwhich, when differentiated using e.g. :func:`jax.grad`, :func:`jax.vjp`, or\n:func:`jax.linearize`, recomputes rather than stores intermediate\nlinearization points, thus potentially saving memory at the cost of extra\ncomputation.\nHere is a simple example:\n>>> import jax\n>>> import jax.numpy as jnp\n>>> @jax.checkpoint\n... def g(x):\n...   y = jnp.sin(x)\n...   z = jnp.sin(y)\n...   return z\n...\n>>> jax.value_and_grad(g)(2.0)\n(Array(0.78907233, dtype=float32, weak_type=True), Array(-0.2556391, dtype=float32, weak_type=True))\nHere, the same value is produced whether or not the :func:`jax.checkpoint`\ndecorator is present. When the decorator is not present, the values\n``jnp.cos(2.0)`` and ``jnp.cos(jnp.sin(2.0))`` are computed on the forward\npass and are stored for use in the backward pass, because they are needed\non the backward pass and depend only on the primal inputs. When using\n:func:`jax.checkpoint`, the forward pass will compute only the primal outputs\nand only the primal inputs (``2.0``) will be stored for the backward pass.\nAt that time, the value ``jnp.sin(2.0)`` is recomputed, along with the values\n``jnp.cos(2.0)`` and ``jnp.cos(jnp.sin(2.0))``.\nWhile :func:`jax.checkpoint` controls what values are stored from the\nforward-pass to be used on the backward pass, the total amount of memory\nrequired to evaluate a function or its VJP depends on many additional internal\ndetails of that function. Those details include which numerical primitives are\nused, how they're composed, where jit and control flow primitives like scan\nare used, and other factors.\nThe :func:`jax.checkpoint` decorator can be applied recursively to express\nsophisticated autodiff rematerialization strategies. For example:\n>>> def recursive_checkpoint(funs):\n...   if len(funs) == 1:\n...     return funs[0]\n...   elif len(funs) == 2:\n...     f1, f2 = funs\n...     return lambda x: f1(f2(x))\n...   else:\n...     f1 = recursive_checkpoint(funs[:len(funs)//2])\n...     f2 = recursive_checkpoint(funs[len(funs)//2:])\n...     return lambda x: f1(jax.checkpoint(f2)(x))\n...\nIf ``fun`` involves Python control flow that depends on argument values,\nit may be necessary to use the ``static_argnums`` parameter. For example,\nconsider a boolean flag argument::\nfrom functools import partial\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, is_training):\nif is_training:\n...\nelse:\n...\nHere, the use of ``static_argnums`` allows the ``if`` statement's condition\nto depends on the value of ``is_training``. The cost to using\n``static_argnums`` is that it introduces re-tracing overheads across calls:\nin the example, ``foo`` is re-traced every time it is called with a new value\nof ``is_training``. In some situations, ``jax.ensure_compile_time_eval``\nis needed as well::\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, y):\nwith jax.ensure_compile_time_eval():\ny_pos = y > 0\nif y_pos:\n...\nelse:\n...\nAs an alternative to using ``static_argnums`` (and\n``jax.ensure_compile_time_eval``), it may be easier to compute some values\noutside the :func:`jax.checkpoint`-decorated function and then close over them.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "clear_caches",
      "signature": "clear_caches()",
      "documentation": {
        "description": "Clear all compilation and staging caches.\nThis doesn't clear the persistent cache; to disable it (e.g. for benchmarks),\nset the jax_enable_compilation_cache config option to False.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "closure_convert",
      "signature": "closure_convert(fun: 'Callable', *example_args) -> 'tuple[Callable, list[Any]]'",
      "documentation": {
        "description": "Closure conversion utility, for use with higher-order custom derivatives.\nTo define custom derivatives such as with ``jax.custom_vjp(f)``, the target\nfunction ``f`` must take, as formal arguments, all values involved in\ndifferentiation. If ``f`` is a higher-order function, in that it accepts as an\nargument a Python function ``g``, then values stored away in ``g``'s closure\nwill not be visible to the custom derivative rules, and attempts at AD\ninvolving these values will fail. One way around this is to convert the\nclosure by extracting these values, and to pass them as explicit formal\narguments across the custom derivative boundary. This utility carries out that\nconversion. More precisely, it closure-converts the function ``fun``\nspecialized to the types of the arguments given in ``example_args``.\nWhen we refer here to \"values in the closure\" of ``fun``, we do not mean the\nvalues that are captured by Python directly when ``fun`` is defined (e.g. the\nPython objects in ``fun.__closure__``, if the attribute exists). Rather, we\nmean values encountered during the execution of ``fun`` on ``example_args``\nthat determine its output. This may include, for instance, arrays captured\ntransitively in Python closures, i.e. in the Python closure of functions\ncalled by ``fun``, the closures of the functions that they call, and so forth.\nThe function ``fun`` must be a pure function.\nExample usage::\ndef minimize(objective_fn, x0):\nconverted_fn, aux_args = closure_convert(objective_fn, x0)\nreturn _minimize(converted_fn, x0, *aux_args)\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef _minimize(objective_fn, x0, *args):\nz = objective_fn(x0, *args)\n# ... find minimizer x_opt ...\nreturn x_opt\ndef fwd(objective_fn, x0, *args):\ny = _minimize(objective_fn, x0, *args)\nreturn y, (y, args)\ndef rev(objective_fn, res, g):\ny, args = res\ny_bar = g\n# ... custom reverse-mode AD ...\nreturn x0_bar, *args_bars\n_minimize.defvjp(fwd, rev)\nArgs:\nfun: Python callable to be converted. Must be a pure function.\nexample_args: Arrays, scalars, or (nested) standard Python\ncontainers (tuples, lists, dicts, namedtuples, i.e., pytrees)\nthereof, used to determine the types of the formal arguments to\n``fun``. This type-specialized form of ``fun`` is the function\nthat will be closure converted.",
        "parameters": {},
        "returns": "A pair comprising (i) a Python callable, accepting the same\narguments as ``fun`` followed by arguments corresponding to the\nvalues hoisted from its closure, and (ii) a list of values hoisted\nfrom the closure.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "copy_to_host_async",
      "signature": "copy_to_host_async(x)",
      "documentation": {
        "description": "Tries to call a ``copy_to_host_async`` method on pytree leaves.\nFor each leaf this method will try to call the ``copy_to_host_async`` method\non the leaf. If the leaf is not a JAX array, or if the leaf does not have a\n``copy_to_host_async`` method, then this method will do nothing to the leaf.\nArgs:\nx: a pytree, usually with at least some JAX array instances at its leaves.",
        "parameters": {},
        "returns": "A pytree with the same structure and values of the input, where the host\ncopy of the values of all JAX array leaves are started.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "custom_gradient",
      "signature": "custom_gradient(fun)",
      "documentation": {
        "description": "Convenience function for defining custom VJP rules (aka custom gradients).\nWhile the canonical way to define custom VJP rules is via ``jax.custom_vjp``,\nthe ``custom_gradient`` convenience wrapper follows TensorFlow's\n``tf.custom_gradient`` API. The difference here is that ``custom_gradient``\ncan be used as a decorator on one function that returns both the primal value\n(representing the output of the mathematical function to be differentiated)\nand the VJP (gradient) function. See\nhttps://www.tensorflow.org/api_docs/python/tf/custom_gradient.\nIf the mathematical function to be differentiated has Haskell-like signature\n``a -> b``, then the Python callable ``fun`` should have the signature\n``a -> (b, CT b --o CT a)`` where we use ``CT x`` to denote a cotangent type\nfor ``x`` and the ``--o`` arrow to denote a linear function. See the example\nbelow. That is, ``fun`` should return a pair where the first element\nrepresents the value of the mathematical function to be differentiated and the\nsecond element is a function to be called on the backward pass of reverse-mode\nautomatic differentiation (i.e. the \"custom gradient\" function).\nThe function returned as the second element of the output of ``fun`` can close\nover intermediate values computed when evaluating the function to be\ndifferentiated. That is, use lexical closure to share work between the forward\npass and the backward pass of reverse-mode automatic differentiation. However,\nit cannot perform Python control flow which depends on the values of the\nclosed-over intermediate values or its cotangent arguments; if the function\nincludes such control flow, an error is raised.\nArgs:\nfun: a Python callable specifying both the mathematical function to be\ndifferentiated and its reverse-mode differentiation rule. It should return\na pair consisting of an output value and a Python callable that represents\nthe custom gradient function.",
        "parameters": {},
        "returns": "A Python callable that accepts the same arguments as ``fun`` and returns the\noutput value specified by the first element of ``fun``'s output pair.\nFor example:\n>>> @jax.custom_gradient\n... def f(x):\n...   return x ** 2, lambda g: (g * x,)\n...\n>>> print(f(3.))\n9.0\n>>> print(jax.grad(f)(3.))\n3.0\nAn example with a function on two arguments, so that the VJP function must\nreturn a tuple of length two:\n>>> @jax.custom_gradient\n... def f(x, y):\n...   return x * y, lambda g: (g * y, g * x)\n...\n>>> print(f(3., 4.))\n12.0\n>>> print(jax.grad(f, argnums=(0, 1))(3., 4.))\n(Array(4., dtype=float32, weak_type=True), Array(3., dtype=float32, weak_type=True))",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "debug_infs",
      "signature": "debug_infs(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_debug_infs` config option.\nAdd inf checks to every operation. When an inf is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the inf.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "debug_key_reuse",
      "signature": "debug_key_reuse(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_debug_key_reuse` config option.\nTurn on experimental key reuse checking. With this configuration enabled, typed PRNG keys (i.e. keys created with jax.random.key()) will have their usage tracked, and incorrect reuse of a previously-used key will lead to an error. Currently enabling this leads to a small Python overhead on every call to a JIT-compiled function with keys as inputs or outputs.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "debug_nans",
      "signature": "debug_nans(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_debug_nans` config option.\nAdd nan checks to every operation. When a nan is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the nan.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "default_backend",
      "signature": "default_backend() -> 'str'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "default_device",
      "signature": "default_device(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_default_device` config option.\nConfigure the default device for JAX operations. Set to a Device object (e.g. ``jax.devices(\"cpu\")[0]``) to use that Device as the default device for JAX operations and jit'd function calls (there is no effect on multi-device computations, e.g. pmapped function calls). Set to None to use the system default device. See :ref:`faq-data-placement` for more information on device placement.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "default_matmul_precision",
      "signature": "default_matmul_precision(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_default_matmul_precision` config option.\nControl the default matmul and conv precision for 32bit inputs.\nSome platforms, like TPU, offer configurable precision levels for matrix multiplication and convolution computations, trading off accuracy for speed. The precision can be controlled for each operation; for example, see the :func:`jax.lax.conv_general_dilated` and :func:`jax.lax.dot` docstrings. But it can be useful to control the default behavior obtained when an operation is not given a specific precision.\nThis option can be used to control the default precision level for computations involved in matrix multiplication and convolution on 32bit inputs. The levels roughly describe the precision at which scalar products are computed. The 'bfloat16' option is the fastest and least precise; 'float32' is similar to full float32 precision; 'tensorfloat32' is intermediate.\nThis parameter can also be used to specify an accumulation \"algorithm\" for functions that perform matrix multiplications, like :func:`jax.lax.dot`. To specify an algorithm, set this option to the name of a :class:`~jax.lax.DotAlgorithmPreset`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "default_prng_impl",
      "signature": "default_prng_impl(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_default_prng_impl` config option.\nSelect the default PRNG implementation, used when one is not explicitly provided at seeding time.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "device_count",
      "signature": "device_count(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "On most platforms, this is the same as :py:func:`jax.local_device_count`.\nHowever, on multi-process platforms where different devices are associated\nwith different processes, this will return the total number of devices across\nall processes.\nArgs:\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the xla backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.\nNumber of devices.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "device_get",
      "signature": "device_get(x: 'Any')",
      "documentation": {
        "description": "Transfer ``x`` to host.\nIf ``x`` is a pytree, then the individual buffers are copied in parallel.\nArgs:\nx: An array, scalar, Array or (nested) standard Python container thereof\nrepresenting the array to be transferred to host.",
        "parameters": {},
        "returns": "An array or (nested) Python container thereof representing the\nvalue of ``x``.",
        "raises": "",
        "see_also": "- device_put\n- device_put_sharded\n- device_put_replicated",
        "notes": "",
        "examples": "Passing a Array:\n>>> import jax\n>>> x = jax.numpy.array([1., 2., 3.])\n>>> jax.device_get(x)\narray([1., 2., 3.], dtype=float32)\nPassing a scalar (has no effect):\n>>> jax.device_get(1)\n1"
      }
    },
    {
      "name": "device_put",
      "signature": "device_put(x, device: 'None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind' = None, *, src: 'None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind' = None, donate: 'bool | Any' = False, may_alias: 'bool | None | Any' = None)",
      "documentation": {
        "description": "Transfers ``x`` to ``device``.\nArgs:\nx: An array, scalar, or (nested) standard Python container thereof.\ndevice: The (optional) :py:class:`Device`, :py:class:`Sharding`, or a\n(nested) :py:class:`Sharding` in standard Python container (must be a tree\nprefix of ``x``), representing the device(s) to which ``x`` should be\ntransferred. If given, then the result is committed to the device(s).\nsrc: The (optional) :py:class:`Device`, :py:class:`Sharding`, or a (nested)\n:py:class:`Sharding` in standard Python container (must be a tree prefix\nof ``x``), representing the device(s) on which ``x`` belongs.\ndonate: bool or a (nested) bool in standard Python container (must be a tree\nprefix of ``x``). If True, ``x`` can be overwritten and marked deleted in\nthe caller. This is best effort. JAX will donate if possible, otherwise it\nwon't. The input buffer (in the future) will always be deleted if donated.\nmay_alias: bool or None or a (nested) bool in standard Python container\n(must be a tree prefix of ``x``). If False, `x` will be copied. If true,\n`x` may be aliased depending on the runtime's implementation.",
        "parameters": {},
        "returns": "A copy of ``x`` that resides on ``device``.\nIf the ``device`` parameter is ``None``, then this operation behaves like the\nidentity function if the operand is on any device already, otherwise it\ntransfers the data to the default device, uncommitted.\nFor more details on data placement see the\n:ref:`FAQ on data placement <faq-data-placement>`.\nThis function is always asynchronous, i.e. returns immediately without\nblocking the calling Python thread until any transfers are completed.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "device_put_replicated",
      "signature": "device_put_replicated(x: 'Any', devices: 'Sequence[xc.Device]')",
      "documentation": {
        "description": "Transfer array(s) to each specified device and form Array(s).\nArgs:\nx: an array, scalar, or (nested) standard Python container thereof\nrepresenting the array to be replicated to form the output.\ndevices: A sequence of :py:class:`Device` instances representing the devices\nto which ``x`` will be transferred.\nThis function is always asynchronous, i.e. returns immediately.",
        "parameters": {},
        "returns": "An Array or (nested) Python container thereof representing the\nvalue of ``x`` broadcasted along a new leading axis of size\n``len(devices)``, with each slice along that new leading axis backed by\nmemory on the device specified by the corresponding entry in ``devices``.",
        "raises": "",
        "see_also": "- device_put\n- device_put_sharded",
        "notes": "",
        "examples": "Passing an array:\n>>> import jax\n>>> devices = jax.local_devices()\n>>> x = jax.numpy.array([1., 2., 3.])\n>>> y = jax.device_put_replicated(x, devices)\n>>> np.allclose(y, jax.numpy.stack([x for _ in devices]))\nTrue"
      }
    },
    {
      "name": "device_put_sharded",
      "signature": "device_put_sharded(shards: 'Sequence[Any]', devices: 'Sequence[xc.Device]')",
      "documentation": {
        "description": "Transfer array shards to specified devices and form Array(s).\nArgs:\nshards: A sequence of arrays, scalars, or (nested) standard Python\ncontainers thereof representing the shards to be stacked together to form\nthe output. The length of ``shards`` must equal the length of ``devices``.\ndevices: A sequence of :py:class:`Device` instances representing the devices\nto which corresponding shards in ``shards`` will be transferred.\nThis function is always asynchronous, i.e. returns immediately.",
        "parameters": {},
        "returns": "A Array or (nested) Python container thereof representing the\nelements of ``shards`` stacked together, with each shard backed by physical\ndevice memory specified by the corresponding entry in ``devices``.",
        "raises": "",
        "see_also": "- device_put\n- device_put_replicated",
        "notes": "",
        "examples": "Passing a list of arrays for ``shards`` results in a sharded array\ncontaining a stacked version of the inputs:\n>>> import jax\n>>> devices = jax.local_devices()\n>>> x = [jax.numpy.ones(5) for device in devices]\n>>> y = jax.device_put_sharded(x, devices)\n>>> np.allclose(y, jax.numpy.stack(x))\nTrue\nPassing a list of nested container objects with arrays at the leaves for\n``shards`` corresponds to stacking the shards at each leaf. This requires\nall entries in the list to have the same tree structure:\n>>> x = [(i, jax.numpy.arange(i, i + 4)) for i in range(len(devices))]\n>>> y = jax.device_put_sharded(x, devices)\n>>> type(y)\n<class 'tuple'>\n>>> y0 = jax.device_put_sharded([a for a, b in x], devices)\n>>> y1 = jax.device_put_sharded([b for a, b in x], devices)\n>>> np.allclose(y[0], y0)\nTrue\n>>> np.allclose(y[1], y1)\nTrue"
      }
    },
    {
      "name": "devices",
      "signature": "devices(backend: 'str | xla_client.Client | None' = None) -> 'list[xla_client.Device]'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": ".. currentmodule:: jaxlib.xla_extension\nEach device is represented by a subclass of :class:`Device` (e.g.\n:class:`CpuDevice`, :class:`GpuDevice`). The length of the returned list is\nequal to ``device_count(backend)``. Local devices can be identified by\ncomparing :attr:`Device.process_index` to the value returned by\n:py:func:`jax.process_index`.\nIf ``backend`` is ``None``, returns all the devices from the default backend.\nThe default backend is generally ``'gpu'`` or ``'tpu'`` if available,\notherwise ``'cpu'``.\nArgs:\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the xla backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.\nList of Device subclasses.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "disable_jit",
      "signature": "disable_jit(disable: 'bool' = True)",
      "documentation": {
        "description": "Context manager that disables :py:func:`jit` behavior under its dynamic context.\nFor debugging it is useful to have a mechanism that disables :py:func:`jit`\neverywhere in a dynamic context. Note that this not only disables explicit\nuses of :func:`jit` by the user, but will also remove any implicit JIT compilation\nused by the JAX library: this includes implicit JIT computation of `body` and\n`cond` functions passed to higher-level primitives like :func:`~jax.lax.scan` and\n:func:`~jax.lax.while_loop`, JIT used in implementations of :mod:`jax.numpy` functions,\nand any other case where :func:`jit` is used within an API's implementation.\nNote however that even under `disable_jit`, individual primitive operations\nwill still be compiled by XLA as in normal eager op-by-op execution.\nValues that have a data dependence on the arguments to a jitted function are\ntraced and abstracted. For example, an abstract value may be a\n:py:class:`ShapedArray` instance, representing the set of all possible arrays\nwith a given shape and dtype, but not representing one concrete array with\nspecific values. You might notice those if you use a benign side-effecting\noperation in a jitted function, like a print:\n>>> import jax\n>>>\n>>> @jax.jit\n... def f(x):\n...   y = x * 2\n...   print(\"Value of y is\", y)\n...   return y + 3\n...\n>>> print(f(jax.numpy.array([1, 2, 3])))  # doctest:+ELLIPSIS\nValue of y is Traced<ShapedArray(int32[3])>with<DynamicJaxprTrace...>\n[5 7 9]\nHere ``y`` has been abstracted by :py:func:`jit` to a :py:class:`ShapedArray`,\nwhich represents an array with a fixed shape and type but an arbitrary value.\nThe value of ``y`` is also traced. If we want to see a concrete value while\ndebugging, and avoid the tracer too, we can use the :py:func:`disable_jit`\ncontext manager:\n>>> import jax\n>>>\n>>> with jax.disable_jit():\n...   print(f(jax.numpy.array([1, 2, 3])))\n...\nValue of y is [2 4 6]\n[5 7 9]",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "effects_barrier",
      "signature": "effects_barrier()",
      "documentation": {
        "description": "Waits until existing functions have completed any side-effects.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_checks",
      "signature": "enable_checks(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_enable_checks` config option.\nTurn on invariant checking for JAX internals. Makes things slower.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_custom_prng",
      "signature": "enable_custom_prng(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_enable_custom_prng` config option (transient).\nEnables an internal upgrade that allows one to define custom pseudo-random number generator implementations.  This will be enabled by default in future versions of JAX, at which point all uses of the flag will be considered deprecated (following the `API compatibility policy <https://jax.readthedocs.io/en/latest/api_compatibility.html>`_).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "enable_custom_vjp_by_custom_transpose",
      "signature": "enable_custom_vjp_by_custom_transpose(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_enable_custom_vjp_by_custom_transpose` config option (transient).\nEnables an internal upgrade that implements `jax.custom_vjp` by reduction to `jax.custom_jvp` and `jax.custom_transpose`.  This will be enabled by default in future versions of JAX, at which point all uses of the flag will be considered deprecated (following the `API compatibility policy <https://jax.readthedocs.io/en/latest/api_compatibility.html>`_).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ensure_compile_time_eval",
      "signature": "ensure_compile_time_eval()",
      "documentation": {
        "description": "Context manager to ensure evaluation at trace/compile time (or error).\nSome JAX APIs like :func:`jax.jit` and :func:`jax.lax.scan` involve staging,\ni.e., delaying the evaluation of numerical expressions (like :mod:`jax.numpy`\nfunction applications) so that instead of performing those computations\neagerly while evaluating the corresponding Python expressions, their\ncomputation is carried out separately, e.g. after optimized compilation. But\nthis delay can be undesirable. For example, numerical values might be needed\nto evaluate Python control flow and so their evaluation cannot be delayed. As\nanother example, it may be beneficial to ensure compile time evaluation (or\n\"constant folding\") for performance reasons.\nThis context manager ensures that JAX computations are evaluated eagerly. If\neager evaluation is not possible, a ``ConcretizationTypeError`` is raised.\nHere's a contrived example::\nimport jax\nimport jax.numpy as jnp\n@jax.jit\ndef f(x):\nwith jax.ensure_compile_time_eval():\ny = jnp.sin(3.0)\nz = jnp.sin(y)\nz_positive = z > 0\nif z_positive:  # z_positive is usable in Python control flow\nreturn jnp.sin(x)\nelse:\nreturn jnp.cos(x)\nHere's a real-world example from https://github.com/jax-ml/jax/issues/3974::\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n@jax.jit\ndef jax_fn(x):\nwith jax.ensure_compile_time_eval():\ny = random.randint(random.key(0), (1000,1000), 0, 100)\ny2 = y @ y\nx2 = jnp.sum(y2) * x\nreturn x2\nA similar behavior can often be achieved simply by 'hoisting' the constant\nexpression out of the corresponding staging API::\ny = random.randint(random.key(0), (1000,1000), 0, 100)\n@jax.jit\ndef jax_fn(x):\ny2 = y @ y\nx2 = jnp.sum(y2)*x\nreturn x2\nBut in some cases it can be more convenient to use this context manager.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "eval_shape",
      "signature": "eval_shape(fun: 'Callable', *args, **kwargs)",
      "documentation": {
        "description": "Compute the shape/dtype of ``fun`` without any FLOPs.\nThis utility function is useful for performing shape inference. Its\ninput/output behavior is defined by::\ndef eval_shape(fun, *args, **kwargs):\nout = fun(*args, **kwargs)\nshape_dtype_struct = lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype)\nreturn jax.tree_util.tree_map(shape_dtype_struct, out)\nBut instead of applying ``fun`` directly, which might be expensive, it uses\nJAX's abstract interpretation machinery to evaluate the shapes without doing\nany FLOPs.\nUsing :py:func:`eval_shape` can also catch shape errors, and will raise same\nshape errors as evaluating ``fun(*args, **kwargs)``.\nArgs:\nfun: The function whose output shape should be evaluated.\n*args: a positional argument tuple of arrays, scalars, or (nested) standard\nPython containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of\nthose types. Since only the ``shape`` and ``dtype`` attributes are\naccessed, one can use :class:`jax.ShapeDtypeStruct` or another container\nthat duck-types as ndarrays (note however that duck-typed objects cannot\nbe namedtuples because those are treated as standard Python containers).\n**kwargs: a keyword argument dict of arrays, scalars, or (nested) standard\nPython containers (pytrees) of those types. As in ``args``, array values\nneed only be duck-typed to have ``shape`` and ``dtype`` attributes.",
        "parameters": {},
        "returns": "out: a nested PyTree containing :class:`jax.ShapeDtypeStruct` objects as leaves.\nFor example:\n>>> import jax\n>>> import jax.numpy as jnp\n>>>\n>>> f = lambda A, x: jnp.tanh(jnp.dot(A, x))\n>>> A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)\n>>> x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)\n>>> out = jax.eval_shape(f, A, x)  # no FLOPs performed\n>>> print(out.shape)\n(2000, 1000)\n>>> print(out.dtype)\nfloat32\nAll arguments passed via :func:`eval_shape` will be treated as dynamic;\nstatic arguments can be included via closure, for example using :func:`functools.partial`:\n>>> import jax\n>>> from jax import lax\n>>> from functools import partial\n>>> import jax.numpy as jnp\n>>>\n>>> x = jax.ShapeDtypeStruct((1, 1, 28, 28), jnp.float32)\n>>> kernel = jax.ShapeDtypeStruct((32, 1, 3, 3), jnp.float32)\n>>>\n>>> conv_same = partial(lax.conv_general_dilated, window_strides=(1, 1), padding=\"SAME\")\n>>> out = jax.eval_shape(conv_same, x, kernel)\n>>> print(out.shape)\n(1, 32, 28, 28)\n>>> print(out.dtype)\nfloat32",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "explain_cache_misses",
      "signature": "explain_cache_misses(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_explain_cache_misses` config option.\nEach time there is a miss on one of the main caches (e.g. the tracing cache), log an explanation.. Logging is performed with `logging`. When this option is set, the log level is WARNING; otherwise the level is DEBUG.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "grad",
      "signature": "grad(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -> 'Callable'",
      "documentation": {
        "description": "Creates a function that evaluates the gradient of ``fun``.\nArgs:\nfun: Function to be differentiated. Its arguments at positions specified by\n``argnums`` should be arrays, scalars, or standard Python containers.\nArgument arrays in the positions specified by ``argnums`` must be of\ninexact (i.e., floating-point or complex) type. It\nshould return a scalar (which includes arrays with shape ``()`` but not\narrays with shape ``(1,)`` etc.)\nargnums: Optional, integer or sequence of integers. Specifies which\npositional argument(s) to differentiate with respect to (default 0).\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.\nholomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\nholomorphic. If True, inputs and outputs must be complex. Default False.\nallow_int: Optional, bool. Whether to allow differentiating with\nrespect to integer valued inputs. The gradient of an integer input will\nhave a trivial vector-space dtype (float0). Default False.",
        "parameters": {},
        "returns": "A function with the same arguments as ``fun``, that evaluates the gradient\nof ``fun``. If ``argnums`` is an integer then the gradient has the same\nshape and type as the positional argument indicated by that integer. If\nargnums is a tuple of integers, the gradient is a tuple of values with the\nsame shapes and types as the corresponding arguments. If ``has_aux`` is True\nthen a pair of (gradient, auxiliary_data) is returned.\nFor example:\n>>> import jax\n>>>\n>>> grad_tanh = jax.grad(jax.numpy.tanh)\n>>> print(grad_tanh(0.2))\n0.961043",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "hessian",
      "signature": "hessian(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False) -> 'Callable'",
      "documentation": {
        "description": "Hessian of ``fun`` as a dense array.\nArgs:\nfun: Function whose Hessian is to be computed.  Its arguments at positions\nspecified by ``argnums`` should be arrays, scalars, or standard Python\ncontainers thereof. It should return arrays, scalars, or standard Python\ncontainers thereof.\nargnums: Optional, integer or sequence of integers. Specifies which\npositional argument(s) to differentiate with respect to (default ``0``).\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.\nholomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\nholomorphic. Default False.",
        "parameters": {},
        "returns": "A function with the same arguments as ``fun``, that evaluates the Hessian of\n``fun``.\n>>> import jax\n>>>\n>>> g = lambda x: x[0]**3 - 2*x[0]*x[1] - x[1]**6\n>>> print(jax.hessian(g)(jax.numpy.array([1., 2.])))\n[[   6.   -2.]\n[  -2. -480.]]\n:py:func:`hessian` is a generalization of the usual definition of the Hessian\nthat supports nested Python containers (i.e. pytrees) as inputs and outputs.\nThe tree structure of ``jax.hessian(fun)(x)`` is given by forming a tree\nproduct of the structure of ``fun(x)`` with a tree product of two copies of\nthe structure of ``x``. A tree product of two tree structures is formed by\nreplacing each leaf of the first tree with a copy of the second. For example:\n>>> import jax.numpy as jnp\n>>> f = lambda dct: {\"c\": jnp.power(dct[\"a\"], dct[\"b\"])}\n>>> print(jax.hessian(f)({\"a\": jnp.arange(2.) + 1., \"b\": jnp.arange(2.) + 2.}))\n{'c': {'a': {'a': Array([[[ 2.,  0.], [ 0.,  0.]],\n[[ 0.,  0.], [ 0., 12.]]], dtype=float32),\n'b': Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],\n[[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32)},\n'b': {'a': Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],\n[[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32),\n'b': Array([[[0.      , 0.      ], [0.      , 0.      ]],\n[[0.      , 0.      ], [0.      , 3.843624]]], dtype=float32)}}}\nThus each leaf in the tree structure of ``jax.hessian(fun)(x)`` corresponds to\na leaf of ``fun(x)`` and a pair of leaves of ``x``. For each leaf in\n``jax.hessian(fun)(x)``, if the corresponding array leaf of ``fun(x)`` has\nshape ``(out_1, out_2, ...)`` and the corresponding array leaves of ``x`` have\nshape ``(in_1_1, in_1_2, ...)`` and ``(in_2_1, in_2_2, ...)`` respectively,\nthen the Hessian leaf has shape ``(out_1, out_2, ..., in_1_1, in_1_2, ...,\nin_2_1, in_2_2, ...)``. In other words, the Python tree structure represents\nthe block structure of the Hessian, with blocks determined by the input and\noutput pytrees.\nIn particular, an array is produced (with no pytrees involved) when the\nfunction input ``x`` and output ``fun(x)`` are each a single array, as in the\n``g`` example above. If ``fun(x)`` has shape ``(out1, out2, ...)`` and ``x``\nhas shape ``(in1, in2, ...)`` then ``jax.hessian(fun)(x)`` has shape\n``(out1, out2, ..., in1, in2, ..., in1, in2, ...)``. To flatten pytrees into\n1D vectors, consider using :py:func:`jax.flatten_util.flatten_pytree`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "host_count",
      "signature": "host_count(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "host_id",
      "signature": "host_id(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "host_ids",
      "signature": "host_ids(backend: 'str | xla_client.Client | None' = None) -> 'list[int]'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jacfwd",
      "signature": "jacfwd(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False) -> 'Callable'",
      "documentation": {
        "description": "Jacobian of ``fun`` evaluated column-by-column using forward-mode AD.\nArgs:\nfun: Function whose Jacobian is to be computed.\nargnums: Optional, integer or sequence of integers. Specifies which\npositional argument(s) to differentiate with respect to (default ``0``).\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.\nholomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\nholomorphic. Default False.",
        "parameters": {},
        "returns": "A function with the same arguments as ``fun``, that evaluates the Jacobian of\n``fun`` using forward-mode automatic differentiation. If ``has_aux`` is True\nthen a pair of (jacobian, auxiliary_data) is returned.\n>>> import jax\n>>> import jax.numpy as jnp\n>>>\n>>> def f(x):\n...   return jnp.asarray(\n...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])\n...\n>>> print(jax.jacfwd(f)(jnp.array([1., 2., 3.])))\n[[ 1.       0.       0.     ]\n[ 0.       0.       5.     ]\n[ 0.      16.      -2.     ]\n[ 1.6209   0.       0.84147]]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jacobian",
      "signature": "jacobian(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False) -> 'Callable'",
      "documentation": {
        "description": "Alias of :func:`jax.jacrev`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jacrev",
      "signature": "jacrev(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False) -> 'Callable'",
      "documentation": {
        "description": "Jacobian of ``fun`` evaluated row-by-row using reverse-mode AD.\nArgs:\nfun: Function whose Jacobian is to be computed.\nargnums: Optional, integer or sequence of integers. Specifies which\npositional argument(s) to differentiate with respect to (default ``0``).\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.\nholomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\nholomorphic. Default False.\nallow_int: Optional, bool. Whether to allow differentiating with\nrespect to integer valued inputs. The gradient of an integer input will\nhave a trivial vector-space dtype (float0). Default False.",
        "parameters": {},
        "returns": "A function with the same arguments as ``fun``, that evaluates the Jacobian of\n``fun`` using reverse-mode automatic differentiation. If ``has_aux`` is True\nthen a pair of (jacobian, auxiliary_data) is returned.\n>>> import jax\n>>> import jax.numpy as jnp\n>>>\n>>> def f(x):\n...   return jnp.asarray(\n...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])\n...\n>>> print(jax.jacrev(f)(jnp.array([1., 2., 3.])))\n[[ 1.       0.       0.     ]\n[ 0.       0.       5.     ]\n[ 0.      16.      -2.     ]\n[ 1.6209   0.       0.84147]]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jax2tf_associative_scan_reductions",
      "signature": "jax2tf_associative_scan_reductions(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax2tf_associative_scan_reductions` config option.\nJAX has two separate lowering rules for the cumulative reduction primitives (cumsum, cumprod, cummax, cummin). On CPUs and GPUs it uses a lax.associative_scan, while for TPUs it uses the HLO ReduceWindow. The latter has a slow implementation on CPUs and GPUs. By default, jax2tf uses the TPU lowering. Set this flag to True to use the associative scan lowering usage, and only if it makes a difference for your application. See the jax2tf README.md for more details.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "jit",
      "signature": "jit(fun: 'Callable', in_shardings: 'Any' = UnspecifiedValue, out_shardings: 'Any' = UnspecifiedValue, static_argnums: 'int | Sequence[int] | None' = None, static_argnames: 'str | Iterable[str] | None' = None, donate_argnums: 'int | Sequence[int] | None' = None, donate_argnames: 'str | Iterable[str] | None' = None, keep_unused: 'bool' = False, device: 'xc.Device | None' = None, backend: 'str | None' = None, inline: 'bool' = False, abstracted_axes: 'Any | None' = None, compiler_options: 'dict[str, Any] | None' = None) -> 'pjit.JitWrapped'",
      "documentation": {
        "description": "Sets up ``fun`` for just-in-time compilation with XLA.\nArgs:\nfun: Function to be jitted. ``fun`` should be a pure function.\nThe arguments and return value of ``fun`` should be arrays, scalar, or\n(nested) standard Python containers (tuple/list/dict) thereof. Positional\narguments indicated by ``static_argnums`` can be any hashable type. Static\narguments are included as part of a compilation cache key, which is why\nhash and equality operators must be defined. JAX keeps a weak reference to\n``fun`` for use as a compilation cache key, so the object ``fun`` must be\nweakly-referenceable.\nin_shardings: optional, a :py:class:`Sharding` or pytree with\n:py:class:`Sharding` leaves and structure that is a tree prefix of the\npositional arguments tuple to ``fun``. If provided, the positional\narguments passed to ``fun`` must have shardings that are compatible with\n``in_shardings`` or an error is raised, and the compiled computation has\ninput shardings corresponding to ``in_shardings``. If not provided, the\ncompiled computation's input shardings are inferred from argument\nshardings.\nout_shardings: optional, a :py:class:`Sharding` or pytree with\n:py:class:`Sharding` leaves and structure that is a tree prefix of the\noutput of ``fun``. If provided, it has the same effect as applying\ncorresponding :py:func:`jax.lax.with_sharding_constraint`s to the output\nof ``fun``.\nstatic_argnums: optional, an int or collection of ints that specify which\npositional arguments to treat as static (trace- and compile-time\nconstant).\nStatic arguments should be hashable, meaning both ``__hash__`` and\n``__eq__`` are implemented, and immutable. Otherwise they can be arbitrary\nPython objects. Calling the jitted function with different values for\nthese constants will trigger recompilation. Arguments that are not\narray-like or containers thereof must be marked as static.\nIf neither ``static_argnums`` nor ``static_argnames`` is provided, no\narguments are treated as static. If ``static_argnums`` is not provided but\n``static_argnames`` is, or vice versa, JAX uses\n:code:`inspect.signature(fun)` to find any positional arguments that\ncorrespond to ``static_argnames``\n(or vice versa). If both ``static_argnums`` and ``static_argnames`` are\nprovided, ``inspect.signature`` is not used, and only actual\nparameters listed in either ``static_argnums`` or ``static_argnames`` will\nbe treated as static.\nstatic_argnames: optional, a string or collection of strings specifying\nwhich named arguments to treat as static (compile-time constant). See the\ncomment on ``static_argnums`` for details. If not\nprovided but ``static_argnums`` is set, the default is based on calling\n``inspect.signature(fun)`` to find corresponding named arguments.\ndonate_argnums: optional, collection of integers to specify which positional\nargument buffers can be overwritten by the computation and marked deleted\nin the caller. It is safe to donate argument buffers if you no longer need\nthem once the computation has started. In some cases XLA can make use of\ndonated buffers to reduce the amount of memory needed to perform a\ncomputation, for example recycling one of your input buffers to store a\nresult. You should not reuse buffers that you donate to a computation; JAX\nwill raise an error if you try to. By default, no argument buffers are\ndonated.\nIf neither ``donate_argnums`` nor ``donate_argnames`` is provided, no\narguments are donated. If ``donate_argnums`` is not provided but\n``donate_argnames`` is, or vice versa, JAX uses\n:code:`inspect.signature(fun)` to find any positional arguments that\ncorrespond to ``donate_argnames``\n(or vice versa). If both ``donate_argnums`` and ``donate_argnames`` are\nprovided, ``inspect.signature`` is not used, and only actual\nparameters listed in either ``donate_argnums`` or ``donate_argnames`` will\nbe donated.\nFor more details on buffer donation see the\n`FAQ <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>`_.\ndonate_argnames: optional, a string or collection of strings specifying\nwhich named arguments are donated to the computation. See the\ncomment on ``donate_argnums`` for details. If not\nprovided but ``donate_argnums`` is set, the default is based on calling\n``inspect.signature(fun)`` to find corresponding named arguments.\nkeep_unused: optional boolean. If `False` (the default), arguments that JAX\ndetermines to be unused by `fun` *may* be dropped from resulting compiled\nXLA executables. Such arguments will not be transferred to the device nor\nprovided to the underlying executable. If `True`, unused arguments will\nnot be pruned.\ndevice: This is an experimental feature and the API is likely to change.\nOptional, the Device the jitted function will run on. (Available devices\ncan be retrieved via :py:func:`jax.devices`.) The default is inherited\nfrom XLA's DeviceAssignment logic and is usually to use\n``jax.devices()[0]``.\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the XLA backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.\ninline: Optional boolean. Specify whether this function should be inlined\ninto enclosing jaxprs. Default False.",
        "parameters": {},
        "returns": "A wrapped version of ``fun``, set up for just-in-time compilation.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "In the following example, ``selu`` can be compiled into a single fused kernel\nby XLA:\n>>> import jax\n>>>\n>>> @jax.jit\n... def selu(x, alpha=1.67, lmbda=1.05):\n...   return lmbda * jax.numpy.where(x > 0, x, alpha * jax.numpy.exp(x) - alpha)\n>>>\n>>> key = jax.random.key(0)\n>>> x = jax.random.normal(key, (10,))\n>>> print(selu(x))  # doctest: +SKIP\n[-0.54485  0.27744 -0.29255 -0.91421 -0.62452 -0.24748\n-0.85743 -0.78232  0.76827  0.59566 ]\nTo pass arguments such as ``static_argnames`` when decorating a function, a\ncommon pattern is to use :func:`functools.partial`:\n>>> from functools import partial\n>>>\n>>> @partial(jax.jit, static_argnames=['n'])\n... def g(x, n):\n...   for i in range(n):\n...     x = x ** 2\n...   return x\n>>>\n>>> g(jnp.arange(4), 3)\nArray([   0,    1,  256, 6561], dtype=int32)"
      }
    },
    {
      "name": "jvp",
      "signature": "jvp(fun: 'Callable', primals, tangents, has_aux: 'bool' = False) -> 'tuple[Any, ...]'",
      "documentation": {
        "description": "Computes a (forward-mode) Jacobian-vector product of ``fun``.\nArgs:\nfun: Function to be differentiated. Its arguments should be arrays, scalars,\nor standard Python containers of arrays or scalars. It should return an\narray, scalar, or standard Python container of arrays or scalars.\nprimals: The primal values at which the Jacobian of ``fun`` should be\nevaluated. Should be either a tuple or a list of arguments,\nand its length should be equal to the number of positional parameters of\n``fun``.\ntangents: The tangent vector for which the Jacobian-vector product should be\nevaluated. Should be either a tuple or a list of tangents, with the same\ntree structure and array shapes as ``primals``.\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.",
        "parameters": {},
        "returns": "If ``has_aux`` is ``False``, returns a ``(primals_out, tangents_out)`` pair,\nwhere ``primals_out`` is ``fun(*primals)``,\nand ``tangents_out`` is the Jacobian-vector product of\n``function`` evaluated at ``primals`` with ``tangents``. The\n``tangents_out`` value has the same Python tree structure and shapes as\n``primals_out``. If ``has_aux`` is ``True``, returns a\n``(primals_out, tangents_out, aux)`` tuple where ``aux``\nis the auxiliary data returned by ``fun``.\nFor example:\n>>> import jax\n>>>\n>>> primals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,))\n>>> print(primals)\n0.09983342\n>>> print(tangents)\n0.19900084",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "legacy_prng_key",
      "signature": "legacy_prng_key(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_legacy_prng_key` config option.\nSpecify the behavior when raw PRNG keys are passed to jax.random APIs.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "linear_transpose",
      "signature": "linear_transpose(fun: 'Callable', *primals, reduce_axes=()) -> 'Callable'",
      "documentation": {
        "description": "Transpose a function that is promised to be linear.\nFor linear functions, this transformation is equivalent to :py:func:`vjp`, but\navoids the overhead of computing the forward pass.\nThe outputs of the transposed function will always have the exact same dtypes\nas ``primals``, even if some values are truncated (e.g., from complex to\nfloat, or from float64 to float32). To avoid truncation, use dtypes in\n``primals`` that match the full range of desired outputs from the transposed\nfunction. Integer dtypes are not supported.\nArgs:\nfun: the linear function to be transposed.\n*primals: a positional argument tuple of arrays, scalars, or (nested)\nstandard Python containers (tuples, lists, dicts, namedtuples, i.e.,\npytrees) of those types used for evaluating the shape/dtype of\n``fun(*primals)``. These arguments may be real scalars/ndarrays, but that\nis not required: only the ``shape`` and ``dtype`` attributes are accessed.\nSee below for an example. (Note that the duck-typed objects cannot be\nnamedtuples because those are treated as standard Python containers.)",
        "parameters": {},
        "returns": "A callable that calculates the transpose of ``fun``. Valid input into this\nfunction must have the same shape/dtypes/structure as the result of\n``fun(*primals)``. Output will be a tuple, with the same\nshape/dtypes/structure as ``primals``.\n>>> import jax\n>>>\n>>> f = lambda x, y: 0.5 * x - 0.5 * y\n>>> scalar = jax.ShapeDtypeStruct(shape=(), dtype=np.dtype(np.float32))\n>>> f_transpose = jax.linear_transpose(f, scalar, scalar)\n>>> f_transpose(1.0)\n(Array(0.5, dtype=float32), Array(-0.5, dtype=float32))",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "linearize",
      "signature": "linearize(fun: 'Callable', *primals, has_aux: 'bool' = False) -> 'tuple[Any, Callable] | tuple[Any, Callable, Any]'",
      "documentation": {
        "description": "Produces a linear approximation to ``fun`` using :py:func:`jvp` and partial eval.\nArgs:\nfun: Function to be differentiated. Its arguments should be arrays, scalars,\nor standard Python containers of arrays or scalars. It should return an\narray, scalar, or standard python container of arrays or scalars.\nprimals: The primal values at which the Jacobian of ``fun`` should be\nevaluated. Should be a tuple of arrays, scalar, or standard Python\ncontainer thereof. The length of the tuple is equal to the number of\npositional parameters of ``fun``.\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the first\nelement is considered the output of the mathematical function to be linearized,\nand the second is auxiliary data. Default False.",
        "parameters": {},
        "returns": "If ``has_aux`` is ``False``, returns a pair where the first element is the value of\n``f(*primals)`` and the second element is a function that evaluates the\n(forward-mode) Jacobian-vector product of ``fun`` evaluated at ``primals`` without\nre-doing the linearization work. If ``has_aux`` is ``True``, returns a\n``(primals_out, lin_fn, aux)`` tuple where ``aux`` is the auxiliary data returned by\n``fun``.\nIn terms of values computed, :py:func:`linearize` behaves much like a curried\n:py:func:`jvp`, where these two code blocks compute the same values::\ny, out_tangent = jax.jvp(f, (x,), (in_tangent,))\ny, f_jvp = jax.linearize(f, x)\nout_tangent = f_jvp(in_tangent)\nHowever, the difference is that :py:func:`linearize` uses partial evaluation\nso that the function ``f`` is not re-linearized on calls to ``f_jvp``. In\ngeneral that means the memory usage scales with the size of the computation,\nmuch like in reverse-mode. (Indeed, :py:func:`linearize` has a similar\nsignature to :py:func:`vjp`!)\nThis function is mainly useful if you want to apply ``f_jvp`` multiple times,\ni.e. to evaluate a pushforward for many different input tangent vectors at the\nsame linearization point. Moreover if all the input tangent vectors are known\nat once, it can be more efficient to vectorize using :py:func:`vmap`, as in::\npushfwd = partial(jvp, f, (x,))\ny, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,))\nBy using :py:func:`vmap` and :py:func:`jvp` together like this we avoid the stored-linearization\nmemory cost that scales with the depth of the computation, which is incurred\nby both :py:func:`linearize` and :py:func:`vjp`.\nHere's a more complete example of using :py:func:`linearize`:\n>>> import jax\n>>> import jax.numpy as jnp\n>>>\n>>> def f(x): return 3. * jnp.sin(x) + jnp.cos(x / 2.)\n...\n>>> jax.jvp(f, (2.,), (3.,))\n(Array(3.2681944, dtype=float32, weak_type=True), Array(-5.007528, dtype=float32, weak_type=True))\n>>> y, f_jvp = jax.linearize(f, 2.)\n>>> print(y)\n3.2681944\n>>> print(f_jvp(3.))\n-5.007528\n>>> print(f_jvp(4.))\n-6.676704",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "live_arrays",
      "signature": "live_arrays(platform=None)",
      "documentation": {
        "description": "Return all live arrays in the backend for `platform`.\nIf platform is None, it is the default backend.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "local_device_count",
      "signature": "local_device_count(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "local_devices",
      "signature": "local_devices(process_index: 'int | None' = None, backend: 'str | xla_client.Client | None' = None, host_id: 'int | None' = None) -> 'list[xla_client.Device]'",
      "documentation": {
        "description": "Like :py:func:`jax.devices`, but only returns devices local to a given process.\nIf ``process_index`` is ``None``, returns devices local to this process.\nArgs:\nprocess_index: the integer index of the process. Process indices can be\nretrieved via ``len(jax.process_count())``.\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the xla backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.",
        "parameters": {},
        "returns": "List of Device subclasses.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "log_compiles",
      "signature": "log_compiles(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_log_compiles` config option.\nLog a message each time `jit` or `pmap` compiles an XLA computation. Logging is performed with `logging`. When this option is set, the log level is WARNING; otherwise the level is DEBUG.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "make_array_from_callback",
      "signature": "make_array_from_callback(shape: 'Shape', sharding: 'Sharding | Layout', data_callback: 'Callable[[Index | None], ArrayLike]', dtype: 'DTypeLike | None' = None) -> 'ArrayImpl'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "``data_callback`` is used to fetch the data for each addressable shard of the\nreturned ``jax.Array``. This function must return concrete arrays, meaning that\n``make_array_from_callback`` has limited compatibility with JAX transformations\nlike :func:`jit` or :func:`vmap`.\nArgs:\nshape : Shape of the ``jax.Array``.\nsharding: A ``Sharding`` instance which describes how the ``jax.Array`` is\nlaid out across devices.\ndata_callback : Callback that takes indices into the global array value as\ninput and returns the corresponding data of the global array value.\nThe data can be returned as any array-like object, e.g. a ``numpy.ndarray``.\ndtype: The dtype of the output ``jax.Array``. If not provided, the dtype of\nthe data for the first addressable shard is used. If there are no\naddressable shards, the ``dtype`` argument must be provided.\nA ``jax.Array`` via data fetched from ``data_callback``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import math\n>>> from jax.sharding import Mesh\n>>> from jax.sharding import PartitionSpec as P\n>>> import numpy as np\n...\n>>> input_shape = (8, 8)\n>>> global_input_data = np.arange(math.prod(input_shape)).reshape(input_shape)\n>>> global_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\n>>> inp_sharding = jax.sharding.NamedSharding(global_mesh, P('x', 'y'))\n...\n>>> def cb(index):\n...  return global_input_data[index]\n...\n>>> arr = jax.make_array_from_callback(input_shape, inp_sharding, cb)\n>>> arr.addressable_data(0).shape\n(4, 2)"
      }
    },
    {
      "name": "make_array_from_process_local_data",
      "signature": "make_array_from_process_local_data(sharding: 'Sharding', local_data: 'np.ndarray', global_shape: 'Shape | None' = None) -> 'ArrayImpl'",
      "documentation": {
        "description": "Creates distributed tensor using the data available in process.\nThis function is a common special case of `make_array_from_callback`. It\nassumes that the data is available in the process and takes care of the\nindex wrangling.\nThe most common case is when the sharding is sharded across the batch\ndimension and each host just loads its corresponding sub-batch. This function\nsupports more general cases as well, such as mixed multi-host and multi-axis\nreplication and sharding but you would need to compute the size and the\ncontents of process-local data correctly to satisfy the sharding constraints.\nIn particular, if any two hosts are replicas, host_local_data should be\nidentical as well.\nThe global_shape is optional. If not provided it will be be inferred from\nthe local_data and sharding, under the assumption that\neach host represents only their own data for uniform sharding. If sharding\nis non-uniform, (see note below) an exception will be raised.\nSetting global_shape explicitly allows for finer grain control and works with\nnon-uniform shardings. Each dimension of global_shape must either match\nhost_local_data, or match the inferred global shape of the sharding (in which\ncase it is equivalent to setting it to None, but is more explicit).\nFor example if dimension `i` is fully sharded then this size would be\n`per_device_shape[i] * jax.local_device_count()`.  Each device will be mapped\ninto local slice of `local_data` array. For example, if given process\naddresses slices (8, 12) and  (24, 28), then these slices will be mapped\ninto (0, 4) and (4, 8) of the `local_data`.\nFor each dimension where global_shapes matches local_shape, each device\nwill lookup the slice in the local_data. For example if\nglobal_shape == local_data.shape, the local data is assumed to be the\nactual target array that will be sharded into device.\nIf global_shape is the same as local_data.shape, then the data must\nbe the same across all hosts.",
        "parameters": {},
        "returns": "Tensor that will have sharding=sharding and of shape global_shape.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from jax.sharding import PartitionSpec as P\n>>> mesh_rows = 2\n>>> mesh_cols =  jax.device_count() // 2\n...\n>>> mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(mesh_rows, mesh_cols), ('x', 'y'))\n>>> sharding = jax.sharding.NamedSharding(mesh, P(('x', 'y'),))\n>>> rows_per_device = 2\n>>> feature_length = 32\n>>> per_device_shape = (rows_per_device, feature_length)\n>>> per_host_shape = (rows_per_device * len(mesh.local_devices), feature_length)\n>>> per_host_generator = lambda : np.arange(np.prod(per_host_shape)).reshape(per_host_shape)\n>>> per_host_data = per_host_generator()  # replace with your own per-host data pipeline that outputs numpy arrays\n>>> global_shape = (rows_per_device * len(sharding.device_set), ) + per_device_shape[1:]\n>>> output_global_array = jax.make_array_from_process_local_data(sharding, per_host_data, global_shape)\n...\n>>> assert output_global_array.addressable_data(0).shape == per_device_shape\n>>> assert output_global_array.shape == global_shape\nNB: While most shardings are uniform, It is possible to design am exotic\nsharding mesh where each process's  devices will be arranged in a non-grid\nlike pattern in some dimensions, or for indices to overlap non-trivially.\nSuch sharding is called \"non-uniform\" in those dimensions. In that case,\nthe global shape along those directions must match local shape as there is\nno meaningful way to represent all needed\nper-process data in non-overlapping fashion. For example for global_shape 4x4\nif sharding looks like this:\n0123\n2103\n4675\n4567\nwith 4 processes, containing devices (0,1), (2, 3), (4, 5), (6, 7) respectively.\nThen the data for each host look like\nxx..    ..xx     ....    ....\n.xx.    x..x     ....    ....\n....    ....     x..x    .xx.\n....    ....     xx..    ..xx\nthe sharding is uniform on rows (each host requires either rows 1-2, or rows 3-4)\nand non-uniform on columns (hosts require overlapping but not matching\nset of columns). Thus local data must have the shape 2x4 or 4x4\nfor all hosts, even though each  host can potentially fit into 2x2 shape.\nIn this case user must provide global_shape explicitly and for\nlocal_shape=(2, 4), potentially valid global shapes are (2, 4) and (4, 4).\nOn the other hand for sharding:\n0213   x.x.  .x.x.  ....  ....\n0213   x.x.  .x.x.  ....  ....\n4657   ....  ....   .x.x  x.x.\n4657   ....  ....   .x.x  x.x.\nfor local_shape=(2, 2) this function can accept a choice of 2x2, 2x4, 4x2\nand 4x4 global shapes. Setting global_shape to None, is equivalent to\nsetting it to (4, 4) in this case.\nArgs:\nsharding: Sharding of the global array.\nlocal_data: Data on the host to be placed on local devices. Each\ndimension should either match global_shape, or match\nnum_addressable_indices(dim).\nglobal_shape: The target shape of the global array. If None,\nwill infer from local_data and sharding."
      }
    },
    {
      "name": "make_array_from_single_device_arrays",
      "signature": "make_array_from_single_device_arrays(shape: 'Shape', sharding: 'Sharding', arrays: 'Sequence[basearray.Array]', *, dtype: 'DTypeLike | None' = None) -> 'ArrayImpl'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "Every device in input ``sharding``\\'s mesh must have an array in ``arrays``\\s.\nArgs:\nshape : Shape of the output ``jax.Array``. This conveys information already included with\n``sharding`` and ``arrays`` and serves as a double check.\nsharding: Sharding: A global Sharding instance which describes how the output jax.Array is laid out across devices.\narrays: Sequence of ``jax.Array``\\s that are each single device addressable. ``len(arrays)``\nmust equal ``len(sharding.addressable_devices)`` and the shape of each array must be the same. For multiprocess code,\neach process will call with a different ``arrays`` argument that corresponds to that processes' data.\nThese arrays are commonly created via ``jax.device_put``.\ndtype: The dtype of the output ``jax.Array``. If not provided, the dtype of the first array in\n``arrays`` is used. If ``arrays`` is empty, the ``dtype`` argument must be provided.\nA global ``jax.Array``, sharded as ``sharding``, with shape equal to ``shape``, and with per-device\ncontents matching ``arrays``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import math\n>>> from jax.sharding import Mesh\n>>> from jax.sharding import PartitionSpec as P\n>>> import numpy as np\n...\n>>> mesh_rows = 2\n>>> mesh_cols =  jax.device_count() // 2\n...\n>>> global_shape = (8, 8)\n>>> mesh = Mesh(np.array(jax.devices()).reshape(mesh_rows, mesh_cols), ('x', 'y'))\n>>> sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n>>> inp_data = np.arange(math.prod(global_shape)).reshape(global_shape)\n...\n>>> arrays = [\n...    jax.device_put(inp_data[index], d)\n...        for d, index in sharding.addressable_devices_indices_map(global_shape).items()]\n...\n>>> arr = jax.make_array_from_single_device_arrays(global_shape, sharding, arrays)\n>>> assert arr.shape == (8,8) # arr.shape is (8,8) regardless of jax.device_count()\nFor cases where you have a local array and want to convert it to a global\njax.Array, use ``jax.make_array_from_process_local_data``."
      }
    },
    {
      "name": "make_jaxpr",
      "signature": "make_jaxpr(fun: 'Callable', static_argnums: 'int | Iterable[int]' = (), axis_env: 'Sequence[tuple[AxisName, int]] | None' = None, return_shape: 'bool' = False, abstracted_axes: 'Any | None' = None) -> 'Callable[..., core.ClosedJaxpr | tuple[core.ClosedJaxpr, Any]]'",
      "documentation": {
        "description": "Creates a function that produces its jaxpr given example args.\nArgs:\nfun: The function whose ``jaxpr`` is to be computed. Its positional\narguments and return value should be arrays, scalars, or standard Python\ncontainers (tuple/list/dict) thereof.\nstatic_argnums: See the :py:func:`jax.jit` docstring.\naxis_env: Optional, a sequence of pairs where the first element is an axis\nname and the second element is a positive integer representing the size of\nthe mapped axis with that name. This parameter is useful when lowering\nfunctions that involve parallel communication collectives, and it\nspecifies the axis name/size environment that would be set up by\napplications of :py:func:`jax.pmap`.\nreturn_shape: Optional boolean, defaults to ``False``. If ``True``, the\nwrapped function returns a pair where the first element is the\n``ClosedJaxpr`` representation of ``fun`` and the second element is a\npytree with the same structure as the output of ``fun`` and where the\nleaves are objects with ``shape`` and ``dtype`` attributes representing\nthe corresponding types of the output leaves.",
        "parameters": {},
        "returns": "A wrapped version of ``fun`` that when applied to example arguments returns\na ``ClosedJaxpr`` representation of ``fun`` on those arguments. If the\nargument ``return_shape`` is ``True``, then the returned function instead\nreturns a pair where the first element is the ``ClosedJaxpr``\nrepresentation of ``fun`` and the second element is a pytree representing\nthe structure, shape, dtypes, and named shapes of the output of ``fun``.\nA ``jaxpr`` is JAX's intermediate representation for program traces. The\n``jaxpr`` language is based on the simply-typed first-order lambda calculus\nwith let-bindings. :py:func:`make_jaxpr` adapts a function to return its\n``jaxpr``, which we can inspect to understand what JAX is doing internally.\nThe ``jaxpr`` returned is a trace of ``fun`` abstracted to\n:py:class:`ShapedArray` level. Other levels of abstraction exist internally.\nWe do not describe the semantics of the ``jaxpr`` language in detail here, but\ninstead give a few examples.\n>>> import jax\n>>>\n>>> def f(x): return jax.numpy.sin(jax.numpy.cos(x))\n>>> print(f(3.0))\n-0.83602\n>>> jax.make_jaxpr(f)(3.0)\n{ lambda ; a:f32[]. let b:f32[] = cos a; c:f32[] = sin b in (c,) }\n>>> jax.make_jaxpr(jax.grad(f))(3.0)\n{ lambda ; a:f32[]. let\nb:f32[] = cos a\nc:f32[] = sin a\n_:f32[] = sin b\nd:f32[] = cos b\ne:f32[] = mul 1.0 d\nf:f32[] = neg e\ng:f32[] = mul f c\nin (g,) }",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "make_mesh",
      "signature": "make_mesh(axis_shapes: 'Sequence[int]', axis_names: 'Sequence[str]', *, devices: 'Sequence[xc.Device] | None' = None, axis_types: 'tuple[mesh_lib.AxisType, ...] | None' = None) -> 'mesh_lib.Mesh'",
      "documentation": {
        "description": "Creates an efficient mesh with the shape and axis names specified.\nThis function attempts to automatically compute a good mapping from a set of\nlogical axes to a physical mesh. For example, on a TPU v3 with 8 devices:\n>>> mesh = jax.make_mesh((8,), ('x'))  # doctest: +SKIP\n>>> [d.id for d in mesh.devices.flat]  # doctest: +SKIP\n[0, 1, 2, 3, 6, 7, 4, 5]\nThe above ordering takes into account the physical topology of TPU v3.\nIt orders the devices into a ring, which yields efficient all-reduces on a\nTPU v3.\nNow, let's see another example with 16 devices of TPU v3:\n>>> mesh = jax.make_mesh((2, 8), ('x', 'y'))  # doctest: +SKIP\n>>> [d.id for d in mesh.devices.flat]  # doctest: +SKIP\n[0, 1, 2, 3, 6, 7, 4, 5, 8, 9, 10, 11, 14, 15, 12, 13]\n>>> mesh = jax.make_mesh((4, 4), ('x', 'y'))  # doctest: +SKIP\n>>> [d.id for d in mesh.devices.flat]  # doctest: +SKIP\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nAs you can see, logical axes (`axis_shapes`) affect the ordering of the\ndevices.\nYou can use `jax.experimental.mesh_utils.create_device_mesh` if you want to\nuse the extra arguments it provides like `contiguous_submeshes` and\n`allow_split_physical_axes`.\nArgs:\naxis_shapes: Shape of the mesh. For example, axis_shape=(4, 2)\naxis_names: Names of the mesh axes. For example, axis_names=('x', 'y')\ndevices: Optional keyword only argument, that allows you to specify the\ndevices you want to create a mesh with.",
        "parameters": {},
        "returns": "A `jax.sharding.Mesh` object.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "named_call",
      "signature": "named_call(fun: 'F', *, name: 'str | None' = None) -> 'F'",
      "documentation": {
        "description": "Adds a user specified name to a function when staging out JAX computations.\nWhen staging out computations for just-in-time compilation to XLA (or other\nbackends such as TensorFlow) JAX runs your Python program but by default does\nnot preserve any of the function names or other metadata associated with it.\nThis can make debugging the staged out (and/or compiled) representation of\nyour program complicated because there is limited context information for each\noperation being executed.\n`named_call` tells JAX to stage the given function out as a subcomputation\nwith a specific name. When the staged out program is compiled with XLA these\nnamed subcomputations are preserved and show up in debugging utilities like\nthe TensorFlow Profiler in TensorBoard. Names are also preserved when staging\nout JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.\nArgs:\nfun: Function to be wrapped. This can be any Callable.\nname: Optional. The prefix to use to name all sub computations created\nwithin the name scope. Use the fun.__name__ if not specified.",
        "parameters": {},
        "returns": "A version of `fun` that is wrapped in a name_scope.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "named_scope",
      "signature": "named_scope(name: 'str') -> 'source_info_util.ExtendNameStackContextManager'",
      "documentation": {
        "description": "A context manager that adds a user specified name to the JAX name stack.\nWhen staging out computations for just-in-time compilation to XLA (or other\nbackends such as TensorFlow) JAX does not, by default, preserve the names\n(or other source metadata) of Python functions it encounters.\nThis can make debugging the staged out (and/or compiled) representation of\nyour program complicated because there is limited context information for each\noperation being executed.\n``named_scope`` tells JAX to stage the given function with additional\nannotations on the underlying operations. JAX internally keeps track of these\nannotations in a name stack. When the staged out program is compiled with XLA\nthese annotations are preserved and show up in debugging utilities like the\nTensorFlow Profiler in TensorBoard. Names are also preserved when staging out\nJAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.\nArgs:\nname: The prefix to use to name all operations created within the name\nscope.\nYields:\nYields ``None``, but enters a context in which `name` will be appended to\nthe active name stack.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "``named_scope`` can be used as a context manager inside compiled functions:\n>>> import jax\n>>>\n>>> @jax.jit\n... def layer(w, x):\n...   with jax.named_scope(\"dot_product\"):\n...     logits = w.dot(x)\n...   with jax.named_scope(\"activation\"):\n...     return jax.nn.relu(logits)\nIt can also be used as a decorator:\n>>> @jax.jit\n... @jax.named_scope(\"layer\")\n... def layer(w, x):\n...   logits = w.dot(x)\n...   return jax.nn.relu(logits)"
      }
    },
    {
      "name": "no_tracing",
      "signature": "no_tracing(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_no_tracing` config option.\nDisallow tracing for JIT compilation.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "numpy_dtype_promotion",
      "signature": "numpy_dtype_promotion(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_numpy_dtype_promotion` config option.\nSpecify the rules used for implicit type promotion in operations between arrays. Options are \"standard\" or \"strict\"; in strict-mode, binary operations between arrays of differing strongly-specified dtypes will result in an error.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "numpy_rank_promotion",
      "signature": "numpy_rank_promotion(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_numpy_rank_promotion` config option.\nControl NumPy-style automatic rank promotion broadcasting (\"allow\", \"warn\", or \"raise\").",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pmap",
      "signature": "pmap(fun: 'Callable', axis_name: 'AxisName | None' = None, *, in_axes: 'int | None | Sequence[Any]' = 0, out_axes: 'Any' = 0, static_broadcasted_argnums: 'int | Iterable[int]' = (), devices: 'Sequence[xc.Device] | None' = None, backend: 'str | None' = None, axis_size: 'int | None' = None, donate_argnums: 'int | Iterable[int]' = (), global_arg_shapes: 'tuple[tuple[int, ...], ...] | None' = None) -> 'Any'",
      "documentation": {
        "description": "Parallel map with support for collective operations.\nThe purpose of :py:func:`pmap` is to express single-program multiple-data\n(SPMD) programs. Applying :py:func:`pmap` to a function will compile the\nfunction with XLA (similarly to :py:func:`jit`), then execute it in parallel\non XLA devices, such as multiple GPUs or multiple TPU cores. Semantically it\nis comparable to :py:func:`vmap` because both transformations map a function\nover array axes, but where :py:func:`vmap` vectorizes functions by pushing the\nmapped axis down into primitive operations, :py:func:`pmap` instead replicates\nthe function and executes each replica on its own XLA device in parallel.\nThe mapped axis size must be less than or equal to the number of local XLA\ndevices available, as returned by :py:func:`jax.local_device_count()` (unless\n``devices`` is specified, see below). For nested :py:func:`pmap` calls, the\nproduct of the mapped axis sizes must be less than or equal to the number of\nXLA devices.\n.. note::\n:py:func:`pmap` compiles ``fun``, so while it can be combined with\n:py:func:`jit`, it's usually unnecessary.\n:py:func:`pmap` requires that all of the participating devices are identical.\nFor example, it is not possible to use :py:func:`pmap` to parallelize a\ncomputation across two different models of GPU. It is currently an error for\nthe same device to participate twice in the same `pmap`.\n**Multi-process platforms:** On multi-process platforms such as TPU pods,\n:py:func:`pmap` is designed to be used in SPMD Python programs, where every\nprocess is running the same Python code such that all processes run the same\npmapped function in the same order. Each process should still call the pmapped\nfunction with mapped axis size equal to the number of *local* devices (unless\n``devices`` is specified, see below), and an array of the same leading axis\nsize will be returned as usual. However, any collective operations in ``fun``\nwill be computed over *all* participating devices, including those on other\nprocesses, via device-to-device communication.  Conceptually, this can be\nthought of as running a pmap over a single array sharded across processes,\nwhere each process \"sees\" only its local shard of the input and output. The\nSPMD model requires that the same multi-process pmaps must be run in the same\norder on all devices, but they can be interspersed with arbitrary operations\nrunning in a single process.\nArgs:\nfun: Function to be mapped over argument axes. Its arguments and return\nvalue should be arrays, scalars, or (nested) standard Python containers\n(tuple/list/dict) thereof. Positional arguments indicated by\n``static_broadcasted_argnums`` can be anything at all, provided they are\nhashable and have an equality operation defined.\naxis_name: Optional, a hashable Python object used to identify the mapped\naxis so that parallel collectives can be applied.\nin_axes: A non-negative integer, None, or nested Python container thereof\nthat specifies which axes of positional arguments to map over. Arguments\npassed as keywords are always mapped over their leading axis (i.e. axis\nindex 0). See :py:func:`vmap` for details.\nout_axes: A non-negative integer, None, or nested Python container thereof\nindicating where the mapped axis should appear in the output. All outputs\nwith a mapped axis must have a non-None ``out_axes`` specification\n(see :py:func:`vmap`).\nstatic_broadcasted_argnums: An int or collection of ints specifying which\npositional arguments to treat as static (compile-time constant).\nOperations that only depend on static arguments will be constant-folded.\nCalling the pmapped function with different values for these constants\nwill trigger recompilation. If the pmapped function is called with fewer\npositional arguments than indicated by ``static_broadcasted_argnums`` then\nan error is raised. Each of the static arguments will be broadcasted to\nall devices. Arguments that are not arrays or containers thereof must be\nmarked as static. Defaults to ().\nStatic arguments must be hashable, meaning both ``__hash__`` and\n``__eq__`` are implemented, and should be immutable.\ndevices: This is an experimental feature and the API is likely to change.\nOptional, a sequence of Devices to map over. (Available devices can be\nretrieved via jax.devices()). Must be given identically for each process\nin multi-process settings (and will therefore include devices across\nprocesses). If specified, the size of the mapped axis must be equal to\nthe number of devices in the sequence local to the given process. Nested\n:py:func:`pmap` s with ``devices`` specified in either the inner or outer\n:py:func:`pmap` are not yet supported.\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the XLA backend. 'cpu', 'gpu', or 'tpu'.\naxis_size: Optional; the size of the mapped axis.\ndonate_argnums: Specify which positional argument buffers are \"donated\" to\nthe computation. It is safe to donate argument buffers if you no longer need\nthem once the computation has finished. In some cases XLA can make use of\ndonated buffers to reduce the amount of memory needed to perform a\ncomputation, for example recycling one of your input buffers to store a\nresult. You should not reuse buffers that you donate to a computation, JAX\nwill raise an error if you try to.\nNote that donate_argnums only work for positional arguments, and keyword\narguments will not be donated.\nFor more details on buffer donation see the\n`FAQ <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>`_.",
        "parameters": {},
        "returns": "A parallelized version of ``fun`` with arguments that correspond to those of\n``fun`` but with extra array axes at positions indicated by ``in_axes`` and\nwith output that has an additional leading array axis (with the same size).\nFor example, assuming 8 XLA devices are available, :py:func:`pmap` can be used\nas a map along a leading array axis:\n>>> import jax.numpy as jnp\n>>>\n>>> out = pmap(lambda x: x ** 2)(jnp.arange(8))  # doctest: +SKIP\n>>> print(out)  # doctest: +SKIP\n[0, 1, 4, 9, 16, 25, 36, 49]\nWhen the leading dimension is smaller than the number of available devices JAX\nwill simply run on a subset of devices:\n>>> x = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2))\n>>> y = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) ** 2\n>>> out = pmap(jnp.dot)(x, y)  # doctest: +SKIP\n>>> print(out)  # doctest: +SKIP\n[[[    4.     9.]\n[   12.    29.]]\n[[  244.   345.]\n[  348.   493.]]\n[[ 1412.  1737.]\n[ 1740.  2141.]]]\nIf your leading dimension is larger than the number of available devices you\nwill get an error:\n>>> pmap(lambda x: x ** 2)(jnp.arange(9))  # doctest: +SKIP\nValueError: ... requires 9 replicas, but only 8 XLA devices are available\nAs with :py:func:`vmap`, using ``None`` in ``in_axes`` indicates that an\nargument doesn't have an extra axis and should be broadcasted, rather than\nmapped, across the replicas:\n>>> x, y = jnp.arange(2.), 4.\n>>> out = pmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None))(x, y)  # doctest: +SKIP\n>>> print(out)  # doctest: +SKIP\n([4., 5.], [8., 8.])\nNote that :py:func:`pmap` always returns values mapped over their leading axis,\nequivalent to using ``out_axes=0`` in :py:func:`vmap`.\nIn addition to expressing pure maps, :py:func:`pmap` can also be used to express\nparallel single-program multiple-data (SPMD) programs that communicate via\ncollective operations. For example:\n>>> f = lambda x: x / jax.lax.psum(x, axis_name='i')\n>>> out = pmap(f, axis_name='i')(jnp.arange(4.))  # doctest: +SKIP\n>>> print(out)  # doctest: +SKIP\n[ 0.          0.16666667  0.33333334  0.5       ]\n>>> print(out.sum())  # doctest: +SKIP\n1.0\nIn this example, ``axis_name`` is a string, but it can be any Python object\nwith ``__hash__`` and ``__eq__`` defined.\nThe argument ``axis_name`` to :py:func:`pmap` names the mapped axis so that\ncollective operations, like :func:`jax.lax.psum`, can refer to it. Axis names\nare important particularly in the case of nested :py:func:`pmap` functions,\nwhere collective operations can operate over distinct axes:\n>>> from functools import partial\n>>> import jax\n>>>\n>>> @partial(pmap, axis_name='rows')\n... @partial(pmap, axis_name='cols')\n... def normalize(x):\n...   row_normed = x / jax.lax.psum(x, 'rows')\n...   col_normed = x / jax.lax.psum(x, 'cols')\n...   doubly_normed = x / jax.lax.psum(x, ('rows', 'cols'))\n...   return row_normed, col_normed, doubly_normed\n>>>\n>>> x = jnp.arange(8.).reshape((4, 2))\n>>> row_normed, col_normed, doubly_normed = normalize(x)  # doctest: +SKIP\n>>> print(row_normed.sum(0))  # doctest: +SKIP\n[ 1.  1.]\n>>> print(col_normed.sum(1))  # doctest: +SKIP\n[ 1.  1.  1.  1.]\n>>> print(doubly_normed.sum((0, 1)))  # doctest: +SKIP\n1.0\nOn multi-process platforms, collective operations operate over all devices,\nincluding those on other processes. For example, assuming the following code\nruns on two processes with 4 XLA devices each:\n>>> f = lambda x: x + jax.lax.psum(x, axis_name='i')\n>>> data = jnp.arange(4) if jax.process_index() == 0 else jnp.arange(4, 8)\n>>> out = pmap(f, axis_name='i')(data)  # doctest: +SKIP\n>>> print(out)  # doctest: +SKIP\n[28 29 30 31] # on process 0\n[32 33 34 35] # on process 1\nEach process passes in a different length-4 array, corresponding to its 4\nlocal devices, and the psum operates over all 8 values. Conceptually, the two\nlength-4 arrays can be thought of as a sharded length-8 array (in this example\nequivalent to jnp.arange(8)) that is mapped over, with the length-8 mapped\naxis given name 'i'. The pmap call on each process then returns the\ncorresponding length-4 output shard.\nThe ``devices`` argument can be used to specify exactly which devices are used\nto run the parallel computation. For example, again assuming a single process\nwith 8 devices, the following code defines two parallel computations, one\nwhich runs on the first six devices and one on the remaining two:\n>>> from functools import partial\n>>> @partial(pmap, axis_name='i', devices=jax.devices()[:6])\n... def f1(x):\n...   return x / jax.lax.psum(x, axis_name='i')\n>>>\n>>> @partial(pmap, axis_name='i', devices=jax.devices()[-2:])\n... def f2(x):\n...   return jax.lax.psum(x ** 2, axis_name='i')\n>>>\n>>> print(f1(jnp.arange(6.)))  # doctest: +SKIP\n[0.         0.06666667 0.13333333 0.2        0.26666667 0.33333333]\n>>> print(f2(jnp.array([2., 3.])))  # doctest: +SKIP\n[ 13.  13.]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "print_environment_info",
      "signature": "print_environment_info(return_string: 'bool' = False) -> 'str | None'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "This is useful information to include when asking a question or filing a bug.\nArgs: return_string (bool) : if True, return the string rather than printing\nto stdout.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "process_count",
      "signature": "process_count(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "process_index",
      "signature": "process_index(backend: 'str | xla_client.Client | None' = None) -> 'int'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "On most platforms, this will always be 0. This will vary on multi-process\nplatforms though.\nArgs:\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the xla backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.\nInteger process index.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "process_indices",
      "signature": "process_indices(backend: 'str | xla_client.Client | None' = None) -> 'list[int]'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "Args:\nbackend: This is an experimental feature and the API is likely to change.\nOptional, a string representing the xla backend: ``'cpu'``, ``'gpu'``, or\n``'tpu'``.\nList of integer process indices.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pure_callback",
      "signature": "pure_callback(callback: 'Callable[..., Any]', result_shape_dtypes: 'Any', *args: 'Any', sharding: 'SingleDeviceSharding | None' = None, vectorized: 'bool | None | DeprecatedArg' = Deprecated, vmap_method: 'str | None' = None, **kwargs: 'Any')",
      "documentation": {
        "description": "Calls a pure Python callback. Works under :func:`jit`/:func:`~vmap`/etc.\nFor more explanation, see `External Callbacks`_.\n``pure_callback`` enables calling a Python function in JIT-ed JAX functions.\nThe input ``callback`` will be passed JAX arrays placed on a local CPU, and\nit should also return JAX arrays on CPU.\nThe callback is treated as functionally pure, meaning it has no side-effects\nand its output value depends only on its argument values. As a consequence, it\nis safe to be called multiple times (e.g. when transformed by :func:`~vmap` or\n:func:`~pmap`), or not to be called at all when e.g. the output of a\n`jit`-decorated function has no data dependence on its value. Pure callbacks\nmay also be reordered if data-dependence allows.\n.. warning::\nIn the context of JAX transformations, Python exceptions should be\nconsidered side-effects: this means that intentionally raising an error\nwithin a `pure_callback` breaks the API contract, and the behavior of\nthe resulting program is undefined.\nWhen `vmap`-ed the behavior will depend on the value of the ``vmap_method``.\n* Calling :func:`~jax.vmap` on a callback without an explicit ``vmap_method``\nis deprecated and it will eventually raise ``NotImplementedError``.\n* ``vmap_method=\"sequential\"`` uses :func:`~jax.lax.map` to loop over\nthe batched arguments, calling ``callback`` once for each batch element.\n* ``vmap_method=\"sequential_unrolled\"`` is like ``sequential``, but the loop\nis unrolled.\n* ``vmap_method=\"expand_dims\"`` calls ``callback`` with new axes of size ``1``\nadded as the leading dimension unbatched inputs.\n* ``vmap_method=\"broadcast_all\"`` behaves like ``expand_dims``, but the\ninputs are tiled to the expected batched shape.\nIf necessary, the legacy behavior provided by the deprecated\n``vectorized=True`` argument can be recovered using\n``vmap_method=\"legacy_vectorized\"``.\nThe current default behavior is to use ``vmap_method=\"sequential\"`` when\nnot specified, but this behavior is deprecated, and in the future, the\ndefault will be to raise a ``NotImplementedError`` unless ``vmap_method`` is\nexplicitly specified.\nArgs:\ncallback: function to execute on the host. The callback is assumed to be a pure\nfunction (i.e. one without side-effects): if an impure function is passed, it\nmay behave in unexpected ways, particularly under transformation. The callable\nwill be passed PyTrees of arrays as arguments, and should return a PyTree of\narrays that matches ``result_shape_dtypes``.\nresult_shape_dtypes: pytree whose leaves have ``shape`` and ``dtype`` attributes,\nwhose structure matches the expected output of the callback function at runtime.\n:class:`jax.ShapeDtypeStruct` is often used to define leaf values.\n*args: arguments to be passed to the callback function\nsharding: optional sharding that specifies the device from which the callback should\nbe invoked.\nvmap_method: string specifying how the callback transforms under\n:func:`~jax.vmap` as described above.\n**kwargs: keyword arguments to be passed to the callback function",
        "parameters": {},
        "returns": "result: a pytree of :class:`jax.Array` objects whose structure matches that of\n``result_shape_dtypes``.",
        "raises": "",
        "see_also": "- :func:`jax.experimental.io_callback`: callback designed for impure functions.\n- :func:`jax.debug.callback`: callback designed for general-purpose debugging.\n- :func:`jax.debug.print`: callback designed for printing.",
        "notes": "",
        "examples": "The behavior of ``pure_callback`` under :func:`~jax.vmap` is controlled by\nthe ``vmap_method`` argument as described above. It is useful to consider\nsome explicit examples that demonstrate the semantics. For example,\nconsider the following function:\n>>> def callback(x, y):\n...   print(jnp.shape(x), jnp.shape(y))\n...   return x + y\n>>> def fun(x, y, *, vmap_method):\n...   shape = jnp.broadcast_shapes(jnp.shape(x), jnp.shape(y))\n...   dtype = jnp.result_type(x, y)\n...   out_type = jax.ShapeDtypeStruct(shape, dtype)\n...   return jax.pure_callback(callback, out_type, x, y,\n...                            vmap_method=vmap_method)\nCalling this with ``vmap_method=\"expand_dims\"`` adds a new axis of size ``1``\nto ``y``:\n>>> from functools import partial\n>>> x = jnp.arange(4)\n>>> y = 1.0\n>>> jax.vmap(partial(fun, vmap_method=\"expand_dims\"), in_axes=(0, None))(x, y)\n(4,) (1,)\nArray([1., 2., 3., 4.], dtype=float32)\nWhereas, ``vmap_method=\"broadcast_all\"`` adds an axis of size ``4`` to\n``y``:\n>>> jax.vmap(partial(fun, vmap_method=\"broadcast_all\"),\n...          in_axes=(0, None))(x, y)\n(4,) (4,)\nArray([1., 2., 3., 4.], dtype=float32)\n.. _External Callbacks: https://jax.readthedocs.io/en/latest/external-callbacks.html"
      }
    },
    {
      "name": "remat",
      "signature": "checkpoint(fun: 'Callable', *, prevent_cse: 'bool' = True, policy: 'Callable[..., bool] | None' = None, static_argnums: 'int | tuple[int, ...]' = ()) -> 'Callable'",
      "documentation": {
        "description": "Make ``fun`` recompute internal linearization points when differentiated.\nThe :func:`jax.checkpoint` decorator, aliased to :func:`jax.remat`, provides a\nway to trade off computation time and memory cost in the context of automatic\ndifferentiation, especially with reverse-mode autodiff like :func:`jax.grad`\nand :func:`jax.vjp` but also with :func:`jax.linearize`.\nWhen differentiating a function in reverse-mode, by default all the\nlinearization points (e.g. inputs to elementwise nonlinear primitive\noperations) are stored when evaluating the forward pass so that they can be\nreused on the backward pass. This evaluation strategy can lead to a high\nmemory cost, or even to poor performance on hardware accelerators where memory\naccess is much more expensive than FLOPs.\nAn alternative evaluation strategy is for some of the linearization points to\nbe recomputed (i.e. rematerialized) rather than stored. This approach can\nreduce memory usage at the cost of increased computation.\nThis function decorator produces a new version of ``fun`` which follows\nthe rematerialization strategy rather than the default store-everything\nstrategy. That is, it returns a new version of ``fun`` which, when\ndifferentiated, doesn't store any of its intermediate linearization points.\nInstead, these linearization points are recomputed from the function's saved\ninputs.\nSee the examples below.\nArgs:\nfun: Function for which the autodiff evaluation strategy is to be changed\nfrom the default of storing all intermediate linearization points to\nrecomputing them. Its arguments and return value should be arrays,\nscalars, or (nested) standard Python containers (tuple/list/dict) thereof.\nprevent_cse: Optional, boolean keyword-only argument indicating whether to\nprevent common subexpression elimination (CSE) optimizations in the HLO\ngenerated from differentiation. This CSE prevention has costs because it\ncan foil other optimizations, and because it can incur high overheads on\nsome backends, especially GPU. The default is True because otherwise,\nunder a :func:`~jax.jit` or :func:`~jax.pmap`, CSE can defeat the purpose\nof this decorator.\nBut in some settings, like when used inside a :func:`~jax.lax.scan`, this\nCSE prevention mechanism is unnecessary, in which case ``prevent_cse`` can\nbe set to False.\nstatic_argnums: Optional, int or sequence of ints, a keyword-only argument\nindicating which argument values on which to specialize for tracing and\ncaching purposes. Specifying arguments as static can avoid\nConcretizationTypeErrors when tracing, but at the cost of more retracing\noverheads. See the example below.\npolicy: Optional, callable keyword-only argument. It should be one of the\nattributes of ``jax.checkpoint_policies``. The callable takes as input a\ntype-level specification of a first-order primitive application and\nreturns a boolean indicating whether the corresponding output value(s) can\nbe saved as residuals (or instead must be recomputed in the (co)tangent\ncomputation if needed).",
        "parameters": {},
        "returns": "A function (callable) with the same input/output behavior as ``fun`` but\nwhich, when differentiated using e.g. :func:`jax.grad`, :func:`jax.vjp`, or\n:func:`jax.linearize`, recomputes rather than stores intermediate\nlinearization points, thus potentially saving memory at the cost of extra\ncomputation.\nHere is a simple example:\n>>> import jax\n>>> import jax.numpy as jnp\n>>> @jax.checkpoint\n... def g(x):\n...   y = jnp.sin(x)\n...   z = jnp.sin(y)\n...   return z\n...\n>>> jax.value_and_grad(g)(2.0)\n(Array(0.78907233, dtype=float32, weak_type=True), Array(-0.2556391, dtype=float32, weak_type=True))\nHere, the same value is produced whether or not the :func:`jax.checkpoint`\ndecorator is present. When the decorator is not present, the values\n``jnp.cos(2.0)`` and ``jnp.cos(jnp.sin(2.0))`` are computed on the forward\npass and are stored for use in the backward pass, because they are needed\non the backward pass and depend only on the primal inputs. When using\n:func:`jax.checkpoint`, the forward pass will compute only the primal outputs\nand only the primal inputs (``2.0``) will be stored for the backward pass.\nAt that time, the value ``jnp.sin(2.0)`` is recomputed, along with the values\n``jnp.cos(2.0)`` and ``jnp.cos(jnp.sin(2.0))``.\nWhile :func:`jax.checkpoint` controls what values are stored from the\nforward-pass to be used on the backward pass, the total amount of memory\nrequired to evaluate a function or its VJP depends on many additional internal\ndetails of that function. Those details include which numerical primitives are\nused, how they're composed, where jit and control flow primitives like scan\nare used, and other factors.\nThe :func:`jax.checkpoint` decorator can be applied recursively to express\nsophisticated autodiff rematerialization strategies. For example:\n>>> def recursive_checkpoint(funs):\n...   if len(funs) == 1:\n...     return funs[0]\n...   elif len(funs) == 2:\n...     f1, f2 = funs\n...     return lambda x: f1(f2(x))\n...   else:\n...     f1 = recursive_checkpoint(funs[:len(funs)//2])\n...     f2 = recursive_checkpoint(funs[len(funs)//2:])\n...     return lambda x: f1(jax.checkpoint(f2)(x))\n...\nIf ``fun`` involves Python control flow that depends on argument values,\nit may be necessary to use the ``static_argnums`` parameter. For example,\nconsider a boolean flag argument::\nfrom functools import partial\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, is_training):\nif is_training:\n...\nelse:\n...\nHere, the use of ``static_argnums`` allows the ``if`` statement's condition\nto depends on the value of ``is_training``. The cost to using\n``static_argnums`` is that it introduces re-tracing overheads across calls:\nin the example, ``foo`` is re-traced every time it is called with a new value\nof ``is_training``. In some situations, ``jax.ensure_compile_time_eval``\nis needed as well::\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, y):\nwith jax.ensure_compile_time_eval():\ny_pos = y > 0\nif y_pos:\n...\nelse:\n...\nAs an alternative to using ``static_argnums`` (and\n``jax.ensure_compile_time_eval``), it may be easier to compute some values\noutside the :func:`jax.checkpoint`-decorated function and then close over them.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "softmax_custom_jvp",
      "signature": "softmax_custom_jvp(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_softmax_custom_jvp` config option (transient).\nUse a new custom_jvp rule for jax.nn.softmax. The new rule should improve memory usage and stability. Set True to use new behavior. See https://github.com/jax-ml/jax/pull/15677  This will be enabled by default in future versions of JAX, at which point all uses of the flag will be considered deprecated (following the `API compatibility policy <https://jax.readthedocs.io/en/latest/api_compatibility.html>`_).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "spmd_mode",
      "signature": "spmd_mode(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_spmd_mode` config option.\nDecides whether Math on `jax.Array`'s that are not fully addressable (i.e. spans across multiple processes) is allowed. The options are: * allow_jit: Default, `pjit` and `jax.jit` computations are allowed     to execute on non-fully addressable `jax.Array`s\n* allow_all: `jnp`, normal math (like `a + b`, etc), `pjit`,     `jax.jit` and all other operations are allowed to     execute on non-fully addressable `jax.Array`s.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "threefry_partitionable",
      "signature": "threefry_partitionable(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_threefry_partitionable` config option (transient).\nEnables internal threefry PRNG implementation changes that render it automatically partitionable in some cases. Without this flag, using the standard jax.random pseudo-random number generation may result in extraneous communication and/or redundant distributed computation. With this flag, the communication overheads disappear in some cases.  This will be enabled by default in future versions of JAX, at which point all uses of the flag will be considered deprecated (following the `API compatibility policy <https://jax.readthedocs.io/en/latest/api_compatibility.html>`_).",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "transfer_guard",
      "signature": "transfer_guard(new_val: 'str') -> 'Iterator[None]'",
      "documentation": {
        "description": "A contextmanager to control the transfer guard level for all transfers.\nFor more information, see\nhttps://jax.readthedocs.io/en/latest/transfer_guard.html\nArgs:\nnew_val: The new thread-local transfer guard level for all transfers.\nYields:\nNone.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "transfer_guard_device_to_device",
      "signature": "transfer_guard_device_to_device(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_transfer_guard_device_to_device` config option.\nSelect the transfer guard level for device-to-device transfers. Default is \"allow\".",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "transfer_guard_device_to_host",
      "signature": "transfer_guard_device_to_host(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_transfer_guard_device_to_host` config option.\nSelect the transfer guard level for device-to-host transfers. Default is \"allow\".",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "transfer_guard_host_to_device",
      "signature": "transfer_guard_host_to_device(new_val: 'Any' = <jax._src.config.NoDefault object at 0x7b22187a0920>)",
      "documentation": {
        "description": "Context manager for `jax_transfer_guard_host_to_device` config option.\nSelect the transfer guard level for host-to-device transfers. Default is \"allow\".",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "typeof",
      "signature": "get_aval(x)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "value_and_grad",
      "signature": "value_and_grad(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -> 'Callable[..., tuple[Any, Any]]'",
      "documentation": {
        "description": "Create a function that evaluates both ``fun`` and the gradient of ``fun``.\nArgs:\nfun: Function to be differentiated. Its arguments at positions specified by\n``argnums`` should be arrays, scalars, or standard Python containers. It\nshould return a scalar (which includes arrays with shape ``()`` but not\narrays with shape ``(1,)`` etc.)\nargnums: Optional, integer or sequence of integers. Specifies which\npositional argument(s) to differentiate with respect to (default 0).\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.\nholomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\nholomorphic. If True, inputs and outputs must be complex. Default False.\nallow_int: Optional, bool. Whether to allow differentiating with\nrespect to integer valued inputs. The gradient of an integer input will\nhave a trivial vector-space dtype (float0). Default False.",
        "parameters": {},
        "returns": "A function with the same arguments as ``fun`` that evaluates both ``fun``\nand the gradient of ``fun`` and returns them as a pair (a two-element\ntuple). If ``argnums`` is an integer then the gradient has the same shape\nand type as the positional argument indicated by that integer. If argnums is\na sequence of integers, the gradient is a tuple of values with the same\nshapes and types as the corresponding arguments. If ``has_aux`` is True\nthen a tuple of ((value, auxiliary_data), gradient) is returned.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "vjp",
      "signature": "vjp(fun: 'Callable', *primals, has_aux: 'bool' = False, reduce_axes=()) -> 'tuple[Any, Callable] | tuple[Any, Callable, Any]'",
      "documentation": {
        "description": "Compute a (reverse-mode) vector-Jacobian product of ``fun``.\n:py:func:`grad` is implemented as a special case of :py:func:`vjp`.\nArgs:\nfun: Function to be differentiated. Its arguments should be arrays, scalars,\nor standard Python containers of arrays or scalars. It should return an\narray, scalar, or standard Python container of arrays or scalars.\nprimals: A sequence of primal values at which the Jacobian of ``fun``\nshould be evaluated. The number of ``primals`` should be equal to the\nnumber of positional parameters of ``fun``. Each primal value should be\nan array, a scalar, or a pytree (standard Python containers) thereof.\nhas_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\nfirst element is considered the output of the mathematical function to be\ndifferentiated and the second element is auxiliary data. Default False.",
        "parameters": {},
        "returns": "If ``has_aux`` is ``False``, returns a ``(primals_out, vjpfun)`` pair, where\n``primals_out`` is ``fun(*primals)``. If ``has_aux`` is ``True``, returns a\n``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data\nreturned by ``fun``.\n``vjpfun`` is a function from a cotangent vector with the same shape as\n``primals_out`` to a tuple of cotangent vectors with the same number and\nshapes as ``primals``, representing the vector-Jacobian product of ``fun``\nevaluated at ``primals``.\n>>> import jax\n>>>\n>>> def f(x, y):\n...   return jax.numpy.sin(x), jax.numpy.cos(y)\n...\n>>> primals, f_vjp = jax.vjp(f, 0.5, 1.0)\n>>> xbar, ybar = f_vjp((-0.7, 0.3))\n>>> print(xbar)\n-0.61430776\n>>> print(ybar)\n-0.2524413",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "vmap",
      "signature": "vmap(fun: 'F', in_axes: 'int | None | Sequence[Any]' = 0, out_axes: 'Any' = 0, axis_name: 'AxisName | None' = None, axis_size: 'int | None' = None, spmd_axis_name: 'AxisName | tuple[AxisName, ...] | None' = None) -> 'F'",
      "documentation": {
        "description": "Vectorizing map. Creates a function which maps ``fun`` over argument axes.\nArgs:\nfun: Function to be mapped over additional axes.\nin_axes: An integer, None, or sequence of values specifying which input\narray axes to map over.\nIf each positional argument to ``fun`` is an array, then ``in_axes`` can\nbe an integer, a None, or a tuple of integers and Nones with length equal\nto the number of positional arguments to ``fun``. An integer or ``None``\nindicates which array axis to map over for all arguments (with ``None``\nindicating not to map any axis), and a tuple indicates which axis to map\nfor each corresponding positional argument. Axis integers must be in the\nrange ``[-ndim, ndim)`` for each array, where ``ndim`` is the number of\ndimensions (axes) of the corresponding input array.\nIf the positional arguments to ``fun`` are container (pytree) types, ``in_axes``\nmust be a sequence with length equal to the number of positional arguments to\n``fun``, and for each argument the corresponding element of ``in_axes`` can\nbe a container with a matching pytree structure specifying the mapping of its\ncontainer elements. In other words, ``in_axes`` must be a container tree prefix\nof the positional argument tuple passed to ``fun``. See this link for more detail:\nhttps://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees\nEither ``axis_size`` must be provided explicitly, or at least one\npositional argument must have ``in_axes`` not None. The sizes of the\nmapped input axes for all mapped positional arguments must all be equal.\nArguments passed as keywords are always mapped over their leading axis\n(i.e. axis index 0).\nSee below for examples.\nout_axes: An integer, None, or (nested) standard Python container\n(tuple/list/dict) thereof indicating where the mapped axis should appear\nin the output. All outputs with a mapped axis must have a non-None\n``out_axes`` specification. Axis integers must be in the range ``[-ndim,\nndim)`` for each output array, where ``ndim`` is the number of dimensions\n(axes) of the array returned by the :func:`vmap`-ed function, which is one\nmore than the number of dimensions (axes) of the corresponding array\nreturned by ``fun``.\naxis_name: Optional, a hashable Python object used to identify the mapped\naxis so that parallel collectives can be applied.\naxis_size: Optional, an integer indicating the size of the axis to be\nmapped. If not provided, the mapped axis size is inferred from arguments.",
        "parameters": {},
        "returns": "Batched/vectorized version of ``fun`` with arguments that correspond to\nthose of ``fun``, but with extra array axes at positions indicated by\n``in_axes``, and a return value that corresponds to that of ``fun``, but\nwith extra array axes at positions indicated by ``out_axes``.\nFor example, we can implement a matrix-matrix product using a vector dot\nproduct:\n>>> import jax.numpy as jnp\n>>>\n>>> vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -> []\n>>> mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -> [b]      (b is the mapped axis)\n>>> mm = vmap(mv, (None, 1), 1)      #  ([b,a], [a,c]) -> [b,c]  (c is the mapped axis)\nHere we use ``[a,b]`` to indicate an array with shape (a,b). Here are some\nvariants:\n>>> mv1 = vmap(vv, (0, 0), 0)   #  ([b,a], [b,a]) -> [b]        (b is the mapped axis)\n>>> mv2 = vmap(vv, (0, 1), 0)   #  ([b,a], [a,b]) -> [b]        (b is the mapped axis)\n>>> mm2 = vmap(mv2, (1, 1), 0)  #  ([b,c,a], [a,c,b]) -> [c,b]  (c is the mapped axis)\nHere's an example of using container types in ``in_axes`` to specify which\naxes of the container elements to map over:\n>>> A, B, C, D = 2, 3, 4, 5\n>>> x = jnp.ones((A, B))\n>>> y = jnp.ones((B, C))\n>>> z = jnp.ones((C, D))\n>>> def foo(tree_arg):\n...   x, (y, z) = tree_arg\n...   return jnp.dot(x, jnp.dot(y, z))\n>>> tree = (x, (y, z))\n>>> print(foo(tree))\n[[12. 12. 12. 12. 12.]\n[12. 12. 12. 12. 12.]]\n>>> from jax import vmap\n>>> K = 6  # batch size\n>>> x = jnp.ones((K, A, B))  # batch axis in different locations\n>>> y = jnp.ones((B, K, C))\n>>> z = jnp.ones((C, D, K))\n>>> tree = (x, (y, z))\n>>> vfoo = vmap(foo, in_axes=((0, (1, 2)),))\n>>> print(vfoo(tree).shape)\n(6, 2, 5)\nHere's another example using container types in ``in_axes``, this time a\ndictionary, to specify the elements of the container to map over:\n>>> dct = {'a': 0., 'b': jnp.arange(5.)}\n>>> x = 1.\n>>> def foo(dct, x):\n...  return dct['a'] + dct['b'] + x\n>>> out = vmap(foo, in_axes=({'a': None, 'b': 0}, None))(dct, x)\n>>> print(out)\n[1. 2. 3. 4. 5.]\nThe results of a vectorized function can be mapped or unmapped. For example,\nthe function below returns a pair with the first element mapped and the second\nunmapped. Only for unmapped results we can specify ``out_axes`` to be ``None``\n(to keep it unmapped).\n>>> print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=(0, None))(jnp.arange(2.), 4.))\n(Array([4., 5.], dtype=float32), 8.0)\nIf the ``out_axes`` is specified for an unmapped result, the result is\nbroadcast across the mapped axis:\n>>> print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=0)(jnp.arange(2.), 4.))\n(Array([4., 5.], dtype=float32), Array([8., 8.], dtype=float32, weak_type=True))\nIf the ``out_axes`` is specified for a mapped result, the result is transposed\naccordingly.\nFinally, here's an example using ``axis_name`` together with collectives:\n>>> xs = jnp.arange(3. * 4.).reshape(3, 4)\n>>> print(vmap(lambda x: lax.psum(x, 'i'), axis_name='i')(xs))\n[[12. 15. 18. 21.]\n[12. 15. 18. 21.]\n[12. 15. 18. 21.]]\nSee the :py:func:`jax.pmap` docstring for more examples involving collectives.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "Array",
      "documentation": {
        "description": "Array base class for JAX\n``jax.Array`` is the public interface for instance checks and type annotation\nof JAX arrays and tracers. Its main applications are in instance checks and\ntype annotations; for example::\nx = jnp.arange(5)\nisinstance(x, jax.Array)  # returns True both inside and outside traced functions.\ndef f(x: Array) -> Array:  # type annotations are valid for traced and non-traced types.\nreturn x\n``jax.Array`` should not be used directly for creation of arrays; instead you\nshould use array creation routines offered in :mod:`jax.numpy`, such as\n:func:`jax.numpy.array`, :func:`jax.numpy.zeros`, :func:`jax.numpy.ones`,\n:func:`jax.numpy.full`, :func:`jax.numpy.arange`, etc.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_data",
          "signature": "addressable_data(self, index: 'int') -> 'Array'",
          "documentation": {
            "description": "Return an array of the addressable data at a particular index.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "all",
          "signature": "_all(self: 'Array', axis: 'reductions.Axis' = None, out: 'None' = None, keepdims: 'bool' = False, *, where: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Test whether all array elements along a given axis evaluate to True.\nRefer to :func:`jax.numpy.all` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "any",
          "signature": "_any(self: 'Array', axis: 'reductions.Axis' = None, out: 'None' = None, keepdims: 'bool' = False, *, where: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Test whether any array elements along a given axis evaluate to True.\nRefer to :func:`jax.numpy.any` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "argmax",
          "signature": "_argmax(self: 'Array', axis: 'int | None' = None, out: 'None' = None, keepdims: 'bool | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the index of the maximum value.\nRefer to :func:`jax.numpy.argmax` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "argmin",
          "signature": "_argmin(self: 'Array', axis: 'int | None' = None, out: 'None' = None, keepdims: 'bool | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the index of the minimum value.\nRefer to :func:`jax.numpy.argmin` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "argpartition",
          "signature": "_argpartition(self: 'Array', kth: 'int', axis: 'int' = -1) -> 'Array'",
          "documentation": {
            "description": "Return the indices that partially sort the array.\nRefer to :func:`jax.numpy.argpartition` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "argsort",
          "signature": "_argsort(self: 'Array', axis: 'int | None' = -1, *, kind: 'None' = None, order: 'None' = None, stable: 'bool' = True, descending: 'bool' = False) -> 'Array'",
          "documentation": {
            "description": "Return the indices that sort the array.\nRefer to :func:`jax.numpy.argsort` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "astype",
          "signature": "_astype(self: 'Array', dtype: 'DTypeLike | None', copy: 'bool' = False, device: 'xc.Device | Sharding | None' = None) -> 'Array'",
          "documentation": {
            "description": "Copy the array and cast to a specified dtype.\nThis is implemented via :func:`jax.lax.convert_element_type`, which may\nhave slightly different behavior than :meth:`numpy.ndarray.astype` in\nsome cases. In particular, the details of float-to-int and int-to-float\ncasts are implementation dependent.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "choose",
          "signature": "_choose(self: 'Array', choices: 'Sequence[ArrayLike]', out: 'None' = None, mode: 'str' = 'raise') -> 'Array'",
          "documentation": {
            "description": "Construct an array choosing from elements of multiple arrays.\nRefer to :func:`jax.numpy.choose` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "clip",
          "signature": "_clip(self: 'Array', min: 'ArrayLike | None' = None, max: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return an array whose values are limited to a specified range.\nRefer to :func:`jax.numpy.clip` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "compress",
          "signature": "_compress(self: 'Array', condition: 'ArrayLike', axis: 'int | None' = None, *, out: 'None' = None, size: 'int | None' = None, fill_value: 'ArrayLike' = 0) -> 'Array'",
          "documentation": {
            "description": "Return selected slices of this array along given axis.\nRefer to :func:`jax.numpy.compress` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conj",
          "signature": "_conj(self: 'Array') -> 'Array'",
          "documentation": {
            "description": "Return the complex conjugate of the array.\nRefer to :func:`jax.numpy.conj` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "_conjugate(self: 'Array') -> 'Array'",
          "documentation": {
            "description": "Return the complex conjugate of the array.\nRefer to :func:`jax.numpy.conjugate` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy",
          "signature": "_copy(self: 'Array') -> 'Array'",
          "documentation": {
            "description": "Return a copy of the array.\nRefer to :func:`jax.numpy.copy` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "copy_to_host_async",
          "signature": "copy_to_host_async(self)",
          "documentation": {
            "description": "Copies an ``Array`` to the host asynchronously.\nFor arrays that live an an accelerator, such as a GPU or a TPU, JAX may\ncache the value of the array on the host. Normally this happens\nbehind the scenes when the value of an on-device array is requested by the\nuser, but waiting to initiate a device-to-host copy until the value is\nrequested requires that JAX block the caller while waiting for the copy to\ncomplete.\n``copy_to_host_async`` requests that JAX populate its on-host cache of an\narray, but does not wait for the copy to complete. This may speed up a\nfuture on-host access to the array's contents.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cumprod",
          "signature": "_cumprod(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the cumulative product of the array.\nRefer to :func:`jax.numpy.cumprod` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "cumsum",
          "signature": "_cumsum(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the cumulative sum of the array.\nRefer to :func:`jax.numpy.cumsum` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "diagonal",
          "signature": "_diagonal(self: 'Array', offset: 'int' = 0, axis1: 'int' = 0, axis2: 'int' = 1) -> 'Array'",
          "documentation": {
            "description": "Return the specified diagonal from the array.\nRefer to :func:`jax.numpy.diagonal` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "dot",
          "signature": "_dot(self: 'Array', b: 'ArrayLike', *, precision: 'lax_internal.PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Compute the dot product of two arrays.\nRefer to :func:`jax.numpy.dot` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "flatten",
          "signature": "_flatten(self: 'Array', order: 'str' = 'C') -> 'Array'",
          "documentation": {
            "description": "Flatten array into a 1-dimensional shape.\nRefer to :func:`jax.numpy.ravel` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "item",
          "signature": "_item(self: 'Array', *args: 'int') -> 'bool | int | float | complex'",
          "documentation": {
            "description": "Copy an element of an array to a standard Python scalar and return it.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "max",
          "signature": "_max(self: 'Array', axis: 'reductions.Axis' = None, out: 'None' = None, keepdims: 'bool' = False, initial: 'ArrayLike | None' = None, where: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the maximum of array elements along a given axis.\nRefer to :func:`jax.numpy.max` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "mean",
          "signature": "_mean(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None, keepdims: 'bool' = False, *, where: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the mean of array elements along a given axis.\nRefer to :func:`jax.numpy.mean` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "min",
          "signature": "_min(self: 'Array', axis: 'reductions.Axis' = None, out: 'None' = None, keepdims: 'bool' = False, initial: 'ArrayLike | None' = None, where: 'ArrayLike | None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the minimum of array elements along a given axis.\nRefer to :func:`jax.numpy.min` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "nonzero",
          "signature": "_nonzero(self: 'Array', *, fill_value: 'None | ArrayLike | tuple[ArrayLike, ...]' = None, size: 'int | None' = None) -> 'tuple[Array, ...]'",
          "documentation": {
            "description": "Return indices of nonzero elements of an array.\nRefer to :func:`jax.numpy.nonzero` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "prod",
          "signature": "_prod(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None, keepdims: 'bool' = False, initial: 'ArrayLike | None' = None, where: 'ArrayLike | None' = None, promote_integers: 'bool' = True) -> 'Array'",
          "documentation": {
            "description": "Return product of the array elements over a given axis.\nRefer to :func:`jax.numpy.prod` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ptp",
          "signature": "_ptp(self: 'Array', axis: 'reductions.Axis' = None, out: 'None' = None, keepdims: 'bool' = False) -> 'Array'",
          "documentation": {
            "description": "Return the peak-to-peak range along a given axis.\nRefer to :func:`jax.numpy.ptp` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "ravel",
          "signature": "_flatten(self: 'Array', order: 'str' = 'C') -> 'Array'",
          "documentation": {
            "description": "Flatten array into a 1-dimensional shape.\nRefer to :func:`jax.numpy.ravel` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "repeat",
          "signature": "_repeat(self: 'Array', repeats: 'ArrayLike', axis: 'int | None' = None, *, total_repeat_length: 'int | None' = None) -> 'Array'",
          "documentation": {
            "description": "Construct an array from repeated elements.\nRefer to :func:`jax.numpy.repeat` for the full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "reshape",
          "signature": "_reshape(self: 'Array', *args: 'Any', order: 'str' = 'C') -> 'Array'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Refer to :func:`jax.numpy.reshape` for full documentation.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "round",
          "signature": "_round(self: 'Array', decimals: 'int' = 0, out: 'None' = None) -> 'Array'",
          "documentation": {
            "description": "Round array elements to a given decimal.\nRefer to :func:`jax.numpy.round` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "searchsorted",
          "signature": "_searchsorted(self: 'Array', v: 'ArrayLike', side: 'str' = 'left', sorter: 'ArrayLike | None' = None, *, method: 'str' = 'scan') -> 'Array'",
          "documentation": {
            "description": "Perform a binary search within a sorted array.\nRefer to :func:`jax.numpy.searchsorted` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "sort",
          "signature": "_sort(self: 'Array', axis: 'int | None' = -1, *, kind: 'None' = None, order: 'None' = None, stable: 'bool' = True, descending: 'bool' = False) -> 'Array'",
          "documentation": {
            "description": "Return a sorted copy of an array.\nRefer to :func:`jax.numpy.sort` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "squeeze",
          "signature": "_squeeze(self: 'Array', axis: 'reductions.Axis' = None) -> 'Array'",
          "documentation": {
            "description": "Remove one or more length-1 axes from array.\nRefer to :func:`jax.numpy.squeeze` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "std",
          "signature": "_std(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None, ddof: 'int' = 0, keepdims: 'bool' = False, *, where: 'ArrayLike | None' = None, correction: 'int | float | None' = None) -> 'Array'",
          "documentation": {
            "description": "Compute the standard deviation along a given axis.\nRefer to :func:`jax.numpy.std` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "sum",
          "signature": "_sum(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None, keepdims: 'bool' = False, initial: 'ArrayLike | None' = None, where: 'ArrayLike | None' = None, promote_integers: 'bool' = True) -> 'Array'",
          "documentation": {
            "description": "Sum of the elements of the array over a given axis.\nRefer to :func:`jax.numpy.sum` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "swapaxes",
          "signature": "_swapaxes(self: 'Array', axis1: 'int', axis2: 'int') -> 'Array'",
          "documentation": {
            "description": "Swap two axes of an array.\nRefer to :func:`jax.numpy.swapaxes` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "take",
          "signature": "_take(self: 'Array', indices: 'ArrayLike', axis: 'int | None' = None, out: 'None' = None, mode: 'str | None' = None, unique_indices: 'bool' = False, indices_are_sorted: 'bool' = False, fill_value: 'StaticScalar | None' = None) -> 'Array'",
          "documentation": {
            "description": "Take elements from an array.\nRefer to :func:`jax.numpy.take` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_device",
          "signature": "_to_device(self: 'Array', device: 'xc.Device | Sharding', *, stream: 'int | Any | None' = None)",
          "documentation": {
            "description": "Return a copy of the array on the specified device\nArgs:\ndevice: :class:`~jax.Device` or :class:`~jax.sharding.Sharding`\nto which the created array will be committed.\nstream: not implemented, passing a non-None value will lead to an error.",
            "parameters": {},
            "returns": "copy of array placed on the specified device or devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "trace",
          "signature": "_trace(self: 'Array', offset: 'int | ArrayLike' = 0, axis1: 'int' = 0, axis2: 'int' = 1, dtype: 'DTypeLike | None' = None, out: 'None' = None) -> 'Array'",
          "documentation": {
            "description": "Return the sum along the diagonal.\nRefer to :func:`jax.numpy.trace` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transpose",
          "signature": "_transpose(self: 'Array', *args: 'Any') -> 'Array'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Refer to :func:`jax.numpy.transpose` for full documentation.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "var",
          "signature": "_var(self: 'Array', axis: 'reductions.Axis' = None, dtype: 'DTypeLike | None' = None, out: 'None' = None, ddof: 'int' = 0, keepdims: 'bool' = False, *, where: 'ArrayLike | None' = None, correction: 'int | float | None' = None) -> 'Array'",
          "documentation": {
            "description": "Compute the variance along a given axis.\nRefer to :func:`jax.numpy.var` for full documentation.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "view",
          "signature": "_view(self: 'Array', dtype: 'DTypeLike | None' = None, type: 'None' = None) -> 'Array'",
          "documentation": {
            "description": "Return a bitwise copy of the array, viewed as a new dtype.\nThis is fuller-featured wrapper around :func:`jax.lax.bitcast_convert_type`.\nIf the source and target dtype have the same bitwidth, the result has the same\nshape as the input array. If the bitwidth of the target dtype is different\nfrom the source, the size of the last axis of the result is adjusted\naccordingly.\n>>> jnp.zeros([1,2,3], dtype=jnp.int16).view(jnp.int8).shape\n(1, 2, 6)\n>>> jnp.zeros([1,2,4], dtype=jnp.int8).view(jnp.int16).shape\n(1, 2, 2)\nConversions involving booleans are not well-defined in all situations. With\nregards to the shape of result as explained above, booleans are treated as\nhaving a bitwidth of 8. However, when converting to a boolean array, the input\nshould only contain 0 or 1 bytes. Otherwise, results may be unpredictable or\nmay change depending on how the result is used.\nThis conversion is guaranteed and safe::\n>>> jnp.array([1, 0, 1], dtype=jnp.int8).view(jnp.bool_)\nArray([ True, False,  True], dtype=bool)\nHowever, there are no guarantees about the results of any expression involving\na view such as this: `jnp.array([1, 2, 3], dtype=jnp.int8).view(jnp.bool_)`.\nIn particular, the results may change between JAX releases and depending on\nthe platform. To safely convert such an array to a boolean array, compare it\nwith `0`::\n>>> jnp.array([1, 2, 0], dtype=jnp.int8) != 0\nArray([ True,  True, False], dtype=bool)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "Device",
      "documentation": {
        "description": "A descriptor of an available device.\nSubclasses are used to represent specific types of devices, e.g. CPUs, GPUs. Subclasses may have additional properties specific to that device type.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "addressable_memories",
          "signature": "addressable_memories(self)",
          "documentation": {
            "description": "addressable_memories(self) -> list",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "default_memory",
          "signature": "default_memory(self)",
          "documentation": {
            "description": "default_memory(self) -> object",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "get_stream_for_external_ready_events",
          "signature": "get_stream_for_external_ready_events(self)",
          "documentation": {
            "description": "get_stream_for_external_ready_events(self) -> int",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "live_buffers",
          "signature": "live_buffers(self)",
          "documentation": {
            "description": "live_buffers(self) -> list",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "memory",
          "signature": "memory(self, kind: str)",
          "documentation": {
            "description": "memory(self, kind: str) -> object",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "memory_stats",
          "signature": "memory_stats(self)",
          "documentation": {
            "description": "memory_stats(self) -> dict | None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transfer_from_outfeed",
          "signature": "transfer_from_outfeed(self, arg: jaxlib.xla_extension.Shape, /)",
          "documentation": {
            "description": "transfer_from_outfeed(self, arg: jaxlib.xla_extension.Shape, /) -> object",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "transfer_to_infeed",
          "signature": "transfer_to_infeed(self, arg: xla::LiteralSlice, /)",
          "documentation": {
            "description": "transfer_to_infeed(self, arg: xla::LiteralSlice, /) -> None",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "NamedSharding",
      "documentation": {
        "description": "A :class:`NamedSharding` expresses sharding using named axes.\nA :class:`NamedSharding` is a pair of a :class:`Mesh` of devices and\n:class:`PartitionSpec` which describes how to shard an array across that\nmesh.\nA :class:`Mesh` is a multidimensional NumPy array of JAX devices,\nwhere each axis of the mesh has a name, e.g. ``'x'`` or ``'y'``.\nA :class:`PartitionSpec` is a tuple, whose elements can be a ``None``,\na mesh axis, or a tuple of mesh axes. Each element describes how an input\ndimension is partitioned across zero or more mesh dimensions. For example,\n``PartitionSpec('x', 'y')`` says that the first dimension of data\nis sharded across ``x`` axis of the mesh, and the second dimension is sharded\nacross ``y`` axis of the mesh.\nThe Distributed arrays and automatic parallelization\n(https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#namedsharding-gives-a-way-to-express-shardings-with-names)\ntutorial has more details and diagrams that explain how\n:class:`Mesh` and :class:`PartitionSpec` are used.\nArgs:\nmesh: A :class:`jax.sharding.Mesh` object.\nspec: A :class:`jax.sharding.PartitionSpec` object.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from jax.sharding import Mesh\n>>> from jax.sharding import PartitionSpec as P\n>>> mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\n>>> spec = P('x', 'y')\n>>> named_sharding = jax.sharding.NamedSharding(mesh, spec)"
      },
      "methods": [
        {
          "name": "addressable_devices_indices_map",
          "signature": "addressable_devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index | None]'",
          "documentation": {
            "description": "A mapping from addressable devices to the slice of array data each contains.\n``addressable_devices_indices_map`` contains that part of\n``device_indices_map`` that applies to the addressable devices.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "check_compatible_aval",
          "signature": "check_compatible_aval(self, aval_shape: 'Shape') -> 'None'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "devices_indices_map",
          "signature": "devices_indices_map(self, global_shape: 'Shape') -> 'Mapping[Device, Index]'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The mapping includes all global devices, i.e., including\nnon-addressable devices from other processes.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_equivalent_to",
          "signature": "is_equivalent_to(self: 'Sharding', other: 'Sharding', ndim: 'int') -> 'bool'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "Two shardings are equivalent if they place the same logical array shards on\nthe same devices.\nFor example, a :class:`NamedSharding` may be equivalent\nto a :class:`PositionalSharding` if both place the same shards of the array\non the same devices.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "shard_shape",
          "signature": "shard_shape(self, global_shape: 'Shape') -> 'Shape'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "The shard shape returned by this function is calculated from\n``global_shape`` and the properties of the sharding.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_memory_kind",
          "signature": "with_memory_kind(self, kind: 'str') -> 'NamedSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "with_spec",
          "signature": "with_spec(self, spec: 'PartitionSpec | Sequence[Any]') -> 'NamedSharding'",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ShapeDtypeStruct",
      "documentation": {
        "description": "A container for the shape, dtype, and other static attributes of an array.\n``ShapeDtypeStruct`` is often used in conjunction with :func:`jax.eval_shape`.\nArgs:\nshape: a sequence of integers representing an array shape\ndtype: a dtype-like object\nsharding: (optional) a :class:`jax.Sharding` object",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Shard",
      "documentation": {
        "description": "A single data shard of an Array.\nAttributes:\ndevice : Which device this shard resides on.\nindex : The index into the global array of this shard.\nreplica_id : Integer id indicating which replica of the global array this\nshard is part of. Always 0 for fully sharded data\n(i.e. when theres only 1 replica).\ndata : The data of this shard. None if ``device`` is non-local.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "custom_jvp",
      "documentation": {
        "description": "Set up a JAX-transformable function for a custom JVP rule definition.\nThis class is meant to be used as a function decorator. Instances are\ncallables that behave similarly to the underlying function to which the\ndecorator was applied, except when a differentiation transformation (like\n:py:func:`jax.jvp` or :py:func:`jax.grad`) is applied, in which case a custom\nuser-supplied JVP rule function is used instead of tracing into and\nperforming automatic differentiation of the underlying function's\nimplementation.\nThere are two instance methods available for defining the custom JVP rule:\n:py:func:`~jax.custom_jvp.defjvp` for defining a *single* custom JVP rule for\nall the function's inputs, and for convenience\n:py:func:`~jax.custom_jvp.defjvps`, which wraps\n:py:func:`~jax.custom_jvp.defjvp`, and allows you to provide separate\ndefinitions for the partial derivatives of the function w.r.t. each of its\narguments.\nFor example::\n@jax.custom_jvp\ndef f(x, y):\nreturn jnp.sin(x) * y\n@f.defjvp\ndef f_jvp(primals, tangents):\nx, y = primals\nx_dot, y_dot = tangents\nprimal_out = f(x, y)\ntangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\nreturn primal_out, tangent_out\nFor a more detailed introduction, see the tutorial_.\n.. _tutorial: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "defjvp",
          "signature": "defjvp(self, jvp: 'Callable[..., tuple[ReturnValue, ReturnValue]]', symbolic_zeros: 'bool' = False) -> 'Callable[..., tuple[ReturnValue, ReturnValue]]'",
          "documentation": {
            "description": "Define a custom JVP rule for the function represented by this instance.\nArgs:\njvp: a Python callable representing the custom JVP rule. When there are no\n``nondiff_argnums``, the ``jvp`` function should accept two arguments,\nwhere the first is a tuple of primal inputs and the second is a tuple of\ntangent inputs. The lengths of both tuples are equal to the number of\nparameters of the :class:`~jax.custom_jvp` function. The ``jvp`` function\nshould produce as output a pair where the first element is the primal\noutput and the second element is the tangent output. Elements of the\ninput and output tuples may be arrays or any nested tuples/lists/dicts\nthereof.\nsymbolic_zeros: boolean, indicating whether the rule should be passed\nobjects representing static symbolic zeros in its tangent argument in\ncorrespondence with unperturbed values; otherwise, only standard JAX\ntypes (e.g. array-likes) are passed. Setting this option to ``True``\nallows a JVP rule to detect whether certain inputs are not involved in\ndifferentiation, but at the cost of needing special handling for these\nobjects (which e.g. can't be passed into jax.numpy functions). Default\n``False``.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ">>> @jax.custom_jvp\n... def f(x, y):\n...   return jnp.sin(x) * y\n...\n>>> @f.defjvp\n... def f_jvp(primals, tangents):\n...   x, y = primals\n...   x_dot, y_dot = tangents\n...   primal_out = f(x, y)\n...   tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n...   return primal_out, tangent_out\n>>> x = jnp.float32(1.0)\n>>> y = jnp.float32(2.0)\n>>> with jnp.printoptions(precision=2):\n...   print(jax.value_and_grad(f)(x, y))\n(Array(1.68, dtype=float32), Array(1.08, dtype=float32))"
          }
        },
        {
          "name": "defjvps",
          "signature": "defjvps(self, *jvps: 'Callable[..., ReturnValue] | None') -> 'None'",
          "documentation": {
            "description": "Convenience wrapper for defining JVPs for each argument separately.\nThis convenience wrapper cannot be used together with ``nondiff_argnums``.\nArgs:\n*jvps: a sequence of functions, one for each positional argument of the\n:class:`~jax.custom_jvp` function. Each function takes as arguments\nthe tangent value for the corresponding primal input, the primal\noutput, and the primal inputs. See the example below.",
            "parameters": {},
            "returns": "None.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ">>> @jax.custom_jvp\n... def f(x, y):\n...   return jnp.sin(x) * y\n...\n>>> f.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n...           lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)\n>>> x = jnp.float32(1.0)\n>>> y = jnp.float32(2.0)\n>>> with jnp.printoptions(precision=2):\n...   print(jax.value_and_grad(f)(x, y))\n(Array(1.68, dtype=float32), Array(1.08, dtype=float32))"
          }
        }
      ]
    },
    {
      "name": "custom_vjp",
      "documentation": {
        "description": "Set up a JAX-transformable function for a custom VJP rule definition.\nThis class is meant to be used as a function decorator. Instances are\ncallables that behave similarly to the underlying function to which the\ndecorator was applied, except when a reverse-mode differentiation\ntransformation (like :py:func:`jax.grad`) is applied, in which case a custom\nuser-supplied VJP rule function is used instead of tracing into and performing\nautomatic differentiation of the underlying function's implementation. There\nis a single instance method, :py:func:`~jax.custom_vjp.defvjp`, which may be\nused to define the custom VJP rule.\nThis decorator precludes the use of forward-mode automatic differentiation.\nFor example::\n@jax.custom_vjp\ndef f(x, y):\nreturn jnp.sin(x) * y\ndef f_fwd(x, y):\nreturn f(x, y), (jnp.cos(x), jnp.sin(x), y)\ndef f_bwd(res, g):\ncos_x, sin_x, y = res\nreturn (cos_x * g * y, sin_x * g)\nf.defvjp(f_fwd, f_bwd)\nFor a more detailed introduction, see the tutorial_.\n.. _tutorial: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "defvjp",
          "signature": "defvjp(self, fwd: 'Callable[..., tuple[ReturnValue, Any]]', bwd: 'Callable[..., tuple[Any, ...]]', symbolic_zeros: 'bool' = False, optimize_remat: 'bool' = False) -> 'None'",
          "documentation": {
            "description": "Define a custom VJP rule for the function represented by this instance.\nArgs:\nfwd: a Python callable representing the forward pass of the custom VJP\nrule. When there are no ``nondiff_argnums``, the ``fwd`` function has\nthe same input signature as the underlying primal function. It should\nreturn as output a pair, where the first element represents the primal\noutput and the second element represents any \"residual\" values to store\nfrom the forward pass for use on the backward pass by the function\n``bwd``. Input arguments and elements of the output pair may be arrays\nor nested tuples/lists/dicts thereof.\nbwd: a Python callable representing the backward pass of the custom VJP\nrule. When there are no ``nondiff_argnums``, the ``bwd`` function takes\ntwo arguments, where the first is the \"residual\" values produced on the\nforward pass by ``fwd``, and the second is the output cotangent with the\nsame structure as the primal function output. The output of ``bwd`` must\nbe a tuple of length equal to the number of arguments of the primal\nfunction, and the tuple elements may be arrays or nested\ntuples/lists/dicts thereof so as to match the structure of the primal\ninput arguments.\nsymbolic_zeros: boolean, determining whether to indicate symbolic zeros\nto the ``fwd`` and ``bwd`` rules. Enabling this option allows custom\nderivative rules to detect when certain inputs, and when certain\noutput cotangents, are not involved in differentiation. If ``True``:\n* ``fwd`` must accept, in place of each leaf value ``x`` in\nthe pytree comprising an argument to the original function,\nan object (of type\n``jax.custom_derivatives.CustomVJPPrimal``) with two\nattributes instead: ``value`` and ``perturbed``. The\n``value`` field is the original primal argument, and\n``perturbed`` is a boolean.  The ``perturbed`` bit indicates\nwhether the argument is involved in differentiation (i.e.,\nif it is ``False``, then the corresponding Jacobian \"column\"\nis zero).\n* ``bwd`` will be passed objects representing static symbolic zeros in\nits cotangent argument in correspondence with unperturbed values;\notherwise, only standard JAX types (e.g. array-likes) are passed.\nSetting this option to ``True`` allows these rules to detect whether\ncertain inputs and outputs are not involved in differentiation, but at\nthe cost of special handling. For instance:\n* The signature of ``fwd`` changes, and the objects it is passed cannot\nbe output from the rule directly.\n* The ``bwd`` rule is passed objects that are not entirely array-like,\nand that cannot be passed to most ``jax.numpy`` functions.\n* Any custom pytree nodes involved in the primal function's arguments\nmust accept, in their unflattening functions, the two-field record\nobjects that are given as input leaves to the ``fwd`` rule.\nDefault ``False``.\noptimize_remat: boolean, an experimental flag to enable an automatic\noptimization when this function is used under :func:`jax.remat`. This\nwill be most useful when the ``fwd`` rule is an opaque call such as a\nPallas kernel or a custom call. Default ``False``.",
            "parameters": {},
            "returns": "None.",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ">>> @jax.custom_vjp\n... def f(x, y):\n...   return jnp.sin(x) * y\n...\n>>> def f_fwd(x, y):\n...   return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n...\n>>> def f_bwd(res, g):\n...   cos_x, sin_x, y = res\n...   return (cos_x * g * y, sin_x * g)\n...\n>>> f.defvjp(f_fwd, f_bwd)\n>>> x = jnp.float32(1.0)\n>>> y = jnp.float32(2.0)\n>>> with jnp.printoptions(precision=2):\n...   print(jax.value_and_grad(f)(x, y))\n(Array(1.68, dtype=float32), Array(1.08, dtype=float32))"
          }
        }
      ]
    }
  ]
}