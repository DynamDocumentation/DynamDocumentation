{
  "description": "No description available",
  "functions": [
    {
      "name": "ConvGeneralDilatedDimensionNumbers",
      "signature": "Union(*args, **kwargs)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "DotDimensionNumbers",
      "signature": "tuple(*args, **kwargs)",
      "documentation": {
        "description": "Built-in immutable sequence.\nIf no argument is given, the constructor returns an empty tuple.\nIf iterable is specified the tuple is initialized from iterable's items.\nIf the argument is a tuple, the return value is the same object.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "PrecisionLike",
      "signature": "Union(*args, **kwargs)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "abs",
      "signature": "abs(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise absolute value: :math:`|x|`.\nThis function lowers directly to the `stablehlo.abs`_ operation.\nArgs:\nx: Input array. Must have signed integer, floating, or complex dtype.",
        "parameters": {},
        "returns": "An array of the same shape as ``x`` containing the elementwise absolute value.\nFor complex valued input, :math:`a + ib`, ``abs(x)`` returns :math:`\\sqrt{a^2+b^2}`.\nSee also:\n- :func:`jax.numpy.abs`: a more flexible NumPy-style ``abs`` implementation.\n.. _stablehlo.abs: https://openxla.org/stablehlo/spec#abs",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "acos",
      "signature": "acos(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise arc cosine: :math:`\\mathrm{acos}(x)`.\nThis function lowers directly to the ``chlo.acos`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the\nelement-wise arc cosine.\nSee also:\n- :func:`jax.lax.cos`: elementwise cosine.\n- :func:`jax.lax.asin`: elementwise arc sine.\n- :func:`jax.lax.atan`: elementwise arc tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "acosh",
      "signature": "acosh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise inverse hyperbolic cosine: :math:`\\mathrm{acosh}(x)`.\nThis function lowers directly to the ``chlo.acosh`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\ninverse hyperbolic cosine.\nSee also:\n- :func:`jax.lax.asinh`: elementwise inverse hyperbolic sine.\n- :func:`jax.lax.atanh`: elementwise inverse hyperbolic tangent.\n- :func:`jax.lax.cosh`: elementwise hyperbolic cosine.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "add",
      "signature": "add(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise addition: :math:`x + y`.\nThis function lowers directly to the `stablehlo.add`_ operation.\nArgs:\nx, y: Input arrays. Must have matching numerical dtypes. If neither\nis a scalar, ``x`` and ``y`` must have the same number of dimensions\nand be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the sum\nof each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.add`: NumPy-style addition supporting inputs\nwith mixed dtypes and ranks.\n.. _stablehlo.add: https://openxla.org/stablehlo/spec#add",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "after_all",
      "signature": "after_all(*operands)",
      "documentation": {
        "description": "Merges one or more XLA token values. Experimental.\nWraps the XLA AfterAll operator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "all_gather",
      "signature": "all_gather(x, axis_name, *, axis_index_groups=None, axis=0, tiled=False)",
      "documentation": {
        "description": "Gather values of x across all replicas.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nThis is equivalent to, but faster than, all_to_all(broadcast(x)).\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would run all gather over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and all groups must be the same size.\naxis: a positional axis into which the chunks along ``axis_name`` will be\nconcatenated.\ntiled: when ``False``, the chunks will be stacked into a fresh positional\naxis at index ``axis`` in the output. When ``True``, ``axis`` has to\nrefer to an existing positional dimension and the chunks will be\nconcatenated into that dimension.",
        "parameters": {},
        "returns": "Array(s) representing the result of an all-gather along the axis\n``axis_name``. Shapes are the same as ``x.shape``, but:\n- when ``tiled`` is ``False``, there is a new dimension equal to the\nsize of axis ``axis_name`` in position ``axis``,\n- when ``tiled`` is ``True``, the size of dimension in position ``axis``\nis multiplied by the size of axis ``axis_name``.\nFor example, with 4 XLA devices available:\n>>> x = np.arange(4)\n>>> y = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[[0 1 2 3]\n[0 1 2 3]\n[0 1 2 3]\n[0 1 2 3]]\nAn example of using axis_index_groups, groups split by even & odd device ids:\n>>> x = np.arange(16).reshape(4, 4)\n>>> print(x)\n[[ 0  1  2  3]\n[ 4  5  6  7]\n[ 8  9 10 11]\n[12 13 14 15]]\n>>> def f(x):\n...   return jax.lax.all_gather(\n...       x, 'i', axis_index_groups=[[0, 2], [3, 1]])\n>>> y = jax.pmap(f, axis_name='i')(x)\n>>> print(y)\n[[[ 0  1  2  3]\n[ 8  9 10 11]]\n[[12 13 14 15]\n[ 4  5  6  7]]\n[[ 0  1  2  3]\n[ 8  9 10 11]]\n[[12 13 14 15]\n[ 4  5  6  7]]]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "all_to_all",
      "signature": "all_to_all(x, axis_name, split_axis, concat_axis, *, axis_index_groups=None, tiled=False)",
      "documentation": {
        "description": "Materialize the mapped axis and map a different axis.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nIn the output, the input mapped axis ``axis_name`` is materialized at the\nlogical axis position ``concat_axis``, and the input unmapped axis at position\n``split_axis`` is mapped with the name ``axis_name``.\nThe group size of the mapped axis size must be equal to the size of the\nunmapped axis; that is, we must have\n``lax.psum(1, axis_name, axis_index_groups=axis_index_groups) == x.shape[axis]``.\nBy default, when ``axis_index_groups=None``, this encompasses all the devices.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\nsplit_axis: int indicating the unmapped axis of ``x`` to map with the name\n``axis_name``.\nconcat_axis: int indicating the position in the output to materialize the\nmapped axis of the input with the name ``axis_name``.\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would run all_to_all over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and all groups must be the same size.\ntiled: when True, all_to_all will divide split_axis into chunks and concatenate\nthem along concat_axis. In particular, no dimensions are added or removed.\nFalse by default.",
        "parameters": {},
        "returns": "When tiled is False, array(s) with shape given by the expression::\nnp.insert(np.delete(x.shape, split_axis), concat_axis, axis_size)\nwhere ``axis_size`` is the size of the mapped axis named ``axis_name`` in\nthe input ``x``, i.e. ``axis_size = lax.psum(1, axis_name)``.\nOtherwise array with shape similar to the input shape, except with split_axis\ndivided by axis size and concat_axis multiplied by axis size.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "approx_max_k",
      "signature": "approx_max_k(operand: jax.Array, k: int, reduction_dimension: int = -1, recall_target: float = 0.95, reduction_input_size_override: int = -1, aggregate_to_topk: bool = True) -> tuple[jax.Array, jax.Array]",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "See https://arxiv.org/abs/2206.14286 for the algorithm details.\nArgs:\noperand : Array to search for max-k. Must be a floating number type.\nk : Specifies the number of max-k.\nreduction_dimension : Integer dimension along which to search. Default: -1.\nrecall_target : Recall target for the approximation.\nreduction_input_size_override : When set to a positive value, it overrides\nthe size determined by ``operand[reduction_dim]`` for evaluating the\nrecall. This option is useful when the given ``operand`` is only a subset\nof the overall computation in SPMD or distributed pipelines, where the\ntrue input size cannot be deferred by the operand shape.\naggregate_to_topk : When true, aggregates approximate results to the top-k\nin sorted order. When false, returns the approximate results unsorted. In\nthis case, the number of the approximate results is implementation defined\nand is greater or equal to the specified ``k``.\nTuple of two arrays. The arrays are the max ``k`` values and the\ncorresponding indices along the ``reduction_dimension`` of the input\n``operand``. The arrays' dimensions are the same as the input ``operand``\nexcept for the ``reduction_dimension``: when ``aggregate_to_topk`` is true,\nthe reduction dimension is ``k``; otherwise, it is greater equals to ``k``\nwhere the size is implementation-defined.\nWe encourage users to wrap ``approx_max_k`` with jit. See the following\nexample for maximal inner production search (MIPS):\n>>> import functools\n>>> import jax\n>>> import numpy as np\n>>> @functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\n... def mips(qy, db, k=10, recall_target=0.95):\n...   dists = jax.lax.dot(qy, db.transpose())\n...   # returns (f32[qy_size, k], i32[qy_size, k])\n...   return jax.lax.approx_max_k(dists, k=k, recall_target=recall_target)\n>>>\n>>> qy = jax.numpy.array(np.random.rand(50, 64))\n>>> db = jax.numpy.array(np.random.rand(1024, 64))\n>>> dot_products, neighbors = mips(qy, db, k=10)",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "approx_min_k",
      "signature": "approx_min_k(operand: jax.Array, k: int, reduction_dimension: int = -1, recall_target: float = 0.95, reduction_input_size_override: int = -1, aggregate_to_topk: bool = True) -> tuple[jax.Array, jax.Array]",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "See https://arxiv.org/abs/2206.14286 for the algorithm details.\nArgs:\noperand : Array to search for min-k. Must be a floating number type.\nk : Specifies the number of min-k.\nreduction_dimension: Integer dimension along which to search. Default: -1.\nrecall_target: Recall target for the approximation.\nreduction_input_size_override : When set to a positive value, it overrides\nthe size determined by ``operand[reduction_dim]`` for evaluating the\nrecall. This option is useful when the given operand is only a subset of\nthe overall computation in SPMD or distributed pipelines, where the true\ninput size cannot be deferred by the ``operand`` shape.\naggregate_to_topk : When true, aggregates approximate results to the top-k\nin sorted order. When false, returns the approximate results unsorted. In\nthis case, the number of the approximate results is implementation defined\nand is greater or equal to the specified ``k``.\nTuple of two arrays. The arrays are the least ``k`` values and the\ncorresponding indices along the ``reduction_dimension`` of the input\n``operand``.  The arrays' dimensions are the same as the input ``operand``\nexcept for the ``reduction_dimension``: when ``aggregate_to_topk`` is true,\nthe reduction dimension is ``k``; otherwise, it is greater equals to ``k``\nwhere the size is implementation-defined.\nWe encourage users to wrap ``approx_min_k`` with jit. See the following example\nfor nearest neighbor search over the squared l2 distance:\n>>> import functools\n>>> import jax\n>>> import numpy as np\n>>> @functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\n... def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):\n...   dists = half_db_norms - jax.lax.dot(qy, db.transpose())\n...   return jax.lax.approx_min_k(dists, k=k, recall_target=recall_target)\n>>>\n>>> qy = jax.numpy.array(np.random.rand(50, 64))\n>>> db = jax.numpy.array(np.random.rand(1024, 64))\n>>> half_db_norm_sq = jax.numpy.linalg.norm(db, axis=1)**2 / 2\n>>> dists, neighbors = l2_ann(qy, db, half_db_norm_sq, k=10)\nIn the example above, we compute ``db^2/2 - dot(qy, db^T)`` instead of\n``qy^2 - 2 dot(qy, db^T) + db^2`` for performance reason. The former uses less\narithmetic and produces the same set of neighbors.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "argmax",
      "signature": "argmax(operand: 'ArrayLike', axis: 'int', index_dtype: 'DTypeLike') -> 'Array'",
      "documentation": {
        "description": "Computes the index of the maximum element along ``axis``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "argmin",
      "signature": "argmin(operand: 'ArrayLike', axis: 'int', index_dtype: 'DTypeLike') -> 'Array'",
      "documentation": {
        "description": "Computes the index of the minimum element along ``axis``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "asin",
      "signature": "asin(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise arc sine: :math:`\\mathrm{asin}(x)`.\nThis function lowers directly to the ``chlo.asin`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the\nelement-wise arc sine.\nSee also:\n- :func:`jax.lax.sin`: elementwise sine.\n- :func:`jax.lax.acos`: elementwise arc cosine.\n- :func:`jax.lax.atan`: elementwise arc tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "asinh",
      "signature": "asinh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise inverse hyperbolic sine: :math:`\\mathrm{asinh}(x)`.\nThis function lowers directly to the ``chlo.asinh`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\ninverse hyperbolic sine.\nSee also:\n- :func:`jax.lax.acosh`: elementwise inverse hyperbolic cosine.\n- :func:`jax.lax.atanh`: elementwise inverse hyperbolic tangent.\n- :func:`jax.lax.sinh`: elementwise hyperbolic sine.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "associative_scan",
      "signature": "associative_scan(fn: 'Callable', elems, reverse: 'bool' = False, axis: 'int' = 0)",
      "documentation": {
        "description": "Performs a scan with an associative binary operation, in parallel.\nFor an introduction to associative scans, see [BLE1990]_.\nArgs:\nfn: A Python callable implementing an associative binary operation with\nsignature ``r = fn(a, b)``. Function `fn` must be associative, i.e., it\nmust satisfy the equation\n``fn(a, fn(b, c)) == fn(fn(a, b), c)``.\nThe inputs and result are (possibly nested Python tree structures of)\narray(s) matching ``elems``. Each array has a dimension in place\nof the ``axis`` dimension. `fn` should be applied elementwise over\nthe ``axis`` dimension (for example, by using :func:`jax.vmap` over the\nelementwise function.)\nThe result ``r`` has the same shape (and structure) as the two inputs\n``a`` and ``b``.\nelems: A (possibly nested Python tree structure of) array(s), each with\nan ``axis`` dimension of size ``num_elems``.\nreverse: A boolean stating if the scan should be reversed with respect to\nthe ``axis`` dimension.\naxis: an integer identifying the axis over which the scan should occur.",
        "parameters": {},
        "returns": "A (possibly nested Python tree structure of) array(s) of the same shape\nand structure as ``elems``, in which the ``k``'th element of ``axis`` is the\nresult of recursively applying ``fn`` to combine the first ``k`` elements\nof ``elems`` along ``axis``. For example, given ``elems = [a, b, c, ...]``,\nthe result would be ``[a, fn(a, b), fn(fn(a, b), c), ...]``.\nExample 1: partial sums of an array of numbers:\n>>> lax.associative_scan(jnp.add, jnp.arange(0, 4))\nArray([0, 1, 3, 6], dtype=int32)\nExample 2: partial products of an array of matrices\n>>> mats = jax.random.uniform(jax.random.key(0), (4, 2, 2))\n>>> partial_prods = lax.associative_scan(jnp.matmul, mats)\n>>> partial_prods.shape\n(4, 2, 2)\nExample 3: reversed partial sums of an array of numbers\n>>> lax.associative_scan(jnp.add, jnp.arange(0, 4), reverse=True)\nArray([6, 6, 5, 3], dtype=int32)\n.. [BLE1990] Blelloch, Guy E. 1990. \"Prefix Sums and Their Applications.\",\nTechnical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon\nUniversity.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "atan",
      "signature": "atan(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise arc tangent: :math:`\\mathrm{atan}(x)`.\nThis function lowers directly to the ``chlo.atan`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the\nelement-wise arc tangent.\nSee also:\n- :func:`jax.lax.tan`: elementwise tangent.\n- :func:`jax.lax.acos`: elementwise arc cosine.\n- :func:`jax.lax.asin`: elementwise arc sine.\n- :func:`jax.lax.atan2`: elementwise 2-term arc tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "atan2",
      "signature": "atan2(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise two-term arc tangent: :math:`\\mathrm{atan}({x \\over y})`.\nThis function lowers directly to the `stablehlo.atan2`_ operation.\nArgs:\nx, y: input arrays. Must have a matching floating-point or complex dtypes. If\nneither is a scalar, the two arrays must have the same number of dimensions\nand be broadcast-compatible.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` and ``y`` containing the element-wise\narc tangent of :math:`x \\over y`, respecting the quadrant indicated by the sign\nof each input.\nSee also:\n- :func:`jax.lax.tan`: elementwise tangent.\n- :func:`jax.lax.atan`: elementwise one-term arc tangent.\n.. _stablehlo.atan2: https://openxla.org/stablehlo/spec#atan2",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "atanh",
      "signature": "atanh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise inverse hyperbolic tangent: :math:`\\mathrm{atanh}(x)`.\nThis function lowers directly to the ``chlo.atanh`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\ninverse hyperbolic tangent.\nSee also:\n- :func:`jax.lax.acosh`: elementwise inverse hyperbolic cosine.\n- :func:`jax.lax.asinh`: elementwise inverse hyperbolic sine.\n- :func:`jax.lax.tanh`: elementwise hyperbolic tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "axis_index",
      "signature": "axis_index(axis_name)",
      "documentation": {
        "description": "Return the index along the mapped axis ``axis_name``.\nArgs:\naxis_name: hashable Python object used to name the mapped axis.",
        "parameters": {},
        "returns": "An integer representing the index.\nFor example, with 8 XLA devices available:\n>>> from functools import partial\n>>> @partial(jax.pmap, axis_name='i')\n... def f(_):\n...   return lax.axis_index('i')\n...\n>>> f(np.zeros(4))\nArray([0, 1, 2, 3], dtype=int32)\n>>> f(np.zeros(8))\nArray([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32)\n>>> @partial(jax.pmap, axis_name='i')\n... @partial(jax.pmap, axis_name='j')\n... def f(_):\n...   return lax.axis_index('i'), lax.axis_index('j')\n...\n>>> x, y = f(np.zeros((4, 2)))\n>>> print(x)\n[[0 0]\n[1 1]\n[2 2]\n[3 3]]\n>>> print(y)\n[[0 1]\n[0 1]\n[0 1]\n[0 1]]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "batch_matmul",
      "signature": "batch_matmul(lhs: 'Array', rhs: 'Array', precision: 'PrecisionLike' = None) -> 'Array'",
      "documentation": {
        "description": "Batch matrix multiplication.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bessel_i0e",
      "signature": "bessel_i0e(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Exponentially scaled modified Bessel function of order 0:\n:math:`\\mathrm{i0e}(x) = e^{-|x|} \\mathrm{i0}(x)`",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bessel_i1e",
      "signature": "bessel_i1e(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Exponentially scaled modified Bessel function of order 1:\n:math:`\\mathrm{i1e}(x) = e^{-|x|} \\mathrm{i1}(x)`",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "betainc",
      "signature": "betainc(a: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], b: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise regularized incomplete beta integral.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bitcast_convert_type",
      "signature": "bitcast_convert_type(operand: 'ArrayLike', new_dtype: 'DTypeLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise bitcast.\nThis function lowers directly to the `stablehlo.bitcast_convert`_ operation.\nThe output shape depends on the size of the input and output dtypes with\nthe following logic::\nif new_dtype.itemsize == operand.dtype.itemsize:\noutput_shape = operand.shape\nif new_dtype.itemsize < operand.dtype.itemsize:\noutput_shape = (*operand.shape, operand.dtype.itemsize // new_dtype.itemsize)\nif new_dtype.itemsize > operand.dtype.itemsize:\nassert operand.shape[-1] * operand.dtype.itemsize == new_dtype.itemsize\noutput_shape = operand.shape[:-1]\nArgs:\noperand: an array or scalar value to be cast\nnew_dtype: the new type. Should be a NumPy type.",
        "parameters": {},
        "returns": "An array of shape `output_shape` (see above) and type `new_dtype`,\nconstructed from the same bits as operand.\nSee also:\n- :func:`jax.lax.convert_element_type`: value-preserving dtype conversion.\n- :func:`jax.Array.view`: NumPy-style API for bitcast type conversion.\n.. _stablehlo.bitcast_convert: https://openxla.org/stablehlo/spec#bitcast_convert",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bitwise_and",
      "signature": "bitwise_and(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise AND: :math:`x \\wedge y`.\nThis function lowers directly to the `stablehlo.and`_ operation.\nArgs:\nx, y: Input arrays. Must have matching boolean or integer dtypes.\nIf neither is a scalar, ``x`` and ``y`` must have the same number\nof dimensions and be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the bitwise\nAND of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.bitwise_and`: NumPy wrapper for this API, also accessible\nvia the ``x & y`` operator on JAX arrays.\n- :func:`jax.lax.bitwise_not`: Elementwise NOT.\n- :func:`jax.lax.bitwise_or`: Elementwise OR.\n- :func:`jax.lax.bitwise_xor`: Elementwise exclusive OR.\n.. _stablehlo.and: https://openxla.org/stablehlo/spec#and",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bitwise_not",
      "signature": "bitwise_not(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise NOT: :math:`\\neg x`.\nThis function lowers directly to the `stablehlo.not`_ operation.\nArgs:\nx: Input array. Must have boolean or integer dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x`` containing the bitwise\ninversion of each entry.\nSee also:\n- :func:`jax.numpy.invert`: NumPy wrapper for this API, also accessible\nvia the ``~x`` operator on JAX arrays.\n- :func:`jax.lax.bitwise_and`: Elementwise AND.\n- :func:`jax.lax.bitwise_or`: Elementwise OR.\n- :func:`jax.lax.bitwise_xor`: Elementwise exclusive OR.\n.. _stablehlo.not: https://openxla.org/stablehlo/spec#not",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bitwise_or",
      "signature": "bitwise_or(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise OR: :math:`x \\vee y`.\nThis function lowers directly to the `stablehlo.or`_ operation.\nArgs:\nx, y: Input arrays. Must have matching boolean or integer dtypes.\nIf neither is a scalar, ``x`` and ``y`` must have the same number\nof dimensions and be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the bitwise\nOR of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.invert`: NumPy wrapper for this API, also accessible\nvia the ``x | y`` operator on JAX arrays.\n- :func:`jax.lax.bitwise_not`: Elementwise NOT.\n- :func:`jax.lax.bitwise_and`: Elementwise AND.\n- :func:`jax.lax.bitwise_xor`: Elementwise exclusive OR.\n.. _stablehlo.or: https://openxla.org/stablehlo/spec#or",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "bitwise_xor",
      "signature": "bitwise_xor(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise exclusive OR: :math:`x \\oplus y`.\nThis function lowers directly to the `stablehlo.xor`_ operation.\nArgs:\nx, y: Input arrays. Must have matching boolean or integer dtypes.\nIf neither is a scalar, ``x`` and ``y`` must have the same number\nof dimensions and be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the bitwise\nXOR of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.bitwise_xor`: NumPy wrapper for this API, also accessible\nvia the ``x ^ y`` operator on JAX arrays.\n- :func:`jax.lax.bitwise_not`: Elementwise NOT.\n- :func:`jax.lax.bitwise_and`: Elementwise AND.\n- :func:`jax.lax.bitwise_or`: Elementwise OR.\n.. _stablehlo.xor: https://openxla.org/stablehlo/spec#xor",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcast",
      "signature": "broadcast(operand: 'ArrayLike', sizes: 'Sequence[int]', out_sharding=None) -> 'Array'",
      "documentation": {
        "description": "Broadcasts an array, adding new leading dimensions\nArgs:\noperand: an array\nsizes: a sequence of integers, giving the sizes of new leading dimensions\nto add to the front of the array.",
        "parameters": {},
        "returns": "An array containing the result.",
        "raises": "",
        "see_also": "jax.lax.broadcast_in_dim : add new dimensions at any location in the array shape.",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcast_in_dim",
      "signature": "broadcast_in_dim(operand: 'ArrayLike', shape: 'Shape', broadcast_dimensions: 'Sequence[int]', out_sharding=None) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `BroadcastInDim\n<https://www.tensorflow.org/xla/operation_semantics#broadcastindim>`_\noperator.\nArgs:\noperand: an array\nshape: the shape of the target array\nbroadcast_dimensions: to which dimension in the target shape each dimension\nof the operand shape corresponds to.  That is, dimension i of the operand\nbecomes dimension broadcast_dimensions[i] of the result.",
        "parameters": {},
        "returns": "An array containing the result.",
        "raises": "",
        "see_also": "jax.lax.broadcast : simpler interface to add new leading dimensions.",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcast_shapes",
      "signature": "broadcast_shapes(*shapes)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "This follows the rules of `NumPy broadcasting`_.\nArgs:\nshapes: one or more tuples of integers containing the shapes of arrays\nto be broadcast.\nA tuple of integers representing the broadcasted shape.",
        "raises": "ValueError: if shapes are not broadcast-compatible.",
        "see_also": "- :func:`jax.numpy.broadcast_shapes`: similar API in the JAX NumPy namespace",
        "notes": "",
        "examples": "Some examples of broadcasting compatible shapes:\n>>> jnp.broadcast_shapes((1,), (4,))\n(4,)\n>>> jnp.broadcast_shapes((3, 1), (4,))\n(3, 4)\n>>> jnp.broadcast_shapes((3, 1), (1, 4), (5, 1, 1))\n(5, 3, 4)\nError when attempting to broadcast incompatible shapes:\n>>> jnp.broadcast_shapes((3, 1), (4, 1))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Incompatible shapes for broadcasting: shapes=[(3, 1), (4, 1)]\n.. _NumPy broadcasting: https://numpy.org/doc/stable/user/basics.broadcasting.html"
      }
    },
    {
      "name": "broadcast_to_rank",
      "signature": "broadcast_to_rank(x: 'ArrayLike', rank: 'int') -> 'Array'",
      "documentation": {
        "description": "Adds leading dimensions of ``1`` to give ``x`` rank ``rank``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "broadcasted_iota",
      "signature": "broadcasted_iota(dtype: 'DTypeLike', shape: 'Shape', dimension: 'int', out_sharding=None) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around ``iota``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cbrt",
      "signature": "cbrt(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise cube root: :math:`\\sqrt[3]{x}`.\nThis function lowers directly to the `stablehlo.cbrt`_ operation.\nArgs:\nx: Input array. Must have floating or complex dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x`` containing the cube root.\nSee also:\n:func:`jax.lax.pow`: Elementwise power.\n:func:`jax.lax.sqrt`: Elementwise square root.\n:func:`jax.lax.rsqrt`: Elementwise reciporical square root.\n.. _stablehlo.cbrt: https://openxla.org/stablehlo/spec#cbrt",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ceil",
      "signature": "ceil(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise ceiling: :math:`\\left\\lceil x \\right\\rceil`.\nThis function lowers directly to the `stablehlo.ceil`_ operation.\nArgs:\nx: input array. Must have floating-point type.",
        "parameters": {},
        "returns": "Array of same shape and dtype as ``x``, containing values rounded\nto the next integer toward positive infinity.\nSee also:\n- :func:`jax.lax.floor`: round to the next integer toward negative infinity\n- :func:`jax.lax.round`: round to the nearest integer\n.. _stablehlo.ceil: https://openxla.org/stablehlo/spec#ceil",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "clamp",
      "signature": "clamp(min: 'ArrayLike', x: 'ArrayLike', max: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise clamp.",
        "parameters": {},
        "returns": "\\mathit{min} & \\text{if } x < \\mathit{min},\\\\\n\\mathit{max} & \\text{if } x > \\mathit{max},\\\\\nx & \\text{otherwise}\n\\end{cases}`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "clz",
      "signature": "clz(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise count-leading-zeros.\nThis function lowers directly to the `stablehlo.count_leading_zeros`_ operation.\nArgs:\nx: Input array. Must have integer dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x``, containing the number of\nset bits in the input.\nSee also:\n- :func:`jax.lax.population_count`: Count the number of set bits in each element.\n.. _stablehlo.count_leading_zeros: https://openxla.org/stablehlo/spec#count_leading_zeros",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "collapse",
      "signature": "collapse(operand: 'Array', start_dimension: 'int', stop_dimension: 'int | None' = None) -> 'Array'",
      "documentation": {
        "description": "Collapses dimensions of an array into a single dimension.\nFor example, if ``operand`` is an array with shape ``[2, 3, 4]``,\n``collapse(operand, 0, 2).shape == [6, 4]``. The elements of the collapsed\ndimension are laid out major-to-minor, i.e., with the lowest-numbered\ndimension as the slowest varying dimension.\nArgs:\noperand: an input array.\nstart_dimension: the start of the dimensions to collapse (inclusive).\nstop_dimension: the end of the dimensions to collapse (exclusive). Pass None\nto collapse all the dimensions after start.",
        "parameters": {},
        "returns": "An array where dimensions ``[start_dimension, stop_dimension)`` have been\ncollapsed (raveled) into a single dimension.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "complex",
      "signature": "complex(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise make complex number: :math:`x + jy`.\nThis function lowers directly to the `stablehlo.complex`_ operation.\nArgs:\nx, y: input arrays. Must have matching floating-point dtypes. If\nneither is a scalar, the two arrays must have the same number\nof dimensions and be broadcast-compatible.",
        "parameters": {},
        "returns": "The complex array with the real part given by ``x``, and the imaginary\npart given by ``y``. For inputs of dtype float32 or float64, the result\nwill have dtype complex64 or complex128 respectively.\nSee also:\n- :func:`jax.lax.real`: elementwise extract real part.\n- :func:`jax.lax.imag`: elementwise extract imaginary part.\n- :func:`jax.lax.conj`: elementwise complex conjugate.\n.. _stablehlo.complex: https://openxla.org/stablehlo/spec#complex",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "composite",
      "signature": "composite(decomposition: 'Callable', name: 'str', version: 'int' = 0)",
      "documentation": {
        "description": "Composite with semantics defined by the decomposition function.\nA composite is a higher-order JAX function that encapsulates an operation made\nup (composed) of other JAX functions. The semantics of the op are implemented\nby the ``decomposition`` function. In other words, the defined composite\nfunction can be replaced with its decomposed implementation without changing\nthe semantics of the encapsulated operation.\nThe compiler can recognize specific composite operations by their ``name``,\n``version``, ``kwargs``, and dtypes to emit more efficient code, potentially\nleveraging hardware-specific instructions or optimizations. If the compiler\ndoesn't recognize the composite, it falls back to compiling the\n``decomposition`` function.\nConsider a \"tangent\" composite operation. Its ``decomposition`` function could\nbe implemented as ``sin(x) / cos(x)``. A hardware-aware compiler could\nrecognize the \"tangent\" composite and emit a single ``tangent`` instruction\ninstead of three separate instructions (``sin``, ``divide``, and ``cos``).\nFor hardware without dedicated tangent support, it would fall back to\ncompiling the decomposition.\nThis is useful for preserving high-level abstractions that would otherwise be\nlost while lowering, which allows for easier pattern-matching in low-level IR.\nArgs:\ndecomposition: function that implements the semantics of the composite op.\nname: name of the encapsulated operation.\nversion: optional int to indicate semantic changes to the composite.",
        "parameters": {},
        "returns": "Callable: Returns a composite function. Note that positional arguments to\nthis function should be interpreted as inputs and keyword arguments should\nbe interpreted as attributes of the op. Any keyword arguments that are\npassed with ``None`` as a value will be omitted from the\n``composite_attributes``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Tangent kernel:\n>>> def my_tangent_composite(x):\n...   return lax.composite(\n...     lambda x: lax.sin(x) / lax.cos(x), name=\"my.tangent\"\n...   )(x)\n>>>\n>>> pi = jnp.pi\n>>> x = jnp.array([0.0, pi / 4, 3 * pi / 4, pi])\n>>> with jnp.printoptions(precision=3, suppress=True):\n...   print(my_tangent_composite(x))\n...   print(lax.tan(x))\n[ 0.  1. -1.  0.]\n[ 0.  1. -1.  0.]\nThe recommended way to create composites is via a decorator. Use ``/`` and\n``*`` in the function signature to be explicit about positional and keyword\narguments, respectively:\n>>> @partial(lax.composite, name=\"my.softmax\")\n... def my_softmax_composite(x, /, *, axis):\n...   return jax.nn.softmax(x, axis)"
      }
    },
    {
      "name": "concatenate",
      "signature": "concatenate(operands: 'Array | Sequence[ArrayLike]', dimension: 'int') -> 'Array'",
      "documentation": {
        "description": "Concatenates a sequence of arrays along `dimension`.\nWraps XLA's `Concatenate\n<https://www.tensorflow.org/xla/operation_semantics#concatenate>`_\noperator.\nArgs:\noperands: a sequence of arrays to concatenate. The arrays must have equal\nshapes, except in the `dimension` axis.\ndimension: the dimension along which to concatenate the arrays.",
        "parameters": {},
        "returns": "An array containing the concatenation.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cond",
      "signature": "_cond(pred, true_fun: 'Callable', false_fun: 'Callable', *operands, operand=<object object at 0x7b2236d7b5d0>)",
      "documentation": {
        "description": "Conditionally apply ``true_fun`` or ``false_fun``.\nWraps XLA's `Conditional\n<https://www.tensorflow.org/xla/operation_semantics#conditional>`_\noperator.\nProvided arguments are correctly typed, ``cond()`` has equivalent\nsemantics to this Python implementation, where ``pred`` must be a\nscalar type::\ndef cond(pred, true_fun, false_fun, *operands):\nif pred:\nreturn true_fun(*operands)\nelse:\nreturn false_fun(*operands)\nIn contrast with :func:`jax.lax.select`, using ``cond`` indicates that only one of\nthe two branches is executed (up to compiler rewrites and optimizations).\nHowever, when transformed with :func:`~jax.vmap` to operate over a batch of\npredicates, ``cond`` is converted to :func:`~jax.lax.select`.\nBoth branches will be traced in all cases (see :ref:`Key concepts: tracing <key-concepts-tracing>`\nfor a discussion of JAX's tracing model).\nArgs:\npred: Boolean scalar type, indicating which branch function to apply.\ntrue_fun: Function (A -> B), to be applied if ``pred`` is True.\nfalse_fun: Function (A -> B), to be applied if ``pred`` is False.\noperands: Operands (A) input to either branch depending on ``pred``. The\ntype can be a scalar, array, or any pytree (nested Python tuple/list/dict)\nthereof.",
        "parameters": {},
        "returns": "Value (B) of either ``true_fun(*operands)`` or ``false_fun(*operands)``,\ndepending on the value of ``pred``. The type can be a scalar, array, or any\npytree (nested Python tuple/list/dict) thereof.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conj",
      "signature": "conj(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise complex conjugate function: :math:`\\overline{x}`.\nThis function lowers to a combination of `stablehlo.real`_, `stablehlo.imag`_,\nand  `stablehlo.complex`_.\nArgs:\nx: input array. Must have complex dtype.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing its complex conjugate.\nSee also:\n- :func:`jax.lax.complex`: elementwise construct complex number.\n- :func:`jax.lax.real`: elementwise extract real part.\n- :func:`jax.lax.imag`: elementwise extract imaginary part.\n- :func:`jax.lax.abs`: elementwise absolute value / complex magnitude.\n.. _stablehlo.real: https://openxla.org/stablehlo/spec#real\n.. _stablehlo.imag: https://openxla.org/stablehlo/spec#imag\n.. _stablehlo.complex: https://openxla.org/stablehlo/spec#complex",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv",
      "signature": "conv(lhs: 'Array', rhs: 'Array', window_strides: 'Sequence[int]', padding: 'str', precision: 'lax.PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around `conv_general_dilated`.\nArgs:\nlhs: a rank `n+2` dimensional input array.\nrhs: a rank `n+2` dimensional array of kernel weights.\nwindow_strides: a sequence of `n` integers, representing the inter-window\nstrides.\npadding: either the string `'SAME'`, the string `'VALID'`.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n:class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``.\npreferred_element_type: Optional. Either ``None``, which means the default\naccumulation type for the input types, or a datatype, indicating to\naccumulate results to and return a result with that datatype.",
        "parameters": {},
        "returns": "An array containing the convolution result.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_dimension_numbers",
      "signature": "conv_dimension_numbers(lhs_shape, rhs_shape, dimension_numbers) -> 'ConvDimensionNumbers'",
      "documentation": {
        "description": "Converts convolution `dimension_numbers` to a `ConvDimensionNumbers`.\nArgs:\nlhs_shape: tuple of nonnegative integers, shape of the convolution input.\nrhs_shape: tuple of nonnegative integers, shape of the convolution kernel.\ndimension_numbers: None or a tuple/list of strings or a ConvDimensionNumbers\nobject.",
        "parameters": {},
        "returns": "A `ConvDimensionNumbers` object that represents `dimension_numbers` in the\ncanonical form used by lax functions.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_general_dilated",
      "signature": "conv_general_dilated(lhs: 'Array', rhs: 'Array', window_strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', lhs_dilation: 'Sequence[int] | None' = None, rhs_dilation: 'Sequence[int] | None' = None, dimension_numbers: 'ConvGeneralDilatedDimensionNumbers' = None, feature_group_count: 'int' = 1, batch_group_count: 'int' = 1, precision: 'lax.PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
      "documentation": {
        "description": "General n-dimensional convolution operator, with optional dilation.\nWraps XLA's `Conv\n<https://www.tensorflow.org/xla/operation_semantics#conv_convolution>`_\noperator.\nArgs:\nlhs: a rank `n+2` dimensional input array.\nrhs: a rank `n+2` dimensional array of kernel weights.\nwindow_strides: a sequence of `n` integers, representing the inter-window\nstrides.\npadding: either the strings `'SAME'`, `'SAME_LOWER'`, or `'VALID'`, or a\nsequence of `n` `(low, high)` integer pairs that give the padding to apply\nbefore and after each spatial dimension. `'SAME'` and `'SAME_LOWER'` add\npadding to produce same output size as the input. The padding is split\nbetween the two sides equally or almost equally. In case the padding is an\nodd number, the extra padding is added at the end for `'SAME'` and at the\nbeginning for `'SAME_LOWER'`.\nlhs_dilation: `None`, or a sequence of `n` integers, giving the dilation\nfactor to apply in each spatial dimension of `lhs`. LHS dilation is also\nknown as transposed convolution.\nrhs_dilation: `None`, or a sequence of `n` integers, giving the dilation\nfactor to apply in each spatial dimension of `rhs`. RHS dilation is also\nknown as atrous convolution.\ndimension_numbers: either `None`, a ``ConvDimensionNumbers`` object, or a\n3-tuple ``(lhs_spec, rhs_spec, out_spec)``, where each element is a string\nof length `n+2`.\nfeature_group_count: integer, default 1. See XLA HLO docs.\nbatch_group_count: integer, default 1. See XLA HLO docs.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, a :class:`~jax.lax.Precision` enum value\n(``Precision.DEFAULT``, ``Precision.HIGH`` or ``Precision.HIGHEST``), a\nstring (e.g. 'highest' or 'fastest', see the\n``jax.default_matmul_precision`` context manager), or a tuple of two\n:class:`~jax.lax.Precision` enums or strings indicating precision of\n``lhs`` and ``rhs``.\npreferred_element_type: Optional. Either ``None``, which means the default\naccumulation type for the input types, or a datatype, indicating to\naccumulate results to and return a result with that datatype.",
        "parameters": {},
        "returns": "An array containing the convolution result.\nIn the string case of ``dimension_numbers``, each character identifies by\nposition:\n- the batch dimensions in ``lhs``, ``rhs``, and the output with the character\n'N',\n- the feature dimensions in `lhs` and the output with the character 'C',\n- the input and output feature dimensions in rhs with the characters 'I'\nand 'O' respectively, and\n- spatial dimension correspondences between lhs, rhs, and the output using\nany distinct characters. The examples below use 'W' and 'H'.\nFor example, to indicate dimension numbers consistent with the ``conv``\nfunction with two spatial dimensions, one could use ``('NCHW', 'OIHW',\n'NCHW')``. As another example, to indicate dimension numbers consistent with\nthe TensorFlow Conv2D operation, one could use ``('NHWC', 'HWIO', 'NHWC')``.\nWhen using the latter form of convolution dimension specification, window\nstrides are associated with spatial dimension character labels according to\nthe order in which the labels appear in the ``rhs_spec`` string, so that\n``window_strides[0]`` is matched with the dimension corresponding to the first\ncharacter appearing in rhs_spec that is not ``'I'`` or ``'O'``.\nIf ``dimension_numbers`` is ``None``, the default is ``('NCHW', 'OIHW',\n'NCHW')`` (for a 2D convolution).",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_general_dilated_local",
      "signature": "conv_general_dilated_local(lhs: 'ArrayLike', rhs: 'ArrayLike', window_strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', filter_shape: 'Sequence[int]', lhs_dilation: 'Sequence[int] | None' = None, rhs_dilation: 'Sequence[int] | None' = None, dimension_numbers: 'convolution.ConvGeneralDilatedDimensionNumbers | None' = None, precision: 'lax.PrecisionLike' = None) -> 'Array'",
      "documentation": {
        "description": "General n-dimensional unshared convolution operator with optional dilation.\nAlso known as locally connected layer, the operation is equivalent to\nconvolution with a separate (unshared) `rhs` kernel used at each output\nspatial location. Docstring below adapted from `jax.lax.conv_general_dilated`.",
        "parameters": {},
        "returns": "An array containing the unshared convolution result.\nIn the string case of `dimension_numbers`, each character identifies by\nposition:\n- the batch dimensions in `lhs`, `rhs`, and the output with the character\n'N',\n- the feature dimensions in `lhs` and the output with the character 'C',\n- the input and output feature dimensions in rhs with the characters 'I'\nand 'O' respectively, and\n- spatial dimension correspondences between `lhs`, `rhs`, and the output using\nany distinct characters. The examples below use 'W' and 'H'.\nFor example, to indicate dimension numbers consistent with the `conv` function\nwith two spatial dimensions, one could use `('NCHW', 'OIHW', 'NCHW')`. As\nanother example, to indicate dimension numbers consistent with the TensorFlow\nConv2D operation, one could use `('NHWC', 'HWIO', 'NHWC')`. When using the\nlatter form of convolution dimension specification, window strides are\nassociated with spatial dimension character labels according to the order in\nwhich the labels appear in the `rhs_spec` string, so that `window_strides[0]`\nis matched with the dimension corresponding to the first character\nappearing in rhs_spec that is not `'I'` or `'O'`.\nIf `dimension_numbers` is `None`, the default is `('NCHW', 'OIHW', 'NCHW')`\n(for a 2D convolution).",
        "raises": "",
        "see_also": "https://www.tensorflow.org/xla/operation_semantics#conv_convolution\nArgs:\nlhs: a rank `n+2` dimensional input array.\nrhs: a rank `n+2` dimensional array of kernel weights. Unlike in regular\nCNNs, its spatial coordinates (`H`, `W`, ...) correspond to output spatial\nlocations, while input spatial locations are fused with the input channel\nlocations in the single `I` dimension, in the order of\n`\"C\" + ''.join(c for c in rhs_spec if c not in 'OI')`, where\n`rhs_spec = dimension_numbers[1]`. For example, if `rhs_spec == \"WHIO\",\nthe unfolded kernel shape is\n`\"[output W][output H]{I[receptive window W][receptive window H]}O\"`.\nwindow_strides: a sequence of `n` integers, representing the inter-window\nstrides.\npadding: either the string `'SAME'`, the string `'VALID'`, or a sequence of\n`n` `(low, high)` integer pairs that give the padding to apply before and\nafter each spatial dimension.\nfilter_shape: a sequence of `n` integers, representing the receptive window\nspatial shape in the order as specified in\n`rhs_spec = dimension_numbers[1]`.\nlhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `lhs`. LHS dilation\nis also known as transposed convolution.\nrhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each input spatial dimension of `rhs`.\nRHS dilation is also known as atrous convolution.\ndimension_numbers: either `None`, a `ConvDimensionNumbers` object, or\na 3-tuple `(lhs_spec, rhs_spec, out_spec)`, where each element is a string\nof length `n+2`.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, a ``lax.Precision`` enum value (``Precision.DEFAULT``,\n``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n``lax.Precision`` enums indicating precision of ``lhs``` and ``rhs``.",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_general_dilated_patches",
      "signature": "conv_general_dilated_patches(lhs: 'ArrayLike', filter_shape: 'Sequence[int]', window_strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', lhs_dilation: 'Sequence[int] | None' = None, rhs_dilation: 'Sequence[int] | None' = None, dimension_numbers: 'convolution.ConvGeneralDilatedDimensionNumbers | None' = None, precision: 'lax.Precision | None' = None, preferred_element_type: 'DType | None' = None) -> 'Array'",
      "documentation": {
        "description": "Extract patches subject to the receptive field of `conv_general_dilated`.\nRuns the input through a convolution with given parameters. The kernel of the\nconvolution is constructed such that the output channel dimension `\"C\"`\ncontains flattened image patches, so instead a single `\"C\"` dimension\nrepresents, for example, three dimensions `\"chw\"` collapsed. The order of\nthese dimensions is `\"c\" + ''.join(c for c in rhs_spec if c not in 'OI')`,\nwhere `rhs_spec == dimension_numbers[1]`, and the size of this `\"C\"`\ndimension is therefore the size of each patch, i.e.\n`np.prod(filter_shape) * lhs.shape[lhs_spec.index('C')]`, where\n`lhs_spec == dimension_numbers[0]`.\nDocstring below adapted from `jax.lax.conv_general_dilated`.",
        "parameters": {},
        "returns": "A rank `n+2` array containing the flattened image patches in the output\nchannel (`\"C\"`) dimension. For example if\n`dimension_numbers = (\"NcHW\", \"OIwh\", \"CNHW\")`, the output has dimension\nnumbers `\"CNHW\" = \"{cwh}NHW\"`, with the size of dimension `\"C\"` equal to\nthe size of each patch\n(`np.prod(filter_shape) * lhs.shape[lhs_spec.index('C')]`).",
        "raises": "",
        "see_also": "https://www.tensorflow.org/xla/operation_semantics#conv_convolution\nArgs:\nlhs: a rank `n+2` dimensional input array.\nfilter_shape: a sequence of `n` integers, representing the receptive window\nspatial shape in the order as specified in\n`rhs_spec = dimension_numbers[1]`.\nwindow_strides: a sequence of `n` integers, representing the inter-window\nstrides.\npadding: either the string `'SAME'`, the string `'VALID'`, or a sequence of\n`n` `(low, high)` integer pairs that give the padding to apply before and\nafter each spatial dimension.\nlhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `lhs`. LHS dilation\nis also known as transposed convolution.\nrhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `rhs`. RHS dilation\nis also known as atrous convolution.\ndimension_numbers: either `None`, or a 3-tuple\n`(lhs_spec, rhs_spec, out_spec)`, where each element is a string\nof length `n+2`. `None` defaults to `(\"NCHWD..., OIHWD..., NCHWD...\")`.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, or a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n``Precision.HIGH`` or ``Precision.HIGHEST``).\npreferred_element_type: Optional. Either ``None``, which means the default\naccumulation type for the input types, or a datatype, indicating to\naccumulate results to and return a result with that datatype.",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_general_permutations",
      "signature": "conv_general_permutations(dimension_numbers)",
      "documentation": {
        "description": "Utility for convolution dimension permutations relative to Conv HLO.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_general_shape_tuple",
      "signature": "conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding, dimension_numbers)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_shape_tuple",
      "signature": "conv_shape_tuple(lhs_shape, rhs_shape, strides, pads, batch_group_count=1)",
      "documentation": {
        "description": "Compute the shape tuple of a conv given input shapes in canonical order.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_transpose",
      "signature": "conv_transpose(lhs: 'Array', rhs: 'Array', strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', rhs_dilation: 'Sequence[int] | None' = None, dimension_numbers: 'ConvGeneralDilatedDimensionNumbers' = None, transpose_kernel: 'bool' = False, precision: 'lax.PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper for calculating the N-d convolution \"transpose\".\nThis function directly calculates a fractionally strided conv rather than\nindirectly calculating the gradient (transpose) of a forward convolution.\nArgs:\nlhs: a rank `n+2` dimensional input array.\nrhs: a rank `n+2` dimensional array of kernel weights.\nstrides: sequence of `n` integers, sets fractional stride.\npadding: 'SAME', 'VALID' will set as transpose of corresponding forward\nconv, or a sequence of `n` integer 2-tuples describing before-and-after\npadding for each `n` spatial dimension.\nrhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `rhs`. RHS dilation\nis also known as atrous convolution.\ndimension_numbers: tuple of dimension descriptors as in\nlax.conv_general_dilated. Defaults to tensorflow convention.\ntranspose_kernel: if True flips spatial axes and swaps the input/output\nchannel axes of the kernel. This makes the output of this function identical\nto the gradient-derived functions like keras.layers.Conv2DTranspose\napplied to the same kernel. For typical use in neural nets this is completely\npointless and just makes input/output channel specification confusing.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n:class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``.\npreferred_element_type: Optional. Either ``None``, which means the default\naccumulation type for the input types, or a datatype, indicating to\naccumulate results to and return a result with that datatype.",
        "parameters": {},
        "returns": "Transposed N-d convolution, with output padding following the conventions of\nkeras.layers.Conv2DTranspose.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_transpose_shape_tuple",
      "signature": "conv_transpose_shape_tuple(lhs_shape, rhs_shape, window_strides, padding, dimension_numbers)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "conv_with_general_padding",
      "signature": "conv_with_general_padding(lhs: 'Array', rhs: 'Array', window_strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', lhs_dilation: 'Sequence[int] | None', rhs_dilation: 'Sequence[int] | None', precision: 'lax.PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around `conv_general_dilated`.\nArgs:\nlhs: a rank `n+2` dimensional input array.\nrhs: a rank `n+2` dimensional array of kernel weights.\nwindow_strides: a sequence of `n` integers, representing the inter-window\nstrides.\npadding: either the string `'SAME'`, the string `'VALID'`, or a sequence of\n`n` `(low, high)` integer pairs that give the padding to apply before and\nafter each spatial dimension.\nlhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `lhs`. LHS dilation\nis also known as transposed convolution.\nrhs_dilation: `None`, or a sequence of `n` integers, giving the\ndilation factor to apply in each spatial dimension of `rhs`. RHS dilation\nis also known as atrous convolution.\nprecision: Optional. Either ``None``, which means the default precision for\nthe backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n:class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``.\npreferred_element_type: Optional. Either ``None``, which means the default\naccumulation type for the input types, or a datatype, indicating to\naccumulate results to and return a result with that datatype.",
        "parameters": {},
        "returns": "An array containing the convolution result.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "convert_element_type",
      "signature": "convert_element_type(operand: 'ArrayLike', new_dtype: 'DTypeLike | dtypes.ExtendedDType') -> 'Array'",
      "documentation": {
        "description": "Elementwise cast.\nThis function lowers directly to the `stablehlo.convert`_ operation, which\nperforms an elementwise conversion from one type to another, similar to a\nC++ ``static_cast``.\nArgs:\noperand: an array or scalar value to be cast.\nnew_dtype: a dtype-like object (e.g. a :class:`numpy.dtype`, a scalar type,\nor a valid dtype name) representing the target dtype.",
        "parameters": {},
        "returns": "An array with the same shape as ``operand``, cast elementwise to ``new_dtype``.\n.. note::\nIf ``new_dtype`` is a 64-bit type and `x64 mode`_ is not enabled,\nthe appropriate 32-bit type will be used in its place.\nIf the input is a JAX array and the input dtype and output dtype match, then\nthe input array will be returned unmodified.\nSee also:\n- :func:`jax.numpy.astype`: NumPy-style dtype casting API.\n- :meth:`jax.Array.astype`: dtype casting as an array method.\n- :func:`jax.lax.bitcast_convert_type`: cast bits directly to a new dtype.\n.. _stablehlo.convert: https://openxla.org/stablehlo/spec#convert\n.. _x64 mode: https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cos",
      "signature": "cos(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise cosine: :math:`\\mathrm{cos}(x)`.\nFor floating-point inputs, this function lowers directly to the\n`stablehlo.cosine`_ operation. For complex inputs, it lowers to a\nsequence of HLO operations implementing the complex cosine.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\ncosine.\nSee also:\n- :func:`jax.lax.sin`: elementwise sine.\n- :func:`jax.lax.tan`: elementwise tangent.\n- :func:`jax.lax.acos`: elementwise arc cosine.\n.. _stablehlo.cosine: https://openxla.org/stablehlo/spec#cosine",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cosh",
      "signature": "cosh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise hyperbolic cosine: :math:`\\mathrm{cosh}(x)`.\nThis function lowers directly to the ``chlo.cosh`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nhyperbolic cosine.\nSee also:\n- :func:`jax.lax.acosh`: elementwise inverse hyperbolic cosine.\n- :func:`jax.lax.sinh`: elementwise hyperbolic sine.\n- :func:`jax.lax.tanh`: elementwise hyperbolic tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "create_token",
      "signature": "create_token(_=None)",
      "documentation": {
        "description": "Creates an XLA token value with no preconditions for sequencing effects.\nExperimental.\nThe argument is ignored. It exists for backward compatibility.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cumlogsumexp",
      "signature": "cumlogsumexp(operand: 'Array', axis: 'int' = 0, reverse: 'bool' = False) -> 'Array'",
      "documentation": {
        "description": "Computes a cumulative logsumexp along `axis`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cummax",
      "signature": "cummax(operand: 'Array', axis: 'int' = 0, reverse: 'bool' = False) -> 'Array'",
      "documentation": {
        "description": "Computes a cumulative maximum along `axis`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cummin",
      "signature": "cummin(operand: 'Array', axis: 'int' = 0, reverse: 'bool' = False) -> 'Array'",
      "documentation": {
        "description": "Computes a cumulative minimum along `axis`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cumprod",
      "signature": "cumprod(operand: 'Array', axis: 'int' = 0, reverse: 'bool' = False) -> 'Array'",
      "documentation": {
        "description": "Computes a cumulative product along `axis`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "cumsum",
      "signature": "cumsum(operand: 'Array', axis: 'int' = 0, reverse: 'bool' = False) -> 'Array'",
      "documentation": {
        "description": "Computes a cumulative sum along `axis`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "custom_linear_solve",
      "signature": "custom_linear_solve(matvec: Callable, b: Any, solve: Callable[[Callable, Any], Any], transpose_solve: Optional[Callable[[Callable, Any], Any]] = None, symmetric=False, has_aux=False)",
      "documentation": {
        "description": "Perform a matrix-free linear solve with implicitly defined gradients.\nThis function allows for overriding or defining gradients for a linear\nsolve directly via implicit differentiation at the solution, rather than by\ndifferentiating *through* the solve operation. This can sometimes be much faster\nor more numerically stable, or differentiating through the solve operation\nmay not even be implemented (e.g., if ``solve`` uses ``lax.while_loop``).\nRequired invariant::\nx = solve(matvec, b)  # solve the linear equation\nassert matvec(x) == b  # not checked\nArgs:\nmatvec: linear function to invert. Must be differentiable.\nb: constant right handle side of the equation. May be any nested structure\nof arrays.\nsolve: higher level function that solves for solution to the linear\nequation, i.e., ``solve(matvec, x) == x`` for all ``x`` of the same form\nas ``b``. This function need not be differentiable.\ntranspose_solve: higher level function for solving the transpose linear\nequation, i.e., ``transpose_solve(vecmat, x) == x``, where ``vecmat`` is\nthe transpose of the linear map ``matvec`` (computed automatically with\nautodiff). Required for backwards mode automatic differentiation, unless\n``symmetric=True``, in which case ``solve`` provides the default value.\nsymmetric: bool indicating if it is safe to assume the linear map\ncorresponds to a symmetric matrix, i.e., ``matvec == vecmat``.\nhas_aux: bool indicating whether the ``solve`` and ``transpose_solve`` functions\nreturn auxiliary data like solver diagnostics as a second argument.",
        "parameters": {},
        "returns": "Result of ``solve(matvec, b)``, with gradients defined assuming that the\nsolution ``x`` satisfies the linear equation ``matvec(x) == b``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "custom_root",
      "signature": "custom_root(f: Callable, initial_guess: Any, solve: Callable[[Callable, Any], Any], tangent_solve: Callable[[Callable, Any], Any], has_aux=False)",
      "documentation": {
        "description": "Differentiably solve for the roots of a function.\nThis is a low-level routine, mostly intended for internal use in JAX.\nGradients of custom_root() are defined with respect to closed-over variables\nfrom the provided function ``f`` via the implicit function theorem:\nhttps://en.wikipedia.org/wiki/Implicit_function_theorem\nArgs:\nf: function for which to find a root. Should accept a single argument,\nreturn a tree of arrays with the same structure as its input.\ninitial_guess: initial guess for a zero of f.\nsolve: function to solve for the roots of f. Should take two positional\narguments, f and initial_guess, and return a solution with the same\nstructure as initial_guess such that func(solution) = 0. In other words,\nthe following is assumed to be true (but not checked)::\nsolution = solve(f, initial_guess)\nerror = f(solution)\nassert all(error == 0)\ntangent_solve: function to solve the tangent system. Should take two\npositional arguments, a linear function ``g`` (the function ``f``\nlinearized at its root) and a tree of array(s) ``y`` with the same\nstructure as initial_guess, and return a solution ``x`` such that\n``g(x)=y``:\n- For scalar ``y``, use ``lambda g, y: y / g(1.0)``.\n- For vector ``y``, you could use a linear solve with the Jacobian, if\ndimensionality of ``y`` is not too large:\n``lambda g, y: np.linalg.solve(jacobian(g)(y), y)``.\nhas_aux: bool indicating whether the ``solve`` function returns\nauxiliary data like solver diagnostics as a second argument.",
        "parameters": {},
        "returns": "The result of calling solve(f, initial_guess) with gradients defined via\nimplicit differentiation assuming ``f(solve(f, initial_guess)) == 0``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "digamma",
      "signature": "digamma(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise digamma: :math:`\\psi(x)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "div",
      "signature": "div(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise division: :math:`x \\over y`.\nThis function lowers directly to the `stablehlo.divide`_ operation.\nInteger division overflow (division by zero or signed division of\nINT_SMIN with -1) produces an implementation defined value.\nArgs:\nx, y: Input arrays. Must have matching numerical dtypes. If neither\nis a scalar, ``x`` and ``y`` must have the same number of dimensions\nand be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the quotient\nof each pair of broadcasted entries. For integer inputs, any fractional\npart is discarded.\nSee also:\n- :func:`jax.numpy.divide`: NumPy-style true division supporting\ninputs with mixed dtypes and ranks.\n- :func:`jax.numpy.floor_divide`: NumPy-style floor division supporting\ninputs with mixed dtypes and ranks.\n.. _stablehlo.divide: https://openxla.org/stablehlo/spec#divide",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "dot",
      "signature": "dot(lhs: 'Array', rhs: 'Array', precision: 'PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'",
      "documentation": {
        "description": "Vector/vector, matrix/vector, and matrix/matrix multiplication.\nWraps XLA's `Dot <https://www.tensorflow.org/xla/operation_semantics#dot>`_\noperator.\nFor more general contraction, see the :func:`jax.lax.dot_general` operator.\nArgs:\nlhs: an array of dimension 1 or 2.\nrhs: an array of dimension 1 or 2.\nprecision: Optional. This parameter controls the numerics of the\ncomputation, and it can be one of the following:\n- ``None``, which means the default precision for the current backend,\n- a :class:`~jax.lax.Precision` enum value or a tuple of two\n:class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and\n``rhs``, or\n- a :class:`~jax.lax.DotAlgorithm` or a\n:class:`~jax.lax.DotAlgorithmPreset` indicating the algorithm that\nmust be used to accumulate the dot product.\npreferred_element_type: Optional. This parameter controls the data type\noutput by the dot product. By default, the output element type of this\noperation will match the ``lhs`` and ``rhs`` input element types under\nthe usual type promotion rules. Setting ``preferred_element_type`` to a\nspecific ``dtype`` will mean that the operation returns that element type.\nWhen ``precision`` is not a :class:`~jax.lax.DotAlgorithm` or\n:class:`~jax.lax.DotAlgorithmPreset`, ``preferred_element_type`` provides\na hint to the compiler to accumulate the dot product using this data type.",
        "parameters": {},
        "returns": "An array containing the product.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "dot_general",
      "signature": "dot_general(lhs: 'ArrayLike', rhs: 'ArrayLike', dimension_numbers: 'DotDimensionNumbers', precision: 'PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None, out_sharding=None) -> 'Array'",
      "documentation": {
        "description": "General dot product/contraction operator.\nWraps XLA's `DotGeneral\n<https://www.tensorflow.org/xla/operation_semantics#dotgeneral>`_\noperator.\nThe semantics of ``dot_general`` are complicated, but most users should not have to\nuse it directly. Instead, you can use higher-level functions like :func:`jax.numpy.dot`,\n:func:`jax.numpy.matmul`, :func:`jax.numpy.tensordot`, :func:`jax.numpy.einsum`,\nand others which will construct appropriate calls to ``dot_general`` under the hood.\nIf you really want to understand ``dot_general`` itself, we recommend reading XLA's\n`DotGeneral  <https://www.tensorflow.org/xla/operation_semantics#dotgeneral>`_\noperator documentation.\nArgs:\nlhs: an array\nrhs: an array\ndimension_numbers: a tuple of tuples of sequences of ints of the form\n``((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims,\nrhs_batch_dims))``\nprecision: Optional. This parameter controls the numerics of the\ncomputation, and it can be one of the following:\n- ``None``, which means the default precision for the current backend,\n- a :class:`~jax.lax.Precision` enum value or a tuple of two\n:class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and\n``rhs``, or\n- a :class:`~jax.lax.DotAlgorithm` or a\n:class:`~jax.lax.DotAlgorithmPreset` indicating the algorithm that\nmust be used to accumulate the dot product.\npreferred_element_type: Optional. This parameter controls the data type\noutput by the dot product. By default, the output element type of this\noperation will match the ``lhs`` and ``rhs`` input element types under\nthe usual type promotion rules. Setting ``preferred_element_type`` to a\nspecific ``dtype`` will mean that the operation returns that element type.\nWhen ``precision`` is not a :class:`~jax.lax.DotAlgorithm` or\n:class:`~jax.lax.DotAlgorithmPreset`, ``preferred_element_type`` provides\na hint to the compiler to accumulate the dot product using this data type.",
        "parameters": {},
        "returns": "An array whose first dimensions are the (shared) batch dimensions, followed\nby the ``lhs`` non-contracting/non-batch dimensions, and finally the ``rhs``\nnon-contracting/non-batch dimensions.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "dynamic_index_in_dim",
      "signature": "dynamic_index_in_dim(operand: 'Array | np.ndarray', index: 'int | Array', axis: 'int' = 0, keepdims: 'bool' = True, *, allow_negative_indices: 'bool' = True) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around dynamic_slice to perform int indexing.\nThis is roughly equivalent to the following Python indexing syntax applied\nalong the specified axis: ``operand[..., index]``.\nArgs:\noperand: an array to slice.\nindex: the (possibly dynamic) start index\naxis: the axis along which to apply the slice (defaults to 0)\nkeepdims: boolean specifying whether the output should have the same rank as\nthe input (default = True)\nallow_negative_indices: boolean specifying whether negative indices are\nallowed. If true, negative indices are taken relative to the end of the\narray. If false, negative indices are out of bounds and the result is\nimplementation defined.",
        "parameters": {},
        "returns": "An array containing the slice.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.index_in_dim`\n- :func:`jax.lax.dynamic_slice`\n- :func:`jax.lax.dynamic_slice_in_dim`",
        "notes": "",
        "examples": "Here is a one-dimensional example:\n>>> x = jnp.arange(5)\n>>> dynamic_index_in_dim(x, 1)\nArray([1], dtype=int32)\n>>> dynamic_index_in_dim(x, 1, keepdims=False)\nArray(1, dtype=int32)\nHere is a two-dimensional example:\n>>> x = jnp.arange(12).reshape(3, 4)\n>>> x\nArray([[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)\n>>> dynamic_index_in_dim(x, 1, axis=1, keepdims=False)\nArray([1, 5, 9], dtype=int32)"
      }
    },
    {
      "name": "dynamic_slice",
      "signature": "dynamic_slice(operand: 'Array | np.ndarray', start_indices: 'Array | np.ndarray | Sequence[ArrayLike]', slice_sizes: 'Shape', *, allow_negative_indices: 'bool | Sequence[bool]' = True) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `DynamicSlice\n<https://www.tensorflow.org/xla/operation_semantics#dynamicslice>`_\noperator.\nArgs:\noperand: an array to slice.\nstart_indices: a list of scalar indices, one per dimension. These values\nmay be dynamic.\nslice_sizes: the size of the slice. Must be a sequence of non-negative\nintegers with length equal to `ndim(operand)`. Inside a JIT compiled\nfunction, only static values are supported (all JAX arrays inside JIT\nmust have statically known size).\nallow_negative_indices: a bool or sequence of bools, one per dimension; if\na bool is passed, it applies to all dimensions. For each dimension,\nif true, negative indices are permitted and are are interpreted relative\nto the end of the array. If false, negative indices are treated as if they\nwere out of bounds and the result is implementation defined, typically\nclamped to the first index.",
        "parameters": {},
        "returns": "An array containing the slice.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.slice`\n- :func:`jax.lax.dynamic_slice_in_dim`\n- :func:`jax.lax.dynamic_index_in_dim`",
        "notes": "",
        "examples": "Here is a simple two-dimensional dynamic slice:\n>>> x = jnp.arange(12).reshape(3, 4)\n>>> x\nArray([[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)\n>>> dynamic_slice(x, (1, 1), (2, 3))\nArray([[ 5,  6,  7],\n[ 9, 10, 11]], dtype=int32)\nNote the potentially surprising behavior for the case where the requested slice\noverruns the bounds of the array; in this case the start index is adjusted to\nreturn a slice of the requested size:\n>>> dynamic_slice(x, (1, 1), (2, 4))\nArray([[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)"
      }
    },
    {
      "name": "dynamic_slice_in_dim",
      "signature": "dynamic_slice_in_dim(operand: 'Array | np.ndarray', start_index: 'ArrayLike', slice_size: 'int', axis: 'int' = 0, *, allow_negative_indices: 'bool' = True) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around :func:`lax.dynamic_slice` applied to one dimension.\nThis is roughly equivalent to the following Python indexing syntax applied\nalong the specified axis: ``operand[..., start_index:start_index + slice_size]``.\nArgs:\noperand: an array to slice.\nstart_index: the (possibly dynamic) start index\nslice_size: the static slice size\naxis: the axis along which to apply the slice (defaults to 0)\nallow_negative_indices: boolean specifying whether negative indices are\nallowed. If true, negative indices are taken relative to the end of the\narray. If false, negative indices are out of bounds and the result is\nimplementation defined.",
        "parameters": {},
        "returns": "An array containing the slice.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.slice_in_dim`\n- :func:`jax.lax.dynamic_slice`\n- :func:`jax.lax.dynamic_index_in_dim`",
        "notes": "",
        "examples": "Here is a one-dimensional example:\n>>> x = jnp.arange(5)\n>>> dynamic_slice_in_dim(x, 1, 3)\nArray([1, 2, 3], dtype=int32)\nLike `jax.lax.dynamic_slice`, out-of-bound slices will be clipped to the\nvalid range:\n>>> dynamic_slice_in_dim(x, 4, 3)\nArray([2, 3, 4], dtype=int32)\nHere is a two-dimensional example:\n>>> x = jnp.arange(12).reshape(3, 4)\n>>> x\nArray([[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)\n>>> dynamic_slice_in_dim(x, 1, 2, axis=1)\nArray([[ 1,  2],\n[ 5,  6],\n[ 9, 10]], dtype=int32)"
      }
    },
    {
      "name": "dynamic_update_index_in_dim",
      "signature": "dynamic_update_index_in_dim(operand: 'Array | np.ndarray', update: 'ArrayLike', index: 'ArrayLike', axis: 'int', *, allow_negative_indices: 'bool' = True) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around :func:`dynamic_update_slice` to update a slice\nof size 1 in a single ``axis``.\nArgs:\noperand: an array to slice.\nupdate: an array containing the new values to write onto `operand`.\nindex: a single scalar index\naxis: the axis of the update.\nallow_negative_indices: boolean specifying whether negative indices are\nallowed. If true, negative indices are taken relative to the end of the\narray. If false, negative indices are out of bounds and the result is\nimplementation defined.",
        "parameters": {},
        "returns": "The updated array",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.dynamic_update_slice`\n- :func:`jax.lax.dynamic_update_index_in_dim`\n- :func:`jax.lax.dynamic_index_in_dim`",
        "notes": "",
        "examples": ">>> x = jnp.zeros(6)\n>>> y = 1.0\n>>> dynamic_update_index_in_dim(x, y, 2, axis=0)\nArray([0., 0., 1., 0., 0., 0.], dtype=float32)\n>>> y = jnp.array([1.0])\n>>> dynamic_update_index_in_dim(x, y, 2, axis=0)\nArray([0., 0., 1., 0., 0., 0.], dtype=float32)\nIf the specified index is out of bounds, the index will be clipped to the\nvalid range:\n>>> dynamic_update_index_in_dim(x, y, 10, axis=0)\nArray([0., 0., 0., 0., 0., 1.], dtype=float32)\nHere is an example of a two-dimensional dynamic index update:\n>>> x = jnp.zeros((4, 4))\n>>> y = jnp.ones(4)\n>>> dynamic_update_index_in_dim(x, y, 1, axis=0)\nArray([[0., 0., 0., 0.],\n[1., 1., 1., 1.],\n[0., 0., 0., 0.],\n[0., 0., 0., 0.]], dtype=float32)\nNote that the shape of the additional axes in ``update`` need not\nmatch the associated dimensions of the ``operand``:\n>>> y = jnp.ones((1, 3))\n>>> dynamic_update_index_in_dim(x, y, 1, 0)\nArray([[0., 0., 0., 0.],\n[1., 1., 1., 0.],\n[0., 0., 0., 0.],\n[0., 0., 0., 0.]], dtype=float32)"
      }
    },
    {
      "name": "dynamic_update_slice",
      "signature": "dynamic_update_slice(operand: 'Array | np.ndarray', update: 'ArrayLike', start_indices: 'Array | Sequence[ArrayLike]', *, allow_negative_indices: 'bool | Sequence[bool]' = True) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `DynamicUpdateSlice\n<https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice>`_\noperator.\nArgs:\noperand: an array to slice.\nupdate: an array containing the new values to write onto `operand`.\nstart_indices: a list of scalar indices, one per dimension.\nallow_negative_indices: a bool or sequence of bools, one per dimension; if\na bool is passed, it applies to all dimensions. For each dimension,\nif true, negative indices are permitted and are are interpreted relative\nto the end of the array. If false, negative indices are treated as if they\nwere out of bounds and the result is implementation defined, typically\nclamped to the first index.",
        "parameters": {},
        "returns": "An array containing the slice.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :attr:`lax.dynamic_update_index_in_dim`\n- :attr:`lax.dynamic_update_slice_in_dim`",
        "notes": "",
        "examples": "Here is an example of updating a one-dimensional slice update:\n>>> x = jnp.zeros(6)\n>>> y = jnp.ones(3)\n>>> dynamic_update_slice(x, y, (2,))\nArray([0., 0., 1., 1., 1., 0.], dtype=float32)\nIf the update slice is too large to fit in the array, the start\nindex will be adjusted to make it fit\n>>> dynamic_update_slice(x, y, (3,))\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)\n>>> dynamic_update_slice(x, y, (5,))\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)\nHere is an example of a two-dimensional slice update:\n>>> x = jnp.zeros((4, 4))\n>>> y = jnp.ones((2, 2))\n>>> dynamic_update_slice(x, y, (1, 2))\nArray([[0., 0., 0., 0.],\n[0., 0., 1., 1.],\n[0., 0., 1., 1.],\n[0., 0., 0., 0.]], dtype=float32)"
      }
    },
    {
      "name": "dynamic_update_slice_in_dim",
      "signature": "dynamic_update_slice_in_dim(operand: 'Array | np.ndarray', update: 'ArrayLike', start_index: 'ArrayLike', axis: 'int', *, allow_negative_indices: 'bool' = True) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around :func:`dynamic_update_slice` to update\na slice in a single ``axis``.\nArgs:\noperand: an array to slice.\nupdate: an array containing the new values to write onto `operand`.\nstart_index: a single scalar index\naxis: the axis of the update.\nallow_negative_indices: boolean specifying whether negative indices are\nallowed. If true, negative indices are taken relative to the end of the\narray. If false, negative indices are out of bounds and the result is\nimplementation defined.",
        "parameters": {},
        "returns": "The updated array",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.dynamic_update_slice`\n- :func:`jax.lax.dynamic_update_index_in_dim`\n- :func:`jax.lax.dynamic_slice_in_dim`",
        "notes": "",
        "examples": ">>> x = jnp.zeros(6)\n>>> y = jnp.ones(3)\n>>> dynamic_update_slice_in_dim(x, y, 2, axis=0)\nArray([0., 0., 1., 1., 1., 0.], dtype=float32)\nIf the update slice is too large to fit in the array, the start\nindex will be adjusted to make it fit:\n>>> dynamic_update_slice_in_dim(x, y, 3, axis=0)\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)\n>>> dynamic_update_slice_in_dim(x, y, 5, axis=0)\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)\nHere is an example of a two-dimensional slice update:\n>>> x = jnp.zeros((4, 4))\n>>> y = jnp.ones((2, 4))\n>>> dynamic_update_slice_in_dim(x, y, 1, axis=0)\nArray([[0., 0., 0., 0.],\n[1., 1., 1., 1.],\n[1., 1., 1., 1.],\n[0., 0., 0., 0.]], dtype=float32)\nNote that the shape of the additional axes in ``update`` need not\nmatch the associated dimensions of the ``operand``:\n>>> y = jnp.ones((2, 3))\n>>> dynamic_update_slice_in_dim(x, y, 1, axis=0)\nArray([[0., 0., 0., 0.],\n[1., 1., 1., 0.],\n[1., 1., 1., 0.],\n[0., 0., 0., 0.]], dtype=float32)"
      }
    },
    {
      "name": "eq",
      "signature": "eq(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise equals: :math:`x = y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=EQ`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching dtypes. If neither is a\nscalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise equal comparison.\nSee also:\n- :func:`jax.numpy.equal`: NumPy wrapper for this API, also accessible\nvia the ``x == y`` operator on JAX arrays.\n- :func:`jax.lax.ne`: elementwise not-equal\n- :func:`jax.lax.ge`: elementwise greater-than-or-equal\n- :func:`jax.lax.gt`: elementwise greater-than\n- :func:`jax.lax.le`: elementwise less-than-or-equal\n- :func:`jax.lax.lt`: elementwise less-than\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "erf",
      "signature": "erf(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise error function: :math:`\\mathrm{erf}(x)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "erf_inv",
      "signature": "erf_inv(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise inverse error function: :math:`\\mathrm{erf}^{-1}(x)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "erfc",
      "signature": "erfc(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise complementary error function:\n:math:`\\mathrm{erfc}(x) = 1 - \\mathrm{erf}(x)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "exp",
      "signature": "exp(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise exponential: :math:`e^x`.\nThis function lowers directly to the  `stablehlo.exponential`_ operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nexponential.\nSee also:\n- :func:`jax.lax.exp2`: elementwise base-2 exponentional: :math:`2^x`.\n- :func:`jax.lax.log`: elementwise natural logarithm: :math:`\\mathrm{log}(x)`.\n.. _stablehlo.exponential: https://openxla.org/stablehlo/spec#exponential",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "exp2",
      "signature": "exp2(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise base-2 exponential: :math:`2^x`.\nThis function is implemented in terms of the `stablehlo.exponential`_\nand `stablehlo.multiply`_ operations.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nbase-2 exponential.\nSee also:\n- :func:`jax.lax.exp`: elementwise exponentional: :math:`e^x`.\n- :func:`jax.lax.log`: elementwise natural logarithm: :math:`\\mathrm{log}(x)`.\n.. _stablehlo.exponential: https://openxla.org/stablehlo/spec#exponential\n.. _stablehlo.multiply: https://openxla.org/stablehlo/spec#multiply",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "expand_dims",
      "signature": "expand_dims(array: 'ArrayLike', dimensions: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Insert any number of size 1 dimensions into an array.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "expm1",
      "signature": "expm1(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise :math:`e^{x} - 1`.\nThis function lowers directly to the `stablehlo.exponential_minus_one`_\noperation. Compared to the naive expression ``lax.exp(x) - 1``, it is\nmore accurate for ``x`` near zero.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nexponential minus 1.\nSee also:\n- :func:`jax.lax.exp`: elementwise exponentional: :math:`e^x`.\n- :func:`jax.lax.log1p`: elementwise :math:`\\mathrm{log}(1 + x)`.\n.. _stablehlo.exponential_minus_one: https://openxla.org/stablehlo/spec#exponential_minus_one",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fft",
      "signature": "fft(x, fft_type: 'FftType | str', fft_lengths: 'Sequence[int]')",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "floor",
      "signature": "floor(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise floor: :math:`\\left\\lfloor x \\right\\rfloor`.\nThis function lowers directly to the `stablehlo.floor`_ operation.\nArgs:\nx: input array. Must have floating-point type.",
        "parameters": {},
        "returns": "Array of same shape and dtype as ``x``, containing values rounded\nto the next integer toward negative infinity.\nSee also:\n- :func:`jax.lax.ceil`: round to the next integer toward positive infinity\n- :func:`jax.lax.round`: round to the nearest integer\n.. _stablehlo.floor: https://openxla.org/stablehlo/spec#floor",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "fori_loop",
      "signature": "fori_loop(lower, upper, body_fun, init_val, *, unroll: 'int | bool | None' = None)",
      "documentation": {
        "description": "Loop from ``lower`` to ``upper`` by reduction to :func:`jax.lax.while_loop`.\nThe `Haskell-like type signature`_ in brief is\n.. code-block:: haskell\nfori_loop :: Int -> Int -> ((Int, a) -> a) -> a -> a\nThe semantics of ``fori_loop`` are given by this Python implementation::\ndef fori_loop(lower, upper, body_fun, init_val):\nval = init_val\nfor i in range(lower, upper):\nval = body_fun(i, val)\nreturn val\nAs the Python version suggests, setting ``upper <= lower`` will produce no\niterations. Negative or custom increments are not supported.\nUnlike that Python version, ``fori_loop`` is implemented in terms of either a\ncall to :func:`jax.lax.while_loop` or a call to :func:`jax.lax.scan`. If the\ntrip count is static (meaning known at tracing time, perhaps because ``lower``\nand ``upper`` are Python integer literals) then the ``fori_loop`` is\nimplemented in terms of :func:`~scan` and reverse-mode autodiff is supported;\notherwise, a ``while_loop`` is used and reverse-mode autodiff is not\nsupported.  See those functions' docstrings for more information.\nAlso unlike the Python analogue, the loop-carried value ``val`` must hold a\nfixed shape and dtype across all iterations (and not just be consistent up to\nNumPy rank/shape broadcasting and dtype promotion rules, for example). In\nother words, the type ``a`` in the type signature above represents an array\nwith a fixed shape and dtype (or a nested tuple/list/dict container data\nstructure with a fixed structure and arrays with fixed shape and dtype at the\nleaves).\n.. note::\n:py:func:`fori_loop` compiles ``body_fun``, so while it can be combined with\n:py:func:`jit`, it's usually unnecessary.\nArgs:\nlower: an integer representing the loop index lower bound (inclusive)\nupper: an integer representing the loop index upper bound (exclusive)\nbody_fun: function of type ``(int, a) -> a``.\ninit_val: initial loop carry value of type ``a``.\nunroll: An optional integer or boolean that determines how much to unroll\nthe loop. If an integer is provided, it determines how many unrolled\nloop iterations to run within a single rolled iteration of the loop. If a\nboolean is provided, it will determine if the loop is competely unrolled\n(i.e. `unroll=True`) or left completely unrolled (i.e. `unroll=False`).\nThis argument is only applicable if the loop bounds are statically known.",
        "parameters": {},
        "returns": "Loop value from the final iteration, of type ``a``.\n.. _Haskell-like type signature: https://wiki.haskell.org/Type_signature",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "full",
      "signature": "full(shape: 'Shape', fill_value: 'ArrayLike', dtype: 'DTypeLike | None' = None, *, sharding: 'Sharding | None' = None) -> 'Array'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "Args:\nshape: sequence of integers, describing the shape of the output array.\nfill_value: the value to fill the new array with.\ndtype: the type of the output array, or `None`. If not `None`, `fill_value`\nwill be cast to `dtype`.\nsharding: an optional sharding specification for the resulting array,\nnote, sharding will currently be ignored in jitted mode, this might change\nin the future.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "full_like",
      "signature": "full_like(x: 'ArrayLike | DuckTypedArray', fill_value: 'ArrayLike', dtype: 'DTypeLike | None' = None, shape: 'Shape | None' = None, sharding: 'Sharding | None' = None) -> 'Array'",
      "documentation": {
        "description": "Create a full array like np.full based on the example array `x`.\nArgs:\nx: example array-like, used for shape and dtype information.\nfill_value: a scalar value to fill the entries of the output array.\ndtype: optional, a dtype parameter for the output ndarray.\nshape: optional, a shape parameter for the output ndarray.\nsharding: an optional sharding specification for the resulting array.\nIf not specified, the output will have the same sharding as the input,\nwith a few exceptions/limitations in particular:\n1. Sharding is not available during tracing, thus this will rely on jit.\n2. If x is weakly typed or uncommitted, will use default sharding.\n3. Shape is not None and is different from x.shape, default will be used.",
        "parameters": {},
        "returns": "An ndarray with the same shape as `x` with its entries set equal to\n`fill_value`, similar to the output of np.full.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gather",
      "signature": "gather(operand: 'ArrayLike', start_indices: 'ArrayLike', dimension_numbers: 'GatherDimensionNumbers', slice_sizes: 'Shape', *, unique_indices: 'bool' = False, indices_are_sorted: 'bool' = False, mode: 'str | GatherScatterMode | None' = None, fill_value=None) -> 'Array'",
      "documentation": {
        "description": "Gather operator.\nWraps `XLA's Gather operator\n<https://www.tensorflow.org/xla/operation_semantics#gather>`_.\n:func:`gather` is a low-level operator with complicated semantics, and most JAX\nusers will never need to call it directly. Instead, you should prefer using\n`Numpy-style indexing`_, and/or :func:`jax.numpy.ndarray.at`, perhaps in combination\nwith :func:`jax.vmap`.\nArgs:\noperand: an array from which slices should be taken\nstart_indices: the indices at which slices should be taken\ndimension_numbers: a `lax.GatherDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices` and the output relate.\nslice_sizes: the size of each slice. Must be a sequence of non-negative\nintegers with length equal to `ndim(operand)`.\nindices_are_sorted: whether `indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements gathered from ``operand`` are\nguaranteed not to overlap with each other. If ``True``, this may improve\nperformance on some backends. JAX does not check this promise: if\nthe elements overlap the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to ``'clip'``,\nindices are clamped so that the slice is within bounds, and when\nset to ``'fill'`` or ``'drop'`` gather returns a slice full of\n``fill_value`` for the affected slice. The behavior for out-of-bounds\nindices when set to ``'promise_in_bounds'`` is implementation-defined.\nfill_value: the fill value to return for out-of-bounds slices when `mode`\nis ``'fill'``. Ignored otherwise. Defaults to ``NaN`` for inexact types,\nthe largest negative value for signed types, the largest positive value\nfor unsigned types, and ``True`` for booleans.",
        "parameters": {},
        "returns": "An array containing the gather output.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "As mentioned above, you should basically never use :func:`gather` directly,\nand instead use NumPy-style indexing expressions to gather values from\narrays.\nFor example, here is how you can extract values at particular indices using\nstraightforward indexing semantics, which will lower to XLA's Gather operator:\n>>> import jax.numpy as jnp\n>>> x = jnp.array([10, 11, 12])\n>>> indices = jnp.array([0, 1, 1, 2, 2, 2])\n>>> x[indices]\nArray([10, 11, 11, 12, 12, 12], dtype=int32)\nFor control over settings like ``indices_are_sorted``, ``unique_indices``, ``mode``,\nand ``fill_value``, you can use the :attr:`jax.numpy.ndarray.at` syntax:\n>>> x.at[indices].get(indices_are_sorted=True, mode=\"promise_in_bounds\")\nArray([10, 11, 11, 12, 12, 12], dtype=int32)\nBy comparison, here is the equivalent function call using :func:`gather` directly,\nwhich is not something typical users should ever need to do:\n>>> from jax import lax\n>>> lax.gather(x, indices[:, None], slice_sizes=(1,),\n...            dimension_numbers=lax.GatherDimensionNumbers(\n...                offset_dims=(),\n...                collapsed_slice_dims=(0,),\n...                start_index_map=(0,)),\n...            indices_are_sorted=True,\n...            mode=lax.GatherScatterMode.PROMISE_IN_BOUNDS)\nArray([10, 11, 11, 12, 12, 12], dtype=int32)\n.. _Numpy-style indexing: https://numpy.org/doc/stable/reference/arrays.indexing.html"
      }
    },
    {
      "name": "ge",
      "signature": "ge(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise greater-than-or-equals: :math:`x \\geq y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=GE`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching non-complex dtypes. If neither is\na scalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise greater-than-or-equal comparison.\nSee also:\n- :func:`jax.numpy.greater_equal`: NumPy wrapper for this API, also\naccessible via the ``x >= y`` operator on JAX arrays.\n- :func:`jax.lax.eq`: elementwise equal\n- :func:`jax.lax.ne`: elementwise not-equal\n- :func:`jax.lax.gt`: elementwise greater-than\n- :func:`jax.lax.le`: elementwise less-than-or-equal\n- :func:`jax.lax.lt`: elementwise less-than\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "gt",
      "signature": "gt(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise greater-than: :math:`x > y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=GT`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching non-complex dtypes. If neither is\na scalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise greater-than comparison.\nSee also:\n- :func:`jax.numpy.greater`: NumPy wrapper for this API, also accessible\nvia the ``x > y`` operator on JAX arrays.\n- :func:`jax.lax.eq`: elementwise equal\n- :func:`jax.lax.ne`: elementwise not-equal\n- :func:`jax.lax.ge`: elementwise greater-than-or-equal\n- :func:`jax.lax.le`: elementwise less-than-or-equal\n- :func:`jax.lax.lt`: elementwise less-than\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "igamma",
      "signature": "igamma(a: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise regularized incomplete gamma function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "igamma_grad_a",
      "signature": "igamma_grad_a(a: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise derivative of the regularized incomplete gamma function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "igammac",
      "signature": "igammac(a: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise complementary regularized incomplete gamma function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "imag",
      "signature": "imag(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise extract imaginary part: :math:`\\mathrm{Im}(x)`.\nThis function lowers directly to the `stablehlo.imag`_ operation.\nArgs:\nx: input array. Must have complex dtype.",
        "parameters": {},
        "returns": "Array of the same shape as ``x`` containing its imaginary part. Will have dtype\nfloat32 if ``x.dtype == complex64``, or float64 if ``x.dtype == complex128``.\nSee also:\n- :func:`jax.lax.complex`: elementwise construct complex number.\n- :func:`jax.lax.real`: elementwise extract real part.\n- :func:`jax.lax.conj`: elementwise complex conjugate.\n.. _stablehlo.imag: https://openxla.org/stablehlo/spec#imag",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "index_in_dim",
      "signature": "index_in_dim(operand: 'Array | np.ndarray', index: 'int', axis: 'int' = 0, keepdims: 'bool' = True) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around :func:`lax.slice` to perform int indexing.\nThis is effectively equivalent to ``operand[..., index]``\nwith the indexing applied on the specified axis.\nArgs:\noperand: an array to index.\nindex: integer index\naxis: the axis along which to apply the index (defaults to 0)\nkeepdims: boolean specifying whether the output array should preserve the\nrank of the input (default=True)",
        "parameters": {},
        "returns": "The subarray at the specified index.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.slice`\n- :func:`jax.lax.slice_in_dim`\n- :func:`jax.lax.dynamic_index_in_dim`",
        "notes": "",
        "examples": "Here is a one-dimensional example:\n>>> x = jnp.arange(4)\n>>> lax.index_in_dim(x, 2)\nArray([2], dtype=int32)\n>>> lax.index_in_dim(x, 2, keepdims=False)\nArray(2, dtype=int32)\nHere are some two-dimensional examples:\n>>> x = jnp.arange(12).reshape(3, 4)\n>>> x\nArray([[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)\n>>> lax.index_in_dim(x, 1)\nArray([[4, 5, 6, 7]], dtype=int32)\n>>> lax.index_in_dim(x, 1, axis=1, keepdims=False)\nArray([1, 5, 9], dtype=int32)"
      }
    },
    {
      "name": "index_take",
      "signature": "index_take(src: 'Array', idxs: 'Array', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "infeed",
      "signature": "infeed(token, shape=None, partitions=None)",
      "documentation": {
        "description": "Consumes an infeed value of `shape` from the host. Experimental.\n`token` is used to sequence infeed and outfeed effects.\n`partitions` may be specified inside a `sharded_jit` function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "integer_pow",
      "signature": "integer_pow(x: 'ArrayLike', y: 'int') -> 'Array'",
      "documentation": {
        "description": "Elementwise power: :math:`x^y`, where :math:`y` is a static integer.\nThis will lower to a sequence of :math:`O[\\log_2(y)]` repetitions of\n`stablehlo.multiply`_.\nArgs:\nx: Input array giving the base value. Must have numerical dtype.\ny: Static scalar integer giving the exponent.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x`` containing the elementwise power.\nSee also:\n:func:`jax.lax.pow`: Elementwise pwoer where ``y`` is an array.\n.. _stablehlo.multiply: https://openxla.org/stablehlo/spec#multiply",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "iota",
      "signature": "iota(dtype: 'DTypeLike', size: 'int') -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `Iota\n<https://www.tensorflow.org/xla/operation_semantics#iota>`_\noperator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "is_finite",
      "signature": "is_finite(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise :math:`\\mathrm{isfinite}`.\nThis function lowers directly to the  `stablehlo.is_finite`_ operation.\nArgs:\nx: input array. Must have floating-point type.",
        "parameters": {},
        "returns": "Array of boolean dtype with the same shape as ``x``, containing ``False`` where\n``x`` is :math:`\\pm\\infty` or :math:`\\mathit{NaN}`, and ``True`` otherwise.\nSee also:\n- :func:`jax.numpy.isinf`: return True where array is infinite.\n- :func:`jax.numpy.isnan`: return True where array is NaN.\n.. _stablehlo.is_finite: https://openxla.org/stablehlo/spec#is_finite",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "le",
      "signature": "le(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise less-than-or-equals: :math:`x \\leq y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=LE`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching non-complex dtypes. If neither is\na scalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise less-than-or-equal comparison.\nSee also:\n- :func:`jax.numpy.less_equal`: NumPy wrapper for this API, also\naccessible via the ``x <= y`` operator on JAX arrays.\n- :func:`jax.lax.eq`: elementwise equal\n- :func:`jax.lax.ne`: elementwise not-equal\n- :func:`jax.lax.ge`: elementwise greater-than-or-equal\n- :func:`jax.lax.gt`: elementwise greater-than\n- :func:`jax.lax.lt`: elementwise less-than\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "lgamma",
      "signature": "lgamma(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise log gamma: :math:`\\mathrm{log}(\\Gamma(x))`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "log",
      "signature": "log(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise natural logarithm: :math:`\\mathrm{log}(x)`.\nThis function lowers directly to the  `stablehlo.log`_ operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nnatural logarithm.\nSee also:\n- :func:`jax.lax.exp`: elementwise exponentional: :math:`e^x`.\n.. _stablehlo.log: https://openxla.org/stablehlo/spec#log",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "log1p",
      "signature": "log1p(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise :math:`\\mathrm{log}(1 + x)`.\nThis function lowers directly to the  `stablehlo.log_plus_one`_ operation.\nCompared to the naive expression ``lax.log(1 + x)``, it is more accurate\nfor ``x`` near zero.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nnatural logarithm of ``x + 1``.\nSee also:\n- :func:`jax.lax.expm1`: elementwise :math:`e^x - 1`.\n- :func:`jax.lax.log`: elementwise natural logarithm :math:`\\mathrm{log}(x)`.\n.. _stablehlo.log_plus_one: https://openxla.org/stablehlo/spec#log_plus_one",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "logistic",
      "signature": "logistic(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise logistic (sigmoid) function: :math:`\\frac{1}{1 + e^{-x}}`.\nThere is no HLO logistic/sigmoid primitive, so this lowers to a sequence\nof HLO arithmetic operations.\nArgs:\nx: input array. Must have floating point or complex dtype.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nlogistic/sigmoid function.\nSee also:\n- :func:`jax.nn.sigmoid`: an alternative API for this functionality.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "lt",
      "signature": "lt(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise less-than: :math:`x < y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=LT`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching non-complex dtypes. If neither is\na scalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise less-than comparison.\nSee also:\n- :func:`jax.numpy.less`: NumPy wrapper for this API, also accessible\nvia the ``x < y`` operator on JAX arrays.\n- :func:`jax.lax.eq`: elementwise equal\n- :func:`jax.lax.ne`: elementwise not-equal\n- :func:`jax.lax.ge`: elementwise greater-than-or-equal\n- :func:`jax.lax.gt`: elementwise greater-than\n- :func:`jax.lax.le`: elementwise less-than-or-equal\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "map",
      "signature": "map(f, xs, *, batch_size: 'int | None' = None)",
      "documentation": {
        "description": "Map a function over leading array axes.\nLike Python's builtin map, except inputs and outputs are in the form of\nstacked arrays. Consider using the :func:`~jax.vmap` transform instead, unless you\nneed to apply a function element by element for reduced memory usage or\nheterogeneous computation with other control flow primitives.\nWhen ``xs`` is an array type, the semantics of :func:`~map` are given by this\nPython implementation::\ndef map(f, xs):\nreturn np.stack([f(x) for x in xs])\nLike :func:`~scan`, :func:`~map` is implemented in terms of JAX primitives so\nmany of the same advantages over a Python loop apply: ``xs`` may be an\narbitrary nested pytree type, and the mapped computation is compiled only\nonce.\nIf ``batch_size`` is provided, the computation is executed in batches of that size\nand parallelized using :func:`~jax.vmap`. This can be used as either a more performant\nversion of ``map`` or as a memory-efficient version of ``vmap``. If the axis is not\ndivisible by the batch size, the remainder is processed in a separate ``vmap`` and\nconcatenated to the result.\n>>> x = jnp.ones((10, 3, 4))\n>>> def f(x):\n...   print('inner shape:', x.shape)\n...   return x + 1\n>>> y = lax.map(f, x, batch_size=3)\ninner shape: (3, 4)\ninner shape: (3, 4)\n>>> y.shape\n(10, 3, 4)\nIn the example above, \"inner shape\" is printed twice, once while tracing the batched\ncomputation and once while tracing the remainder computation.\nArgs:\nf: a Python function to apply element-wise over the first axis or axes of\n``xs``.\nxs: values over which to map along the leading axis.\nbatch_size: (optional) integer specifying the size of the batch for each step to execute\nin parallel.",
        "parameters": {},
        "returns": "Mapped values.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "max",
      "signature": "max(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise maximum: :math:`\\mathrm{max}(x, y)`.\nThis function lowers directly to the `stablehlo.maximum`_ operation for\nnon-complex inputs. For complex numbers, this uses a lexicographic\ncomparison on the `(real, imaginary)` pairs.\nArgs:\nx, y: Input arrays. Must have matching dtypes. If neither is a scalar,\n``x`` and ``y`` must have the same rank and be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the elementwise\nmaximum.\nSee also:\n- :func:`jax.numpy.maximum`: more flexibly NumPy-style maximum.\n- :func:`jax.lax.reduce_max`: maximum along an axis of an array.\n- :func:`jax.lax.min`: elementwise minimum.\n.. _stablehlo.maximum: https://openxla.org/stablehlo/spec#maximum",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "min",
      "signature": "min(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise minimum: :math:`\\mathrm{min}(x, y)`\nThis function lowers directly to the `stablehlo.minimum`_ operation for\nnon-complex inputs. For complex numbers, this uses a lexicographic\ncomparison on the `(real, imaginary)` pairs.\nArgs:\nx, y: Input arrays. Must have matching dtypes. If neither is a scalar,\n``x`` and ``y`` must have the same rank and be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the elementwise\nminimum.\nSee also:\n- :func:`jax.numpy.minimum`: more flexibly NumPy-style minimum.\n- :func:`jax.lax.reduce_min`: minimum along an axis of an array.\n- :func:`jax.lax.max`: elementwise maximum.\n.. _stablehlo.minimum: https://openxla.org/stablehlo/spec#minimum",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "mul",
      "signature": "mul(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise multiplication: :math:`x \\times y`.\nThis function lowers directly to the `stablehlo.multiply`_ operation.\nArgs:\nx, y: Input arrays. Must have matching numerical dtypes. If neither\nis a scalar, ``x`` and ``y`` must have the same number of dimensions\nand be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the product\nof each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.multiply`: NumPy-style multiplication supporting\ninputs with mixed dtypes and ranks.\n.. _stablehlo.multiply: https://openxla.org/stablehlo/spec#multiply",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ne",
      "signature": "ne(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise not-equals: :math:`x \\neq y`.\nThis function lowers directly to the `stablehlo.compare`_ operation\nwith ``comparison_direction=NE`` and ``compare_type`` set according\nto the input dtype.\nArgs:\nx, y: Input arrays. Must have matching dtypes. If neither is a\nscalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "A boolean array of shape ``lax.broadcast_shapes(x.shape, y.shape)``\ncontaining the elementwise not-equal comparison.\nSee also:\n- :func:`jax.numpy.not_equal`: NumPy wrapper for this API, also accessible\nvia the ``x != y`` operator on JAX arrays.\n- :func:`jax.lax.eq`: elementwise equal\n- :func:`jax.lax.ge`: elementwise greater-than-or-equal\n- :func:`jax.lax.gt`: elementwise greater-than\n- :func:`jax.lax.le`: elementwise less-than-or-equal\n- :func:`jax.lax.lt`: elementwise less-than\n.. _stablehlo.compare: https://openxla.org/stablehlo/spec#compare",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "neg",
      "signature": "neg(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise negation: :math:`-x`.\nThis function lowers directly to the `stablehlo.negate`_ operation.\nArgs:\nx: input array",
        "parameters": {},
        "returns": "Array of same shape and dtype as ``x``, containing the element-wise negative.",
        "raises": "",
        "see_also": "",
        "notes": "For unsigned integer inputs, this function returns ``2 ** nbits - x``, where\n``nbits`` is the number of bits in the integer representation.\n.. _stablehlo.negate: https://openxla.org/stablehlo/spec#negate",
        "examples": ""
      }
    },
    {
      "name": "nextafter",
      "signature": "nextafter(x1: 'ArrayLike', x2: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "This function lowers directly to the ``chlo.next_after`` operation.\nArgs:\nx1, x2: input arrays. Must have a matching floating-point dtypes. If neither is\na scalar, must have the same number of dimensions and be broadcast-compatible.\nArray of the same dtype and broadcasted shape of the inputs, containing the\nnext representable floating-point value after ``x1`` in the direction of\n``x2``.",
        "raises": "",
        "see_also": "",
        "notes": "In some environments flush-denormal-to-zero semantics is used.\nThis means that, around zero, this function returns strictly non-zero\nvalues which appear as zero in any operations. Consider this example::\n>>> from jax import lax\n>>> lax.nextafter(0.0, 1.0)  # denormal numbers are representable\nArray(1.e-45, dtype=float32, weak_type=True)\n>>> lax.nextafter(0.0, 1.0) * 1  # but are flushed to zero\nArray(0., dtype=float32, weak_type=True)\nFor the smallest usable (i.e. normal) float, use ``tiny`` of ``jnp.finfo``.",
        "examples": ""
      }
    },
    {
      "name": "optimization_barrier",
      "signature": "optimization_barrier(operand, /)",
      "documentation": {
        "description": "Prevents the compiler from moving operations across the barrier.\nOptimization barriers have a number of possible uses:\n* An optimization barrier ensures that all inputs are evaluated before any\noperators that depend on the barrier's outputs. This can be used to enforce\na particular order of operations.\n* An optimization barrier prevents common subexpression elimination. This is\nused by JAX to implement rematerialization.\n* Optimization barriers prevent compiler fusions. That is, operations before\nthe barrier may not be fused into the same kernel as operations after the\nbarrier by the compiler.\nJAX does not define derivative or batching rules for an optimization barrier.\nOptimization barriers have no effect outside a compiled function.\nArgs:\noperand: a pytree of JAX values.",
        "parameters": {},
        "returns": "A pytree of JAX values, with the same structure and contents as ``operand``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Prevents common-subexpression elimination between the two calls to `sin`:\n>>> def f(x):\n...   return jax.lax.optimization_barrier(jax.lax.sin(x)) + jax.lax.sin(x)\n>>> jax.jit(f)(0.)\nArray(0., dtype=float32, weak_type=True)"
      }
    },
    {
      "name": "outfeed",
      "signature": "outfeed(token, xs, partitions=None)",
      "documentation": {
        "description": "Outfeeds value `xs` to the host. Experimental.\n`token` is used to sequence infeed and outfeed effects.\n`partitions` may be specified inside a `sharded_jit` or `pjit` function.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pad",
      "signature": "pad(operand: 'ArrayLike', padding_value: 'ArrayLike', padding_config: 'Sequence[tuple[int, int, int]]') -> 'Array'",
      "documentation": {
        "description": "Applies low, high, and/or interior padding to an array.\nWraps XLA's `Pad\n<https://www.tensorflow.org/xla/operation_semantics#pad>`_\noperator.\nArgs:\noperand: an array to be padded.\npadding_value: the value to be inserted as padding. Must have the same dtype\nas ``operand``.\npadding_config: a sequence of ``(low, high, interior)`` tuples of integers,\ngiving the amount of low, high, and interior (dilation) padding to insert\nin each dimension.",
        "parameters": {},
        "returns": "The ``operand`` array with padding value ``padding_value`` inserted in each\ndimension according to the ``padding_config``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> from jax import lax\n>>> import jax.numpy as jnp\nPad a 1-dimensional array with zeros, We'll specify two zeros in front and\nthree at the end:\n>>> x = jnp.array([1, 2, 3, 4])\n>>> lax.pad(x, 0, [(2, 3, 0)])\nArray([0, 0, 1, 2, 3, 4, 0, 0, 0], dtype=int32)\nPad a 1-dimensional array with *interior* zeros; i.e. insert a single zero\nbetween each value:\n>>> lax.pad(x, 0, [(0, 0, 1)])\nArray([1, 0, 2, 0, 3, 0, 4], dtype=int32)\nPad a 2-dimensional array with the value ``-1`` at front and end, with a pad\nsize of 2 in each dimension:\n>>> x = jnp.array([[1, 2, 3],\n...                [4, 5, 6]])\n>>> lax.pad(x, -1, [(2, 2, 0), (2, 2, 0)])\nArray([[-1, -1, -1, -1, -1, -1, -1],\n[-1, -1, -1, -1, -1, -1, -1],\n[-1, -1,  1,  2,  3, -1, -1],\n[-1, -1,  4,  5,  6, -1, -1],\n[-1, -1, -1, -1, -1, -1, -1],\n[-1, -1, -1, -1, -1, -1, -1]], dtype=int32)"
      }
    },
    {
      "name": "padtype_to_pads",
      "signature": "padtype_to_pads(in_shape, window_shape, window_strides, padding)",
      "documentation": {
        "description": "Convert padding string to list of pairs of pad values.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pbroadcast",
      "signature": "pbroadcast(x, axis_name, source)",
      "documentation": {
        "description": "Perform a collective broadcast and replicate from ``source``.\nThis is equivalent to\n```\ndef pbroadcast(x, axis_name, source):\nmasked = jnp.where(axis_index(axis_name) == source, x, zeros_like(x))\nreturn psum(masked, axis_name)\n```\nbut implemented in a hardware optimized way.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nThis function is an analog of the CollectiveBroadcast HLO.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\nsource: int, representing which index into ``axis_name`` that should be copied.",
        "parameters": {},
        "returns": "Array(s) with ``x`` being copied from the ``source`` index slice of ``axis_name``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "platform_dependent",
      "signature": "platform_dependent(*args: 'Any', default: 'Callable[..., _T] | None' = None, **per_platform: 'Callable[..., _T]')",
      "documentation": {
        "description": "Stages out platform-specific code.\nIn JAX the actual platform on which a computation is run is determined\nvery late, e.g., based on where the data is located. When using AOT\nlowering or serialization, the computation may be compiled and executed\non a different machine, or even on a platform that is not available at\nlowering time. This means that it is not safe to write platform-dependent\ncode using Python conditionals, e.g., based on the current default\nJAX platform. Instead, one can use ``platform_dependent``:\nUsage::\ndef cpu_code(*args): ...\ndef tpu_code(*args): ...\ndef other_platforms_code(*args): ...\nres = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code,\ndefault=other_platforms_code)\nWhen the staged out code is executed on a CPU, this is equivalent to\n``cpu_code(*args)``, on a TPU is equivalent to ``tpu_code(*args)`` and on\nany other platform to ``other_platforms_code(*args)``.\nUnlike a Python conditional, all alternatives are traced\nand staged out to Jaxpr. This is similar to, and is implemented in terms of,\n:func:`~switch`, from which it inherits the behavior\nunder transformations.\nUnlike a :func:`~switch` the choice of what gets executed is made earlier:\nin most cases during lowering when the lowering platform is known; in the\nrare case of multi-platform lowering and serialization, the StableHLO code\nwill contain a conditional on the actual platform. This conditional is\nresolved just in time prior to compilation when the compilation platform is\nknown. This means that the compiler actually never sees a conditional.\nArgs:\n*args: JAX arrays passed to each of the branches. May be PyTrees.\n**per_platform: branches to use for different platforms. The branches are\nJAX callables invoked with ``*args``. The keywords are platform names,\ne.g., 'cpu', 'tpu', 'cuda', 'rocm'.\ndefault: optional default branch to use for a platform not mentioned in\n``per_platform``. If there is no ``default`` there will be an error when\nthe code is lowered for a platform not mentioned in ``per_platform``.",
        "parameters": {},
        "returns": "The value ``per_platform[execution_platform](*args)``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pmax",
      "signature": "pmax(x, axis_name, *, axis_index_groups=None)",
      "documentation": {
        "description": "Compute an all-reduce max on ``x`` over the pmapped axis ``axis_name``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would perform pmaxes over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and on TPUs all groups must be the same size.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` representing the result of an\nall-reduce max along the axis ``axis_name``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pmean",
      "signature": "pmean(x, axis_name, *, axis_index_groups=None)",
      "documentation": {
        "description": "Compute an all-reduce mean on ``x`` over the pmapped axis ``axis_name``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would perform pmeans over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and on TPUs all groups must be the same size.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` representing the result of an\nall-reduce mean along the axis ``axis_name``.\nFor example, with 4 XLA devices available:\n>>> x = np.arange(4)\n>>> y = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[1.5 1.5 1.5 1.5]\n>>> y = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[0.        0.6666667 1.3333334 2.       ]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pmin",
      "signature": "pmin(x, axis_name, *, axis_index_groups=None)",
      "documentation": {
        "description": "Compute an all-reduce min on ``x`` over the pmapped axis ``axis_name``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would perform pmins over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and on TPUs all groups must be the same size.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` representing the result of an\nall-reduce min along the axis ``axis_name``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "polygamma",
      "signature": "polygamma(m: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise polygamma: :math:`\\psi^{(m)}(x)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "population_count",
      "signature": "population_count(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise popcount, count the number of set bits in each element.\nThis function lowers directly to the `stablehlo.popcnt`_ operation.\nArgs:\nx: Input array. Must have integer dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x``, containing the number of\nset bits in the input.\nSee also:\n- :func:`jax.lax.clz`: Elementwise count leading zeros.\n- :func:`jax.numpy.bitwise_count`: More flexible NumPy-style API for bit counts.\n.. _stablehlo.popcnt: https://openxla.org/stablehlo/spec#popcnt",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pow",
      "signature": "pow(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise power: :math:`x^y`.\nThis function lowers directly to the `stablehlo.pow`_ operation, along with\na `stablehlo.convert`_ when the argument dtypes do not match.\nArgs:\nx: Input array giving the base value. Must have floating or complex type.\ny: Input array giving the exponent value. Must have integer, floating, or\ncomplex type. Its dtype will be cast to that of ``x.dtype`` if necessary.\nIf neither ``x`` nor ``y`` is a scalar, then ``x`` and ``y`` must have\nthe same number of dimensions and be broadcast-compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` containing the elementwise power.\nSee also:\n:func:`jax.lax.integer_pow`: Elementwise power where ``y`` is a static integer.\n.. _stablehlo.convert: https://openxla.org/stablehlo/spec#convert\n.. _stablehlo.pow: https://openxla.org/stablehlo/spec#pow",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ppermute",
      "signature": "ppermute(x, axis_name, perm)",
      "documentation": {
        "description": "Perform a collective permutation according to the permutation ``perm``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nThis function is an analog of the CollectivePermute HLO.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\nperm: list of pairs of ints, representing\n``(source_index, destination_index)``\npairs that encode how the mapped axis named ``axis_name`` should be\nshuffled. The integer values are treated as indices into the mapped axis\n``axis_name``. Any two pairs should not have the same source index or the\nsame destination index. For each index of the axis ``axis_name`` that does\nnot correspond to a destination index in ``perm``, the corresponding\nvalues in the result are filled with zeros of the appropriate type.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` with slices along the axis\n``axis_name`` gathered from ``x`` according to the permutation ``perm``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pshuffle",
      "signature": "pshuffle(x, axis_name, perm)",
      "documentation": {
        "description": "Convenience wrapper of jax.lax.ppermute with alternate permutation encoding\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\nperm: list of ints encoding sources for the permutation to be applied to\nthe axis named ``axis_name``, so that the output at axis index i\ncomes from the input at axis index perm[i]. Every integer in [0, N) should\nbe included exactly once for axis size N.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` with slices along the axis\n``axis_name`` gathered from ``x`` according to the permutation ``perm``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "psum",
      "signature": "psum(x, axis_name, *, axis_index_groups=None)",
      "documentation": {
        "description": "Compute an all-reduce sum on ``x`` over the pmapped axis ``axis_name``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nInputs of boolean dtype are converted to integers before the reduction.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would perform psums over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x`` representing the result of an\nall-reduce sum along the axis ``axis_name``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "For example, with 4 XLA devices available:\n>>> x = np.arange(4)\n>>> y = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[6 6 6 6]\n>>> y = jax.pmap(lambda x: x / jax.lax.psum(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[0.         0.16666667 0.33333334 0.5       ]\nSuppose we want to perform ``psum`` among two groups, one with ``device0`` and ``device1``, the other with ``device2`` and ``device3``,\n>>> y = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\n>>> print(y)\n[1 1 5 5]\nAn example using 2D-shaped x. Each row is data from one device.\n>>> x = np.arange(16).reshape(4, 4)\n>>> print(x)\n[[ 0  1  2  3]\n[ 4  5  6  7]\n[ 8  9 10 11]\n[12 13 14 15]]\nFull ``psum`` across all devices:\n>>> y = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[[24 28 32 36]\n[24 28 32 36]\n[24 28 32 36]\n[24 28 32 36]]\nPerform ``psum`` among two groups:\n>>> y = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\n>>> print(y)\n[[ 4  6  8 10]\n[ 4  6  8 10]\n[20 22 24 26]\n[20 22 24 26]]"
      }
    },
    {
      "name": "psum_scatter",
      "signature": "psum_scatter(x, axis_name, *, scatter_dimension=0, axis_index_groups=None, tiled=False)",
      "documentation": {
        "description": "Like ``psum(x, axis_name)`` but each device retains only part of the result.\nFor example, ``psum_scatter(x, axis_name, scatter_dimension=0, tiled=False)``\ncomputes the same value as ``psum(x, axis_name)[axis_index(axis_name)]``, but\nit is more efficient. Thus the ``psum`` result is left scattered along the\nmapped axis.\nOne efficient algorithm for computing ``psum(x, axis_name)`` is to perform a\n``psum_scatter`` followed by an ``all_gather``, essentially evaluating\n``all_gather(psum_scatter(x, axis_name))``. So we can think of\n``psum_scatter`` as \"the first half\" of a ``psum``.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a mapped axis (see the\n:func:`jax.pmap` documentation for more details).\nscatter_dimension: a positional axis into which the all-reduce result along\n``axis_name`` will be scattered.\naxis_index_groups: optional list of lists of integers containing axis\nindices. For example, for an axis of size 4,\n``axis_index_groups=[[0, 1], [2, 3]]`` would run reduce-scatter over the\nfirst two and the last two axis indices. Groups must cover all axis\nindices exactly once, and all groups must be the same size.\ntiled: boolean representing whether to use rank-preserving 'tiled' behavior.\nWhen ``False`` (the default value), the size of dimension in\n``scatter_dimension`` must match the size of axis ``axis_name`` (or the\ngroup size if ``axis_index_groups`` is given). After scattering the\nall-reduce result along ``scatter_dimension``, the output is squeezed by\nremoving ``scatter_dimension``, so the result has lower rank than the\ninput. When ``True``, the size of dimension in ``scatter_dimension`` must\nbe divisible by the size of axis ``axis_name`` (or the group size if\n``axis_index_groups`` is given), and the ``scatter_dimension`` axis is\npreserved (so the result has the same rank as the input).",
        "parameters": {},
        "returns": "Array(s) with the similar shape as ``x``, except the size of dimension in\nposition ``scatter_dimension`` is divided by the size of axis ``axis_name``\n(when ``tiled=True``), or the dimension in position ``scatter_dimension`` is\neliminated (when ``tiled=False``).\nFor example, with 4 XLA devices available:\n>>> x = np.arange(16).reshape(4, 4)\n>>> print(x)\n[[ 0  1  2  3]\n[ 4  5  6  7]\n[ 8  9 10 11]\n[12 13 14 15]]\n>>> y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i'), axis_name='i')(x)\n>>> print(y)\n[24 28 32 36]\nif using tiled:\n>>> y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i', tiled=True), axis_name='i')(x)\n>>> print(y)\n[[24]\n[28]\n[32]\n[36]]\nAn example of using axis_index_groups:\n>>> def f(x):\n...   return jax.lax.psum_scatter(\n...       x, 'i', axis_index_groups=[[0, 2], [3, 1]], tiled=True)\n>>> y = jax.pmap(f, axis_name='i')(x)\n>>> print(y)\n[[ 8 10]\n[20 22]\n[12 14]\n[16 18]]",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "pswapaxes",
      "signature": "pswapaxes(x, axis_name, axis, *, axis_index_groups=None)",
      "documentation": {
        "description": "Swap the pmapped axis ``axis_name`` with the unmapped axis ``axis``.\nIf ``x`` is a pytree then the result is equivalent to mapping this function to\neach leaf in the tree.\nThe group size of the mapped axis size must be equal to the size of the\nunmapped axis; that is, we must have\n``lax.psum(1, axis_name, axis_index_groups=axis_index_groups) == x.shape[axis]``.\nBy default, when ``axis_index_groups=None``, this encompasses all the devices.\nThis function is a special case of ``all_to_all`` where the pmapped axis of\nthe input is placed at the position ``axis`` in the output. That is, it is\nequivalent to ``all_to_all(x, axis_name, axis, axis)``.\nArgs:\nx: array(s) with a mapped axis named ``axis_name``.\naxis_name: hashable Python object used to name a pmapped axis (see the\n:func:`jax.pmap` documentation for more details).\naxis: int indicating the unmapped axis of ``x`` to map with the name\n``axis_name``.\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would run pswapaxes over the first\ntwo and last two replicas). Groups must cover all axis indices exactly\nonce, and all groups must be the same size.",
        "parameters": {},
        "returns": "Array(s) with the same shape as ``x``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ragged_all_to_all",
      "signature": "ragged_all_to_all(operand, output, input_offsets, send_sizes, output_offsets, recv_sizes, *, axis_name, axis_index_groups=None)",
      "documentation": {
        "description": "Ragged version of :func:`all_to_all` collective.\nWe say data are \"ragged\" when they can be represented as a list of arrays\nwhose shapes differ only in the size of the leading axis. For example, these\ndata are ragged, comprising four component arrays::\nragged_data = [jnp.arange(3), jnp.arange(1), jnp.arange(4), jnp.arange(1)]\nWe often instead want a contiguous representation, e.g. for batching. But\nbecause the shapes of the components differ, we can't apply ``jnp.stack`` to\nrepresent these data by a single rectangular array with the leading axis\nindexing the component arrays. So instead of stacking, we concatenate along\nthe leading axis and keep track of offsets and sizes.\nThat is, we can represent ragged data contiguously using a triple of dense\narrays ``(data, offsets, sizes)``:\n* ``data``: the concatenated component arrays,\n* ``offsets``: 1D array of indices into the leading axis of ``data``\nindicating where the data for each component array begins,\n* ``sizes``: 1D array of sizes of the leading axis of each component array.\nWe refer to this triple as a ragged array. (Offsets can't be computed from\nsizes in general to allow for internal padding.)\nFor example::\ndata: f32[8,3] = jnp.array([\n[a,b,c], [d,e,f], [g,h,i], [j,k,l], [m,n,o], [p,q,r], [s,t,u], [v,w,x],\n])\noffsets: i32[3] = jnp.array([0, 1, 4])\nsizes: i32[3] = jnp.array([1, 3, 4])\n# To extract the first component array, of type f32[1,3]\ndata[offsets[0]:offsets[0]+sizes[0]]\n# To extract the second component array, of type f32[3,3]\ndata[offsets[1]:offsets[1]+sizes[1]]\n# To extract the third component array, of type f32[4,3]\ndata[offsets[2]:offsets[2]+sizes[2]]\nThe ``ragged_all_to_all`` collective operation communicates slices of ragged\narrays between devices. Each caller is both a sender and a receiver. The\n``input_offsets`` and ``send_sizes`` arguments indicate the slices of the\ncaller's ``operand`` to be sent. Received results are returned in an array\nthat has the same value of the argument ``output`` except with received values\nwritten at some slices. The ``output_offsets`` argument does *not* indicate\nthe offsets at which all the received results are written; instead,\n``output_offsets`` indicates the offsets at which the *sent* slices are\nwritten on their corresponding receivers. The sizes of received slices are\nindicated by ``recv_sizes``. See below for details.\nThe arrays ``input_offsets``, ``send_sizes``,``output_offsets``, and\n``recv_sizes`` must all be the same length, and that length must be divisible\nby the size of the mapped axis ``axis_name``. Moreover, ``send_sizes`` and\n``recv_sizes`` must satisfy::\njnp.all(send_sizes == jax.lax.all_to_all(recv_sizes, axis_name, 0, 0, tiled=True))\nSpecifically, given a call::\nresult = ragged_all_to_all(operand, output, input_offsets, send_sizes,\noutput_offsets, recv_sizes, axis_name)\nthe caller sends data like::\nassert len(input_offsets) == len(send_sizes) == len(output_offsets) == len(recv_sizes)\nN = len(input_offsets)\nslices_per_device, leftover = divmod(N, lax.axis_size(axis_name))\nassert not leftover\nfor i in range(N):\ndst_idx = i // slices_per_device\nSEND(data=operand[input_offsets[i]:input_offsets[i]+send_sizes[i]],\naxis_name=axis_name, to_axis_index=dst_idx)\nand receives data in ``result`` like::\nresult = output\noutput_offsets_ = jax.lax.all_to_all(output_offsets, axis_name, 0, 0, tiled=True)\nfor i in range(N):\nsrc_idx = i // slices_per_device\nresult = result.at[output_offsets_[i]:output_offsets_[i]+recv_sizes[i]\n].set(RECEIVE(axis_name=axis_name, from_axis_index=src_idx))\nwhere ``SEND`` and ``RECEIVE`` are pseudocode. Notice that a caller's local\n``output_offsets`` does not indicate the offsets at which its local ``result``\nis updated; instead, it indicates where the corresponding sent slices are\nwritten on their destination instances. To compute the local offsets at which\nreceived data are written, we apply an ``all_to_all`` on ``output_offsets``.\nFor example, if we apply a ``ragged_all_to_all`` along an axis of size 2, with\nthese arguments in each mapped function instance::\naxis index 0:\noperand = [1, 2, 2]\noutput = [0, 0, 0, 0]\ninput_offsets = [0, 1]\nsend_sizes = [1, 2]\noutput_offsets = [0, 0]\nrecv_sizes = [1, 1]\naxis index 1:\noperand = [3, 4, 0]\noutput = [0, 0, 0, 0]\ninput_offsets = [0, 1]\nsend_sizes = [1, 1]\noutput_offsets = [1, 2]\nrecv_sizes = [2, 1]\nthen::\naxis index 0:\nresult = [1, 3, 0, 0]\naxis index 1:\nresult = [2, 2, 4, 0]\nArgs:\noperand: data array of shape (N, A, B, ...) representing concatenated\n(possibly padded) ragged data to be sent.\noutput: data array of shape (M, A, B, ...) to update with received data.\ninput_offsets: 1D integer array of shape (K,) representing the offsets of\nleading-axis slices into ``operand`` to be sent.\nsend_sizes: 1D integer array array of shape (K,) representing the sizes of\nleading-axis slices into ``operand`` to be sent.\noutput_offsets: 1D integer array of shape (K,) representing where the\ncorresponding sent data is written on each corresponding receiver.\nrecv_sizes: 1D integer array of shape (K,) representing sizes of\nleading-axis slices into ``output`` to update with received data.\naxis_name: name of the mapped axis over which to perform the communication.\naxis_index_groups: optional list of lists containing axis indices (e.g. for\nan axis of size 4, [[0, 1], [2, 3]] would run ragged all to all over the\nfirst two and last two replicas). Groups must cover all axis indices\nexactly once, and all groups must be the same size. Otherwise, the\nbehavior is undefined.",
        "parameters": {},
        "returns": "Array of shape (M, A, B, ...) with the same value as the ``output`` except\nwith received data written into slices starting at\n``all_to_all(output_offsets, axis_name, 0, 0, tiled=True)`` and with size\n``recv_sizes``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ragged_dot",
      "signature": "ragged_dot(lhs: 'Array', rhs: 'Array', group_sizes: 'Array', precision: 'PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None, group_offset: 'Array | None' = None) -> 'Array'",
      "documentation": {
        "description": "Ragged matrix multiplication.\nArgs:\nlhs: (m, k) shaped array.\nrhs: (g, k, n) shaped array.\ngroup_sizes: (g,) shaped array with integer element type, where g denotes   number of groups. The ith element indicates the size of ith group.\nprecision: Optional. Consistent with precision argument for :func:`jax.lax.dot`.\npreferred_element_type: Optional. Consistent with precision argument for :func:`jax.lax.dot`.\ngroup_offset: Optional. (1,) shaped array that indicates the group in group_sizes to start computing from. If not specified, defaults to [0].\nResults:\n(m, n) shaped array with preferred_element_type element type.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "ragged_dot_general",
      "signature": "ragged_dot_general(lhs: 'Array', rhs: 'Array', group_sizes: 'Array', ragged_dot_dimension_numbers: 'RaggedDotDimensionNumbers', precision: 'PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None, group_offset: 'Array | None' = None) -> 'Array'",
      "documentation": {
        "description": "Ragged matrix multiplication.\nRagged dot takes three arrays---``lhs``, ``rhs``, and ``group_sizes``---and\na ``ragged_dot_dimension_numbers`` argument. Like `dot_general`, ``lhs`` and\n``rhs`` are allowed arbitrary batch and contracting dimensions. Additionally,\n``lhs`` is required to have one ragged dimension, and ``rhs`` may have at\nmost one group dimension.\nLet `g` be the number of groups in the lhs ragged dimension. Ragged dot has\nthree modes, depending on the kind of the lhs ragged dimension:\n1. `[b...,m...,k...], [g,b...,k...,n...], [b...,x...,g] -> [b...,m...,n...]`.\nHere the ragged dimension is a non-contracting dimension (`m`) of ``lhs``,\nand `x...` are the lhs non-contracting dims outer to the ragged dim.\n2. `[b...,m...,k...], [b...,k...,n...], [b...,x...,g] -> [g,b...,m...,n...]`.\nHere the ragged dimension is a contracting dimension (`k`) of ``lhs`` and\n``rhs``, and `x...` are the lhs contracting dims outer to the ragged dim.\n3. `[b...,m...,k...], [b...,k...,n...], [x...,g] -> [b...,m...,n...]`.\nHere the ragged dimension is a batch dimension (`b`) of ``lhs`` and\n``rhs``, and `x...` are the lhs batch dims outer to the ragged dim.\nIf ``group_sizes`` is passed-in with shape `[g]`, it is broadcasted according\nto the rules above.\nArgs:\nlhs: an array\nrhs: an array\ngroup_sizes: an array with integer element type\nragged_dot_dimension_numbers: a ``RaggedDotDimensionNumbers`` object to\nspecify the dot dimension numbers, lhs ragged dimension, and rhs group\ndimension.\nprecision: Optional. Consistent with precision argument for\n:func:`jax.lax.dot`.\npreferred_element_type: Optional. Consistent with precision argument for\n:func:`jax.lax.dot`.\ngroup_offset: Optional. (1,) shaped array that indicates the group in\ngroup_sizes to start computing from. If not specified, defaults to [0].\nResults:\nAn array whose shape is the same as that produced by `dot_general`, with an\nextra leading dimension of size `g` in the case where the lhs ragged\ndimension is a contracting dimension.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "random_gamma_grad",
      "signature": "random_gamma_grad(a: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise derivative of samples from `Gamma(a, 1)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "real",
      "signature": "real(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise extract real part: :math:`\\mathrm{Re}(x)`.\nThis function lowers directly to the `stablehlo.real`_ operation.\nArgs:\nx: input array. Must have complex dtype.",
        "parameters": {},
        "returns": "Array of the same shape as ``x`` containing its real part. Will have dtype\nfloat32 if ``x.dtype == complex64``, or float64 if ``x.dtype == complex128``.\nSee also:\n- :func:`jax.lax.complex`: elementwise construct complex number.\n- :func:`jax.lax.imag`: elementwise extract imaginary part.\n- :func:`jax.lax.conj`: elementwise complex conjugate.\n.. _stablehlo.real: https://openxla.org/stablehlo/spec#real",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reciprocal",
      "signature": "reciprocal(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise reciprocal: :math:`1 \\over x`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce",
      "signature": "reduce(operands: 'Any', init_values: 'Any', computation: 'Callable[[Any, Any], Any]', dimensions: 'Sequence[int]') -> 'Any'",
      "documentation": {
        "description": "Wraps XLA's `Reduce\n<https://www.tensorflow.org/xla/operation_semantics#reduce>`_\noperator.\n``init_values`` and ``computation`` together must form a `monoid\n<https://en.wikipedia.org/wiki/Monoid>`_\nfor correctness. That is ``init_values`` must be an identity of\n``computation``, and ``computation`` must be associative. XLA may exploit both\nof these properties during code generation; if either is violated the result\nis undefined.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_and",
      "signature": "reduce_and(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the bitwise AND of elements over one or more array axes.\nArgs:\noperand: array over which to compute the reduction. Must have boolean\nor integer dtype.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to reduce. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.\nSee also:\n- :func:`jax.numpy.bitwise_and.reduce`: more flexible NumPy-style logical\nreduction API, built around :func:`jax.lax.reduce_and`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_max`,\n:func:`jax.lax.reduce_min`, :func:`jax.lax.reduce_or`, :func:`jax.lax.reduce_xor`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_max",
      "signature": "reduce_max(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the maximum of elements over one or more array axes.\nArgs:\noperand: array over which to compute maximum.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to reduce. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.\nSee also:\n- :func:`jax.numpy.max`: more flexible NumPy-style max-reduction API, built\naround :func:`jax.lax.reduce_max`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_min`,\n:func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_or`, :func:`jax.lax.reduce_xor`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_min",
      "signature": "reduce_min(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the minimum of elements over one or more array axes.\nArgs:\noperand: array over which to compute minimum.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to reduce. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.\nSee also:\n- :func:`jax.numpy.min`: more flexible NumPy-style min-reduction API, built\naround :func:`jax.lax.reduce_min`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_max`,\n:func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_or`, :func:`jax.lax.reduce_xor`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_or",
      "signature": "reduce_or(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the bitwise OR of elements over one or more array axes.\nArgs:\noperand: array over which to compute the reduction. Must have boolean\nor integer dtype.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to reduce. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.\nSee also:\n- :func:`jax.numpy.bitwise_or.reduce`: more flexible NumPy-style logical\nreduction API, built around :func:`jax.lax.reduce_or`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_max`,\n:func:`jax.lax.reduce_min`, :func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_xor`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_precision",
      "signature": "reduce_precision(operand: 'float | ArrayLike', exponent_bits: 'int', mantissa_bits: 'int') -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `ReducePrecision\n<https://www.tensorflow.org/xla/operation_semantics#reduceprecision>`_\noperator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_prod",
      "signature": "reduce_prod(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the product of elements over one or more array axes.\nArgs:\noperand: array over which to sum. Must have numerical dtype.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to sum. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.",
        "raises": "",
        "see_also": "",
        "notes": "Unlike :func:`jax.numpy.prod`, :func:`jax.lax.reduce_prod` does not upcast\nnarrow-width types for accumulation, so products of 8-bit or 16-bit types\nmay be subject to rounding errors.\nSee also:\n- :func:`jax.numpy.prod`: more flexible NumPy-style product API, built\naround :func:`jax.lax.reduce_prod`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_max`, :func:`jax.lax.reduce_min`,\n:func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_or`, :func:`jax.lax.reduce_xor`.",
        "examples": ""
      }
    },
    {
      "name": "reduce_sum",
      "signature": "reduce_sum(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the sum of elements over one or more array axes.\nArgs:\noperand: array over which to sum. Must have numerical dtype.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to sum. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.",
        "raises": "",
        "see_also": "",
        "notes": "Unlike :func:`jax.numpy.sum`, :func:`jax.lax.reduce_sum` does not upcast\nnarrow-width types for accumulation, so sums of 8-bit or 16-bit types\nmay be subject to rounding errors.\nSee also:\n- :func:`jax.numpy.sum`: more flexible NumPy-style summation API, built\naround :func:`jax.lax.reduce_sum`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_max`, :func:`jax.lax.reduce_min`,\n:func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_or`, :func:`jax.lax.reduce_xor`.",
        "examples": ""
      }
    },
    {
      "name": "reduce_window",
      "signature": "reduce_window(operand, init_value, computation: 'Callable', window_dimensions: 'core.Shape', window_strides: 'Sequence[int]', padding: 'str | Sequence[tuple[int, int]]', base_dilation: 'Sequence[int] | None' = None, window_dilation: 'Sequence[int] | None' = None) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `ReduceWindowWithGeneralPadding\n<https://www.tensorflow.org/xla/operation_semantics#reducewindow>`_\noperator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_window_shape_tuple",
      "signature": "reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides, padding, base_dilation=None, window_dilation=None)",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reduce_xor",
      "signature": "reduce_xor(operand: 'ArrayLike', axes: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Compute the bitwise XOR of elements over one or more array axes.\nArgs:\noperand: array over which to compute the reduction. Must have boolean\nor integer dtype.\naxes: sequence of zero or more unique integers specifying the axes over\nwhich to reduce. Each entry must satisfy ``0 <= axis < operand.ndim``.",
        "parameters": {},
        "returns": "An array of the same dtype as ``operand``, with shape corresponding\nto the dimensions of ``operand.shape`` with ``axes`` removed.\nSee also:\n- :func:`jax.numpy.bitwise_xor.reduce`: more flexible NumPy-style logical\nreduction API, built around :func:`jax.lax.reduce_xor`.\n- Other low-level :mod:`jax.lax` reduction operators:\n:func:`jax.lax.reduce_sum`, :func:`jax.lax.reduce_prod`, :func:`jax.lax.reduce_max`,\n:func:`jax.lax.reduce_min`, :func:`jax.lax.reduce_and`, :func:`jax.lax.reduce_or`.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "rem",
      "signature": "rem(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise remainder: :math:`x \\bmod y`.\nThis function lowers directly to the `stablehlo.remainder`_ operation.\nThe sign of the result is taken from the dividend, and the absolute value\nof the result is always less than the divisor's absolute value.\nInteger division overflow (remainder by zero or remainder of INT_SMIN with -1)\nproduces an implementation defined value.\nArgs:\nx, y: Input arrays. Must have matching int or float dtypes. If neither\nis a scalar, ``x`` and ``y`` must have the same number of dimensions\nand be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the remainder.\nSee also:\n- :func:`jax.numpy.remainder`: NumPy-style remainder with different\nsign semantics.\n.. _stablehlo.remainder: https://openxla.org/stablehlo/spec#remainder",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "reshape",
      "signature": "reshape(operand: 'ArrayLike', new_sizes: 'Shape', dimensions: 'Sequence[int] | None' = None, out_sharding: 'NamedSharding | P | None' = None) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `Reshape\n<https://www.tensorflow.org/xla/operation_semantics#reshape>`_\noperator.\nFor inserting/removing dimensions of size 1, prefer using ``lax.squeeze`` /\n``lax.expand_dims``. These preserve information about axis identity that may\nbe useful for advanced transformation rules.\nArgs:\noperand: array to be reshaped.\nnew_sizes: sequence of integers specifying the resulting shape. The size\nof the final array must match the size of the input.\ndimensions: optional sequence of integers specifying the permutation order of\nthe input shape. If specified, the length must match ``operand.shape``.",
        "parameters": {},
        "returns": "out: reshaped array.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Simple reshaping from one to two dimensions:\n>>> x = jnp.arange(6)\n>>> y = reshape(x, (2, 3))\n>>> y\nArray([[0, 1, 2],\n[3, 4, 5]], dtype=int32)\nReshaping back to one dimension:\n>>> reshape(y, (6,))\nArray([0, 1, 2, 3, 4, 5], dtype=int32)\nReshaping to one dimension with permutation of dimensions:\n>>> reshape(y, (6,), (1, 0))\nArray([0, 3, 1, 4, 2, 5], dtype=int32)"
      }
    },
    {
      "name": "rev",
      "signature": "rev(operand: 'ArrayLike', dimensions: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `Rev\n<https://www.tensorflow.org/xla/operation_semantics#rev_reverse>`_\noperator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "rng_bit_generator",
      "signature": "rng_bit_generator(key, shape, dtype=<class 'numpy.uint32'>, algorithm=<RandomAlgorithm.RNG_DEFAULT: 0>)",
      "documentation": {
        "description": "Stateless PRNG bit generator. Experimental and its use is discouraged.",
        "parameters": {},
        "returns": "(what is required to be an integer type) using the platform specific\ndefault algorithm or the one specified.\nIt provides direct access to the RngBitGenerator primitive exposed by XLA\n(https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator) for low\nlevel API access.\nMost users should use `jax.random` instead for a stable and more user\nfriendly API.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "rng_uniform",
      "signature": "rng_uniform(a, b, shape)",
      "documentation": {
        "description": "Stateful PRNG generator. Experimental and its use is discouraged.",
        "parameters": {},
        "returns": "b <= a, then the result is undefined, and different implementations may\nreturn different results.\nYou should use jax.random for most purposes; this function exists only for\nniche use cases with special performance requirements.\nThis API may be removed at any time.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "round",
      "signature": "round(x: 'ArrayLike', rounding_method: 'RoundingMethod' = <RoundingMethod.AWAY_FROM_ZERO: 0>) -> 'Array'",
      "documentation": {
        "description": "Elementwise round.\nRounds values to the nearest integer. This function lowers directly to the\n`stablehlo.round`_ operation.\nArgs:\nx: an array or scalar value to round. Must have floating-point type.\nrounding_method: the method to use when rounding halfway values\n(e.g., ``0.5``). See :class:`jax.lax.RoundingMethod` for possible values.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x``, containing the elementwise\nrounding of ``x``.\nSee also:\n- :func:`jax.lax.floor`: round to the next integer toward negative infinity\n- :func:`jax.lax.ceil`: round to the next integer toward positive infinity",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ">>> import jax.numpy as jnp\n>>> from jax import lax\n>>> x = jnp.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n>>> jax.lax.round(x)  # defaults method is AWAY_FROM_ZERO\nArray([-2., -1., -1.,  0.,  1.,  1.,  2.], dtype=float32)\n>>> jax.lax.round(x, rounding_method=jax.lax.RoundingMethod.TO_NEAREST_EVEN)\nArray([-2., -1., -0.,  0.,  0.,  1.,  2.], dtype=float32)\n.. _stablehlo.round: https://openxla.org/stablehlo/spec#round"
      }
    },
    {
      "name": "rsqrt",
      "signature": "rsqrt(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise reciprocal square root:  :math:`1 \\over \\sqrt{x}`.\nThis function lowers directly to the `stablehlo.rsqrt`_ operation.\nArgs:\nx: Input array. Must have floating or complex dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x`` containing the\nreciporical square root.\nSee also:\n:func:`jax.lax.pow`: Elementwise power.\n:func:`jax.lax.sqrt`: Elementwise square root.\n:func:`jax.lax.cbrt`: Elementwise cube root.\n.. _stablehlo.rsqrt: https://openxla.org/stablehlo/spec#rsqrt",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scan",
      "signature": "scan(f: 'Callable[[Carry, X], tuple[Carry, Y]]', init: 'Carry', xs: 'X | None' = None, length: 'int | None' = None, reverse: 'bool' = False, unroll: 'int | bool' = 1, _split_transpose: 'bool' = False) -> 'tuple[Carry, Y]'",
      "documentation": {
        "description": "Scan a function over leading array axes while carrying along state.\nThe `Haskell-like type signature`_ in brief is\n.. code-block:: haskell\nscan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\nwhere for any array type specifier ``t``, ``[t]`` represents the type with an additional\nleading axis, and if ``t`` is a pytree (container) type with array leaves then ``[t]``\nrepresents the type with the same pytree structure and corresponding leaves\neach with an additional leading axis.\nWhen the type of ``xs`` (denoted `a` above) is an array type or None, and the type\nof ``ys`` (denoted `b` above) is an array type, the semantics of :func:`~scan` are\ngiven roughly by this Python implementation::\ndef scan(f, init, xs, length=None):\nif xs is None:\nxs = [None] * length\ncarry = init\nys = []\nfor x in xs:\ncarry, y = f(carry, x)\nys.append(y)\nreturn carry, np.stack(ys)\nUnlike that Python version, both ``xs`` and ``ys`` may be arbitrary pytree\nvalues, and so multiple arrays can be scanned over at once and produce multiple\noutput arrays. ``None`` is actually a special case of this, as it represents an\nempty pytree.\nAlso unlike that Python version, :func:`~scan` is a JAX primitive and is\nlowered to a single WhileOp. That makes it useful for reducing\ncompilation times for JIT-compiled functions, since native Python\nloop constructs in an :func:`~jax.jit` function are unrolled, leading to large\nXLA computations.\nFinally, the loop-carried value ``carry`` must hold a fixed shape and dtype\nacross all iterations (and not just be consistent up to NumPy rank/shape\nbroadcasting and dtype promotion rules, for example). In other words, the type\n``c`` in the type signature above represents an array with a fixed shape and\ndtype (or a nested tuple/list/dict container data structure with a fixed\nstructure and arrays with fixed shape and dtype at the leaves).\n.. note::\n:py:func:`scan` compiles ``f``, so while it can be combined with\n:py:func:`jit`, it's usually unnecessary.\nArgs:\nf: a Python function to be scanned of type ``c -> a -> (c, b)``, meaning\nthat ``f`` accepts two arguments where the first is a value of the loop\ncarry and the second is a slice of ``xs`` along its leading axis, and that\n``f`` returns a pair where the first element represents a new value for\nthe loop carry and the second represents a slice of the output.\ninit: an initial loop carry value of type ``c``, which can be a scalar,\narray, or any pytree (nested Python tuple/list/dict) thereof, representing\nthe initial loop carry value. This value must have the same structure as\nthe first element of the pair returned by ``f``.\nxs: the value of type ``[a]`` over which to scan along the leading axis,\nwhere ``[a]`` can be an array or any pytree (nested Python\ntuple/list/dict) thereof with consistent leading axis sizes.\nlength: optional integer specifying the number of loop iterations, which\nmust agree with the sizes of leading axes of the arrays in ``xs`` (but can\nbe used to perform scans where no input ``xs`` are needed).\nreverse: optional boolean specifying whether to run the scan iteration\nforward (the default) or in reverse, equivalent to reversing the leading\naxes of the arrays in both ``xs`` and in ``ys``.\nunroll: optional positive int or bool specifying, in the underlying\noperation of the scan primitive, how many scan iterations to unroll within\na single iteration of a loop. If an integer is provided, it determines how\nmany unrolled loop iterations to run within a single rolled iteration of\nthe loop. If a boolean is provided, it will determine if the loop is\ncompetely unrolled (i.e. `unroll=True`) or left completely rolled (i.e.\n`unroll=False`).\n_split_transpose: experimental optional bool specifying whether to further\nsplit the transpose into a scan (computing activation gradients), and a\nmap (computing gradients corresponding to the array arguments). Enabling\nthis may increase memory requirements, and so is an experimental feature\nthat may evolve or even be rolled back.",
        "parameters": {},
        "returns": "A pair of type ``(c, [b])`` where the first element represents the final\nloop carry value and the second element represents the stacked outputs of\nthe second output of ``f`` when scanned over the leading axis of the inputs.\n.. _Haskell-like type signature: https://wiki.haskell.org/Type_signature",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter",
      "signature": "scatter(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-update operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where updates\nreplace values from `operand`.\nIf multiple updates are performed to the same index of operand, they may be\napplied in any order.\n:func:`scatter` is a low-level operator with complicated semantics, and most\nJAX users will never need to call it directly. Instead, you should prefer using\n:func:`jax.numpy.ndarray.at` for more familiary NumPy-style indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices`, `updates` and the output\nrelate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "As mentioned above, you should basically never use :func:`scatter` directly,\nand instead perform scatter-style operations using NumPy-style indexing\nexpressions via :attr:`jax.numpy.ndarray.at`.\nHere is and example of updating entries in an array using :attr:`jax.numpy.ndarray.at`,\nwhich lowers to an XLA Scatter operation:\n>>> x = jnp.zeros(5)\n>>> indices = jnp.array([1, 2, 4])\n>>> values = jnp.array([2.0, 3.0, 4.0])\n>>> x.at[indices].set(values)\nArray([0., 2., 3., 0., 4.], dtype=float32)\nThis syntax also supports several of the optional arguments to :func:`scatter`,\nfor example:\n>>> x.at[indices].set(values, indices_are_sorted=True, mode='promise_in_bounds')\nArray([0., 2., 3., 0., 4.], dtype=float32)\nBy comparison, here is the equivalent function call using :func:`scatter` directly,\nwhich is not something typical users should ever need to do:\n>>> lax.scatter(x, indices[:, None], values,\n...             dimension_numbers=lax.ScatterDimensionNumbers(\n...                 update_window_dims=(),\n...                 inserted_window_dims=(0,),\n...                 scatter_dims_to_operand_dims=(0,)),\n...             indices_are_sorted=True,\n...             mode=lax.GatherScatterMode.PROMISE_IN_BOUNDS)\nArray([0., 2., 3., 0., 4.], dtype=float32)"
      }
    },
    {
      "name": "scatter_add",
      "signature": "scatter_add(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-add operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where\naddition is used to combine updates and values from `operand`.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `scatter_indices`, `updates` and the output\nrelate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_apply",
      "signature": "scatter_apply(operand: 'Array', scatter_indices: 'Array', func: 'Callable[[Array], Array]', dimension_numbers: 'ScatterDimensionNumbers', *, update_shape: 'Shape' = (), indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-apply operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where values\nfrom ``operand`` are replaced with ``func(operand)``, with duplicate indices\nresulting in multiple applications of ``func``.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nNote that in the current implementation, ``scatter_apply`` is not compatible\nwith automatic differentiation.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nfunc: unary function that will be applied at each index.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices`, `updates` and the output\nrelate.\nupdate_shape: the shape of the updates at the given indices.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the result of applying `func` to `operand` at the given indices.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_max",
      "signature": "scatter_max(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-max operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where\nthe `max` function is used to combine updates and values from `operand`.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices`, `updates` and the output\nrelate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_min",
      "signature": "scatter_min(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-min operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where\nthe `min` function is used to combine updates and values from `operand`.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices`, `updates` and the output\nrelate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_mul",
      "signature": "scatter_mul(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-multiply operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where\nmultiplication is used to combine updates and values from `operand`.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes\nhow dimensions of `operand`, `start_indices`, `updates` and the output\nrelate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve performance on\nsome backends. JAX does not check this promise: if the updated elements\noverlap when ``unique_indices`` is ``True`` the behavior is undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when\nset to 'fill' or 'drop' out-of-bounds updates are dropped. The behavior\nfor out-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "scatter_sub",
      "signature": "scatter_sub(operand: 'ArrayLike', scatter_indices: 'ArrayLike', updates: 'ArrayLike', dimension_numbers: 'ScatterDimensionNumbers', *, indices_are_sorted: 'bool' = False, unique_indices: 'bool' = False, mode: 'str | GatherScatterMode | None' = None) -> 'Array'",
      "documentation": {
        "description": "Scatter-sub operator.\nWraps `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_, where\nsubtraction is used to combine updates and values from `operand`.\nThe semantics of scatter are complicated, and its API might change in the\nfuture. For most use cases, you should prefer the\n:attr:`jax.numpy.ndarray.at` property on JAX arrays which uses\nthe familiar NumPy indexing syntax.\nArgs:\noperand: an array to which the scatter should be applied\nscatter_indices: an array that gives the indices in `operand` to which each\nupdate in `updates` should be applied.\nupdates: the updates that should be scattered onto `operand`.\ndimension_numbers: a `lax.ScatterDimensionNumbers` object that describes how\ndimensions of `operand`, `start_indices`, `updates` and the output relate.\nindices_are_sorted: whether `scatter_indices` is known to be sorted. If\ntrue, may improve performance on some backends.\nunique_indices: whether the elements to be updated in ``operand`` are\nguaranteed to not overlap with each other. If true, may improve\nperformance on some backends. JAX does not check this promise: if the\nupdated elements overlap when ``unique_indices`` is ``True`` the behavior\nis undefined.\nmode: how to handle indices that are out of bounds: when set to 'clip',\nindices are clamped so that the slice is within bounds, and when set to\n'fill' or 'drop' out-of-bounds updates are dropped. The behavior for\nout-of-bounds indices when set to 'promise_in_bounds' is\nimplementation-defined.",
        "parameters": {},
        "returns": "An array containing the sum of `operand` and the scattered updates.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "select",
      "signature": "select(pred: 'ArrayLike', on_true: 'ArrayLike', on_false: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Selects between two branches based on a boolean predicate.\nWraps XLA's `Select\n<https://www.tensorflow.org/xla/operation_semantics#select>`_\noperator.\nIn general :func:`~jax.lax.select` leads to evaluation of both branches, although\nthe compiler may elide computations if possible. For a similar function that\nusually evaluates only a single branch, see :func:`~jax.lax.cond`.\nArgs:\npred: boolean array\non_true: array containing entries to return where ``pred`` is True. Must have\nthe same shape as ``pred``, and the same shape and dtype as ``on_false``.\non_false: array containing entries to return where ``pred`` is False. Must have\nthe same shape as ``pred``, and the same shape and dtype as ``on_true``.",
        "parameters": {},
        "returns": "result: array with same shape and dtype as ``on_true`` and ``on_false``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "select_n",
      "signature": "select_n(which: 'ArrayLike', *cases: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Selects array values from multiple cases.\nGeneralizes XLA's `Select\n<https://www.tensorflow.org/xla/operation_semantics#select>`_\noperator. Unlike XLA's version, the operator is variadic and can select\nfrom many cases using an integer `pred`.\nArgs:\nwhich: determines which case should be returned. Must be an array containing\neither a boolean or integer values. May either be a scalar or have\nshape matching ``cases``. For each array element, the value of ``which``\ndetermines which of ``cases`` is taken. ``which`` must be in the range\n``[0 .. len(cases))``; for values outside that range the behavior is\nimplementation-defined.\n*cases: a non-empty list of array cases. All must have equal dtypes and\nequal shapes.",
        "parameters": {},
        "returns": "An array with shape and dtype equal to the cases, whose values are chosen\naccording to ``which``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "shift_left",
      "signature": "shift_left(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise left shift: :math:`x \\ll y`.\nThis function lowers directly to the `stablehlo.shift_left`_ operation.\nArgs:\nx, y: Input arrays. Must have matching integer dtypes. If neither is a\nscalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the element-wise\nleft shift of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.left_shift`: NumPy wrapper for this API, also accessible\nvia the ``x << y`` operator on JAX arrays.\n- :func:`jax.lax.shift_right_arithmetic`: Elementwise arithmetic right shift.\n- :func:`jax.lax.shift_right_logical`: Elementwise logical right shift.\n.. _stablehlo.shift_left: https://openxla.org/stablehlo/spec#shift_left",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "shift_right_arithmetic",
      "signature": "shift_right_arithmetic(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise arithmetic right shift: :math:`x \\gg y`.\nThis function lowers directly to the `stablehlo.shift_right_arithmetic`_ operation.\nArgs:\nx, y: Input arrays. Must have matching integer dtypes. If neither is a\nscalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the element-wise\narithmetic right shift of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.right_shift`: NumPy wrapper for this API when applied to\nsigned integers, also accessible via the ``x >> y`` operator on JAX arrays\nwith signed integer dtype.\n- :func:`jax.lax.shift_left`: Elementwise left shift.\n- :func:`jax.lax.shift_right_logical`: Elementwise logical right shift.\n.. _stablehlo.shift_right_arithmetic: https://openxla.org/stablehlo/spec#shift_right_arithmetic",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "shift_right_logical",
      "signature": "shift_right_logical(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise logical right shift: :math:`x \\gg y`.\nThis function lowers directly to the `stablehlo.shift_right_logical`_ operation.\nArgs:\nx, y: Input arrays. Must have matching integer dtypes. If neither is a\nscalar, ``x`` and ``y`` must have the same number of dimensions and\nbe broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the element-wise\nlogical right shift of each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.right_shift`: NumPy wrapper for this API when applied to\nunsigned integers, also accessible via the ``x >> y`` operator on JAX arrays\nwith unsigned integer dtype.\n- :func:`jax.lax.shift_left`: Elementwise left shift.\n- :func:`jax.lax.shift_right_arithmetic`: Elementwise arithmetic right shift.\n.. _stablehlo.shift_right_logical: https://openxla.org/stablehlo/spec#shift_right_logical",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "sign",
      "signature": "sign(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise sign.\nThis function lowers directly to the `stablehlo.sign`_ operation.\nArgs:\nx: input array",
        "parameters": {},
        "returns": "Array of same shape and dtype as ``x``, containing the sign\nof the value, as defined in Notes below.",
        "raises": "",
        "see_also": "",
        "notes": "For floating-point inputs, returns\n.. math::\n\\mathrm{sign}(x) = \\begin{cases}\n-1 & x < 0\\\\\n-0 & x = -0\\\\\n\\mathit{NaN} & x = \\mathit{NaN}\\\\\n+0 & x = +0\\\\\n1 & x > 0\n\\end{cases}\nFor signed integer inputs, returns\n.. math::\n\\mathrm{sign}(x) = \\begin{cases}\n-1 & x < 0\\\\\n0 & x = 0\\\\\n1 & x > 0\n\\end{cases}\nFor complex inputs, returns the complex phase, i.e.\n:math:`\\mathrm{sign}(x) = x / |x|`.\n.. _stablehlo.sign: https://openxla.org/stablehlo/spec#sign",
        "examples": ""
      }
    },
    {
      "name": "sin",
      "signature": "sin(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise sine: :math:`\\mathrm{sin}(x)`.\nFor floating-point inputs, this function lowers directly to the\n`stablehlo.sine`_ operation. For complex inputs, it lowers to a\nsequence of HLO operations implementing the complex sine.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nsine.\nSee also:\n- :func:`jax.lax.cos`: elementwise cosine.\n- :func:`jax.lax.tan`: elementwise tangent.\n- :func:`jax.lax.asin`: elementwise arc sine.\n.. _stablehlo.sine: https://openxla.org/stablehlo/spec#sine",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "sinh",
      "signature": "sinh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise hyperbolic sine: :math:`\\mathrm{sinh}(x)`.\nThis function lowers directly to the ``chlo.sinh`` operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nhyperbolic sine.\nSee also:\n- :func:`jax.lax.asinh`: elementwise inverse hyperbolic sine.\n- :func:`jax.lax.cosh`: elementwise hyperbolic cosine.\n- :func:`jax.lax.tanh`: elementwise hyperbolic tangent.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "slice",
      "signature": "slice(operand: 'ArrayLike', start_indices: 'Sequence[int]', limit_indices: 'Sequence[int]', strides: 'Sequence[int] | None' = None) -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `Slice\n<https://www.tensorflow.org/xla/operation_semantics#slice>`_\noperator.\nArgs:\noperand: an array to slice\nstart_indices: a sequence of ``operand.ndim`` start indices.\nlimit_indices: a sequence of ``operand.ndim`` limit indices.\nstrides: an optional sequence of ``operand.ndim`` strides.",
        "parameters": {},
        "returns": "The sliced array",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.slice_in_dim`\n- :func:`jax.lax.index_in_dim`\n- :func:`jax.lax.dynamic_slice`",
        "notes": "",
        "examples": "Here are some examples of simple two-dimensional slices:\n>>> x = jnp.arange(12).reshape(3, 4)\n>>> x\nArray([[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]], dtype=int32)\n>>> lax.slice(x, (1, 0), (3, 2))\nArray([[4, 5],\n[8, 9]], dtype=int32)\n>>> lax.slice(x, (0, 0), (3, 4), (1, 2))\nArray([[ 0,  2],\n[ 4,  6],\n[ 8, 10]], dtype=int32)\nThese two examples are equivalent to the following Python slicing syntax:\n>>> x[1:3, 0:2]\nArray([[4, 5],\n[8, 9]], dtype=int32)\n>>> x[0:3, 0:4:2]\nArray([[ 0,  2],\n[ 4,  6],\n[ 8, 10]], dtype=int32)"
      }
    },
    {
      "name": "slice_in_dim",
      "signature": "slice_in_dim(operand: 'Array | np.ndarray', start_index: 'int | None', limit_index: 'int | None', stride: 'int' = 1, axis: 'int' = 0) -> 'Array'",
      "documentation": {
        "description": "Convenience wrapper around :func:`lax.slice` applying to only one dimension.\nThis is effectively equivalent to ``operand[..., start_index:limit_index:stride]``\nwith the indexing applied on the specified axis.\nArgs:\noperand: an array to slice.\nstart_index: an optional start index (defaults to zero)\nlimit_index: an optional end index (defaults to operand.shape[axis])\nstride: an optional stride (defaults to 1)\naxis: the axis along which to apply the slice (defaults to 0)",
        "parameters": {},
        "returns": "An array containing the slice.",
        "raises": "",
        "see_also": "- :attr:`jax.numpy.ndarray.at`\n- :func:`jax.lax.slice`\n- :func:`jax.lax.index_in_dim`\n- :func:`jax.lax.dynamic_slice_in_dim`",
        "notes": "",
        "examples": "Here is a one-dimensional example:\n>>> x = jnp.arange(4)\n>>> lax.slice_in_dim(x, 1, 3)\nArray([1, 2], dtype=int32)\nHere are some two-dimensional examples:\n>>> x = jnp.arange(12).reshape(4, 3)\n>>> x\nArray([[ 0,  1,  2],\n[ 3,  4,  5],\n[ 6,  7,  8],\n[ 9, 10, 11]], dtype=int32)\n>>> lax.slice_in_dim(x, 1, 3)\nArray([[3, 4, 5],\n[6, 7, 8]], dtype=int32)\n>>> lax.slice_in_dim(x, 1, 3, axis=1)\nArray([[ 1,  2],\n[ 4,  5],\n[ 7,  8],\n[10, 11]], dtype=int32)"
      }
    },
    {
      "name": "sort",
      "signature": "sort(operand: 'Array | Sequence[Array]', dimension: 'int' = -1, is_stable: 'bool' = True, num_keys: 'int' = 1) -> 'Array | tuple[Array, ...]'",
      "documentation": {
        "description": "Wraps XLA's `Sort\n<https://www.tensorflow.org/xla/operation_semantics#sort>`_ operator.\nFor floating point inputs, -0.0 and 0.0 are treated as equivalent, and NaN values\nare sorted to the end of the array. For complex inputs, the sort order is\nlexicographic over the real and imaginary parts, with the real part primary.\nArgs:\noperand : Array or sequence of arrays\ndimension : integer dimension along which to sort. Default: -1.\nis_stable : boolean specifying whether to use a stable sort. Default: True.\nnum_keys : number of operands to treat as sort keys. Default: 1.\nFor num_keys > 1, the sort order will be determined lexicographically using\nthe first `num_keys` arrays, with the first key being primary.\nThe remaining operands will be returned with the same permutation.",
        "parameters": {},
        "returns": "operand : sorted version of the input or inputs.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "sort_key_val",
      "signature": "sort_key_val(keys: 'Array', values: 'ArrayLike', dimension: 'int' = -1, is_stable: 'bool' = True) -> 'tuple[Array, Array]'",
      "documentation": {
        "description": "Sorts ``keys`` along ``dimension`` and applies the same permutation to ``values``.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "split",
      "signature": "split(operand: 'ArrayLike', sizes: 'Sequence[int]', axis: 'int' = 0) -> 'Sequence[Array]'",
      "documentation": {
        "description": "Splits an array along ``axis``.\nArgs:\noperand: an array to split\nsizes: the sizes of the split arrays. The sum of the sizes must be equal\nto the size of the ``axis`` dimension of ``operand``.\naxis: the axis along which to split the array.",
        "parameters": {},
        "returns": "A sequence of ``len(sizes)`` arrays. If ``sizes`` is\n``[s1, s2, ...]``, this function returns chunks of sizes ``s1``, ``s2``,\ntaken along ``axis``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "sqrt",
      "signature": "sqrt(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise square root: :math:`\\sqrt{x}`.\nThis function lowers directly to the `stablehlo.sqrt`_ operation.\nArgs:\nx: Input array. Must have floating or complex dtype.",
        "parameters": {},
        "returns": "An array of the same shape and dtype as ``x`` containing the square root.\nSee also:\n:func:`jax.lax.pow`: Elementwise power.\n:func:`jax.lax.cbrt`: Elementwise cube root.\n:func:`jax.lax.rsqrt`: Elementwise reciporical square root.\n.. _stablehlo.sqrt: https://openxla.org/stablehlo/spec#sqrt",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "square",
      "signature": "square(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise square: :math:`x^2`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "squeeze",
      "signature": "squeeze(array: 'ArrayLike', dimensions: 'Sequence[int]') -> 'Array'",
      "documentation": {
        "description": "Squeeze any number of size 1 dimensions from an array.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "stop_gradient",
      "signature": "stop_gradient(x: 'T') -> 'T'",
      "documentation": {
        "description": "Stops gradient computation.\nOperationally ``stop_gradient`` is the identity function, that is, it returns\nargument `x` unchanged. However, ``stop_gradient`` prevents the flow of\ngradients during forward or reverse-mode automatic differentiation. If there\nare multiple nested gradient computations, ``stop_gradient`` stops gradients\nfor all of them. For some discussion of where this is useful, refer to\n:ref:`stopping-gradients`.\nArgs:\nx: array or pytree of arrays",
        "parameters": {},
        "returns": "input value is returned unchanged, but within autodiff will be treated as\na constant.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Consider a simple function that returns the square of the input value:\n>>> def f1(x):\n...   return x ** 2\n>>> x = jnp.float32(3.0)\n>>> f1(x)\nArray(9.0, dtype=float32)\n>>> jax.grad(f1)(x)\nArray(6.0, dtype=float32)\nThe same function with ``stop_gradient`` around ``x`` will be equivalent\nunder normal evaluation, but return a zero gradient because ``x`` is\neffectively treated as a constant:\n>>> def f2(x):\n...   return jax.lax.stop_gradient(x) ** 2\n>>> f2(x)\nArray(9.0, dtype=float32)\n>>> jax.grad(f2)(x)\nArray(0.0, dtype=float32)\nThis is used in a number of places within the JAX codebase; for example\n:func:`jax.nn.softmax` internally normalizes the input by its maximum\nvalue, and this maximum value is wrapped in ``stop_gradient`` for\nefficiency. Refer to :ref:`stopping-gradients` for more discussion of\nthe applicability of ``stop_gradient``."
      }
    },
    {
      "name": "sub",
      "signature": "sub(x: 'ArrayLike', y: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise subtraction: :math:`x - y`.\nThis function lowers directly to the `stablehlo.subtract`_ operation.\nArgs:\nx, y: Input arrays. Must have matching numerical dtypes. If neither\nis a scalar, ``x`` and ``y`` must have the same number of dimensions\nand be broadcast compatible.",
        "parameters": {},
        "returns": "An array of the same dtype as ``x`` and ``y`` containing the difference\nof each pair of broadcasted entries.\nSee also:\n- :func:`jax.numpy.subtract`: NumPy-style subtraction supporting\ninputs with mixed dtypes and ranks.\n.. _stablehlo.subtract: https://openxla.org/stablehlo/spec#subtract",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "switch",
      "signature": "switch(index, branches: 'Sequence[Callable]', *operands, operand=<object object at 0x7b2236d7b5d0>)",
      "documentation": {
        "description": "Apply exactly one of the ``branches`` given by ``index``.\nIf ``index`` is out of bounds, it is clamped to within bounds.\nHas the semantics of the following Python::\ndef switch(index, branches, *operands):\nindex = clamp(0, index, len(branches) - 1)\nreturn branches[index](*operands)\nInternally this wraps XLA's `Conditional\n<https://www.tensorflow.org/xla/operation_semantics#conditional>`_\noperator. However, when transformed with :func:`~jax.vmap` to operate over a\nbatch of predicates, ``cond`` is converted to :func:`~jax.lax.select`.\nArgs:\nindex: Integer scalar type, indicating which branch function to apply.\nbranches: Sequence of functions (A -> B) to be applied based on ``index``.\nAll branches must return the same output structure.\noperands: Operands (A) input to whichever branch is applied.",
        "parameters": {},
        "returns": "Value (B) of ``branch(*operands)`` for the branch that was selected based\non ``index``.",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "tan",
      "signature": "tan(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise tangent: :math:`\\mathrm{tan}(x)`.\nThis function lowers directly to the `stablehlo.tangent`_ operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\ntangent.\nSee also:\n- :func:`jax.lax.cos`: elementwise cosine.\n- :func:`jax.lax.sin`: elementwise sine.\n- :func:`jax.lax.atan`: elementwise arc tangent.\n- :func:`jax.lax.atan2`: elementwise 2-term arc tangent.\n.. _stablehlo.tangent: https://openxla.org/stablehlo/spec#tangent",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "tanh",
      "signature": "tanh(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "Elementwise hyperbolic tangent: :math:`\\mathrm{tanh}(x)`.\nThis function lowers directly to the `stablehlo.tanh`_ operation.\nArgs:\nx: input array. Must have floating-point or complex type.",
        "parameters": {},
        "returns": "Array of the same shape and dtype as ``x`` containing the element-wise\nhyperbolic tangent.\nSee also:\n- :func:`jax.lax.atanh`: elementwise inverse hyperbolic tangent.\n- :func:`jax.lax.cosh`: elementwise hyperbolic cosine.\n- :func:`jax.lax.sinh`: elementwise hyperbolic sine.\n.. _stablehlo.tanh: https://openxla.org/stablehlo/spec#tanh",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "top_k",
      "signature": "top_k(operand: 'ArrayLike', k: 'int') -> 'tuple[Array, Array]'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "Args:\noperand: N-dimensional array of non-complex type.\nk: integer specifying the number of top entries.\nA tuple ``(values, indices)`` where\n- ``values`` is an array containing the top k values along the last axis.\n- ``indices`` is an array containing the indices corresponding to values.\nSee also:\n- :func:`jax.lax.approx_max_k`\n- :func:`jax.lax.approx_min_k`",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Find the largest three values, and their indices, within an array:\n>>> x = jnp.array([9., 3., 6., 4., 10.])\n>>> values, indices = jax.lax.top_k(x, 3)\n>>> values\nArray([10.,  9.,  6.], dtype=float32)\n>>> indices\nArray([4, 0, 2], dtype=int32)"
      }
    },
    {
      "name": "transpose",
      "signature": "transpose(operand: 'ArrayLike', permutation: 'Sequence[int] | np.ndarray') -> 'Array'",
      "documentation": {
        "description": "Wraps XLA's `Transpose\n<https://www.tensorflow.org/xla/operation_semantics#transpose>`_\noperator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "while_loop",
      "signature": "while_loop(cond_fun: 'Callable[[T], BooleanNumeric]', body_fun: 'Callable[[T], T]', init_val: 'T') -> 'T'",
      "documentation": {
        "description": "Call ``body_fun`` repeatedly in a loop while ``cond_fun`` is True.\nThe `Haskell-like type signature`_ in brief is\n.. code-block:: haskell\nwhile_loop :: (a -> Bool) -> (a -> a) -> a -> a\nThe semantics of ``while_loop`` are given by this Python implementation::\ndef while_loop(cond_fun, body_fun, init_val):\nval = init_val\nwhile cond_fun(val):\nval = body_fun(val)\nreturn val\nUnlike that Python version, ``while_loop`` is a JAX primitive and is lowered\nto a single WhileOp. That makes it useful for reducing compilation times\nfor jit-compiled functions, since native Python loop constructs in an ``@jit``\nfunction are unrolled, leading to large XLA computations.\nAlso unlike the Python analogue, the loop-carried value ``val`` must hold a\nfixed shape and dtype across all iterations (and not just be consistent up to\nNumPy rank/shape broadcasting and dtype promotion rules, for example). In\nother words, the type ``a`` in the type signature above represents an array\nwith a fixed shape and dtype (or a nested tuple/list/dict container data\nstructure with a fixed structure and arrays with fixed shape and dtype at the\nleaves).\nAnother difference from using Python-native loop constructs is that\n``while_loop`` is not reverse-mode differentiable because XLA computations\nrequire static bounds on memory requirements.\n.. note::\n:py:func:`while_loop` compiles ``cond_fun`` and ``body_fun``, so while it\ncan be combined with :py:func:`jit`, it's usually unnecessary.\nArgs:\ncond_fun: function of type ``a -> Bool``.\nbody_fun: function of type ``a -> a``.\ninit_val: value of type ``a``, a type that can be a scalar, array, or any\npytree (nested Python tuple/list/dict) thereof, representing the initial\nloop carry value.",
        "parameters": {},
        "returns": "The output from the final iteration of body_fun, of type ``a``.\n.. _Haskell-like type signature: https://wiki.haskell.org/Type_signature",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "with_sharding_constraint",
      "signature": "with_sharding_constraint(x, shardings)",
      "documentation": {
        "description": "Mechanism to constrain the sharding of an Array inside a jitted computation\nThis is a strict constraint for the GSPMD partitioner and not a hint. For examples\nof how to use this function, see `Distributed arrays and automatic parallelization`_.\nInside of a jitted computation, with_sharding_constraint makes it possible to\nconstrain intermediate values to an uneven sharding. However, if such an\nunevenly sharded value is output by the jitted computation, it will come out\nas fully replicated, no matter the sharding annotation given.\nArgs:\nx: PyTree of jax.Arrays which will have their shardings constrained\nshardings: PyTree of sharding specifications. Valid values are the same as for\nthe ``in_shardings`` argument of :func:`jax.experimental.pjit`.",
        "parameters": {},
        "returns": "x_with_shardings: PyTree of jax.Arrays with specified sharding constraints.\n.. _Distributed arrays and automatic parallelization: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "zeros_like_array",
      "signature": "zeros_like_array(x: 'ArrayLike') -> 'Array'",
      "documentation": {
        "description": "",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    },
    {
      "name": "zeta",
      "signature": "zeta(x: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex], q: Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number, bool, int, float, complex]) -> jax.Array",
      "documentation": {
        "description": "Elementwise Hurwitz zeta function: :math:`\\zeta(x, q)`",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      }
    }
  ],
  "classes": [
    {
      "name": "ConvDimensionNumbers",
      "documentation": {
        "description": "Describes batch, spatial, and feature dimensions of a convolution.\nArgs:\nlhs_spec: a tuple of nonnegative integer dimension numbers containing\n`(batch dimension, feature dimension, spatial dimensions...)`.\nrhs_spec: a tuple of nonnegative integer dimension numbers containing\n`(out feature dimension, in feature dimension, spatial dimensions...)`.\nout_spec: a tuple of nonnegative integer dimension numbers containing\n`(batch dimension, feature dimension, spatial dimensions...)`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DotAlgorithm",
      "documentation": {
        "description": "Specify the algorithm used for computing dot products.\nWhen used to specify the ``precision`` input to :func:`~jax.lax.dot`,\n:func:`~jax.lax.dot_general`, and other dot product functions, this data\nstructure is used for controlling the properties of the algorithm used for\ncomputing the dot product. This API controls the precision used for the\ncomputation, and allows users to access hardware-specific accelerations.\nSupport for these algorithms is platform dependent, and using an unsupported\nalgorithm will raise a Python exception when the computation is compiled. The\nalgorithms that are known to be supported on at least some platforms are\nlisted in the :class:`~jax.lax.DotAlgorithmPreset` enum, and these are a\ngood starting point for experimenting with this API.\nA \"dot algorithm\" is specified by the following parameters:\n* ``lhs_precision_type`` and ``rhs_precision_type``, the data types that the\nLHS and RHS of the operation are rounded to.\n* ``accumulation_type`` the data type used for accumulation.\n* ``lhs_component_count``, ``rhs_component_count``, and\n``num_primitive_operations`` apply to algorithms that decompose the LHS\nand/or RHS into multiple components and execute multiple operations on\nthose values, usually to emulate a higher precision. For algorithms with no\ndecomposition, these values should be set to ``1``.\n* ``allow_imprecise_accumulation`` to specify if accumulation in lower\nprecision is permitted for some steps (e.g.\n``CUBLASLT_MATMUL_DESC_FAST_ACCUM``).\nThe `StableHLO spec <https://openxla.org/stablehlo/spec#dot_general>`_ for\nthe dot operation doesn't require that the precision types be the same as the\nstorage types for the inputs or outputs, but some plaforms may require that\nthese types match. Furthermore, the return type of\n:func:`~jax.lax.dot_general` is always defined by the ``accumulation_type``\nparameter of the input algorithm, if specified.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": "Accumulate two 16-bit floats using a 32-bit float accumulator:\n>>> algorithm = DotAlgorithm(\n...     lhs_precision_type=np.float16,\n...     rhs_precision_type=np.float16,\n...     accumulation_type=np.float32,\n... )\n>>> lhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype=np.float16)\n>>> rhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype=np.float16)\n>>> dot(lhs, rhs, precision=algorithm)  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float16)\nOr, equivalently, using a preset:\n>>> algorithm = DotAlgorithmPreset.F16_F16_F32\n>>> dot(lhs, rhs, precision=algorithm)  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float16)\nPresets can also be specified by name:\n>>> dot(lhs, rhs, precision=\"F16_F16_F32\")  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float16)\nThe ``preferred_element_type`` parameter can be used to return the output\nwithout downcasting the accumulation type:\n>>> dot(lhs, rhs, precision=\"F16_F16_F32\", preferred_element_type=np.float32)  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float32)"
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "DotAlgorithmPreset",
      "documentation": {
        "description": "An enum of known algorithms for computing dot products.\nThis ``Enum`` provides a named set of :class:`~jax.lax.DotAlgorithm` objects\nthat are known to be supported on at least platform. See the\n:class:`~jax.lax.DotAlgorithm` documentation for more details about the\nbehavior of these algorithms.\nAn algorithm can be selected from this list when calling :func:`~jax.lax.dot`,\n:func:`~jax.lax.dot_general`, or most other JAX dot product functions, by\npassing either a member of this ``Enum`` or it's name as a string using the\n``precision`` argument.\nFor example, users can specify the preset using this ``Enum`` directly:\n>>> lhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype=np.float16)\n>>> rhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype=np.float16)\n>>> algorithm = DotAlgorithmPreset.F16_F16_F32\n>>> dot(lhs, rhs, precision=algorithm)  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float16)\nor, equivalently, they can be specified by name:\n>>> dot(lhs, rhs, precision=\"F16_F16_F32\")  # doctest: +SKIP\narray([ 1.,  4.,  9., 16.], dtype=float16)\nThe names of the presets are typically ``LHS_RHS_ACCUM`` where ``LHS`` and\n``RHS`` are the element types of the ``lhs`` and ``rhs`` inputs\nrespectively, and ``ACCUM`` is the element type of the accumulator. Some\npresets have an extra suffix, and the meaning of each of these is\ndocumented below. The supported presets are:",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "FftType",
      "documentation": {
        "description": "Describes which FFT operation to perform.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "as_integer_ratio",
          "signature": "as_integer_ratio(self, /)",
          "documentation": {
            "description": "Return a pair of integers, whose ratio is equal to the original int.\nThe ratio is in lowest terms and has a positive denominator.\n>>> (10).as_integer_ratio()\n(10, 1)\n>>> (-10).as_integer_ratio()\n(-10, 1)\n>>> (0).as_integer_ratio()\n(0, 1)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_count",
          "signature": "bit_count(self, /)",
          "documentation": {
            "description": "Number of ones in the binary representation of the absolute value of self.\nAlso known as the population count.\n>>> bin(13)\n'0b1101'\n>>> (13).bit_count()\n3",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_length",
          "signature": "bit_length(self, /)",
          "documentation": {
            "description": "Number of bits necessary to represent self in binary.\n>>> bin(37)\n'0b100101'\n>>> (37).bit_length()\n6",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "conjugate(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "from_bytes",
          "signature": "from_bytes(bytes, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return the integer represented by the given array of bytes.\nbytes\nHolds the array of bytes to convert.  The argument must either\nsupport the buffer protocol or be an iterable object producing bytes.\nBytes and bytearray are examples of built-in objects that support the\nbuffer protocol.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nIndicates whether two's complement is used to represent the integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_integer",
          "signature": "is_integer(self, /)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bytes",
          "signature": "to_bytes(self, /, length=1, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return an array of bytes representing an integer.\nlength\nLength of bytes object to use.  An OverflowError is raised if the\ninteger is not representable with the given number of bytes.  Default\nis length 1.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nDetermines whether two's complement is used to represent the integer.\nIf signed is False and a negative integer is given, an OverflowError\nis raised.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GatherDimensionNumbers",
      "documentation": {
        "description": "Describes the dimension number arguments to an `XLA's Gather operator\n<https://www.tensorflow.org/xla/operation_semantics#gather>`_. See the XLA\ndocumentation for more details of what the dimension numbers mean.\nArgs:\noffset_dims: the set of dimensions in the `gather` output that offset into\nan array sliced from `operand`. Must be a tuple of integers in ascending\norder, each representing a dimension number of the output.\ncollapsed_slice_dims: the set of dimensions `i` in `operand` that have\n`slice_sizes[i] == 1` and that should not have a corresponding dimension\nin the output of the gather. Must be a tuple of integers in ascending\norder.\nstart_index_map: for each dimension in `start_indices`, gives the\ncorresponding dimension in the `operand` that is to be sliced. Must be a\ntuple of integers with size equal to `start_indices.shape[-1]`.\noperand_batching_dims: the set of batching dimensions `i` in `operand` that\nhave `slice_sizes[i] == 1` and that should have a corresponding dimension\nin both the `start_indices` (at the same index in\n`start_indices_batching_dims`) and output of the gather. Must be a tuple\nof integers in ascending order.\nstart_indices_batching_dims: the set of batching dimensions `i` in\n`start_indices` that should have a corresponding dimension in both the\n`operand` (at the same index in `operand_batching_dims`) and output of the\ngather. Must be a tuple of integers (order is fixed based on\ncorrespondence with `operand_batching_dims`).\nUnlike XLA's `GatherDimensionNumbers` structure, `index_vector_dim` is\nimplicit; there is always an index vector dimension and it must always be the\nlast dimension. To gather scalar indices, add a trailing dimension of size 1.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "GatherScatterMode",
      "documentation": {
        "description": "Describes how to handle out-of-bounds indices in a gather or scatter.\nPossible values are:\nCLIP:\nIndices will be clamped to the nearest in-range value, i.e., such that the\nentire window to be gathered is in-range.\nFILL_OR_DROP:\nIf any part of a gathered window is out of bounds, the entire window\nthat is returned, even those elements that were otherwise in-bounds, will be\nfilled with a constant.\nIf any part of a scattered window is out of bounds, the entire window\nwill be discarded.\nPROMISE_IN_BOUNDS:\nThe user promises that indices are in bounds. No additional checking will be\nperformed. In practice, with the current XLA implementation this means\nthat out-of-bounds gathers will be clamped but out-of-bounds scatters will\nbe discarded. Gradients will not be correct if indices are out-of-bounds.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "Precision",
      "documentation": {
        "description": "Precision enum for lax matrix multiply related functions.\nThe device-dependent `precision` argument to JAX functions generally\ncontrols the tradeoff between speed and accuracy for array computations on\naccelerator backends, (i.e. TPU and GPU). Has no impact on CPU backends.\nThis only has an effect on float32 computations, and does not affect the\ninput/output datatypes. Members are:\nDEFAULT:\nFastest mode, but least accurate. On TPU: performs float32 computations in\nbfloat16. On GPU: uses tensorfloat32 if available (e.g. on A100 and H100\nGPUs), otherwise standard float32 (e.g. on V100 GPUs). Aliases:\n``'default'``, ``'fastest'``.\nHIGH:\nSlower but more accurate. On TPU: performs float32 computations in 3\nbfloat16 passes. On GPU: uses tensorfloat32 where available, otherwise\nfloat32. Aliases: ``'high'``..\nHIGHEST:\nSlowest but most accurate. On TPU: performs float32 computations in 6\nbfloat16. Aliases: ``'highest'``. On GPU: uses float32.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "RaggedDotDimensionNumbers",
      "documentation": {
        "description": "Describes ragged, group, and dot dimensions for ragged dot general.\nArgs:\ndot_dimension_numbers: a tuple of tuples of sequences of ints of the form\n`((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims,\nrhs_batch_dims))`.\nlhs_ragged_dimensions: a sequence of ints indicating the 'lhs' ragged\ndimensions.\nrhs_group_dimensions: a sequence of ints indicating the 'rhs' group\ndimensions.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": []
    },
    {
      "name": "RandomAlgorithm",
      "documentation": {
        "description": "Describes which PRNG algorithm to use for rng_bit_generator.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "as_integer_ratio",
          "signature": "as_integer_ratio(self, /)",
          "documentation": {
            "description": "Return a pair of integers, whose ratio is equal to the original int.\nThe ratio is in lowest terms and has a positive denominator.\n>>> (10).as_integer_ratio()\n(10, 1)\n>>> (-10).as_integer_ratio()\n(-10, 1)\n>>> (0).as_integer_ratio()\n(0, 1)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_count",
          "signature": "bit_count(self, /)",
          "documentation": {
            "description": "Number of ones in the binary representation of the absolute value of self.\nAlso known as the population count.\n>>> bin(13)\n'0b1101'\n>>> (13).bit_count()\n3",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_length",
          "signature": "bit_length(self, /)",
          "documentation": {
            "description": "Number of bits necessary to represent self in binary.\n>>> bin(37)\n'0b100101'\n>>> (37).bit_length()\n6",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "conjugate(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "from_bytes",
          "signature": "from_bytes(bytes, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return the integer represented by the given array of bytes.\nbytes\nHolds the array of bytes to convert.  The argument must either\nsupport the buffer protocol or be an iterable object producing bytes.\nBytes and bytearray are examples of built-in objects that support the\nbuffer protocol.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nIndicates whether two's complement is used to represent the integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_integer",
          "signature": "is_integer(self, /)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bytes",
          "signature": "to_bytes(self, /, length=1, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return an array of bytes representing an integer.\nlength\nLength of bytes object to use.  An OverflowError is raised if the\ninteger is not representable with the given number of bytes.  Default\nis length 1.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nDetermines whether two's complement is used to represent the integer.\nIf signed is False and a negative integer is given, an OverflowError\nis raised.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "RoundingMethod",
      "documentation": {
        "description": "Rounding strategies for handling halfway values (e.g., 0.5) in\n:func:`jax.lax.round`.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "as_integer_ratio",
          "signature": "as_integer_ratio(self, /)",
          "documentation": {
            "description": "Return a pair of integers, whose ratio is equal to the original int.\nThe ratio is in lowest terms and has a positive denominator.\n>>> (10).as_integer_ratio()\n(10, 1)\n>>> (-10).as_integer_ratio()\n(-10, 1)\n>>> (0).as_integer_ratio()\n(0, 1)",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_count",
          "signature": "bit_count(self, /)",
          "documentation": {
            "description": "Number of ones in the binary representation of the absolute value of self.\nAlso known as the population count.\n>>> bin(13)\n'0b1101'\n>>> (13).bit_count()\n3",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "bit_length",
          "signature": "bit_length(self, /)",
          "documentation": {
            "description": "Number of bits necessary to represent self in binary.\n>>> bin(37)\n'0b100101'\n>>> (37).bit_length()\n6",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "conjugate",
          "signature": "conjugate(...)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "from_bytes",
          "signature": "from_bytes(bytes, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return the integer represented by the given array of bytes.\nbytes\nHolds the array of bytes to convert.  The argument must either\nsupport the buffer protocol or be an iterable object producing bytes.\nBytes and bytearray are examples of built-in objects that support the\nbuffer protocol.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nIndicates whether two's complement is used to represent the integer.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "is_integer",
          "signature": "is_integer(self, /)",
          "documentation": {
            "description": "",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "to_bytes",
          "signature": "to_bytes(self, /, length=1, byteorder='big', *, signed=False)",
          "documentation": {
            "description": "Return an array of bytes representing an integer.\nlength\nLength of bytes object to use.  An OverflowError is raised if the\ninteger is not representable with the given number of bytes.  Default\nis length 1.\nbyteorder\nThe byte order used to represent the integer.  If byteorder is 'big',\nthe most significant byte is at the beginning of the byte array.  If\nbyteorder is 'little', the most significant byte is at the end of the\nbyte array.  To request the native byte order of the host system, use\n`sys.byteorder' as the byte order value.  Default is to use 'big'.\nsigned\nDetermines whether two's complement is used to represent the integer.\nIf signed is False and a negative integer is given, an OverflowError\nis raised.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    },
    {
      "name": "ScatterDimensionNumbers",
      "documentation": {
        "description": "Describes the dimension number arguments to an `XLA's Scatter operator\n<https://www.tensorflow.org/xla/operation_semantics#scatter>`_. See the XLA\ndocumentation for more details of what the dimension numbers mean.\nArgs:\nupdate_window_dims: the set of dimensions in the `updates` that are window\ndimensions. Must be a tuple of integers in ascending\norder, each representing a dimension number.\ninserted_window_dims: the set of size 1 window dimensions that must be\ninserted into the shape of `updates`. Must be a tuple of integers in\nascending order, each representing a dimension number of the output. These\nare the mirror image of `collapsed_slice_dims` in the case of `gather`.\nscatter_dims_to_operand_dims: for each dimension in `scatter_indices`, gives\nthe corresponding dimension in `operand`. Must be a sequence of integers\nwith size equal to `scatter_indices.shape[-1]`.\noperand_batching_dims: the set of batching dimensions `i` in `operand` that\nshould have a corresponding dimension in both the `scatter_indices` (at\nthe same index in `scatter_indices_batching_dims`) and `updates`. Must be\na tuple of integers in ascending order. These are the mirror image of\n`operand_batching_dims` in the case of `gather`.\nscatter_indices_batching_dims: the set of batching dimensions `i` in\n`scatter_indices` that should have a corresponding dimension in both the\n`operand` (at the same index in `operand_batching_dims`) and output of the\ngather. Must be a tuple of integers (order is fixed based on\ncorrespondence with `input_batching_dims`). These are the mirror image of\n`start_indices_batching_dims` in the case of `gather`.\nUnlike XLA's `ScatterDimensionNumbers` structure, `index_vector_dim` is\nimplicit; there is always an index vector dimension and it must always be the\nlast dimension. To scatter scalar indices, add a trailing dimension of size 1.",
        "parameters": {},
        "returns": "",
        "raises": "",
        "see_also": "",
        "notes": "",
        "examples": ""
      },
      "methods": [
        {
          "name": "count",
          "signature": "count(self, value, /)",
          "documentation": {
            "description": "Return number of occurrences of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        },
        {
          "name": "index",
          "signature": "index(self, value, start=0, stop=9223372036854775807, /)",
          "documentation": {
            "description": "Return first index of value.",
            "parameters": {},
            "returns": "",
            "raises": "",
            "see_also": "",
            "notes": "",
            "examples": ""
          }
        }
      ]
    }
  ]
}